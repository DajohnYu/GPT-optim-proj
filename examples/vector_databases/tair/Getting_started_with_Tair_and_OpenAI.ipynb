{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tair as a vector database for OpenAI embeddings\n",
    "\n",
    "This notebook guides you step by step on using Tair as a vector database for OpenAI embeddings.\n",
    "\n",
    "This notebook presents an end-to-end process of:\n",
    "1. Using precomputed embeddings created by OpenAI API.\n",
    "2. Storing the embeddings in a cloud instance of Tair.\n",
    "3. Converting raw text query to an embedding with OpenAI API.\n",
    "4. Using Tair to perform the nearest neighbour search in the created collection.\n",
    "\n",
    "### What is Tair\n",
    "\n",
    "[Tair](https://www.alibabacloud.com/help/en/tair/latest/what-is-tair) is a cloud native in-memory database service that is developed by Alibaba Cloud. Tair is compatible with open source Redis and provides a variety of data models and enterprise-class capabilities to support your real-time online scenarios. Tair also introduces persistent memory-optimized instances that are based on the new non-volatile memory (NVM) storage medium. These instances can reduce costs by 30%, ensure data persistence, and provide almost the same performance as in-memory databases. Tair has been widely used in areas such as government affairs, finance, manufacturing, healthcare, and pan-Internet to meet their high-speed query and computing requirements.\n",
    "\n",
    "[Tairvector](https://www.alibabacloud.com/help/en/tair/latest/tairvector) is an in-house data structure that provides high-performance real-time storage and retrieval of vectors. TairVector provides two indexing algorithms: Hierarchical Navigable Small World (HNSW) and Flat Search. Additionally, TairVector supports multiple distance functions, such as Euclidean distance, inner product, and Jaccard distance. Compared with traditional vector retrieval services, TairVector has the following advantages:\n",
    "- Stores all data in memory and supports real-time index updates to reduce latency of read and write operations.\n",
    "- Uses an optimized data structure in memory to better utilize storage capacity.\n",
    "- Functions as an out-of-the-box data structure in a simple and efficient architecture without complex modules or dependencies.\n",
    "\n",
    "### Deployment options\n",
    "\n",
    "- Using [Tair Cloud Vector Database](https://www.alibabacloud.com/help/en/tair/latest/getting-started-overview). [Click here](https://www.alibabacloud.com/product/tair) to fast deploy it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "For the purposes of this exercise we need to prepare a couple of things:\n",
    "\n",
    "1. Tair cloud server instance.\n",
    "2. The 'tair' library to interact with the tair database.\n",
    "3. An [OpenAI API key](https://beta.openai.com/account/api-keys).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements\n",
    "\n",
    "This notebook obviously requires the `openai` and `tair` packages, but there are also some other additional libraries we will use. The following command installs them all:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:05:05.718972Z",
     "start_time": "2023-02-16T12:04:30.434820Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "! pip install openai tair pandas wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your OpenAI API key\n",
    "\n",
    "The OpenAI API key is used for vectorization of the documents and queries.\n",
    "\n",
    "If you don't have an OpenAI API key, you can get one from [https://beta.openai.com/account/api-keys](https://beta.openai.com/account/api-keys).\n",
    "\n",
    "Once you get your key, please add it to variables as `OPENAI_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:05:05.730338Z",
     "start_time": "2023-02-16T12:05:05.723351Z"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass.getpass(\"Input your OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might validate if the server was launched successfully by running a simple curl command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl https://api.openai.com/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
    "  -d '{\"model\": \"gpt-3.5-turbo\",\"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Tair\n",
    "First add it to your environment variables.\n",
    "\n",
    "Connecting to a running instance of Tair server is easy with the official Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The format of url: redis://[[username]:[password]]@localhost:6379/0\n",
    "TAIR_URL = getpass.getpass(\"Input your tair url:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tair import Tair as TairClient\n",
    "\n",
    "# connect to tair from url and create a client\n",
    "\n",
    "url = os.environ.get(\"TAIR_URL\")\n",
    "print(\"url =\", url)\n",
    "client = TairClient.from_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the connection by ping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:05:06.848488Z",
     "start_time": "2023-02-16T12:05:06.832612Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:05:37.371951Z",
     "start_time": "2023-02-16T12:05:06.851634Z"
    },
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "embeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n",
    "\n",
    "# The file is ~700 MB so this will take some time\n",
    "wget.download(embeddings_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded file has to then be extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:06:01.538851Z",
     "start_time": "2023-02-16T12:05:37.376042Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "zip_file_path = os.path.join(current_directory, \"vector_database_wikipedia_articles_embedded.zip\")\n",
    "output_directory = os.path.join(current_directory, \"../../data\")\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(output_directory)\n",
    "\n",
    "\n",
    "# check the csv file exist\n",
    "file_name = \"vector_database_wikipedia_articles_embedded.csv\"\n",
    "data_directory = os.path.join(current_directory, \"../../data\")\n",
    "file_path = os.path.join(data_directory, file_name)\n",
    "\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"The file {file_name} exists in the data directory.\")\n",
    "else:\n",
    "    print(f\"The file {file_name} does not exist in the data directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index\n",
    "\n",
    "Tair stores data in indexes where each object is described by one key. Each key contains a vector and multiple attribute_keys.\n",
    "\n",
    "We will start with creating two indexes, one for **title_vector** and one for **content_vector**, and then we will fill it with our precomputed embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index parameters\n",
    "index = \"openai_test\"\n",
    "embedding_dim = 1536\n",
    "distance_type = \"L2\"\n",
    "index_type = \"HNSW\"\n",
    "data_type = \"FLOAT32\"\n",
    "\n",
    "# Create two indexes, one for title_vector and one for content_vector, skip if already exists\n",
    "index_names = [index + \"_title_vector\", index+\"_content_vector\"]\n",
    "for index_name in index_names:\n",
    "    index_connection = client.tvs_get_index(index_name)\n",
    "    if index_connection is not None:\n",
    "        print(\"Index already exists\")\n",
    "    else:\n",
    "        client.tvs_create_index(name=index_name, dim=embedding_dim, distance_type=distance_type,\n",
    "                                index_type=index_type, data_type=data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "In this section we are going to load the data prepared previous to this session, so you don't have to recompute the embeddings of Wikipedia articles with your own credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your local CSV file\n",
    "csv_file_path = '../../data/vector_database_wikipedia_articles_embedded.csv'\n",
    "article_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Read vectors from strings back into a list\n",
    "article_df['title_vector'] = article_df.title_vector.apply(literal_eval).values\n",
    "article_df['content_vector'] = article_df.content_vector.apply(literal_eval).values\n",
    "\n",
    "# add/update data to indexes\n",
    "for i in range(len(article_df)):\n",
    "    # add data to index with title_vector\n",
    "    client.tvs_hset(index=index_names[0], key=article_df.id[i].item(), vector=article_df.title_vector[i], is_binary=False,\n",
    "                    **{\"url\": article_df.url[i], \"title\": article_df.title[i], \"text\": article_df.text[i]})\n",
    "    # add data to index with content_vector\n",
    "    client.tvs_hset(index=index_names[1], key=article_df.id[i].item(), vector=article_df.content_vector[i], is_binary=False,\n",
    "                    **{\"url\": article_df.url[i], \"title\": article_df.title[i], \"text\": article_df.text[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:40.675202Z",
     "start_time": "2023-02-16T12:30:40.655654Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Check the data count to make sure all the points have been stored\n",
    "for index_name in index_names:\n",
    "    stats = client.tvs_get_index(index_name)\n",
    "    count = int(stats[\"current_record_count\"]) - int(stats[\"delete_record_count\"])\n",
    "    print(f\"Count in {index_name}:{count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search data\n",
    "\n",
    "Once the data is put into Tair we will start querying the collection for the closest vectors. We may provide an additional parameter `vector_name` to switch from title to content based search. Since the precomputed embeddings were created with `text-embedding-ada-002` OpenAI model, we also have to use it during search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:38.024370Z",
     "start_time": "2023-02-16T12:30:37.712816Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_tair(client, query, vector_name=\"title_vector\", top_k=5):\n",
    "\n",
    "    # Creates embedding vector from user query\n",
    "    embedded_query = openai.Embedding.create(\n",
    "        input= query,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "    )[\"data\"][0]['embedding']\n",
    "    embedded_query = np.array(embedded_query)\n",
    "\n",
    "    # search for the top k approximate nearest neighbors of vector in an index\n",
    "    query_result = client.tvs_knnsearch(index=index+\"_\"+vector_name, k=top_k, vector=embedded_query)\n",
    "\n",
    "    return query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:39.379566Z",
     "start_time": "2023-02-16T12:30:38.031041Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "query_result = query_tair(client=client, query=\"modern art in Europe\", vector_name=\"title_vector\")\n",
    "for i in range(len(query_result)):\n",
    "    title = client.tvs_hmget(index, query_result[i][0].decode('utf-8'), \"title\")\n",
    "    print(f\"{i + 1}. {title[0].decode('utf-8')} (Distance: {round(query_result[i][1],3)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T12:30:40.652676Z",
     "start_time": "2023-02-16T12:30:39.382555Z"
    }
   },
   "outputs": [],
   "source": [
    "# This time we'll query using content vector\n",
    "query_result = query_tair(client=client, query=\"Famous battles in Scottish history\", vector_name=\"content_vector\")\n",
    "for i in range(len(query_result)):\n",
    "    title = client.tvs_hmget(index, query_result[i][0].decode('utf-8'), \"title\")\n",
    "    print(f\"{i + 1}. {title[0].decode('utf-8')} (Distance: {round(query_result[i][1],3)})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
