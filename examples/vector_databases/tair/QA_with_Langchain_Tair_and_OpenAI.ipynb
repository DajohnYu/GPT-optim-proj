{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with Langchain, Tair and OpenAI\n",
    "This notebook presents how to implement a Question Answering system with Langchain, Tair as a knowledge based and OpenAI embeddings. If you are not familiar with Tair, itâ€™s better to check out the [Getting_started_with_Tair_and_OpenAI.ipynb](Getting_started_with_Tair_and_OpenAI.ipynb) notebook.\n",
    "\n",
    "This notebook presents an end-to-end process of:\n",
    "- Calculating the embeddings with OpenAI API.\n",
    "- Storing the embeddings in an Tair instance to build a knowledge base.\n",
    "- Converting raw text query to an embedding with OpenAI API.\n",
    "- Using Tair to perform the nearest neighbour search in the created collection to find some context.\n",
    "- Asking LLM to find the answer in a given context.\n",
    "\n",
    "All the steps will be simplified to calling some corresponding Langchain methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "For the purposes of this exercise we need to prepare a couple of things:\n",
    "[Tair cloud instance](https://www.alibabacloud.com/help/en/tair/latest/what-is-tair).\n",
    "[Langchain](https://github.com/hwchase17/langchain) as a framework.\n",
    "An OpenAI API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements\n",
    "This notebook requires the following Python packages: `openai`, `tiktoken`, `langchain` and `tair`.\n",
    "- `openai` provides convenient access to the OpenAI API.\n",
    "- `tiktoken` is a fast BPE tokeniser for use with OpenAI's models.\n",
    "- `langchain` helps us to build applications with LLM more easily.\n",
    "- `tair` library is used to interact with the tair vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T10:21:40.843630Z",
     "start_time": "2023-05-06T10:21:38.796769Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install openai tiktoken langchain tair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T10:21:40.974668Z",
     "start_time": "2023-05-06T10:21:40.845980Z"
    }
   },
   "outputs": [],
   "source": [
    "! export OPENAI_API_KEY=\"your API key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T10:21:40.975073Z",
     "start_time": "2023-05-06T10:21:40.971906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test that your OpenAI API key is correctly set as an environment variable\n",
    "# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\n",
    "import os\n",
    "\n",
    "# Note. alternatively you can set a temporary env variable like this:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
    "    print(\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY environment variable not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your OpenAI API key\n",
    "The OpenAI API key is used for vectorization of the documents and queries.\n",
    "\n",
    "If you don't have an OpenAI API key, you can get one from [https://platform.openai.com/account/api-keys ).\n",
    "\n",
    "Once you get your key, please add it to your environment variables as `OPENAI_API_KEY` by running following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your Tair URL\n",
    "To build the Tair connection, you need to have `TAIR_URL`. You need to export it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T10:21:41.574807Z",
     "start_time": "2023-05-06T10:21:40.976664Z"
    }
   },
   "outputs": [],
   "source": [
    "# The format of url: redis://[[username]:[password]]@localhost:6379/0\n",
    "! export TAIR_URL=\"your_tair_url\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "url = os.environ.get(\"TAIR_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "In this section we are going to load the data containing some natural questions and answers to them. All the data will be used to create a Langchain application with Tair being the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "# All the examples come from https://ai.google.com/research/NaturalQuestions\n",
    "# This is a sample of the training set that we download and extract for some\n",
    "# further processing.\n",
    "wget.download(\"https://storage.googleapis.com/dataset-natural-questions/questions.json\")\n",
    "wget.download(\"https://storage.googleapis.com/dataset-natural-questions/answers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"questions.json\", \"r\") as fp:\n",
    "    questions = json.load(fp)\n",
    "\n",
    "with open(\"answers.json\", \"r\") as fp:\n",
    "    answers = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain definition\n",
    "\n",
    "Langchain is already integrated with Tair and performs all the indexing for given list of documents. In our case we are going to store the set of answers we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Tair\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain import VectorDBQA, OpenAI\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "doc_store = Tair.from_texts(\n",
    "    texts=answers, embedding=embeddings, tair_url=url,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage all the possible answers are already stored in Tair, so we can define the whole QA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "qa = VectorDBQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    vectorstore=doc_store,\n",
    "    return_source_documents=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search data\n",
    "\n",
    "Once the data is put into Tair we can start asking some questions. A question will be automatically vectorized by OpenAI model, and the created vector will be used to find some possibly matching answers in Tair. Once retrieved, the most similar answers will be incorporated into the prompt sent to OpenAI Large Language Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(52)\n",
    "selected_questions = random.choices(questions, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in selected_questions:\n",
    "    print(\">\", question)\n",
    "    print(qa.run(question), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom prompt templates\n",
    "\n",
    "The `stuff` chain type in Langchain uses a specific prompt with question and context documents incorporated. This is what the default prompt looks like:\n",
    "\n",
    "```text\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "```\n",
    "\n",
    "We can, however, provide our prompt template and change the behaviour of the OpenAI LLM, while still using the `stuff` chain type. It is important to keep `{context}` and `{question}` as placeholders.\n",
    "\n",
    "#### Experimenting with custom prompts\n",
    "\n",
    "We can try using a different prompt template, so the model:\n",
    "1. Responds with a single-sentence answer if it knows it.\n",
    "2. Suggests a random song title if it doesn't know the answer to our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "custom_prompt = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. Please provide\n",
    "a short single-sentence summary answer only. If you don't know the answer or if it's\n",
    "not present in given context, don't try to make up an answer, but suggest me a random\n",
    "unrelated song title I could listen to.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "custom_prompt_template = PromptTemplate(\n",
    "    template=custom_prompt, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_qa = VectorDBQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    vectorstore=doc_store,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt_template},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(41)\n",
    "for question in random.choices(questions, k=5):\n",
    "    print(\">\", question)\n",
    "    print(custom_qa.run(question), end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
