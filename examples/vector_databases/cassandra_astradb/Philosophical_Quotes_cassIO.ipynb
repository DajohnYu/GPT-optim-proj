{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46589cdf-1ab6-4028-b07c-08b75acd98e5",
   "metadata": {},
   "source": [
    "# Philosophy with Vector Embeddings, OpenAI and Cassandra / Astra DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3496d07-f473-4008-9133-1a54b818c8d3",
   "metadata": {},
   "source": [
    "In this quickstart you will learn how to build a \"philosophy quote finder & generator\" using OpenAI's vector embeddings and DataStax Astra DB (_or a vector-capable Apache Cassandra® cluster, if you prefer_) as the vector store for data persistence.\n",
    "\n",
    "The basic workflow of this notebook is outlined below. You will evaluate and store the vector embedding for a number of quotes by famous philosopher and later use them to build a powerful search engine and, after that, a generator of new quotes!\n",
    "\n",
    "The notebook exemplifies some of the standard usage patterns of vector search -- and also shows how easy is it to get started with the [Vector capabilities of Astra DB](https://astra.datastax.com/).\n",
    "\n",
    "_Choose-your-framework_\n",
    "\n",
    "Please note that this notebook uses the [CassIO library](https://cassio.org), but we cover other choices of technology to accomplish the same task. Check out this folder's [README](https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/cassandra_astradb) for other options. This notebook can run either as a Colab notebook or with as a regular Jupyter notebook.\n",
    "\n",
    "Table of contents:\n",
    "- Setup\n",
    "- Get DB connection\n",
    "- Connect to OpenAI\n",
    "- Load quotes into the Vector Store\n",
    "- Use case 1: **quote search engine**\n",
    "- Use case 2: **quote generator**\n",
    "- (Optional) exploit partitioning in the Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf17cc-eef4-4021-b72a-4d3832a9b4a7",
   "metadata": {},
   "source": [
    "### How it works\n",
    "\n",
    "**Indexing**\n",
    "\n",
    "Each quote is made into an embedding vector with OpenAI's `Embedding`. These are saved in the Vector Store along with some metadata, including the author's name and a few other pre-computed tags, for later customization of the search.\n",
    "\n",
    "![1_vector_indexing](https://user-images.githubusercontent.com/14221764/262085997-215c3854-a004-45f0-8afc-51b924b059a0.png)\n",
    "\n",
    "**Search**\n",
    "\n",
    "To find a quote similar to the provided search quote, the latter is made into an embedding vector on the fly, and this vector is used to query the store for similar vectors ... i.e. similar quotes that were previously indexed. The search can optionally be constrained by additional metadata (\"find me quotes by Spinoza similar to this one ...\")\n",
    "\n",
    "![2_vector_search](https://user-images.githubusercontent.com/14221764/262086005-5824d690-b8a4-4cbe-a6fd-a43fa785f8dc.png)\n",
    "\n",
    "The key point here is that \"similar quotes\" translates, in vector space, to vectors that are metrically close to each other: thus, vector similarity search effectively implements semantic similarity. _This is the key reason vector embeddings are so powerful._\n",
    "\n",
    "The sketch below tries to convey this idea. Each quote, once it's made into a vector, is a point in space. Well, in this case it's on a sphere, since OpenAI's embedding vectors, as most others, are normalized to _unit length_. Oh, and the sphere is actually not three-dimensional, rather 1536-dimensional!\n",
    "\n",
    "![3_vector_space](https://user-images.githubusercontent.com/14221764/262086007-f417c44b-c048-47f8-9dbd-140472798b6d.png)\n",
    "\n",
    "**Generation**\n",
    "\n",
    "Given a suggestion (a topic or a tentative quote), the search step is performed, and the first returned results (quotes) are fed into an LLM prompt which asks the generative model to invent a new text along the lines of the passed examples _and_ the initial suggestion.\n",
    "\n",
    "![4_quote_generation](https://user-images.githubusercontent.com/14221764/262087157-784117ff-7c56-45bc-9c76-577d09aea19a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10493f44-565d-4f23-8bfd-1a7335392c2b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a14f95-4683-4d0c-a251-0df7b43ca975",
   "metadata": {},
   "source": [
    "First install some required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39afdb74-56e4-44ff-9c72-ab2669780113",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install cassio openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb99e33-5cb7-416f-8dca-da18e0cb108d",
   "metadata": {},
   "source": [
    "## Get DB connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8edc1-4633-491b-9ed3-11163ec24e46",
   "metadata": {},
   "source": [
    "A couple of secrets are required to create a `Session` object (a connection to your Astra DB instance).\n",
    "\n",
    "_(Note: some steps will be slightly different on Google Colab and on local Jupyter, that's why we detect the runtime type.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7429ed4-b3fe-44b0-ad00-60883df32070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f2eec1-b784-4cea-9006-03cfe7b31e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    IS_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    IS_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7615e522-574f-427e-9f7f-87fc721207a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please provide the full path to your Secure Connect Bundle zipfile:  /path/to/secure-connect-DATABASE.zip\n",
      "Please provide your Database Token ('AstraCS:...' string):  ········\n",
      "Please provide the Keyspace name for your Database:  my_keyspace\n"
     ]
    }
   ],
   "source": [
    "# We need your database's Secure Connect Bundle zip file:\n",
    "from getpass import getpass\n",
    "if IS_COLAB:\n",
    "    # Upload your Secure Connect Bundle zipfile:\n",
    "    import os\n",
    "    from google.colab import files\n",
    "\n",
    "\n",
    "    print('Please upload your Secure Connect Bundle zipfile: ')\n",
    "    uploaded = files.upload()\n",
    "    if uploaded:\n",
    "        astraBundleFileTitle = list(uploaded.keys())[0]\n",
    "        ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
    "        )\n",
    "else:\n",
    "    # you are running a local-jupyter notebook:\n",
    "    ASTRA_DB_SECURE_BUNDLE_PATH = input(\"Please provide the full path to your Secure Connect Bundle zipfile: \")\n",
    "\n",
    "ASTRA_DB_APPLICATION_TOKEN = getpass(\"Please provide your Database Token ('AstraCS:...' string): \")\n",
    "ASTRA_DB_KEYSPACE = input(\"Please provide the Keyspace name for your Database: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c4e5ec-2ab2-4d41-b3ec-c946469fed8b",
   "metadata": {},
   "source": [
    "#### Creation of the DB connection\n",
    "\n",
    "This is how you create a connection to Astra DB:\n",
    "\n",
    "_(Incidentally, you could also use any Cassandra cluster (as long as it provides Vector capabilities), just by changing the parameters to the following `Cluster` instantiation.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949ab020-90c8-499b-a139-f69f07af50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Cluster(\n",
    "    cloud={\n",
    "        \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
    "    },\n",
    "    auth_provider=PlainTextAuthProvider(\n",
    "        \"token\",\n",
    "        ASTRA_DB_APPLICATION_TOKEN,\n",
    "    ),\n",
    ")\n",
    "\n",
    "session = cluster.connect()\n",
    "keyspace = ASTRA_DB_KEYSPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60829851-bd48-4461-9243-974f76304933",
   "metadata": {},
   "source": [
    "#### Creation of the Vector Store through CassIO\n",
    "\n",
    "We need a table which support vectors and is equipped with metadata. Let's call it \"philosophers\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db837dc-cd49-41e2-8b5d-edb17ccc470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector store with cassIO\n",
    "from cassio.table import MetadataVectorCassandraTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "691f1a07-cab4-42a1-baba-f17b561ddd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_table = MetadataVectorCassandraTable(\n",
    "    session,\n",
    "    keyspace,\n",
    "    \"philosophers\",\n",
    "    vector_dimension=1536,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da86f91a-88a6-4997-b0f8-9da0816f8ece",
   "metadata": {},
   "source": [
    "## Connect to OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b664b5-fd84-492e-a7bd-4dda3863b48a",
   "metadata": {},
   "source": [
    "#### Setup your secret key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37fe7653-dd64-4494-83e1-5702ec41725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your OpenAI API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = getpass(\"Please enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8065a42a-0ece-4453-b771-1dbef6d8a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f2821-7f3f-4dcd-8e0c-49aa397e36f4",
   "metadata": {},
   "source": [
    "### A test call for embeddings\n",
    "\n",
    "Let us quickly check how one can get the embedding vectors for a list of input texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bf89454-9a55-4202-ab6b-ea15b2048f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "result = openai.Embedding.create(\n",
    "    input=[\n",
    "        \"This is a sentence\",\n",
    "        \"A second sentence\"\n",
    "    ],\n",
    "    engine=embedding_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50a8e6f0-0aa7-4ffc-94e9-702b68566815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(result.data)              = 2\n",
      "result.data[1].embedding      = [-0.010772699490189552, 0.0013737495755776763, 0.003638...\n",
      "len(result.data[1].embedding) = 1536\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(result.data)              = {len(result.data)}\")\n",
    "print(f\"result.data[1].embedding      = {str(result.data[1].embedding)[:55]}...\")\n",
    "print(f\"len(result.data[1].embedding) = {len(result.data[1].embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f09c42-fff3-4aa2-922b-043739b4b06a",
   "metadata": {},
   "source": [
    "## Load quotes into the Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f3d58-74c2-458b-903d-3d12e61b7846",
   "metadata": {},
   "source": [
    "Let's get a JSON file containing our quotes. We already prepared this collection and put it into this repo for quick loading.\n",
    "\n",
    "_(Note: we adapted the following from a Kaggle dataset -- which we acknowledge -- and also added a few tags to each quote.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94ff33fb-4b52-4c15-ab74-4af4fe973cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "if IS_COLAB:\n",
    "    # load from Web request to (github) repo\n",
    "    json_url = \"https://raw.githubusercontent.com/hemidactylus/openai-cookbook/SL-cassandra_astra_vector/examples/vector_databases/cassandra_astradb/sources/philo_quotes.json\"\n",
    "    quote_dict = json.loads(requests.get(json_url).text)    \n",
    "else:\n",
    "    # load from local repo\n",
    "    quote_dict = json.load(open(\"./sources/philo_quotes.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b08b1-e3db-4c7c-9d7c-2ada7c8bc71d",
   "metadata": {},
   "source": [
    "A quick inspection of the input data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ab84ccb-3363-4bdc-9484-0d68c25a58ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapted from this Kaggle dataset: https://www.kaggle.com/datasets/mertbozkurt5/quotes-by-philosophers (License: CC BY-NC-SA 4.0)\n",
      "\n",
      "Quotes loaded: 450.\n",
      "By author:\n",
      "  aristotle (50)\n",
      "  freud (50)\n",
      "  hegel (50)\n",
      "  kant (50)\n",
      "  nietzsche (50)\n",
      "  plato (50)\n",
      "  sartre (50)\n",
      "  schopenhauer (50)\n",
      "  spinoza (50)\n",
      "\n",
      "Some examples:\n",
      "  aristotle:\n",
      "    True happiness comes from gaining insight and grow ... (tags: knowledge)\n",
      "    The roots of education are bitter, but the fruit i ... (tags: education, knowledge)\n",
      "  freud:\n",
      "    We are what we are because we have been what we ha ... (tags: history)\n",
      "    From error to error one discovers the entire truth ... (tags: )\n"
     ]
    }
   ],
   "source": [
    "print(quote_dict[\"source\"])\n",
    "\n",
    "total_quotes = sum(len(quotes) for quotes in quote_dict[\"quotes\"].values())\n",
    "print(f\"\\nQuotes loaded: {total_quotes}.\\nBy author:\")\n",
    "print(\"\\n\".join(f\"  {author} ({len(quotes)})\" for author, quotes in quote_dict[\"quotes\"].items()))\n",
    "\n",
    "print(\"\\nSome examples:\")\n",
    "for author, quotes in list(quote_dict[\"quotes\"].items())[:2]:\n",
    "    print(f\"  {author}:\")\n",
    "    for quote in quotes[:2]:\n",
    "        print(f\"    {quote['body'][:50]} ... (tags: {', '.join(quote['tags'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c4a99f-fe8c-4101-a1cd-66aeaf05f6cb",
   "metadata": {},
   "source": [
    "#### Reduce dataset if you wish\n",
    "\n",
    "If you want to run a smaller-scale demo, feel free to adjust the numbers below and run the cell -- it will simply reduce the amount of quotes to insert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6e8854e-8f4c-4093-bbaa-4b5d88cb0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters and run this cell to run on a shortened data set\n",
    "AUTHORS_TO_USE = 9  # all: 9\n",
    "QUOTES_PER_AUTHOR = 6  # all: 50\n",
    "\n",
    "quote_dict[\"quotes\"] = {\n",
    "    author: quotes[:QUOTES_PER_AUTHOR]\n",
    "    for author, quotes in list(quote_dict[\"quotes\"].items())[:AUTHORS_TO_USE]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15745dc8-e7c1-4781-933b-41eef6ddd657",
   "metadata": {},
   "source": [
    "### Insert quotes into vector store\n",
    "\n",
    "We will compute the embeddings for the quotes and save them into the Vector Store, along with the text itself and the metadata we plan to use later. Note that we add the author as a metadata field in addition to the \"tags\" already found with the quote itself.\n",
    "\n",
    "To optimize speed and reduce the calls, we choose to perform batched calls to the embedding OpenAI service, with one batch per author.\n",
    "\n",
    "_(Note: for faster execution, Cassandra and CassIO would let you do concurrent inserts, which we don't do here for a more straightforward demo code.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68e80e81-886b-45a4-be61-c33b8028bcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aristotle: ****** Done.\n",
      "freud: ****** Done.\n",
      "hegel: ****** Done.\n",
      "kant: ****** Done.\n",
      "nietzsche: ****** Done.\n",
      "plato: ****** Done.\n",
      "sartre: ****** Done.\n",
      "schopenhauer: ****** Done.\n",
      "spinoza: ****** Done.\n",
      "Finished inserting.\n"
     ]
    }
   ],
   "source": [
    "for philosopher, quotes in quote_dict[\"quotes\"].items():\n",
    "    print(f\"{philosopher}: \", end=\"\")\n",
    "    result = openai.Embedding.create(\n",
    "        input=[quote[\"body\"] for quote in quotes],\n",
    "        engine=embedding_model_name,\n",
    "    )\n",
    "    for quote_idx, (quote, q_data) in enumerate(zip(quotes, result.data)):\n",
    "        v_table.put(\n",
    "            row_id=f\"q_{philosopher}_{quote_idx}\",\n",
    "            body_blob=quote[\"body\"],\n",
    "            vector=q_data.embedding,\n",
    "            metadata={**{tag: True for tag in quote[\"tags\"]}, **{\"author\": philosopher}},\n",
    "        )\n",
    "        print(\"*\", end='')\n",
    "    print(\" Done.\")\n",
    "print(\"Finished inserting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ee629-b6b9-4a77-8c58-c3b93403a6a6",
   "metadata": {},
   "source": [
    "## Use case 1: **quote search engine**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b12b3-2557-4826-af5a-16e6cd9a4531",
   "metadata": {},
   "source": [
    "For the quote-search functionality, we need first to make the input quote into a vector, and then use it to query the store (besides handling the optional metadata into the search call, that is).\n",
    "\n",
    "Let's encapsulate the search-engine functionality into a function for ease of re-use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6fcf182-3ab7-4d28-9472-dce35cc38182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_quote_and_author(query_quote, n, author=None, tags=None):\n",
    "    query_vector = openai.Embedding.create(\n",
    "        input=[query_quote],\n",
    "        engine=embedding_model_name,\n",
    "    ).data[0].embedding\n",
    "    metadata = {}\n",
    "    if author:\n",
    "        metadata[\"author\"] = author\n",
    "    if tags:\n",
    "        for tag in tags:\n",
    "            metadata[tag] = True\n",
    "    #\n",
    "    results = v_table.ann_search(\n",
    "        query_vector,\n",
    "        n=n,\n",
    "        metadata=metadata,\n",
    "    )\n",
    "    return [\n",
    "        (result[\"body_blob\"], result[\"metadata\"][\"author\"])\n",
    "        for result in results\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2539262d-100b-4e8d-864d-e9c612a73e91",
   "metadata": {},
   "source": [
    "#### Putting search to test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634165c-0882-4281-bc60-ab96261a500d",
   "metadata": {},
   "source": [
    "Passing just a quote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6722c2c0-3e54-4738-80ce-4d1149e95414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('To live is to suffer, to survive is to find some meaning in the suffering.',\n",
       "  'nietzsche'),\n",
       " ('The meager satisfaction that man can extract from reality leaves him starving.',\n",
       "  'freud'),\n",
       " ('The valor that struggles is better than the weakness that endures.',\n",
       "  'hegel')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_quote_and_author(\"We struggle all our life for nothing\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50828e4c-9bb5-4489-9fe9-87da5fbe1f18",
   "metadata": {},
   "source": [
    "Search restricted to an author:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da9c705f-5c12-42b3-a038-202f89a3c6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('To live is to suffer, to survive is to find some meaning in the suffering.',\n",
       "  'nietzsche'),\n",
       " ('Everything the State says is a lie, and everything it has it has stolen.',\n",
       "  'nietzsche')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_quote_and_author(\"We struggle all our life for nothing\", 2, author=\"nietzsche\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3857ea-6dfe-489a-9b86-4e5e0534960f",
   "metadata": {},
   "source": [
    "Search constrained to a tag (out of those we saved earlier with the quotes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abcfaec9-8f42-4789-a5ed-1073fa2932c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Everything the State says is a lie, and everything it has it has stolen.',\n",
       "  'nietzsche'),\n",
       " ('He who seeks equality between unequals seeks an absurdity.', 'spinoza')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_quote_and_author(\"We struggle all our life for nothing\", 2, tags=[\"politics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71871251-169f-4d3f-a687-65f836a9a8fe",
   "metadata": {},
   "source": [
    "## Use case 2: **quote generator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a9cd63-a131-4819-bf41-c8ffa0b1e1ca",
   "metadata": {},
   "source": [
    "For this task we need another component from OpenAI, namely an LLM to generate the quote for us (based on input we obtain by querying the Vector Store).\n",
    "\n",
    "We also need a working template for the prompt that will be filled for the generate-quote LLM completion task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6dd366d-665a-45fd-917b-b6b5312b0865",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "generation_prompt_template = \"\"\"\"Generate a single short philosophical on the given topic,\n",
    "similar in spirit and form to the provided actual example quotes.\n",
    "Do not exceed 20-30 words in your quote.\n",
    "\n",
    "REFERENCE TOPIC: \"{topic}\"\n",
    "\n",
    "ACTUAL EXAMPLES:\n",
    "{examples}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53073a9e-16de-4e49-9e97-ff31b9b250c2",
   "metadata": {},
   "source": [
    "Like we did for search, this functionality is best wrapped into a handy function (which internally uses search):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "397e6ebd-b30e-413b-be63-81a62947a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quote(topic, n=2, author=None, tags=None):\n",
    "    quotes = find_quote_and_author(query_quote=topic, n=n, author=author, tags=tags)\n",
    "    if quotes:\n",
    "        prompt = generation_prompt_template.format(\n",
    "            topic=topic,\n",
    "            examples=\"\\n\".join(f\"  - {quote[0]}\" for quote in quotes),\n",
    "        )\n",
    "        # a little logging:\n",
    "        print(\"** quotes found:\")\n",
    "        for q, a in quotes:\n",
    "            print(f\"**    - {q} ({a})\")\n",
    "        print(\"** end of logging\")\n",
    "        #\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=completion_model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=320,\n",
    "        )\n",
    "        return response.choices[0].message.content.replace('\"', '').strip()\n",
    "    else:\n",
    "        print(\"** no quotes found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bcc157-e5d4-43ef-8028-d4dcc8a72b9c",
   "metadata": {},
   "source": [
    "#### Putting quote generation to test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6b3f38-089d-486d-b32c-e665c725faa8",
   "metadata": {},
   "source": [
    "Just passing a text (a \"quote\", but we can actually just suggest a topic since its vector embedding will still end up at the right place in the vector space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "806ba758-8988-410e-9eeb-b9c6799e6b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** quotes found:\n",
      "**    - The roots of education are bitter, but the fruit is sweet. (aristotle)\n",
      "**    - The valor that struggles is better than the weakness that endures. (hegel)\n",
      "** end of logging\n",
      "\n",
      "A new generated quote:\n",
      "Power corrupts, but virtue is the compass that guides politics toward justice and progress.\n"
     ]
    }
   ],
   "source": [
    "q_topic = generate_quote(\"politics and virtue\")\n",
    "print(\"\\nA new generated quote:\")\n",
    "print(q_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca032d30-4538-4d0b-aea1-731fb32d2d4b",
   "metadata": {},
   "source": [
    "Use inspiration from just a single philosopher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c2e2d4e-865f-4b2d-80cd-a695271415d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** quotes found:\n",
      "**    - The assumption that animals are without rights, and the illusion that our treatment of them has no moral significance, is a positively outrageous example of Western crudity and barbarity. Universal compassion is the only guarantee of morality. (schopenhauer)\n",
      "**    - It is difficult to keep quiet if you have nothing to do (schopenhauer)\n",
      "** end of logging\n",
      "\n",
      "A new generated quote:\n",
      "The greatness of a society lies in its treatment of animals; our compassion towards them reflects our true morality.\n"
     ]
    }
   ],
   "source": [
    "q_topic = generate_quote(\"animals\", author=\"schopenhauer\")\n",
    "print(\"\\nA new generated quote:\")\n",
    "print(q_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86bc6b3-8258-4f26-96df-29deb898d55e",
   "metadata": {},
   "source": [
    "## (Optional) Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25036179-d7fe-4b19-8a32-22eec4ff57ef",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08435bae-1bb9-4c14-ba21-7b4a7bdee3f5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have seen how to use OpenAI for the vector embeddings and Astra DB / Cassandra for storage and build a sophisticated philosophical search engine and quote generator.\n",
    "\n",
    "This example used [CassIO](https://cassio.org) to interface with the Vector Store - but this is not the only choice. Check the [README](https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/cassandra_astradb) for other options and integration with popular frameworks.\n",
    "\n",
    "To find out more on how Astra DB can be a key ingredient in your ML/GenAI applications, visit [Astra DB]()'s web page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
