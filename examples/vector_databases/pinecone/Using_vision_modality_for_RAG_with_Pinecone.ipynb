{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Enhancing Retrieval-Augmented Generation for Documents with Integrated Images, Graphics and Tables\n",
    "\n",
    "Implementing Retrieval-Augmented Generation (RAG) presents unique challenges when working with documents rich in images, graphics and tables. Traditional RAG models excel with textual data but often falter when visual elements play a crucial role in conveying information. In this cookbook, we bridge that gap by leveraging vision models to extract and interpret visual content, ensuring that the generated responses are as informative and accurate as possible.\n",
    "\n",
    "Our approach involves parsing documents into images and utilizing metadata tagging to identify pages containing images, graphics and tables. When a semantic search retrieves such a page, we pass the page image to a vision model instead of relying solely on text. This method enhances the model's ability to understand and answer user queries that pertain to visual data.\n",
    "\n",
    "In this cookbook, we will explore and demonstrate the following key concepts:\n",
    "\n",
    "##### 1. Setting Up a Vector Store with Pinecone:\n",
    "- Learn how to initialize and configure Pinecone to store vector embeddings efficiently.\n",
    "\n",
    "##### 2. Parsing PDFs and Extracting Visual Information:\n",
    "- Discover techniques for converting PDF pages into images.\n",
    "- Use GPT-4o vision modality to extract textual information from pages with images, graphics or tables.  \n",
    "\n",
    "##### 3. Generating Embeddings: \n",
    "- Utilize embedding models to create vector representations of textual data. \n",
    "- Flag the pages that have visual content so that we set a metadata flag on vector store, and retrieve images to pass on the GPT-4o using vision modality. \n",
    "\n",
    "##### 4. Uploading embeddings to Pinecone: \n",
    "- Upload these embeddings to Pinecone for storage and retrieval. \n",
    "\n",
    "##### 5. Performing Semantic Search for Relevant Pages:\n",
    "- Implement semantic search on page text to find pages that best match the user's query. \n",
    "- Provide the matching page text to GPT-4o as context to answer user's query.  \n",
    "\n",
    "##### 6. Handling Pages with Visual Content (Optional Step):\n",
    "- Learn how to pass the image using GPT-4o vision modality for question answering with additional context. \n",
    "- Understand how this process improves the accuracy of responses involving visual data.\n",
    " \n",
    "By the end of this cookbook, you will have a robust understanding of how to implement RAG systems capable of processing and interpreting documents with complex visual elements. This knowledge will empower you to build AI solutions that deliver richer, more accurate information, enhancing user satisfaction and engagement.\n",
    "\n",
    "We will use the World Bank report - [A Better Bank for a Better World: Annual Report 2024](https://documents1.worldbank.org/curated/en/099101824180532047/pdf/BOSIB13bdde89d07f1b3711dd8e86adb477.pdf) to illustrate the concepts as this document has a mix of images, tables and graphics data. "
   ],
   "id": "411bc0cd6d650113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Setting up a Vector Store with Pinecone \n",
    "In this section, we'll set up a vector store using Pinecone to store and manage our embeddings efficiently. Pinecone is a vector database optimized for handling high-dimensional vector data, which is essential for tasks like semantic search and similarity matching.\n",
    "\n",
    "**Prerequisites** \n",
    "1. Sign-up for Pinecone and obtain an API key by following the instructions here [Pinecone Database Quickstart](https://docs.pinecone.io/guides/get-started/quickstart)  \n",
    "2. Install the Pinecone SDK using `pip install \"pinecone[grpc]\"`. gRPC (gRPC Remote Procedure Call) is a high-performance, open-source universal RPC framework that uses HTTP/2 for transport, Protocol Buffers (protobuf) as the interface definition language, and enables client-server communication in a distributed system. It is designed to make inter-service communication more efficient and suitable for microservices architectures.   \n",
    "\n",
    "**Store the API Key Securely**  \n",
    "1. Store the API key in an .env file for security purposes in you project directory as follows:  \n",
    " `PINECONE_API_KEY=your-api-key-here`.   \n",
    " 2. Install `pip install python-dotenv` to read the API Key from the .env file. \n",
    "\n",
    "**Creat2 the Pinecone Index**   \n",
    "We'll use the `create_index` function to initialize our embeddings database on Pinecone. There are two crucial parameters to consider:\n",
    "\n",
    "1. Dimension: This must match the dimensionality of the embeddings produced by your chosen model. For example, OpenAI's text-embedding-ada-002 model produces embeddings with 1536 dimensions, while text-embedding-3-large produces embeddings with 3072 dimensions. We'll use the text-embedding-3-large model, so we'll set the dimension to 3072.\n",
    "\n",
    "2. Metric: The distance metric determines how similarity is calculated between vectors. Pinecone supports several metrics, including cosine, dotproduct, and euclidean. For this cookbook, we'll use the cosine similarity metric. You can learn more about distance metrics in the [Pinecone Distance Metrics documentation](https://docs.pinecone.io/guides/indexes/understanding-indexes#distance-metrics).\n"
   ],
   "id": "f1f1a7f1a653ef32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "# Import the Pinecone library\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Initialize a Pinecone client with your API key\n",
    "pc = Pinecone(api_key)\n",
    "\n",
    "# Create a serverless index\n",
    "index_name = \"my-test-index\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Wait for the index to be ready\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)\n"
   ],
   "id": "698c0591038c8d47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Navigate to Indexes list on [Pinecone](https://app.pinecone.io/) and you should be able to view `my-test-index` in the list of indexes. ",
   "id": "fb5542a36f05623c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Parsing PDFs and Extracting Visual Information:\n",
    "\n",
    "In this section, we will parse our PDF document the World Bank report - [A Better Bank for a Better World: Annual Report 2024](https://documents1.worldbank.org/curated/en/099101824180532047/pdf/BOSIB13bdde89d07f1b3711dd8e86adb477.pdf) and extract textual and visual information, such as describing images, graphics, and tables. The process involves three main steps:\n",
    "\n",
    "1. **Parse the PDF into individual pages:** We split the PDF into separate pages for easier processing.\n",
    "2. **Convert PDF pages to images:** This enabled vision GPT-4o vision capability to analyze the page as an image.  \n",
    "3. **Process images and tables:** Provide instructions to GPT-4o to extract text, and also describe the images, graphics or tables in the document. \n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "Before proceeding, make sure you have the following packages installed. Also ensure your OpenAI API key is set up as an environment variable. You may also need to install Poppler for PDF rendering.  \n",
    "\n",
    "`pip install PyPDF2 pdf2image pytesseract pandas tqdm`\n",
    " \n",
    "**Steps Breakdown:**\n",
    "\n",
    "**1. Downloading and Chunking the PDF:**  \n",
    "- The `chunk_document` function downloads the PDF from the provided URL and splits it into individual pages using PyPDF2.\n",
    "- Each page is stored as a separate PDF byte stream in a list.\n",
    "\n",
    "**2. Converting PDF Pages to Images:** \n",
    "- The `convert_page_to_image` function takes the PDF bytes of a single page and converts it into an image using pdf2image.\n",
    "- The image is saved locally in an 'images' directory for further processing.\n",
    "\n",
    "**3. Extracting Text Using OCR:**\n",
    "- The `extract_text_from_image` function uses GPT-4o vision capability to extract text from the image of the page.\n",
    "- This method can extract textual information even from scanned documents.\n",
    "\n",
    "**4. Processing the Entire Document:** \n",
    "- The process_document function orchestrates the processing of each page.\n",
    "- It uses a progress bar (tqdm) to show the processing status.\n",
    "- The extracted information from each page is collected into a list and then converted into a Pandas DataFrame.\n"
   ],
   "id": "f22e8baf98bba582"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from pdf2image import convert_from_bytes\n",
    "from io import BytesIO\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "document_to_parse = \"https://documents1.worldbank.org/curated/en/099101824180532047/pdf/BOSIB13bdde89d07f1b3711dd8e86adb477.pdf\"\n",
    "\n",
    "# OpenAI client \n",
    "oai_client = OpenAI()\n",
    "\n",
    "def chunk_document(document_url):\n",
    "    # Download the PDF document\n",
    "    response = requests.get(document_url)\n",
    "    pdf_data = response.content\n",
    "\n",
    "    # Read the PDF data using PyPDF2\n",
    "    pdf_reader = PdfReader(BytesIO(pdf_data))\n",
    "    page_chunks = []\n",
    "\n",
    "    for page_number, page in enumerate(pdf_reader.pages, start=1):\n",
    "        pdf_writer = PdfWriter()\n",
    "        pdf_writer.add_page(page)\n",
    "        pdf_bytes_io = BytesIO()\n",
    "        pdf_writer.write(pdf_bytes_io)\n",
    "        pdf_bytes_io.seek(0)\n",
    "        pdf_bytes = pdf_bytes_io.read()\n",
    "        page_chunk = {\n",
    "            'pageNumber': page_number,\n",
    "            'pdfBytes': pdf_bytes\n",
    "        }\n",
    "        page_chunks.append(page_chunk)\n",
    "\n",
    "    return page_chunks\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(local_image_path):\n",
    "    with open(local_image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "def convert_page_to_image(pdf_bytes, page_number):\n",
    "    # Convert the PDF page to an image\n",
    "    images = convert_from_bytes(pdf_bytes)\n",
    "    image = images[0]  # There should be only one page\n",
    "\n",
    "    # Define the directory to save images (relative to your script)\n",
    "    images_dir = 'images'  # Use relative path here\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "    # Save the image to the images directory\n",
    "    image_file_name = f\"page_{page_number}.png\"\n",
    "    image_file_path = os.path.join(images_dir, image_file_name)\n",
    "    image.save(image_file_path, 'PNG')\n",
    "\n",
    "    # Return the relative image path\n",
    "    return image_file_path\n",
    "\n",
    "\n",
    "def get_vision_response(prompt, image_path):\n",
    "\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image(image_path)\n",
    "\n",
    "    response = oai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def process_document(document_url):\n",
    "    try:\n",
    "        # Update document status to 'Processing'\n",
    "        print(\"Document processing started\")\n",
    "\n",
    "        # Get per-page chunks\n",
    "        page_chunks = chunk_document(document_url)\n",
    "        total_pages = len(page_chunks)\n",
    "\n",
    "        # Prepare a list to collect page data\n",
    "        page_data_list = []\n",
    "\n",
    "        # Add progress bar here\n",
    "        for page_chunk in tqdm(page_chunks, total=total_pages, desc='Processing Pages'):\n",
    "            page_number = page_chunk['pageNumber']\n",
    "            pdf_bytes = page_chunk['pdfBytes']\n",
    "\n",
    "            # Convert page to image\n",
    "            image_path = convert_page_to_image(pdf_bytes, page_number)\n",
    "\n",
    "            # Prepare question for vision API\n",
    "            system_prompt = (\n",
    "                \"The user will provide you an image of a document file. Perform the following actions: \"\n",
    "                \"1. Transcribe the text on the page. **TRANSCRIPTION OF THE TEXT:**\"\n",
    "                \"2. If there is a chart, describe the image and include the text **DESCRIPTION OF THE IMAGE OR CHART**\"\n",
    "                \"3. If there is a table, transcribe the table and include the text **TRANSCRIPTION OF THE TABLE**\"\n",
    "            )\n",
    "\n",
    "            # Get vision API response\n",
    "            vision_response = get_vision_response(system_prompt, image_path)\n",
    "\n",
    "            # Extract text from vision response\n",
    "            text = vision_response.choices[0].message.content\n",
    "\n",
    "            # Collect page data\n",
    "            page_data = {\n",
    "                'PageNumber': page_number,\n",
    "                'ImagePath': image_path,\n",
    "                'PageText': text\n",
    "            }\n",
    "            page_data_list.append(page_data)\n",
    "\n",
    "        # Create DataFrame from page data\n",
    "        pdf_df = pd.DataFrame(page_data_list)\n",
    "        print(\"Document processing completed.\")\n",
    "        print(\"DataFrame created with page data.\")\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return pdf_df\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"Error processing document: {err}\")\n",
    "        # Update document status to 'Error'\n",
    "\n",
    "\n",
    "df = process_document(document_to_parse)"
   ],
   "id": "9a58943248b3eb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's examine the DataFrame to ensure that the pages have been processed correctly. For brevity, we will retrieve and display only the first five rows. Additionally, you should be able to see the page images generated in the 'images' directory.",
   "id": "2f760dd52b8ad996"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Convert the DataFrame to an HTML table and display it\n",
    "display(HTML(df.head().to_html()))"
   ],
   "id": "ba17abe814692406",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's take a look at a sample page, such as page 21, which contains embedded images and text. We can observe that the vision modality effectively extracted and described the visual information. For instance, the pie chart on this page is accurately described as:\n",
    "\n",
    "`\"FIGURE 6: MIDDLE EAST AND NORTH AFRICA IBRD AND IDA LENDING BY SECTOR - FISCAL 2024 SHARE OF TOTAL OF $4.6 BILLION\" is a circular chart, resembling a pie chart, illustrating the percentage distribution of funds among different sectors. The sectors include:`"
   ],
   "id": "a0cb52367eef3222"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter and print rows where pageNumber is 21\n",
    "filtered_rows = df[df['PageNumber'] == 21]\n",
    "for text in filtered_rows.PageText:\n",
    "    print(text)"
   ],
   "id": "31a0a3ca3f9012fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Generating Embeddings: \n",
    "\n",
    "In this section, we focus on transforming the textual content extracted from each page of the document into vector embeddings. These embeddings capture the semantic meaning of the text, enabling efficient similarity searches and various Natural Language Processing (NLP) tasks. We also identify pages containing visual elements, such as images, graphics, or tables, and flag them for special handling.\n",
    "\n",
    "**Steps Breakdown:**\n",
    "\n",
    "**1. Adding a flag for visual content**   \n",
    "  \n",
    "To process pages containing visual information, in Step 2 we used the vision modality to extract content from charts, tables, and images. By including specific instructions in our prompt, we ensure that the model adds markers such as `DESCRIPTION OF THE IMAGE OR CHART` or `TRANSCRIPTION OF THE TABLE` when describing visual content. In this step, if such a marker is detected, we set the Visual_Input_Processed flag to 'Y'; otherwise, it remains 'N'.\n",
    "\n",
    "While the vision modality captures most visual information effectively, some details—particularly in complex visuals like engineering drawings—may be lost in translation. In Step 6, we will use this flag to determine when to pass the image of the page to GPT-4 Vision as additional context. This is an optional enhancement that can significantly improve the effectiveness of a RAG solution.  \n",
    "\n",
    "**2. Generating Embeddings with OpenAI's Embedding Model**  \n",
    "\n",
    "We use OpenAI's embedding model, `text-embedding-3-large`, to generate high-dimensional embeddings that represent the semantic content of each page. \n",
    "\n",
    "Note: It is crucial to ensure that the dimensions of the embedding model you use are consistent with the configuration of your Pinecone vector store. In our case, we set up the Pinecone database with 3072 dimensions to match the default dimensions of `text-embedding-3-large`.  \n"
   ],
   "id": "33919ba1ef16f993"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add a column to flag pages with visual content\n",
    "df['Visual_Input_Processed'] = df['PageText'].apply(\n",
    "    lambda x: 'Y' if '**DESCRIPTION OF THE IMAGE OR CHART:**' in x or '**TRANSCRIPTION OF THE TABLE:**' in x else 'N'\n",
    ")\n",
    "\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embedding(text_input):\n",
    "\n",
    "    response = oai_client.embeddings.create(\n",
    "        input=text_input,\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "# Generate embeddings with a progress bar to display progress\n",
    "embeddings = []\n",
    "for text in tqdm(df['PageText'], desc='Generating Embeddings'):\n",
    "    embedding = get_embedding(text)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# Add the embeddings to the DataFrame\n",
    "df['Embeddings'] = embeddings"
   ],
   "id": "209bf18a8849b61d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can verify that our logic correctly flagged pages requiring visual input. For instance, page 21, which we previously examined, has the Visual_Input_Needed flag set to \"Y\".",
   "id": "a1357b68b8cc62a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "filtered_rows = df[df['PageNumber'] == 21]\n",
    "print(filtered_rows.Visual_Input_Processed)"
   ],
   "id": "952c187d77892913",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 4. Uploading embeddings to Pinecone: \n",
    "\n",
    "In this section, we will upload the embeddings we've generated for each page of our document to Pinecone. Along with the embeddings, we'll include relevant metadata tags that describe each page, such as the page number, text content, image paths, and whether the page includes graphics. \n",
    "\n",
    "**Steps Breakdown:**  \n",
    "\n",
    "**1. Create Metadata Fields:**  \n",
    "Metadata enhances our ability to perform more granular searches, find the text or image associated with the vector, and enables filtering within the vector database.\n",
    "* pageId: Combines the document_id and pageNumber to create a unique identifier for each page. We will use this as a unique identifier for our embeddings. \n",
    "* pageNumber: The numerical page number within the document.\n",
    "* text: The extracted text content from the page.\n",
    "* ImagePath: The file path to the image associated with the page.\n",
    "* GraphicIncluded: A boolean or flag indicating whether the page includes graphical elements that may require visual processing.\n",
    "\n",
    "**2. Upload embeddings:**  \n",
    "We will use Pinecone API to in function `upsert_vector` to \"upserts\" the values - \n",
    "\n",
    "* A unique identifier\n",
    "* Embeddings\n",
    "* Metadata as defined above \n",
    "\n",
    "Note: \"Upsert\" is a combination of the words \"update\" and \"insert.\" In database operations, an upsert is an atomic operation that updates an existing record if it exists or inserts a new record if it doesn't. This is particularly useful when you want to ensure that your database has the most recent data without having to perform separate checks for insertion or updating."
   ],
   "id": "fb055f21c11d7098"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'pc', 'index_name', and 'df' are defined elsewhere in your code\n",
    "index = pc.Index(index_name)\n",
    "document_id = 'WB_Report'\n",
    "\n",
    "# Define the async function correctly\n",
    "def upsert_vector(identifier, embedding, metadata):\n",
    "    try:\n",
    "        index.upsert([\n",
    "            {\n",
    "                'id': identifier,\n",
    "                'values': embedding,\n",
    "                'metadata': metadata\n",
    "            }\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"Error upserting vector with ID {identifier}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc='Uploading to Pinecone'):\n",
    "    pageNumber = row['PageNumber']\n",
    "    \n",
    "    # Create meta-data tags to be added to Pinecone \n",
    "    metadata = {\n",
    "        'pageId': f\"{document_id}-{pageNumber}\",\n",
    "        'pageNumber': pageNumber,\n",
    "        'text': row['PageText'],\n",
    "        'ImagePath': row['ImagePath'],\n",
    "        'GraphicIncluded': row['Visual_Input_Needed']\n",
    "    }\n",
    "\n",
    "    upsert_vector(metadata['pageId'] , row['Embeddings'], metadata)  \n",
    "    "
   ],
   "id": "af65e9d39caf979f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Navigate to Indexes list on [Pinecone](https://app.pinecone.io/) and you should be able to view the vectors upserted into the database with metadata.",
   "id": "dd6ce7b0e4a8c5a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Performing Semantic Search for Relevant Pages:\n",
    "In this section, we implement a semantic search to find the most relevant pages in our document that answer a user's question. This approach uses the embeddings stored in the Pinecone vector database to retrieve pages based on the semantic similarity of their content to the user's query. By doing so, we can effectively search textual content, and provide it as context to GPT-4o for answering user's question.\n",
    "\n",
    "**Steps Breakdown:**  \n",
    "\n",
    "**1. Generate an Embedding for the User's Question** \n",
    "\n",
    "* We use OpenAI's embedding model to generate a high-dimensional vector representation of the user's question. \n",
    "* This vector captures the semantic meaning of the question, allowing us to perform an efficient similarity search against our stored embeddings. \n",
    "* The embedding is crucial for ensuring that the search query is semantically aligned with the content of the document, even if the exact words do not match.\n",
    "\n",
    "**2. Query the Pinecone Index for Relevant Pages** \n",
    "\n",
    "* Using the generated embedding, we query the Pinecone index to find the most relevant pages. \n",
    "* Pinecone performs a similarity search by comparing the question's embedding to the embeddings stored in the vector database using `cosine` similarity. If you recall, we set this as `metric` parameter in Step 1 when we created our Pinecone database. \n",
    "* We specify the number of top matches to retrieve, typically based on a balance between coverage and relevance. For instance, retrieving the top 3-5 pages is often sufficient to provide a comprehensive answer without overwhelming the model with too much context. \n",
    "\n",
    "\n",
    "**3. Compile the Metadata of Matched Pages to Provide Context** \n",
    "\n",
    "* Once the relevant embeddings are identified, we gather their associated metadata, including the extracted text and the page number.\n",
    "* This metadata is essential for structuring the context provided to GPT-4o. \n",
    "* We also format the compiled information as a JSON to make it easy for the LLM to interpret.\n",
    "\n",
    "**4. Use the GPT-4 Vision Model to Generate an Answer** \n",
    "\n",
    "* Finally, we pass the compiled context to the GPT-4o. \n",
    "* The model uses the context to generate an informative, coherent, and contextually relevant answer to the user's question.\n",
    "* The retrieved context helps the LLM answer questions with greater accuracy, as it has access to relevant information from the document. "
   ],
   "id": "a4d8d84f316a85a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def get_response_to_question(user_question, pc_index):\n",
    "    # Get embedding of the question to find the relevant page with the information \n",
    "    question_embedding = get_embedding(user_question) \n",
    "\n",
    "    # get response vector embeddings \n",
    "    response = pc_index.query(\n",
    "            vector=question_embedding,\n",
    "            top_k=2,\n",
    "            include_values=True,\n",
    "            include_metadata=True\n",
    "        )\n",
    "    \n",
    "    # Collect the metadata from the matches\n",
    "    context_metadata = [match['metadata'] for match in response['matches']]\n",
    "    \n",
    "    # Convert the list of metadata dictionaries to a JSON string\n",
    "    context_json = json.dumps(context_metadata, indent=3)\n",
    "    \n",
    "    prompt = f\"\"\"You are a helpful assistant. Use the following context and images to answer the question. In the answer, include the reference to the document, and page number you found the information on between <source></source> tags. If you don't find the information, you can say \"I couldn't find the information\"\n",
    "\n",
    "    question: {user_question}\n",
    "    \n",
    "    <SOURCES>\n",
    "    {context_json}\n",
    "    </SOURCES>\n",
    "    \"\"\"\n",
    "    \n",
    "    completion = oai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content"
   ],
   "id": "ff6ec1bf85570568",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's pose a question that requires information from a diagram. In this case, the relevant details are found within a pie chart.",
   "id": "34f768cacad77b07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"What percentage was allocated to social protections in Western and Central Africa?\"\n",
    "answer = get_response_to_question(question, index)\n",
    "\n",
    "print(answer)"
   ],
   "id": "8f2152d5a6e152d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's make it more challenging by asking a question that requires interpretation of information presented in a table. In our Step 2, we extracted this information using the GPT-4o vision modality. ",
   "id": "347ee5875a9d9d8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"What was the increase in access to electricity between 2000 and 2012 in Western and Central Africa?\"\n",
    "answer = get_response_to_question(question, index)\n",
    "\n",
    "print(answer)"
   ],
   "id": "3852345cdc10b063",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This approach worked well. However, there may be cases where information is embedded within images or graphics that lose fidelity when translated to text, such as complex engineering drawings.\n",
    "\n",
    "By using the GPT-4o Vision modality, we can pass the image of the page directly to the model as context. In the next section, we will explore how to improve the accuracy of model responses using image inputs."
   ],
   "id": "8981b499322cee5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Handling Pages with Visual Content (Optional Step):\n",
    "When metadata indicates the presence of an image, graphic or a table, we can pass the image as the context to GPT-4o instead of the extracted text. This approach can be useful in cases where text description of the visual information is not sufficient to convey the context. It can be the case for complex graphics such as engineering drawings or complex diagrams. \n",
    "\n",
    "**Steps Breakdown:** \n",
    "\n",
    "The difference between this Step and Step 5, is that we've added additional logic to identify when `Visual_Input_Processed` flag is set for an embedding. In that case, instead of passing the text as the context, we pass the image of the page using GPT-4o vision modality as the context. \n",
    "\n",
    "Note: This approach does increase both latency and cost, as processing image inputs is more resource intensive and expensive. Therefore, it should only be used if the desired results cannot be achieved with the text-only modality as outlined in Step 5 above."
   ],
   "id": "c2d6bbe66e284e12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "def get_response_to_question_with_images(user_question, pc_index):\n",
    "    # Get embedding of the question to find the relevant page with the information \n",
    "    question_embedding = get_embedding(user_question) \n",
    "\n",
    "    # Get response vector embeddings \n",
    "    response = pc_index.query(\n",
    "        vector=question_embedding,\n",
    "        top_k=2,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # Collect the metadata from the matches\n",
    "    context_metadata = [match['metadata'] for match in response['matches']]\n",
    "    \n",
    "    # Build the message content\n",
    "    message_content = []\n",
    "\n",
    "    # Add the initial prompt\n",
    "    initial_prompt = f\"\"\"You are a helpful assistant. Use the text and images provided by the user to answer the question. In the answer, include the reference to the page number or title of the section you the answer. If you don't find the information, you can say \"I couldn't find the information\"\n",
    "\n",
    "    question: {user_question}\n",
    "    \"\"\"\n",
    "    \n",
    "    message_content.append({\"type\": \"text\", \"text\": initial_prompt})\n",
    "\n",
    "        # Process each metadata item to include text or images based on 'Visual_Input_Processed'\n",
    "    for metadata in context_metadata:\n",
    "        visual_flag = metadata.get('Visual_Input_Processed', 'N')\n",
    "        page_number = metadata.get('PageNumber', 'Unknown')\n",
    "        \n",
    "        if visual_flag == 'Y':\n",
    "            # Include the image\n",
    "            image_path = metadata.get('ImagePath', None)\n",
    "            try:\n",
    "                base64_image = encode_image(image_path)\n",
    "                image_type = 'png'  # Adjust if your images are in a different format\n",
    "                message_content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/{image_type};base64,{base64_image}\"\n",
    "                    },\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error encoding image at {image_path}: {e}\")\n",
    "        else:\n",
    "            # Include the text\n",
    "            page_text = metadata.get('PageText', '')\n",
    "            message_content.append({\"type\": \"text\", \"text\": f\"Page {page_number}: {page_text}\"})\n",
    "\n",
    "    # Prepare the messages for the API call\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message_content\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    completion = oai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content\n"
   ],
   "id": "1d2c19cf61fdf198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's examin the same questions we asked for the text only sementic search. ",
   "id": "d42175c54db3888"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"What percentage was allocated to social protections in Western and Central Africa?\"\n",
    "answer = get_response_to_question_with_images(question, index)\n",
    "\n",
    "print(answer)"
   ],
   "id": "6e636006f43c724",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"What was the increase in access to electricity between 2000 and 2012 in Western and Central Africa?\"\n",
    "answer = get_response_to_question_with_images(question, index)\n",
    "\n",
    "print(answer)"
   ],
   "id": "7120e7ff370e30f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "In this cookbook, we embarked on a journey to enhance Retrieval-Augmented Generation (RAG) systems for documents rich in images, graphics and tables. Traditional RAG models, while proficient with textual data, often overlook the wealth of information conveyed through visual elements. By integrating vision models and leveraging metadata tagging, we've bridged this gap, enabling AI to interpret and utilize visual content effectively.\n",
    "\n",
    "We began by setting up a vector store using Pinecone, establishing a foundation for efficient storage and retrieval of vector embeddings. Parsing PDFs and extracting visual information allowed us to convert document pages into images and identify those containing crucial visual data. By generating embeddings and flagging pages with visual content, we created a robust metadata filtering system within our vector store.\n",
    "\n",
    "Uploading these embeddings to Pinecone facilitated seamless integration with our RAG system. Through semantic search, we retrieved relevant pages that matched user queries, ensuring that both textual and visual information were considered. Handling pages with visual content by passing them to vision models enhanced the accuracy and depth of the responses, particularly for queries dependent on images or tables.\n",
    "\n",
    "Using the World Bank's **A Better Bank for a Better World: Annual Report 2024** as our guiding example, we demonstrated how these techniques come together to process and interpret complex documents. This approach not only enriches the information provided to users but also significantly enhances user satisfaction and engagement by delivering more comprehensive and accurate responses.\n",
    "\n",
    "By following the concepts outlined in this cookbook, you are now equipped to build RAG systems capable of processing and interpreting documents with intricate visual elements. This advancement opens up new possibilities for AI applications across various domains where visual data plays a pivotal role."
   ],
   "id": "8dcfbb6a47322a21"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
