{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning OpenAI Models for Retrieval Augmented Generation (RAG) with Qdrant and Few-Shot Learning\n",
    "\n",
    "The aim of this blog is to walk through a comprehensive example of how to fine-tune OpenAI models for Retrieval Augmented Generation (RAG). We will also be integrating Qdrant and Few-Shot Learning to boost the model's performance and reduce hallucinations. This could serve as a practical guide for ML practitioners, data scientists, and researchers interested in leveraging the power of OpenAI models for specific use-cases. 🤩\n",
    "\n",
    "To begin, we've selected a dataset where we've a guarantee that the retrieval is perfect. We've selected a subset of the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) dataset, which is a collection of questions and answers about Wikipedia articles. We've also included samples where the answer is not present in the context, to demonstrate how RAG handles this case.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setting up the Environment\n",
    "\n",
    "### Section A: Zero-Shot Learning\n",
    "2. Data Preparation\n",
    "3. OpenAI Model Fine-Tuning\n",
    "4. Baseline Results\n",
    "\n",
    "### Section B: Few-Shot Learning\n",
    "5. Using Qdrant to Improve RAG Prompt\n",
    "6. Fine-Tuning OpenAI Model with Qdrant\n",
    "7. Comparison and Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up\n",
    "\n",
    "### Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas openai tqdm tenacity matplotlib scikit-learn tiktoken python-dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import openai\n",
    "import tiktoken\n",
    "from tenacity import retry, wait_exponential\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation: SQuADv2 Data Subsets\n",
    "\n",
    "For the purpose of demonstration, we'll make small slices from the train and validation splits of the [SQuADv2](https://rajpurkar.github.io/SQuAD-explorer/) dataset. This dataset has questions and contexts where the answer is not present in the context, to help us evaluate how LLM handles this case.\n",
    "\n",
    "We'll read the data from the JSON files and create a dataframe with the following columns: `question`, `context`, `answer`, `is_impossible`.\n",
    "\n",
    "### Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p local_cache\n",
    "# !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O local_cache/train.json\n",
    "# !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O local_cache/dev.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read JSON to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dataframe_with_titles(json_data):\n",
    "    qas = []\n",
    "    context = []\n",
    "    is_impossible = []\n",
    "    answers = []\n",
    "    titles = []\n",
    "\n",
    "    for article in json_data['data']:\n",
    "        title = article['title']\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                qas.append(qa['question'].strip())\n",
    "                context.append(paragraph['context'])\n",
    "                is_impossible.append(qa['is_impossible'])\n",
    "                \n",
    "                ans_list = []\n",
    "                for ans in qa['answers']:\n",
    "                    ans_list.append(ans['text'])\n",
    "                answers.append(ans_list)\n",
    "                titles.append(title)\n",
    "\n",
    "    df = pd.DataFrame({'title': titles, 'question': qas, 'context': context, 'is_impossible': is_impossible, 'answers': answers})\n",
    "    return df\n",
    "\n",
    "def get_diverse_sample(df, sample_size=100, random_state=42):\n",
    "    sample_df = df.groupby(['title', 'is_impossible']).apply(lambda x: x.sample(min(len(x), max(1, sample_size // 50)), random_state=random_state)).reset_index(drop=True)\n",
    "    \n",
    "    if len(sample_df) < sample_size:\n",
    "        remaining_sample_size = sample_size - len(sample_df)\n",
    "        remaining_df = df.drop(sample_df.index).sample(remaining_sample_size, random_state=random_state)\n",
    "        sample_df = pd.concat([sample_df, remaining_df]).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return sample_df.sample(min(sample_size, len(sample_df)), random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "train_df = json_to_dataframe_with_titles(json.load(open('local_cache/train.json')))\n",
    "val_df = json_to_dataframe_with_titles(json.load(open('local_cache/dev.json')))\n",
    "\n",
    "df = get_diverse_sample(val_df, sample_size=100, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Baseline Model Performance\n",
    "\n",
    "### Utility Functions: Zero Shot Prompt, API Call to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get prompt messages\n",
    "def get_prompt(row):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\n",
    "    Question: {row.question}\\n\\n\n",
    "    Context: {row.context}\\n\\n\n",
    "    Answer:\\n\"\"\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "# Function with tenacity for retries\n",
    "@retry(wait=wait_exponential(multiplier=1, min=2, max=6))\n",
    "def api_call(messages, model):\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stop=[\"\\n\\n\"],\n",
    "        max_tokens=100,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "\n",
    "# Main function to answer question\n",
    "def answer_question(row, prompt_func=get_prompt, model=\"gpt-3.5-turbo-0613\"):\n",
    "    messages = prompt_func(row)\n",
    "    response = api_call(messages, model)\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:33<01:35,  1.28s/it]"
     ]
    }
   ],
   "source": [
    "# Use progress_apply with tqdm for progress bar\n",
    "df[\"generated_answer\"] = df.progress_apply(answer_question, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"local_cache/100_val.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning the OpenAI Model\n",
    "\n",
    "For the complete fine-tuning process, please refer to the [OpenAI Fine-Tuning Docs](https://platform.openai.com/docs/guides/fine-tuning/use-a-fine-tuned-model).\n",
    "\n",
    "### Prepare the Fine-Tuning Data\n",
    "\n",
    "We need to prepare the data for fine-tuning. We'll use a few samples from train split of same dataset as before, but we'll add the answer to the context. This will help the model learn to retrieve the answer from the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_jsonl(df):\n",
    "    def create_jsonl_entry(row):\n",
    "        answer = row[\"answers\"][0] if row[\"answers\"] else \"I don't know\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\n",
    "            Question: {row.question}\\n\\n\n",
    "            Context: {row.context}\\n\\n\n",
    "            Answer:\\n\"\"\",\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "        return json.dumps({\"messages\": messages})\n",
    "\n",
    "    jsonl_output = df.progress_apply(create_jsonl_entry, axis=1)\n",
    "    return \"\\n\".join(jsonl_output)\n",
    "\n",
    "train_sample = get_diverse_sample(train_df, sample_size=100, random_state=42)\n",
    "\n",
    "with open(\"local_cache/100_train.jsonl\", \"w\") as f:\n",
    "    f.write(dataframe_to_jsonl(train_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Verify the Fine-Tuning Data\n",
    "\n",
    "The script below will verify that the data is in the format that OpenAI expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the data path and open the JSONL file\n",
    "\n",
    "data_path = \"local_cache/100_train.jsonl\"\n",
    "\n",
    "# Load dataset\n",
    "with open(data_path) as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "\n",
    "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "# Token counting functions\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "print(\"See pricing page to estimate total costs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Fine-Tuning Data to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = openai.File.create(\n",
    "    file=open(\"local_cache/100_train.jsonl\", \"r\"),\n",
    "    purpose=\"fine-tune\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 Wait: For file to be uploaded and then processed by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while file_object.status!='processed':\n",
    "    time.sleep(5)\n",
    "    file_object.refresh()\n",
    "file_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Fine Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_job = openai.FineTuningJob.create(\n",
    "    training_file=file_object[\"id\"], model=\"gpt-3.5-turbo\", suffix=\"100train20230906\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 Wait: For the fine-tuning job to complete and status to be \"succeeded\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while ft_job.status!='succeeded':\n",
    "    time.sleep(15)\n",
    "    ft_job.refresh()\n",
    "    print(\"Status: \", ft_job.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = openai.FineTuningJob.retrieve(ft_job[\"id\"]).fine_tuned_model\n",
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Hi, how can I help you today?\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you answer the following question based on the given context? If not, say, I don't know:\\n\\nQuestion: What is the capital of France?\\n\\nContext: The capital of Mars is Gaia. Answer:\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ft_generated_answer\"] = df.progress_apply(answer_question, model=model_id, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Baseline Model Performance\n",
    "\n",
    "To evaluate the model's performance, compare the predicted answer to the actual answers -- if any of the actual answers are present in the predicted answer, then it's a match. We've also created error categories to help you understand where the model is struggling.\n",
    "\n",
    "1. Expected and Right: The model responsded the correct answer. It may have also included other answers that were not in the context.\n",
    "2. Expected but \"IDK\": The model responded with \"I don't know\" (IDK) while the answer was present in the context. *This is a model error* and better than giving the wrong answer. We exclude this from the overall error rate.\n",
    "3. Expected but Wrong: The model responded with an incorrect answer. *This is a model ERROR.*\n",
    "4. Hallucination: The model responded with an answer, when \"I don't know\" was expected. **This is a model error.** \n",
    "5. Did not expect and IDK: The model responded with \"I don't know\" (IDK) and the answer was not present in the context. *This is a model WIN.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrixEvaluator:\n",
    "    def __init__(self, df, answers_column=\"generated_answer\"):\n",
    "        self.df = df\n",
    "        self.y_pred = []\n",
    "        self.labels = [\n",
    "            \"Expected and Right\",\n",
    "            \"Expected but IDK\",\n",
    "            \"Expected but Wrong\",\n",
    "            \"Hallucination\",\n",
    "            \"Did not Expect and IDK\",\n",
    "        ]\n",
    "        self.answers_column = answers_column\n",
    "\n",
    "    def _evaluate_single_row(self, row):\n",
    "        is_impossible = row[\"is_impossible\"]\n",
    "        generated_answer = row[self.answers_column].lower()\n",
    "        actual_answers = [ans.lower() for ans in row[\"answers\"]]\n",
    "\n",
    "        y_pred = (\n",
    "            \"Expected and Right\"\n",
    "            if not is_impossible\n",
    "            and any(ans in generated_answer for ans in actual_answers)\n",
    "            else \"Expected but IDK\"\n",
    "            if not is_impossible and generated_answer == \"i don't know\"\n",
    "            else \"Expected but Wrong\"\n",
    "            if not is_impossible and generated_answer not in actual_answers\n",
    "            else \"Hallucination\"\n",
    "            if is_impossible and generated_answer != \"i don't know\"\n",
    "            else \"Did not Expect and IDK\"\n",
    "        )\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate_answers(self):\n",
    "        self.y_pred = self.df.apply(self._evaluate_single_row, axis=1)\n",
    "\n",
    "    def generate_matrices(self, use_percentages=False):\n",
    "        # Using value_counts to create a Series of frequencies, then reindexing to include missing labels with count 0\n",
    "        freq_series = self.y_pred.value_counts().reindex(self.labels, fill_value=0)\n",
    "        if use_percentages:\n",
    "            total = freq_series.sum()\n",
    "            freq_series = (freq_series / total * 100).apply(\"{0:.2f}%\".format)\n",
    "        return freq_series\n",
    "\n",
    "\n",
    "def evaluate_model(df, answers_column):\n",
    "    \"\"\"\n",
    "    Evaluate the confusion matrix for a given DataFrame and answer column.\n",
    "    \"\"\"\n",
    "    evaluator = ConfusionMatrixEvaluator(df, answers_column=answers_column)\n",
    "    evaluator.evaluate_answers()\n",
    "    error_categories = evaluator.generate_matrices(use_percentages=True)\n",
    "    return error_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(df, \"generated_answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(df, \"ft_generated_answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the results to a JSON file\n",
    "df.to_json(\"local_cache/100_val_ft.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overall_error(df, answer_columns):\n",
    "    error_categories = [\"Expected but Wrong\", \"Hallucination\"]\n",
    "    labels = [\"Overall Error\"]\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35 / len(answer_columns)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i, col in enumerate(answer_columns):\n",
    "        results = evaluate_model(df, col)\n",
    "        matrix_error = sum(\n",
    "            [float(results[col].loc[cat].replace(\"%\", \"\")) for cat in error_categories]\n",
    "        )\n",
    "        ax.bar(x + (i - len(answer_columns)/2) * width, [matrix_error], width, label=col)\n",
    "\n",
    "    ax.set_ylabel(\"Error (%) - Lower is Better\")\n",
    "    ax.set_title(\"Overall Error Comparison\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot only the overall error\n",
    "plot_overall_error(df, answer_columns=[\"generated_answer\", \"ft_generated_answer\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Learning\n",
    "\n",
    "\n",
    "We'll select a few examples from the dataset, including cases where the answer is not present in the context. We'll then use these examples to create a prompt that we can use to fine-tune the model. We'll then measure the performance of the fine-tuned model.\n",
    "\n",
    "## 5. Fine-Tuning OpenAI Model with Qdrant\n",
    "\n",
    "So far, we've been using the OpenAI model to answer questions where the answer is present in the context. But what if we want to answer questions where the answer is not present in the context? This is where few-shot learning comes in. Few-shot learning is a type of transfer learning that allows us to answer questions where the answer is not present in the context. We can do this by providing a few examples of the answer we're looking for, and the model will learn to answer questions where the answer is not present in the context."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the Training Data\n",
    "\n",
    "Embeddings are a way to represent sentences as an array of floats. We'll use the embeddings to find the most similar questions to the ones we're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from qdrant_client.http.models import PointStruct\n",
    "from qdrant_client.http.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've the Qdrant imports in place, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(\n",
    "    url=os.getenv(\"QDRANT_URL\"), api_key=os.getenv(\"QDRANT_API_KEY\"), timeout=6000, prefer_grpc=True\n",
    ")\n",
    "\n",
    "# collection_name = \"squadv2-cookbook\"\n",
    "\n",
    "# # Create the collection\n",
    "# qdrant_client.recreate_collection(\n",
    "#     collection_name=collection_name,\n",
    "#     vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed.embedding import DefaultEmbedding\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "embedding_model = DefaultEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the Questions\n",
    "\n",
    "We embed the entire training set questions. We'll use the question to question similarity to find the most similar questions to the question we're looking for. This is a workflow which is used in RAG to leverage the OpenAI model ability of incontext learning with more examples. This is what we call Few Shot Learning here.\n",
    "\n",
    "### ❗️ Important Note: This step can take upto 3 hours to complete. Please be patient. If you see Out of Memory errors or Kernel Crashes, please reduce the batch size to 32, restart the kernel and run the notebook again. This code needs to be run only ONCE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_points_from_dataframe(df: pd.DataFrame) -> List[PointStruct]:\n",
    "#     batch_size = 512\n",
    "#     questions = df[\"question\"].tolist()\n",
    "#     total_batches = len(questions) // batch_size + 1\n",
    "    \n",
    "#     pbar = tqdm(total=len(questions), desc=\"Generating embeddings\")\n",
    "    \n",
    "#     # Generate embeddings in batches to improve performance\n",
    "#     embeddings = []\n",
    "#     for i in range(total_batches):\n",
    "#         start_idx = i * batch_size\n",
    "#         end_idx = min((i + 1) * batch_size, len(questions))\n",
    "#         batch = questions[start_idx:end_idx]\n",
    "        \n",
    "#         batch_embeddings = embedding_model.embed(batch, batch_size=batch_size)\n",
    "#         embeddings.extend(batch_embeddings)\n",
    "#         pbar.update(len(batch))\n",
    "        \n",
    "#     pbar.close()\n",
    "    \n",
    "#     # Convert embeddings to list of lists\n",
    "#     embeddings_list = [embedding.tolist() for embedding in embeddings]\n",
    "    \n",
    "#     # Create a temporary DataFrame to hold the embeddings and existing DataFrame columns\n",
    "#     temp_df = df.copy()\n",
    "#     temp_df[\"embeddings\"] = embeddings_list\n",
    "#     temp_df[\"id\"] = temp_df.index\n",
    "    \n",
    "#     # Generate PointStruct objects using DataFrame apply method\n",
    "#     points = temp_df.progress_apply(\n",
    "#         lambda row: PointStruct(\n",
    "#             id=row[\"id\"],\n",
    "#             vector=row[\"embeddings\"],\n",
    "#             payload={\n",
    "#                 \"question\": row[\"question\"],\n",
    "#                 \"title\": row[\"title\"],\n",
    "#                 \"context\": row[\"context\"],\n",
    "#                 \"is_impossible\": row[\"is_impossible\"],\n",
    "#                 \"answers\": row[\"answers\"],\n",
    "#             },\n",
    "#         ),\n",
    "#         axis=1,\n",
    "#     ).tolist()\n",
    "\n",
    "#     return points\n",
    "\n",
    "# points = generate_points_from_dataframe(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Embeddings to Qdrant\n",
    "\n",
    "Note that configuring Qdrant is outside the scope of this notebook. Please refer to the [Qdrant](https://qdrant.tech) for more information. We used a timeout of 600 seconds for the upload, and grpc compression to speed up the upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation_info = qdrant_client.upsert(\n",
    "#     collection_name=collection_name, wait=True, points=points\n",
    "# )\n",
    "# print(operation_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Qdrant to Improve RAG Prompt\n",
    "\n",
    "Now that we've uploaded the embeddings to Qdrant, we can use Qdrant to find the most similar questions to the question we're looking for. We'll use the top 5 most similar questions to create a prompt that we can use to fine-tune the model. We'll then measure the performance of the fine-tuned model on the same validation set, but with few shot prompting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_few_shot_prompt(row):\n",
    "\n",
    "    query, row_context = row[\"question\"], row[\"context\"]\n",
    "\n",
    "    embeddings = list(embedding_model.embed([query]))\n",
    "    query_embedding = embeddings[0].tolist()\n",
    "\n",
    "    num_of_qa_to_retrieve = 5\n",
    "\n",
    "    # Pick the most similar question from the collection\n",
    "    q1 = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        with_payload=True,\n",
    "        limit=num_of_qa_to_retrieve,\n",
    "        query_filter=models.Filter(\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"is_impossible\",\n",
    "                    match=models.MatchValue(\n",
    "                        value=False,\n",
    "                    ),\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Pick the next best question which has same title but impossible answer/I don't know answer\n",
    "    q2 = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        query_filter=models.Filter(\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"is_impossible\",\n",
    "                    match=models.MatchValue(\n",
    "                        value=True,\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        with_payload=True,\n",
    "        limit=num_of_qa_to_retrieve,\n",
    "    )\n",
    "\n",
    "\n",
    "    instruction = \"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\\n\\n\"\"\"\n",
    "    # If there is a next best question, add it to the prompt\n",
    "    \n",
    "    def q_to_prompt(q):\n",
    "        question, context = q.payload[\"question\"], q.payload[\"context\"]\n",
    "        answer = q.payload[\"answers\"][0] if len(q.payload[\"answers\"]) > 0 else \"I don't know\"\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"\"\"Question: {question}\\n\\nContext: {context}\\n\\nAnswer:\"\"\"\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "\n",
    "    rag_prompt = []\n",
    "    # If the next best question is not the same as the question, add it to the prompt\n",
    "    if len(q2) >= 1:\n",
    "        rag_prompt += q_to_prompt(q2[1])\n",
    "    if len(q1) >= 1:\n",
    "        rag_prompt += q_to_prompt(q1[1])\n",
    "        rag_prompt += q_to_prompt(q1[2])\n",
    "    if len(q2) >= 1:\n",
    "        rag_prompt += q_to_prompt(q2[2])\n",
    "    \n",
    "    \n",
    "\n",
    "    rag_prompt += [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Question: {query}\\n\\nContext: {row_context}\\n\\nAnswer:\"\"\"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    rag_prompt = [{\"role\": \"system\", \"content\": instruction}] + rag_prompt\n",
    "    return rag_prompt\n",
    "\n",
    "train_sample[\"few_shot_prompt_1K\"] = train_sample.progress_apply(get_few_shot_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample[\"few_shot_prompt_1K\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample[\"few_shot_prompt_1K\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-Tuning OpenAI Model with Qdrant\n",
    "\n",
    "### Upload the Fine-Tuning Data to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the OpenAI File format i.e. JSONL from train_sample\n",
    "def dataframe_to_jsonl(df):\n",
    "    def create_jsonl_entry(row):\n",
    "        messages = row[\"few_shot_prompt_1K\"]\n",
    "        return json.dumps({\"messages\": messages})\n",
    "\n",
    "    jsonl_output = df.progress_apply(create_jsonl_entry, axis=1)\n",
    "    return \"\\n\".join(jsonl_output)\n",
    "\n",
    "with open(\"local_cache/100_train_few_shot.jsonl\", \"w\") as f:\n",
    "    f.write(dataframe_to_jsonl(train_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIFineTuner:\n",
    "    def __init__(self, training_file_path, model_name, suffix):\n",
    "        self.training_file_path = training_file_path\n",
    "        self.model_name = model_name\n",
    "        self.suffix = suffix\n",
    "        self.file_object = None\n",
    "        self.fine_tuning_job = None\n",
    "        self.model_id = None\n",
    "\n",
    "    def create_openai_file(self):\n",
    "        self.file_object = openai.File.create(\n",
    "            file=open(self.training_file_path, \"r\"),\n",
    "            purpose=\"fine-tune\",\n",
    "        )\n",
    "\n",
    "    def wait_for_file_processing(self, sleep_time=20):\n",
    "        while self.file_object.status != 'processed':\n",
    "            time.sleep(sleep_time)\n",
    "            self.file_object.refresh()\n",
    "            print(\"File Status: \", self.file_object.status)\n",
    "\n",
    "    def create_fine_tuning_job(self):\n",
    "        self.fine_tuning_job = openai.FineTuningJob.create(\n",
    "            training_file=self.file_object[\"id\"],\n",
    "            model=self.model_name,\n",
    "            suffix=self.suffix,\n",
    "        )\n",
    "\n",
    "    def wait_for_fine_tuning(self, sleep_time=45):\n",
    "        while self.fine_tuning_job.status != 'succeeded':\n",
    "            time.sleep(sleep_time)\n",
    "            self.fine_tuning_job.refresh()\n",
    "            print(\"Job Status: \", self.fine_tuning_job.status)\n",
    "\n",
    "    def retrieve_fine_tuned_model(self):\n",
    "        self.model_id = openai.FineTuningJob.retrieve(self.fine_tuning_job[\"id\"]).fine_tuned_model\n",
    "        return self.model_id\n",
    "\n",
    "    def fine_tune_model(self):\n",
    "        self.create_openai_file()\n",
    "        self.wait_for_file_processing()\n",
    "        self.create_fine_tuning_job()\n",
    "        self.wait_for_fine_tuning()\n",
    "        return self.retrieve_fine_tuned_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuner = OpenAIFineTuner(\n",
    "        training_file_path=\"local_cache/100_train_few_shot.jsonl\",\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        suffix=\"trnfewshot20230907\"\n",
    "    )\n",
    "\n",
    "model_id = fine_tuner.fine_tune_model()\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ft_generated_answer_few_shot\"] = df.progress_apply(answer_question, model=model_id, prompt_func=get_few_shot_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"ft_generated_answer_few_shot\", \"question\", \"context\", \"is_impossible\"]].iloc[4].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate this using the Evaluator\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = ConfusionMatrixEvaluator(\n",
    "    df, answers_column=\"ft_generated_answer_few_shot\"\n",
    ")\n",
    "evaluator.evaluate_answers()\n",
    "error_categories = evaluator.generate_matrices(use_percentages=True)\n",
    "error_categories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
