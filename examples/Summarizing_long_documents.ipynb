{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing Long Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to demonstrate how to summarize large documents with a controllable level of detail.\n",
    " \n",
    "If you give a GPT model the task of summarizing a long document (e.g. 10k or more tokens), you'll tend to get back a relatively short summary that isn't proportional to the length of the document. For instance, a summary of a 20k token document will not be twice as long as a summary of a 10k token document. One way we can fix this is to split our document up into pieces, and produce a summary piecewise. After many queries to a GPT model, the full summary can be reconstructed. By controlling the number of text chunks and their sizes, we can ultimately control the level of detail in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:19:35.305706Z",
     "start_time": "2024-04-10T05:19:35.303535Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Optional\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:19:35.325026Z",
     "start_time": "2024-04-10T05:19:35.322414Z"
    }
   },
   "outputs": [],
   "source": [
    "# open dataset containing part of the text of the Wikipedia page for the United States\n",
    "with open(\"data/artificial_intelligence_wikipedia.txt\", \"r\") as file:\n",
    "    artificial_intelligence_wikipedia_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:19:35.364483Z",
     "start_time": "2024-04-10T05:19:35.348213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14630"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load encoding and check the length of dataset\n",
    "encoding = tiktoken.encoding_for_model('gpt-4-turbo')\n",
    "len(encoding.encode(artificial_intelligence_wikipedia_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a simple utility to wrap calls to the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:19:35.375619Z",
     "start_time": "2024-04-10T05:19:35.365818Z"
    }
   },
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_chat_completion(messages, model='gpt-4-turbo'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define some utilities to chunk a large document into smaller pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:19:35.382790Z",
     "start_time": "2024-04-10T05:19:35.376721Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    encoding = tiktoken.encoding_for_model('gpt-4-turbo')\n",
    "    return encoding.encode(text)\n",
    "\n",
    "\n",
    "# This function chunks a text into smaller pieces based on a maximum token count and a delimiter.\n",
    "def chunk_on_delimiter(input_string: str,\n",
    "                       max_tokens: int, delimiter: str) -> List[str]:\n",
    "    chunks = input_string.split(delimiter)\n",
    "    combined_chunks, _, dropped_chunk_count = combine_chunks_with_no_minimum(\n",
    "        chunks, max_tokens, chunk_delimiter=delimiter, add_ellipsis_for_overflow=True\n",
    "    )\n",
    "    if dropped_chunk_count > 0:\n",
    "        print(f\"warning: {dropped_chunk_count} chunks were dropped due to overflow\")\n",
    "    combined_chunks = [f\"{chunk}{delimiter}\" for chunk in combined_chunks]\n",
    "    return combined_chunks\n",
    "\n",
    "\n",
    "# This function combines text chunks into larger blocks without exceeding a specified token count. It returns the combined text blocks, their original indices, and the count of chunks dropped due to overflow.\n",
    "def combine_chunks_with_no_minimum(\n",
    "        chunks: List[str],\n",
    "        max_tokens: int,\n",
    "        chunk_delimiter=\"\\n\\n\",\n",
    "        header: Optional[str] = None,\n",
    "        add_ellipsis_for_overflow=False,\n",
    ") -> Tuple[List[str], List[int]]:\n",
    "    dropped_chunk_count = 0\n",
    "    output = []  # list to hold the final combined chunks\n",
    "    output_indices = []  # list to hold the indices of the final combined chunks\n",
    "    candidate = (\n",
    "        [] if header is None else [header]\n",
    "    )  # list to hold the current combined chunk candidate\n",
    "    candidate_indices = []\n",
    "    for chunk_i, chunk in enumerate(chunks):\n",
    "        chunk_with_header = [chunk] if header is None else [header, chunk]\n",
    "        if len(tokenize(chunk_delimiter.join(chunk_with_header))) > max_tokens:\n",
    "            print(f\"warning: chunk overflow\")\n",
    "            if (\n",
    "                    add_ellipsis_for_overflow\n",
    "                    and len(tokenize(chunk_delimiter.join(candidate + [\"...\"]))) <= max_tokens\n",
    "            ):\n",
    "                candidate.append(\"...\")\n",
    "                dropped_chunk_count += 1\n",
    "            continue  # this case would break downstream assumptions\n",
    "        # estimate token count with the current chunk added\n",
    "        extended_candidate_token_count = len(tokenize(chunk_delimiter.join(candidate + [chunk])))\n",
    "        # If the token count exceeds max_tokens, add the current candidate to output and start a new candidate\n",
    "        if extended_candidate_token_count > max_tokens:\n",
    "            output.append(chunk_delimiter.join(candidate))\n",
    "            output_indices.append(candidate_indices)\n",
    "            candidate = chunk_with_header  # re-initialize candidate\n",
    "            candidate_indices = [chunk_i]\n",
    "        # otherwise keep extending the candidate\n",
    "        else:\n",
    "            candidate.append(chunk)\n",
    "            candidate_indices.append(chunk_i)\n",
    "    # add the remaining candidate to output if it's not empty\n",
    "    if (header is not None and len(candidate) > 1) or (header is None and len(candidate) > 0):\n",
    "        output.append(chunk_delimiter.join(candidate))\n",
    "        output_indices.append(candidate_indices)\n",
    "    return output, output_indices, dropped_chunk_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a utility to summarize text with a controllable level of detail (note the `detail` parameter).\n",
    "\n",
    "The function first determines the number of chunks by interpolating between a minimum and a maximum chunk count based on a controllable `detail` parameter. It then splits the text into chunks and summarizes each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:19:35.390876Z",
     "start_time": "2024-04-10T05:19:35.385076Z"
    }
   },
   "outputs": [],
   "source": [
    "def summarize(text: str,\n",
    "              detail: float = 0,\n",
    "              model: str = 'gpt-4-turbo',\n",
    "              additional_instructions: Optional[str] = None,\n",
    "              minimum_chunk_size: Optional[int] = 500,\n",
    "              chunk_delimiter: str = \".\",\n",
    "              summarize_recursively=False,\n",
    "              verbose=False):\n",
    "    \"\"\"\n",
    "    Summarizes a given text by splitting it into chunks, each of which is summarized individually. \n",
    "    The level of detail in the summary can be adjusted, and the process can optionally be made recursive.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to be summarized.\n",
    "    - detail (float, optional): A value between 0 and 1 indicating the desired level of detail in the summary.\n",
    "      0 leads to a higher level summary, and 1 results in a more detailed summary. Defaults to 0.\n",
    "    - model (str, optional): The model to use for generating summaries. Defaults to 'gpt-3.5-turbo'.\n",
    "    - additional_instructions (Optional[str], optional): Additional instructions to provide to the model for customizing summaries.\n",
    "    - minimum_chunk_size (Optional[int], optional): The minimum size for text chunks. Defaults to 500.\n",
    "    - chunk_delimiter (str, optional): The delimiter used to split the text into chunks. Defaults to \".\".\n",
    "    - summarize_recursively (bool, optional): If True, summaries are generated recursively, using previous summaries for context.\n",
    "    - verbose (bool, optional): If True, prints detailed information about the chunking process.\n",
    "\n",
    "    Returns:\n",
    "    - str: The final compiled summary of the text.\n",
    "\n",
    "    The function first determines the number of chunks by interpolating between a minimum and a maximum chunk count based on the `detail` parameter. \n",
    "    It then splits the text into chunks and summarizes each chunk. If `summarize_recursively` is True, each summary is based on the previous summaries, \n",
    "    adding more context to the summarization process. The function returns a compiled summary of all chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    # check detail is set correctly\n",
    "    assert 0 <= detail <= 1\n",
    "\n",
    "    # interpolate the number of chunks based to get specified level of detail\n",
    "    max_chunks = len(chunk_on_delimiter(text, minimum_chunk_size, chunk_delimiter))\n",
    "    min_chunks = 1\n",
    "    num_chunks = int(min_chunks + detail * (max_chunks - min_chunks))\n",
    "\n",
    "    # adjust chunk_size based on interpolated number of chunks\n",
    "    document_length = len(tokenize(text))\n",
    "    chunk_size = max(minimum_chunk_size, document_length // num_chunks)\n",
    "    text_chunks = chunk_on_delimiter(text, chunk_size, chunk_delimiter)\n",
    "    if verbose:\n",
    "        print(f\"Splitting the text into {len(text_chunks)} chunks to be summarized.\")\n",
    "        print(f\"Chunk lengths are {[len(tokenize(x)) for x in text_chunks]}\")\n",
    "\n",
    "    # set system message\n",
    "    system_message_content = \"Rewrite this text in summarized form.\"\n",
    "    if additional_instructions is not None:\n",
    "        system_message_content += f\"\\n\\n{additional_instructions}\"\n",
    "\n",
    "    accumulated_summaries = []\n",
    "    for chunk in tqdm(text_chunks):\n",
    "        if summarize_recursively and accumulated_summaries:\n",
    "            # Creating a structured prompt for recursive summarization\n",
    "            accumulated_summaries_string = '\\n\\n'.join(accumulated_summaries)\n",
    "            user_message_content = f\"Previous summaries:\\n\\n{accumulated_summaries_string}\\n\\nText to summarize next:\\n\\n{chunk}\"\n",
    "        else:\n",
    "            # Directly passing the chunk for summarization without recursive context\n",
    "            user_message_content = chunk\n",
    "\n",
    "        # Constructing messages based on whether recursive summarization is applied\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message_content},\n",
    "            {\"role\": \"user\", \"content\": user_message_content}\n",
    "        ]\n",
    "\n",
    "        # Assuming this function gets the completion and works as expected\n",
    "        response = get_chat_completion(messages, model=model)\n",
    "        accumulated_summaries.append(response)\n",
    "\n",
    "    # Compile final summary from partial summaries\n",
    "    final_summary = '\\n\\n'.join(accumulated_summaries)\n",
    "\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this utility to produce summaries with varying levels of detail. By increasing `detail` from 0 to 1 we get progressively longer summaries of the underlying document. A higher value for the `detail` parameter results in a more detailed summary because the utility first splits the document into a greater number of chunks. Each chunk is then summarized, and the final summary is a concatenation of all the chunk summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:19:47.541096Z",
     "start_time": "2024-04-10T05:19:35.391911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the text into 1 chunks to be summarized.\n",
      "Chunk lengths are [14631]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.68s/it]\n"
     ]
    }
   ],
   "source": [
    "summary_with_detail_0 = summarize(artificial_intelligence_wikipedia_text, detail=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:19:58.724212Z",
     "start_time": "2024-04-10T05:19:47.542129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the text into 9 chunks to be summarized.\n",
      "Chunk lengths are [1817, 1807, 1823, 1810, 1806, 1827, 1814, 1829, 103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:33<00:00, 10.39s/it]\n"
     ]
    }
   ],
   "source": [
    "summary_with_detail_pt25 = summarize(artificial_intelligence_wikipedia_text, detail=0.25, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:20:16.216023Z",
     "start_time": "2024-04-10T05:19:58.725014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the text into 17 chunks to be summarized.\n",
      "Chunk lengths are [897, 890, 914, 876, 893, 906, 893, 902, 909, 907, 905, 889, 902, 890, 901, 880, 287]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [02:26<00:00,  8.64s/it]\n"
     ]
    }
   ],
   "source": [
    "summary_with_detail_pt5 = summarize(artificial_intelligence_wikipedia_text, detail=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:22:57.760218Z",
     "start_time": "2024-04-10T05:21:44.921275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the text into 31 chunks to be summarized.\n",
      "Chunk lengths are [492, 427, 485, 490, 496, 478, 473, 497, 496, 501, 499, 497, 493, 470, 472, 494, 489, 492, 481, 485, 471, 500, 486, 498, 478, 469, 498, 468, 493, 478, 103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [04:08<00:00,  8.02s/it]\n"
     ]
    }
   ],
   "source": [
    "summary_with_detail_1 = summarize(artificial_intelligence_wikipedia_text, detail=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original document is nearly 15k tokens long. Notice how large the gap is between the length of `summary_with_detail_0` and `summary_with_detail_1`. It's nearly 25 times longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:22:57.782389Z",
     "start_time": "2024-04-10T05:22:57.763041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[235, 2529, 4336, 6742]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lengths of summaries\n",
    "[len(tokenize(x)) for x in\n",
    " [summary_with_detail_0, summary_with_detail_pt25, summary_with_detail_pt5, summary_with_detail_1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the summaries to see how the level of detail changes when the `detail` parameter is increased from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:22:57.785881Z",
     "start_time": "2024-04-10T05:22:57.783455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) is the simulation of human intelligence in machines, designed to perform tasks that typically require human intelligence. This includes applications like advanced search engines, recommendation systems, speech interaction, autonomous vehicles, and more. AI was first significantly researched by Alan Turing and became an academic discipline in 1956. The field has experienced cycles of high expectations followed by disillusionment and reduced funding, known as \"AI winters.\" Interest in AI surged post-2012 with advancements in deep learning and again post-2017 with the development of the transformer architecture, leading to a boom in AI research and applications in the early 2020s.\n",
      "\n",
      "AI's increasing integration into various sectors is influencing societal and economic shifts towards automation and data-driven decision-making, impacting areas such as employment, healthcare, and privacy. Ethical and safety concerns about AI have prompted discussions on regulatory policies.\n",
      "\n",
      "AI research involves various sub-fields focused on specific goals like reasoning, learning, and perception, using techniques from mathematics, logic, and other disciplines. Despite its broad applications, AI's complexity and potential risks, such as privacy issues, misinformation, and ethical challenges, remain areas of active investigation and debate.\n"
     ]
    }
   ],
   "source": [
    "print(summary_with_detail_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:22:57.788969Z",
     "start_time": "2024-04-10T05:22:57.786691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) is the demonstration of intelligence by machines, particularly in computer systems, and involves methods that allow these machines to perceive their environment and make decisions to achieve specific goals. AI is utilized in various sectors including industry, government, and science, with applications ranging from web search engines and recommendation systems to autonomous vehicles and AI in gaming. The field, which began as an academic discipline in 1956, has experienced cycles of high expectations followed by periods of reduced interest and funding, known as AI winters. Interest in AI surged post-2012 with advancements in deep learning and the transformer architecture, leading to a significant boom in AI development in the early 2020s.\n",
      "\n",
      "AI's increasing integration into daily life and various sectors is driving shifts towards automation and data-driven decision-making, raising important questions about its long-term impacts, ethical considerations, and the need for regulatory oversight. AI research encompasses several sub-fields focused on specific goals like reasoning, learning, and natural language processing, using a variety of techniques from formal logic to artificial neural networks. Challenges in AI include developing efficient reasoning algorithms, representing knowledge accurately, and making decisions under uncertainty. AI also heavily interacts with fields such as psychology, linguistics, and neuroscience to achieve its goals.\n",
      "\n",
      "The text discusses various aspects of artificial intelligence (AI), including its applications and techniques. Early AI research, influenced by Noam Chomsky's generative grammar, faced challenges in word-sense disambiguation and relied on limited domains known as \"micro-worlds.\" Margaret Masterman emphasized the importance of meaning over grammar in language understanding, advocating for the use of thesauri over dictionaries. Modern NLP techniques now include word embedding, transformers, and generative pre-trained transformer (GPT) models, which by 2023 could achieve human-level scores on various tests.\n",
      "\n",
      "Machine perception involves using sensor input to deduce aspects of the world, encompassing abilities like speech and object recognition, and computer vision. Social intelligence in AI focuses on recognizing and simulating human emotions, with applications like virtual assistants designed to mimic conversational and emotional interactions.\n",
      "\n",
      "General AI aims to solve a broad range of problems with human-like versatility, using techniques such as search and optimization, including state space and local search methods like gradient descent and evolutionary computation. Logic plays a crucial role in AI for reasoning and knowledge representation, utilizing propositional and predicate logic, and dealing with challenges in inference.\n",
      "\n",
      "Probabilistic methods address reasoning under uncertainty, employing tools like Bayesian networks and decision theory to support decision-making and planning. AI also uses classifiers and statistical learning methods for applications like pattern recognition, where algorithms like decision trees and support vector machines categorize data based on learned patterns.\n",
      "\n",
      "The naive Bayes classifier is highly utilized at Google for its scalability, alongside neural networks which serve as classifiers. Artificial neural networks, mimicking the human brain's neurons, are trained to recognize patterns and process data through multiple layers, including input, hidden, and output layers. They use algorithms like backpropagation for training and can model complex relationships in data. Feedforward neural networks pass signals in one direction, while recurrent neural networks can process sequences due to their feedback loops, with long short-term memory networks being particularly effective.\n",
      "\n",
      "Deep learning, a subset of neural networks, involves multiple layers that help in extracting progressively higher-level features from input data, significantly enhancing tasks like image and speech recognition, and natural language processing. The success of deep learning since the early 2010s is attributed to increased computational power and large datasets rather than new theoretical advancements.\n",
      "\n",
      "Generative Pre-trained Transformers (GPT) are advanced language models pre-trained on vast text corpora to generate human-like text, useful in applications like chatbots. They are trained further to improve accuracy and reduce errors.\n",
      "\n",
      "In terms of hardware, GPUs have become central to training AI models, replacing CPUs due to their efficiency in handling large datasets. AI applications are pervasive across various domains including search engines, online advertising, virtual assistants, autonomous vehicles, and healthcare, where they improve diagnostics and patient care.\n",
      "\n",
      "In gaming, AI has progressed to beating human champions in complex games like Chess, Jeopardy, Go, and even real-time strategy games like StarCraft II. Military applications of AI are also expanding, enhancing capabilities in surveillance, logistics, and combat operations.\n",
      "\n",
      "Overall, AI's integration into various sectors is profound, driven by advancements in machine learning, deep learning, and hardware capabilities, shaping a future where AI's influence continues to grow across all aspects of society.\n",
      "\n",
      "In March 2023, a survey revealed that 58% of US adults were aware of ChatGPT, with 14% having used it. The realism of AI-based text-to-image generators like Midjourney, DALL-E, and Stable Diffusion led to viral trends, including a fake photo of Pope Francis in a puffer coat and other hoaxes. AI has been successfully applied across various industries, including agriculture where it aids in irrigation, pest control, and crop monitoring, and in astronomy for data analysis and discovery tasks.\n",
      "\n",
      "Ethical concerns about AI include potential biases, privacy issues, and the misuse of data. AI systems have been criticized for privacy violations, such as Amazon's use of private conversations to improve speech recognition technology. Generative AI has also raised copyright issues, with debates over the legality of using unlicensed copyrighted works for training AI.\n",
      "\n",
      "Misinformation spread by AI algorithms on platforms like YouTube and Facebook has been a significant issue, leading to increased user engagement with false information. Algorithmic bias is another concern, with systems like COMPAS showing racial bias in judicial decision-making. These biases arise from training on historical data that may reflect past prejudices.\n",
      "\n",
      "Overall, while AI offers substantial benefits and advancements in various fields, it also presents significant ethical, privacy, and societal challenges that need to be addressed.\n",
      "\n",
      "Machine learning, while powerful, is limited in its ability to make decisions for a better future as it is inherently descriptive rather than prescriptive. It often fails to detect bias and unfairness due to a lack of diversity among AI developers. The Association for Computing Machinery highlighted at its 2022 conference that AI systems should not be used until they are proven to be free from bias. AI systems are complex and often lack transparency, making it difficult to understand how decisions are made. This complexity can lead to unintended consequences, such as misclassifications in medical diagnostics. The right to an explanation for decisions made by algorithms is emphasized, yet achieving transparency remains a challenge. AI also poses risks in terms of security and employment, with potential misuse by bad actors and significant impacts on job markets. Moreover, AI's rapid development could lead to existential risks if superintelligent systems act against human interests. Overall, while AI offers significant benefits, these come with substantial challenges and risks that need careful management.\n",
      "\n",
      "Yuval Noah Harari highlights that AI can pose existential risks through its influence over non-physical aspects of civilization like ideologies and economies, using language to potentially spread misinformation and manipulate beliefs. While some experts, including prominent figures like Stephen Hawking and Elon Musk, express concerns about the existential risks of AI, others like Juergen Schmidhuber and Andrew Ng offer a more optimistic view, emphasizing AI's benefits and dismissing doomsday scenarios. The field of machine ethics aims to ensure AI systems make ethical decisions, with various frameworks and principles developed to guide the ethical implementation of AI technologies.\n",
      "\n",
      "Regulation of AI is becoming a significant focus globally, with numerous countries developing strategies and laws to manage AI's integration and ensure it aligns with human rights and democratic values. Public opinion on AI varies widely, with differing levels of support for regulatory measures. The first global AI Safety Summit in 2023 called for international cooperation to address AI risks, reflecting growing global engagement with AI governance issues.\n",
      "\n",
      "The history of AI traces back to ancient philosophical and mathematical studies of logic, leading to significant developments in the mid-20th century, including Turing's theory of computation and the establishment of AI research as a formal discipline at a 1956 Dartmouth workshop. This foundational period set the stage for ongoing advancements and debates in the field of artificial intelligence.\n",
      "\n",
      "Herbert Simon and Marvin Minsky initially predicted that machines would soon match human capabilities in all tasks, but they underestimated the challenge. AI research faced setbacks in the 1970s due to government funding cuts influenced by criticism and perceived lack of progress, leading to the first \"AI winter.\" Interest in AI was rekindled in the 1980s with the success of expert systems and further inspired by Japan's fifth generation computer project, leading to renewed government funding. However, the collapse of the Lisp Machine market in 1987 led to another AI winter.\n",
      "\n",
      "During the 1980s, skepticism grew about the symbolic AI approach, which used high-level symbols to represent knowledge. Researchers began exploring \"sub-symbolic\" methods and connectionism, including neural networks, which gained significant attention after successful applications like Yann LeCun's convolutional neural networks for digit recognition.\n",
      "\n",
      "AI's reputation improved in the late 1990s and early 2000s with the adoption of more mathematical methods and narrow applications, leading to practical solutions in various fields. The field of artificial general intelligence (AGI) emerged in the early 2000s, aiming to create versatile, fully intelligent machines. Deep learning became dominant by 2012, driven by hardware improvements and large data availability, leading to significant advancements and increased funding and interest in AI.\n",
      "\n",
      "By the mid-2010s, AI began addressing ethical issues and fairness in technology use. High-profile successes like AlphaGo and GPT-3 in the late 2010s and early 2020s spurred a new AI boom, with substantial investments from large companies.\n",
      "\n",
      "Philosophically, AI has been defined in various ways, focusing on external behavior rather than internal thought processes, as suggested by Alan Turing with the Turing test. AI research has largely been guided by practical problem-solving and achieving defined goals, with ongoing debates about the best approaches and definitions of intelligence.\n",
      "\n",
      "The debate on AI's development includes concerns about sub-symbolic AI, which, like human intuition, can be prone to errors such as algorithmic bias. Critics like Noam Chomsky suggest that research into symbolic AI is essential for achieving general intelligence due to its transparency compared to sub-symbolic AI. The field of neuro-symbolic AI aims to integrate both approaches.\n",
      "\n",
      "Historically, the AI community was divided into \"Neats,\" who believe intelligent behavior can be described with simple principles, and \"Scruffies,\" who believe it involves solving many complex problems. This debate, prominent in the 1970s and 1980s, has seen a blend of both perspectives in modern AI.\n",
      "\n",
      "Soft computing, which emerged in the late 1980s, focuses on techniques like genetic algorithms and neural networks to handle problems where precise solutions are intractable. This approach has dominated successful AI applications in the 21st century.\n",
      "\n",
      "The AI field is also split between developing narrow AI, which addresses specific problems, and pursuing broader goals like artificial general intelligence (AGI) and superintelligence. AGI remains a challenging and elusive goal, with current AI research achieving more success in narrow applications.\n",
      "\n",
      "Philosophical discussions in AI explore whether machines can possess consciousness or mental states similar to humans. Mainstream AI research generally views these questions as irrelevant to its practical goals. However, these issues are central to the philosophy of mind and are often explored in AI fiction.\n",
      "\n",
      "The concept of machine rights and welfare is gaining attention, with debates on whether advanced AI systems should have rights similar to humans or animals, especially if they can experience suffering.\n",
      "\n",
      "Looking to the future, the possibility of an \"intelligence explosion\" leading to superintelligence poses both opportunities and risks. The concept of transhumanism suggests a future where humans and machines merge, enhancing capabilities beyond natural human limits.\n",
      "\n",
      "In popular culture, AI has been a theme since antiquity, with stories often depicting AI as either a threat or a beneficial force. Isaac Asimov's Three Laws of Robotics is a notable example of integrating machine ethics into narrative, although these laws are often criticized for practical limitations in real-world applications.\n",
      "\n",
      "Numerous works, including Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, and Philip K. Dick's novel Do Androids Dream of Electric Sheep?, utilize AI to explore the essence of humanity. These works present artificial beings capable of feeling and suffering, prompting a reevaluation of human subjectivity in the context of advanced technology.\n"
     ]
    }
   ],
   "source": [
    "print(summary_with_detail_pt25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this utility also allows passing additional instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:33:18.789246Z",
     "start_time": "2024-04-10T05:22:57.789764Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:38<00:00,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- AI is intelligence demonstrated by machines, especially computer systems.\n",
      "- AI technology applications include search engines, recommendation systems, speech interaction, autonomous vehicles, creative tools, and strategy games.\n",
      "- Alan Turing initiated substantial AI research, termed \"machine intelligence.\"\n",
      "- AI became an academic discipline in 1956, experiencing cycles of optimism and \"AI winters.\"\n",
      "- Post-2012, deep learning and post-2017 transformer architectures revitalized AI, leading to a boom in the early 2020s.\n",
      "- AI influences societal and economic shifts towards automation and data-driven decision-making across various sectors.\n",
      "- AI research goals: reasoning, knowledge representation, planning, learning, natural language processing, perception, and robotics support.\n",
      "- AI techniques include search, optimization, logic, neural networks, and statistical methods.\n",
      "- AI sub-problems focus on traits like reasoning, problem-solving, knowledge representation, planning, decision-making, learning, and perception.\n",
      "- Early AI research mimicked human step-by-step reasoning; modern AI handles uncertain information using probability and economics.\n",
      "- Knowledge representation in AI involves ontologies and knowledge bases to support intelligent querying and reasoning.\n",
      "- Planning in AI involves goal-directed behavior and decision-making based on utility maximization.\n",
      "- Learning in AI includes machine learning, supervised and unsupervised learning, reinforcement learning, and deep learning.\n",
      "- Natural language processing (NLP) in AI has evolved from rule-based systems to modern deep learning techniques.\n",
      "- AI perception involves interpreting sensor data for tasks like speech recognition and computer vision.\n",
      "- General AI aims to solve diverse problems with human-like versatility.\n",
      "- AI search techniques include state space search, local search, and adversarial search for game-playing.\n",
      "- Logic in AI uses formal systems like propositional and predicate logic for reasoning and knowledge representation.\n",
      "- Probabilistic methods in AI address decision-making and planning under uncertainty using tools like Bayesian networks and Markov decision processes.\n",
      "- Classifiers in AI categorize data into predefined classes based on pattern matching and supervised learning.\n",
      "\n",
      "- Neural networks: Interconnected nodes, similar to brain neurons, with input, hidden layers, and output.\n",
      "- Deep neural networks: At least 2 hidden layers.\n",
      "- Training techniques: Commonly use backpropagation.\n",
      "- Feedforward networks: Signal passes in one direction.\n",
      "- Recurrent networks: Output fed back into input for short-term memory.\n",
      "- Perceptrons: Single layer of neurons.\n",
      "- Convolutional networks: Strengthen connections between close neurons, important in image processing.\n",
      "- Deep learning: Multiple layers extract features progressively, used in various AI subfields.\n",
      "- GPT (Generative Pre-trained Transformers): Large language models pre-trained on text, used in chatbots.\n",
      "- Specialized AI hardware: GPUs replaced CPUs for training large-scale machine learning models.\n",
      "- AI applications: Used in search engines, online ads, virtual assistants, autonomous vehicles, language translation, facial recognition.\n",
      "- AI in healthcare: Increases patient care, used in medical research and drug discovery.\n",
      "- AI in games: Used in chess, Jeopardy!, Go, and real-time strategy games.\n",
      "- Military AI: Enhances command, control, and operations, used in coordination and threat detection.\n",
      "- Generative AI: Creates realistic images and texts, used in creative arts.\n",
      "- AI ethics and risks: Concerns over privacy, surveillance, copyright, misinformation, and algorithmic bias.\n",
      "- Algorithmic bias: Can cause discrimination if trained on biased data, fairness in machine learning is a critical area of study.\n",
      "\n",
      "- AI engineers demographics: 4% black, 20% women.\n",
      "- ACM FAccT 2022: Recommends limiting use of self-learning neural networks due to bias.\n",
      "- AI complexity: Designers often can't explain decision-making processes.\n",
      "- Misleading AI outcomes: Skin disease identifier misclassifies images with rulers as \"cancerous\"; AI misclassifies asthma patients as low risk for pneumonia.\n",
      "- Right to explanation: Essential for accountability, especially in medical and legal fields.\n",
      "- DARPA's XAI program (2014): Aims to make AI decisions understandable.\n",
      "- Transparency solutions: SHAP, LIME, multitask learning, deconvolution, DeepDream.\n",
      "- AI misuse: Authoritarian surveillance, misinformation, autonomous weapons.\n",
      "- AI in warfare: 30 nations support UN ban on autonomous weapons; over 50 countries researching battlefield robots.\n",
      "- Technological unemployment: AI could increase long-term unemployment; conflicting expert opinions on job risk from automation.\n",
      "- Existential risks of AI: Potential to lose control over superintelligent AI; concerns from Stephen Hawking, Bill Gates, Elon Musk.\n",
      "- Ethical AI development: Importance of aligning AI with human values and ethics.\n",
      "- AI regulation: Increasing global legislative activity; first global AI Safety Summit in 2023.\n",
      "- Historical perspective: AI research dates back to antiquity, significant developments in mid-20th century.\n",
      "\n",
      "- 1974: U.S. and British governments ceased AI exploratory research due to criticism and funding pressures.\n",
      "- 1985: AI market value exceeded $1 billion.\n",
      "- 1987: Collapse of Lisp Machine market led to a second, prolonged AI winter.\n",
      "- 1990: Yann LeCun demonstrated successful use of convolutional neural networks for recognizing handwritten digits.\n",
      "- Early 2000s: AI reputation restored through specific problem-solving and formal methods.\n",
      "- 2012: Deep learning began dominating AI benchmarks.\n",
      "- 2015-2019: Machine learning research publications increased by 50%.\n",
      "- 2016: Fairness and misuse of technology became central issues in AI.\n",
      "- 2022: Approximately $50 billion annually invested in AI in the U.S.; 800,000 AI-related job openings in the U.S.\n",
      "- Turing test proposed by Alan Turing in 1950 to measure machine's ability to simulate human conversation.\n",
      "- AI defined as the study of agents that perceive their environment and take actions to achieve goals.\n",
      "- 2010s: Statistical machine learning overshadowed other AI approaches.\n",
      "- Symbolic AI excelled in high-level reasoning but failed in tasks like object recognition and commonsense reasoning.\n",
      "- Late 1980s: Introduction of soft computing techniques.\n",
      "- Debate between pursuing narrow AI (specific problem-solving) versus artificial general intelligence (AGI).\n",
      "- 2017: EU considered granting \"electronic personhood\" to advanced AI systems.\n",
      "- Predictions of merging humans and machines into cyborgs, a concept known as transhumanism.\n",
      "\n",
      "- Focus on how AI and technology, as depicted in \"Ex Machina\" and Philip K. Dick's \"Do Androids Dream of Electric Sheep?\", alter human subjectivity.\n",
      "- No specific numerical data provided.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "summary_with_additional_instructions = summarize(artificial_intelligence_wikipedia_text, detail=0.1,\n",
    "                                                 additional_instructions=\"Write in point form and focus on numerical data.\")\n",
    "print(summary_with_additional_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that the utility allows for recursive summarization, where each summary is based on the previous summaries, adding more context to the summarization process. This can be enabled by setting the `summarize_recursively` parameter to True. This is more computationally expensive, but can increase consistency and coherence of the combined summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T05:33:30.123036Z",
     "start_time": "2024-04-10T05:33:18.791253Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:41<00:00,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) is the simulation of human intelligence in machines, designed to perform tasks that typically require human intelligence. This includes applications like advanced search engines, recommendation systems, speech interaction, autonomous vehicles, and strategic game analysis. AI was established as a distinct academic discipline in 1956 and has experienced cycles of high expectations followed by disillusionment and decreased funding, known as \"AI winters.\" Interest in AI surged post-2012 with advancements in deep learning and again post-2017 with the development of transformer architectures, leading to significant progress in the early 2020s.\n",
      "\n",
      "AI's increasing integration into various sectors is influencing societal and economic shifts towards automation and data-driven decision-making, affecting areas such as employment, healthcare, and education. This raises important ethical and safety concerns, prompting discussions on regulatory policies.\n",
      "\n",
      "AI research encompasses various sub-fields focused on specific goals like reasoning, learning, natural language processing, perception, and robotics, using techniques from search and optimization, logic, and probabilistic methods. The field also draws from psychology, linguistics, philosophy, and neuroscience. AI aims to achieve general intelligence, enabling machines to perform any intellectual task that a human can do.\n",
      "\n",
      "Artificial intelligence (AI) simulates human intelligence in machines to perform tasks that typically require human intellect, such as advanced search engines, recommendation systems, and autonomous vehicles. AI research, which began as a distinct academic discipline in 1956, includes sub-fields like natural language processing and robotics, employing techniques from various scientific domains. AI has significantly advanced due to deep learning and the development of transformer architectures, notably improving applications in computer vision, speech recognition, and other areas.\n",
      "\n",
      "Neural networks, central to AI, mimic the human brain's neuron network to recognize patterns and learn from data, using multiple layers in deep learning to extract complex features. These networks have evolved into sophisticated models like GPT (Generative Pre-trained Transformers) for natural language processing, enhancing applications like chatbots.\n",
      "\n",
      "AI's integration into sectors like healthcare, military, and agriculture has led to innovations like precision medicine and smart farming but also raised ethical concerns regarding privacy, bias, and the potential for misuse. Issues like data privacy, algorithmic bias, and the generation of misinformation are critical challenges as AI becomes pervasive in society. AI's potential and risks necessitate careful management and regulation to harness benefits while mitigating adverse impacts.\n",
      "\n",
      "AI, or artificial intelligence, simulates human intelligence in machines to perform complex tasks, such as operating autonomous vehicles and analyzing strategic games. Since its establishment as an academic discipline in 1956, AI has seen periods of high expectations and subsequent disillusionment, known as \"AI winters.\" Recent advancements in deep learning and transformer architectures have significantly advanced AI capabilities in areas like computer vision and speech recognition.\n",
      "\n",
      "AI's integration into various sectors, including healthcare and agriculture, has led to innovations like precision medicine and smart farming but has also raised ethical concerns about privacy, bias, and misuse. The complexity of AI systems, particularly deep neural networks, often makes it difficult for developers to explain their decision-making processes, leading to transparency issues. This lack of transparency can result in unintended consequences, such as misclassifications in medical diagnostics.\n",
      "\n",
      "The potential for AI to be weaponized by bad actors, such as authoritarian governments or terrorists, poses significant risks. AI's reliance on large tech companies for computational power and the potential for technological unemployment are also critical issues. Despite these challenges, AI also offers opportunities for enhancing human well-being if ethical considerations are integrated throughout the design and implementation stages.\n",
      "\n",
      "Regulation of AI is emerging globally, with various countries adopting AI strategies to ensure the technology aligns with human rights and democratic values. The first global AI Safety Summit in 2023 emphasized the need for international cooperation to manage AI's risks and challenges effectively.\n",
      "\n",
      "In the 1970s, AI research faced significant setbacks due to criticism from influential figures like Sir James Lighthill and funding cuts from the U.S. and British governments, leading to the first \"AI winter.\" The field saw a resurgence in the 1980s with the success of expert systems and renewed government funding, but suffered another setback with the collapse of the Lisp Machine market in 1987, initiating a second AI winter. During this period, researchers began exploring \"sub-symbolic\" approaches, including neural networks, which gained prominence in the 1990s with successful applications like Yann LeCun’s convolutional neural networks for digit recognition.\n",
      "\n",
      "By the early 21st century, AI was revitalized by focusing on narrow, specific problems, leading to practical applications and integration into various sectors. The field of artificial general intelligence (AGI) emerged, aiming to create versatile, fully intelligent machines. The 2010s saw deep learning dominate AI research, driven by hardware improvements and large datasets, which significantly increased interest and investment in AI.\n",
      "\n",
      "Philosophically, AI has been defined in various ways, focusing on external behavior rather than internal experience, aligning with Alan Turing's proposal of the Turing test. The field has debated the merits of symbolic vs. sub-symbolic AI, with ongoing discussions about machine consciousness and the ethical implications of potentially sentient AI. The concept of AI rights and welfare has also emerged, reflecting concerns about the moral status of advanced AI systems.\n",
      "\n",
      "Overall, AI research has oscillated between periods of intense optimism and profound setbacks, with current trends heavily favoring practical applications through narrow AI, while continuing to explore the broader implications and potential of general and superintelligent AI systems.\n",
      "\n",
      "Artificial Intelligence (AI) and its portrayal in media, such as the film \"Ex Machina\" and Philip K. Dick's novel \"Do Androids Dream of Electric Sheep?\", explore how technology, particularly AI, can alter our understanding of human subjectivity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recursive_summary = summarize(artificial_intelligence_wikipedia_text, detail=0.1, summarize_recursively=True)\n",
    "print(recursive_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
