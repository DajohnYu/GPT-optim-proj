{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Voice Translation of Audio Files into Different Languages \n",
    "\n",
    "Have you ever wanted to translate a podcast into your native language? Translating and dubbing audio content can make it more accessible to a global audience. This guide will walk you through the process of converting an English audio file into Hindi using OpenAI's APIs.\n",
    "\n",
    "The steps to dub audio content are: transcribing the audio, translating it into the target script, converting the text into speech in the target language, and benchmarking to ensure quality and accuracy.   \n",
    "\n",
    "The process flow can be illustrated as follows:\n",
    "\n",
    "![Voice Translations Steps](./images/voice-translation-steps.png)\n",
    "\n",
    "A note on semantics used in this Cookbook regarding **Language** and written **Script**. These words are generally used interchangeably, though it's important to understand the distinction, given the task at hand. \n",
    "- **Language** refers to the spoken or written system of communication. For instance, Hindi and Marathi are different languages, but both use the Devanagari script. Similarly, English and French are different languages, but are written in Latin script. \n",
    "- **Script** refers to the set of characters or symbols used to write the language. For example, Serbian language traditionally written in Cyrillic Script, is also written in Latin script.\n",
    "\n",
    "\n",
    "In this cookbook, we will walk through the following 5 steps to dub an audio podcast from English to Hindi. \n",
    "\n",
    "1. **Transcribe** the audio file into text with Whisper \n",
    "2. **Translate** the English language text to Hindi in Devanagari script    \n",
    "3. **Text-to-speech** conversion of the Devanagari script into spoken Hindi language \n",
    "4. **Translation benchmarking** (BLEU or ROUGE) \n",
    "5. **Interpret and improve** scores by adjusting prompting parameters in steps 1-3 as needed  \n",
    "\n",
    "\n",
    "Before we get started, make sure you have the `openai` library installed, and your OpenAI API key is configured as an environment variable. "
   ],
   "id": "4265ef1326248608"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 1: Transcribe the audio file into text with Whisper\n",
    "\n",
    "[Whisper](https://github.com/openai/whisper) is an automatic speech recognition (ASR) system developed by OpenAI, accessible through both an API and an open-source model. It can transcribe audio files with high accuracy across multiple languages. OpenAI Whisper API provides two speech to text endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2. The code below invokes the API, passes the audio file as parameter, and receives the transcription in English in return. \n",
    "\n",
    "![Text-to-speech](./images/Whisper.png)\n",
    "\n",
    "[*Learn more about Whisper here](https://platform.openai.com/docs/guides/speech-to-text) \n"
   ],
   "id": "d84f1356379946f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T00:33:29.462138Z",
     "start_time": "2024-09-27T00:33:19.434877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# We'll use pre-recorded English language audio of 2023 OpenAI dev day keynote \n",
    "# You could change this path to the audio file you want to translate \n",
    "audio_file = \"../gpt4o/data/keynote_recap.mp3\"\n",
    "\n",
    "audio_file= open(audio_file, \"rb\")\n",
    "\n",
    "# Get the transcription from Whisper model \n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\",  \n",
    "  file=audio_file\n",
    ")\n",
    "\n",
    "# Retrieve the transcribed text and print the output\n",
    "transcription_english_first_pass = transcription.text\n",
    "\n",
    "print(transcription_english_first_pass)"
   ],
   "id": "7d10f34bb10ab45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to our first-ever OpenAI Dev Day. Today, we are launching a new model, GPT-4 Turbo. GPT-4 Turbo supports up to 128,000 tokens of context. We have a new feature called JSON mode, which ensures that the model will respond with valid JSON. You can now call many functions at once, and it'll do better at following instructions in general. You want these models to be able to access better knowledge about the world. So do we. So we're launching retrieval in the platform. You can bring knowledge from outside documents or databases into whatever you're building. GPT-4 Turbo has knowledge about the world up to April of 2023, and we will continue to improve that over time. Dolly 3, GPT-4 Turbo with Vision, and the new Text-to-Speech model are all going into the API today. Today, we're launching a new program called Custom Models. With Custom Models, our researchers will work closely with the company to help them make a great custom model, especially for them and their use case using our tools, higher rate limits. We're doubling the tokens per minute for all of our established GPT-4 customers so that it's easier to do more, and you'll be able to request changes to further rate limits and quotas directly in your API account settings. And GPT-4 Turbo is considerably cheaper than GPT-4 by a factor of 3x for prompt tokens and 2x for completion tokens, starting today. We're thrilled to introduce GPTs. GPTs are tailored versions of chat GPT for a specific purpose. And because they combine instructions, expanded knowledge, and actions, they can be more helpful to you. They can work better in many contexts, and they can give you better control. We know that many people who want to build a GPT don't know how to code. We've made it so that you can program the GPT just by having a conversation. You can make private GPTs. You can share your creations publicly with a link for anyone to use. Or if you're on chat GPT Enterprise, you can make GPTs just for your company. And later this month, we're going to launch the GPT Store. So those are the GPTs, and we can't wait to see what you'll build. We're bringing the same concept to the API. The Assistance API includes persistent threads, so they don't have to figure out how to deal with long conversation history, built-in retrieval, code interpreter, a working Python interpreter in a sandbox environment, and of course, the improved function calling. As intelligence gets integrated everywhere, we will all have superpowers on demand. We're excited to see what you all will do with this technology, and to discover the new future that we're all going to architect together. We hope that you'll come back next year. What we launched today is going to look very quaint relative to what we're busy creating for you now. Thank you for all that you do. Thanks for coming here today.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Note:** the model transcribed \"Dall-e\" as \"Dolly\". A common issue with similar sounding words for speech-to-text APIs is that they may misidentify the term without additional context.  \n",
    "\n",
    "There are two techniques to provide context to the model to correct such transcription errors: "
   ],
   "id": "3a8c242021ce3964"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Option 1: Provide the correct transcription in the `prompt` parameter. \n",
    "Use `prompt` in Whisper API call to improve the quality of the transcripts generated by the model. Prompts can be very helpful for correcting specific words or acronyms that the model may misspell in the audio transcription.\n",
    "\n",
    "You could build a \"glossary\" of terms that the model is likely to transcribe inaccurately, given the context of the text. Over time, terms can be added to the glossary, improving accuracy.\n",
    "\n",
    "[Learn more about prompting Whisper model here](https://platform.openai.com/docs/guides/speech-to-text/prompting)"
   ],
   "id": "f571bfa842f92665"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T00:33:43.429291Z",
     "start_time": "2024-09-27T00:33:33.474316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path to the source audio file  \n",
    "audio_file = \"../gpt4o/data/keynote_recap.mp3\"\n",
    "\n",
    "# Transcription errors can be stored in a glossary of transcription errors (comma separated)\n",
    "glossary_of_transcription_errors = \"Dall-e\"\n",
    "\n",
    "# Invoke the Whisper model to get the transcription\n",
    "audio_file= open(audio_file, \"rb\")\n",
    "\n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "    prompt=glossary_of_transcription_errors,   \n",
    "  file=audio_file\n",
    ")\n",
    "\n",
    "# Retrieve the transcribed text and print the output\n",
    "english_transcription_second_pass = transcription.text\n",
    "print(english_transcription_second_pass)"
   ],
   "id": "261ed580658bd77f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to our first ever OpenAI Dev Day. Today, we are launching a new model, GPT-4 Turbo. GPT-4 Turbo supports up to 128,000 tokens of context. We have a new feature called JSON mode, which ensures that the model will respond with valid JSON. You can now call many functions at once, and it will do better at following instructions in general. You want these models to be able to access better knowledge about the world. So do we. So we're launching retrieval in the platform. You can bring knowledge from outside documents or databases into whatever you're building. GPT-4 Turbo has knowledge about the world up to April of 2023, and we will continue to improve that over time. Dall-e 3, GPT-4 Turbo with Vision, and the new Text-to-Speech model are all going into the API today. Today, we're launching a new program called Custom Models. With Custom Models, our researchers will work closely with a company to help them make a great custom model, especially for them and their use case using our tools. Higher rate limits. We're doubling the tokens per minute for all of our established GPT-4 customers so that it's easier to do more, and you'll be able to request changes to further rate limits and quotas directly in your API account settings. And GPT-4 Turbo is considerably cheaper than GPT-4 by a factor of 3x for prompt tokens and 2x for completion tokens starting today. We're thrilled to introduce GPTs. GPTs are tailored versions of ChatGPT for a specific purpose. And because they combine instructions, expanded knowledge, and actions, they can be more helpful to you. They can work better in many contexts, and they can give you better control. We know that many people who want to build a GPT don't know how to code. We've made it so that you can program the GPT just by having a conversation. You can make private GPTs. You can share your creations publicly with a link for anyone to use. Or, if you're on ChatGPT Enterprise, you can make GPTs just for your company. And later this month, we're going to launch the GPT Store. So those are GPTs, and we can't wait to see what you'll build. We're bringing the same concept to the API. The Assistance API includes persistent threads, so they don't have to figure out how to deal with long conversation history, built-in retrieval, code interpreter, a working Python interpreter in a sandbox environment, and, of course, the improved function calling. As intelligence gets integrated everywhere, we will all have superpowers on demand. We're excited to see what you all will do with this technology, and to discover the new future that we're all going to architect together. We hope that you'll come back next year. What we launched today is going to look very quaint relative to what we're busy creating for you now. Thank you for all that you do. Thanks for coming here today. Thank you.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now the model accurately transcribed word \"Dall-e\" as we provided the correct transcription in the prompt for the given context.\n",
    " "
   ],
   "id": "671ccdbfa5c5238d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Option 2: Use GPT model for post-processing output \n",
    "Another popular technique to improve the quality of output and correct spelling mistakes is to use GPT model for post-processing as described below."
   ],
   "id": "796d292e3fedb213"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T00:34:20.635659Z",
     "start_time": "2024-09-27T00:34:13.242876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = f\"You are a helpful assistant. Your task is to correct any spelling discrepancies and grammar in the transcribed text. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided. Some words that may be misspelled include: {glossary_of_transcription_errors}\" \n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": transcription_english_first_pass\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "transcription_english_third_pass = response.choices[0].message.content\n",
    "\n",
    "print(transcription_english_third_pass)"
   ],
   "id": "8eac35e5c4adb6ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to our first-ever OpenAI Dev Day. Today, we are launching a new model, GPT-4 Turbo. GPT-4 Turbo supports up to 128,000 tokens of context. We have a new feature called JSON mode, which ensures that the model will respond with valid JSON. You can now call many functions at once, and it'll do better at following instructions in general. You want these models to be able to access better knowledge about the world. So do we. So we're launching retrieval in the platform. You can bring knowledge from outside documents or databases into whatever you're building. GPT-4 Turbo has knowledge about the world up to April of 2023, and we will continue to improve that over time. DALL-E 3, GPT-4 Turbo with Vision, and the new Text-to-Speech model are all going into the API today. Today, we're launching a new program called Custom Models. With Custom Models, our researchers will work closely with the company to help them make a great custom model, especially for them and their use case using our tools. Higher rate limits: we're doubling the tokens per minute for all of our established GPT-4 customers so that it's easier to do more, and you'll be able to request changes to further rate limits and quotas directly in your API account settings. And GPT-4 Turbo is considerably cheaper than GPT-4 by a factor of 3x for prompt tokens and 2x for completion tokens, starting today. We're thrilled to introduce GPTs. GPTs are tailored versions of ChatGPT for a specific purpose. And because they combine instructions, expanded knowledge, and actions, they can be more helpful to you. They can work better in many contexts, and they can give you better control. We know that many people who want to build a GPT don't know how to code. We've made it so that you can program the GPT just by having a conversation. You can make private GPTs. You can share your creations publicly with a link for anyone to use. Or if you're on ChatGPT Enterprise, you can make GPTs just for your company. And later this month, we're going to launch the GPT Store. So those are the GPTs, and we can't wait to see what you'll build. We're bringing the same concept to the API. The Assistance API includes persistent threads, so you don't have to figure out how to deal with long conversation history, built-in retrieval, code interpreter, a working Python interpreter in a sandbox environment, and of course, the improved function calling. As intelligence gets integrated everywhere, we will all have superpowers on demand. We're excited to see what you all will do with this technology, and to discover the new future that we're all going to architect together. We hope that you'll come back next year. What we launched today is going to look very quaint relative to what we're busy creating for you now. Thank you for all that you do. Thanks for coming here today.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2. Translate the English language text to Hindi in Devanagari script\n",
    "\n",
    "The next step is to translate the text from the source language to the target language script. In this case, we prompt the GPT-4o model to translate the text from English (written in Latin script) to Hindi (written in Devanagari script).\n",
    "\n",
    "For certain new words in a language, there may not be a direct translation in the target language. In such cases, we prompt the model to retain these words in the original language. We can also explicitly provide examples of words to keep in the original script (e.g., English). These terms can be stored as a glossary, which can be expanded as we continue translating the text."
   ],
   "id": "5d294fa712197b74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T00:34:36.765189Z",
     "start_time": "2024-09-27T00:34:26.797284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "glossary_of_terms_to_keep_in_original_language = \"Some words to keep in English include - Turbo, OpenAI, token, GPT, Dall-e, Python\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": f\"You are an assistant that translates text from English to Hindi. The user will provide content for you to translate. You may keep certain words in English when a direct translation doesn't exist. {glossary_of_terms_to_keep_in_original_language} \"},\n",
    "    {\"role\": \"user\", \"content\": transcription.text},\n",
    "  ]\n",
    ")\n",
    "\n",
    "hindi_transcription = response.choices[0].message.content\n",
    "\n",
    "print(hindi_transcription)"
   ],
   "id": "330baa9a6e55530c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "हमारे पहले OpenAI Dev Day में आपका स्वागत है। आज, हम एक नया मॉडल लॉन्च कर रहे हैं, GPT-4 Turbo। GPT-4 Turbo 128,000 tokens तक के संदर्भ को समर्थन करता है। हमारे पास एक नई फीचर है, जिसे JSON mode कहा जाता है, जो सुनिश्चित करती है कि मॉडल वैध JSON के साथ प्रतिक्रिया देगा। आप अब कई functions को एक साथ कॉल कर सकते हैं, और यह सामान्य निर्देशों का पालन करने में भी बेहतर होगा। आप चाहते हैं कि ये मॉडल दुनिया के बारे में बेहतर ज्ञान तक पहुंच सकें। हम भी ऐसा ही चाहते हैं। इसलिए हम प्लेटफ़ॉर्म में retrieval लॉन्च कर रहे हैं। आप बाहरी दस्तावेजों या डेटाबेस से ज्ञान को अपने निर्माण में ला सकते हैं। GPT-4 Turbo के पास अप्रैल 2023 तक का दुनिया का ज्ञान है, और हम इसे समय के साथ सुधारते रहेंगे। Dall-e 3, GPT-4 Turbo with Vision, और नया Text-to-Speech मॉडल आज API में जा रहे हैं। \n",
      "\n",
      "आज, हम एक नया कार्यक्रम लॉन्च कर रहे हैं जिसे Custom Models कहा जाता है। Custom Models के साथ, हमारे शोधकर्ता किसी कंपनी के साथ मिलकर काम करेंगे ताकि वे उनके उपकरणों का उपयोग करके उनके विशेष उपयोग के मामले के लिए एक महान कस्टम मॉडल बना सकें। उच्च दर सीमाएँ। हम अपने सभी स्थापित GPT-4 ग्राहकों के लिए प्रति मिनट टोकन को दुगना कर रहे हैं ताकि अधिक करना आसान हो जाए, और आप अपनी API खाता सेटिंग्स में सीधे दर सीमाओं और कोटा में बदलाव का अनुरोध कर सकें। और GPT-4 Turbo GPT-4 की तुलना में काफी सस्ता है, prompt tokens के लिए 3x और completion tokens के लिए 2x, आज से। \n",
      "\n",
      "हम GPTs को पेश करने के लिए उत्साहित हैं। GPTs विशेष उद्देश्यों के लिए ChatGPT के tailored versions हैं। और क्योंकि वे निर्देशों, विस्तारित ज्ञान और क्रियाओं को जोड़ते हैं, वे आपके लिए अधिक सहायक हो सकते हैं। वे कई संदर्भों में बेहतर काम सकते हैं, और आपको बेहतर नियंत्रण दे सकते हैं। हम जानते हैं कि कई लोग जो GPT बनाना चाहते हैं, वे कोड करना नहीं जानते हैं। हमने इसे ऐसा बना दिया है कि आप केवल बातचीत करके GPT प्रोग्राम कर सकते हैं। आप निजी GPTs बना सकते हैं। आप अपने निर्माण को सार्वजनिक रूप से लिंक के साथ साझा कर सकते हैं ताकि कोई भी इसका उपयोग कर सके। या, यदि आप ChatGPT Enterprise पर हैं, तो आप केवल अपनी कंपनी के लिए GPT बना सकते हैं। और इस महीने के अंत में, हम GPT Store लॉन्च करने जा रहे हैं। ये हैं GPTs, और हम यह देखने के लिए इंतजार नहीं कर सकते कि आप क्या बनाएंगे। \n",
      "\n",
      "हम समान कॉन्सेप्ट को API में ला रहे हैं। Assistance API में persistent threads शामिल हैं, जिससे उन्हें लंबी बातचीत के इतिहास से निपटने का तरीका नहीं निकालना पड़ता, built-in retrieval, code interpreter, एक sandbox environment में काम करने वाला Python interpreter, और, निश्चित रूप से, improved function calling। जैसे-जैसे बुद्धिमत्ता हर जगह एकीकृत होती जाती है, हम सभी के पास मांग पर superpowers होंगी। हम इस तकनीक के साथ आप सभी के द्वारा किए जाने वाले कार्य को देखने के लिए उत्साहित हैं, और साथ में हम जो नया भविष्य तैयार करने जा रहे हैं उसे देखने के लिए भी। हम उम्मीद करते हैं कि आप अगले साल वापस आएंगे। जो हम आज लॉन्च कर रहे हैं, वह उस चीज के सामने बहुत मामूली लगेगा जिसे हम अभी आपके लिए बना रहे हैं। आप सभी का धन्यवाद। आज यहां आने के लिए धन्यवाद। धन्यवाद।\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The transcribed text is a combination of Hindi and English, represented in their respective scripts: Devanagari for Hindi and Latin for English. This approach ensures more natural-sounding speech with the correct pronunciation of both languages' words.",
   "id": "d7d71ed42afacb8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Text-to-speech conversion of the written script into spoken language\n",
    "\n",
    "OpenAI's text-to-speech (TTS) model can take the written script as input and produce spoken output with a native-sounding Hindi accent (where the script represents the Hindi language), intermingled with a native-sounding English accent (where the script represents the English language).    \n",
    "\n",
    "![Text-to-speech](./images/Text-to-speech.png)\n",
    " \n",
    "\n",
    "*[Learn more about tts model here](https://platform.openai.com/docs/guides/text-to-speech)"
   ],
   "id": "67068cca4fe5211b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T00:35:40.219930Z",
     "start_time": "2024-09-27T00:35:07.349336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_file_path = \"./sounds/output.mp3\"\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    voice=\"alloy\",\n",
    "    input=hindi_transcription,\n",
    ")\n",
    "\n",
    "response.write_to_file(output_file_path)"
   ],
   "id": "dc7922ea7f33b4d3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T00:39:26.524113Z",
     "start_time": "2024-09-27T00:35:57.600595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from playsound import playsound\n",
    "\n",
    "playsound(output_file_path)"
   ],
   "id": "e5545d55bf8be52a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 4. Translation benchmarking (e.g., BLEU or ROUGE) \n",
    "\n",
    "We can assess the quality of the translated text by comparing it to a reference translation using evaluation metrics like BLEU and ROUGE. \n",
    "\n",
    "**BLEU (Bilingual Evaluation Understudy)**: Measures the overlap of n-grams between the candidate and reference translations. Scores range from 0 to 100, with higher scores indicating better quality.\n",
    "\n",
    "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Commonly used for summarization evaluation. Measures the overlap of n-grams and the longest common subsequence between the candidate and reference texts.\n",
    "\n",
    "Ideally, a reference translation (a human-translated version) of the original text is needed for an accurate evaluation. However, developing such evaluations can be challenging, as it requires time and effort from bilingual humans proficient in both languages.\n",
    "\n",
    "An alternative is to transcribe the output audio file from the target language back into the original language to assess the quality of the translation. The Whisper API provides a `translations` endpoint to transcribe the audio back into English."
   ],
   "id": "9f25e38cbf8dbd26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T00:39:39.160422Z",
     "start_time": "2024-09-27T00:39:28.312498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Translate the audio output file generated by tts to English text using Whisper \n",
    "\n",
    "audio_file= open(output_file_path, \"rb\")\n",
    "transcription = client.audio.translations.create(\n",
    "  model=\"whisper-1\",  \n",
    "  file=audio_file,\n",
    ")\n",
    "\n",
    "english_re_transcription = transcription.text\n",
    "\n",
    "print(english_re_transcription)"
   ],
   "id": "d662ac09494780a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to our first OpenAI Dev Day. Today, we are launching a new model, GPT-4 Turbo. GPT-4 Turbo supports up to 128,000 tokens. We have a new feature called JSON mode, which ensures that the model will react with a method. You can now call multiple functions at once, and it will be great for following general instructions. You want this model to reach the world with the best knowledge possible. We want that too. That's why we are launching Retrieval on the platform. You can bring your knowledge from external documents or databases into your development. GPT-4 Turbo has the knowledge of the world up to April 22, 2023, and we will keep improving it over time. DALI 3, GPT-4 Turbo with Vision, and a new text-to-speech model is going into the API today. Today, we are launching a new program called Custom Models. With Custom Models, our researchers will work with a company so that they can use their products to create a great custom model for their special needs. Uchdar Seemai. We are doubling the minute token for all our established GPT-4 customers so that it becomes easier to do more. And you can request changes in your API account settings directly in Uchdar Seemai and Quota. And GPT-4 Turbo is much cheaper than GPT-4. 3x for prompt tokens and 2x for completion tokens from today. We are excited to present GPTs. GPTs are tailored versions of ChatGPT for special needs. And because they connect instructions, extended life, and actions, they can be more helpful for you. They can be more effective in many situations and give you better control. We know that many people who want to make a GPT don't know how to code. We've made it so that you can just talk and program a GPT. You can make your own GPTs. You can share your creation with a link so that anyone can use it. Or if you're on ChatGPT Enterprise, you can just make a GPT for your company. And at the end of this month, we're going to launch a GPT store. These are the GPTs. And we can't wait to see what you make. We're bringing the same concept into the API. Persistent threads are included in the Assistance API, so that they don't have to figure out how to deal with a long conversation. Built-in retrieval, code interpreter, a Python interpreter that works in a sandbox environment, and of course, improved function calling. As intelligence becomes a reality everywhere, we will all have superpowers at our fingertips. With this technique, we are excited to see the work you all do. And to see the new future we're going to create. We hope you'll be back next year. What we're launching today will look very ordinary in front of what we're creating for you. Thank you all. Thank you for coming here today. Thank you.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With the text re-translated into English from the Hindi audio, we can run the evaluation metrics by comparing it to the original English transcription.",
   "id": "fd673dbf860cdf62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T00:39:49.869822Z",
     "start_time": "2024-09-27T00:39:46.372597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# We'll use the original english transcription as the reference text \n",
    "reference_text = transcription_english_third_pass\n",
    "\n",
    "candidate_text = english_re_transcription\n",
    "\n",
    "# BLEU Score Evaluation\n",
    "bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "print(f\"BLEU Score: {bleu.score}\")\n",
    "\n",
    "# ROUGE Score Evaluation\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference_text, candidate_text)\n",
    "print(f\"ROUGE-1 Score: {scores['rouge1'].fmeasure}\")\n",
    "print(f\"ROUGE-L Score: {scores['rougeL'].fmeasure}\")\n"
   ],
   "id": "b661a404001c660d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 47.553973306995736\n",
      "ROUGE-1 Score: 0.8320463320463322\n",
      "ROUGE-L Score: 0.7297297297297297\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5. Interpret and improve scores by adjusting prompting parameters in steps 1-3 as needed\n",
    "\n",
    "In this example, both BLEU and ROUGE scores indicate that the quality of the voice translation is between very good and excellent.\n",
    "\n",
    "**Interpreting BLEU Scores:** While there is no universally accepted scale, some interpretations suggest:\n",
    "\n",
    "0 to 10: Poor quality translation; significant errors and lack of fluency.\n",
    "\n",
    "10 to 20: Low quality; understandable in parts but contains many errors.\n",
    "\n",
    "20 to 30: Fair quality; conveys the general meaning but lacks precision and fluency.\n",
    "\n",
    "30 to 40: Good quality; understandable and relatively accurate with minor errors.\n",
    "\n",
    "40 to 50: Very good quality; accurate and fluent with very few errors.\n",
    "\n",
    "50 and above: Excellent quality; closely resembles human translation.\n",
    "\n",
    "**Interpreting ROUGE scores:** The interpretation of a \"good\" ROUGE score can vary depending on the task, dataset, and domain. The following guidelines indicate a good outcome:\n",
    "\n",
    "ROUGE-1 (unigram overlap): Scores between 0.5 to 0.6 are generally considered good for abstractive summarization tasks.\n",
    "\n",
    "ROUGE-L (Longest Common Subsequence): Scores around 0.4 to 0.5 are often regarded as good, reflecting the model's ability to capture the structure of the reference text.\n",
    "\n",
    "If the score for your translation is unsatisfactory, consider the following questions:\n",
    "\n",
    "#### 1. Is the source audio accurately transcribed? \n",
    "Revisit parameters `glossary_of_transcription_errors` and add text that has incorrect transcriptions.  \n",
    "\n",
    "#### 2. Is the transcribed text free of grammatical errors? \n",
    "Consider using a post-processing step with the GPT model to refine the transcription by removing grammatical mistakes and adding appropriate punctuation.\n",
    "\n",
    "#### 3. Are there words that make sense to keep in the original language?  \n",
    "There may be new terms or concepts for which a translation in target language may not exist or is not universally understood. Revisit `glossary_of_terms_to_keep_in_original_language` and add such terms."
   ],
   "id": "cbbe2e0cafb384fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "To recap, this cookbook provides a step-by-step process for translating and dubbing audio from one language to another, making content more accessible to a global audience. The example we used is the voice translation of an audio file from English to Hindi. \n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "**1. Transcription**: Using OpenAI's Whisper to transcribe the English audio into text.\n",
    "\n",
    "**2. Translation**: Converting the transcribed English text into Hindi, specifically in the Devanagari script.\n",
    "\n",
    "**3. Text-to-Speech**: Transforming the translated text into spoken Hindi using a text-to-speech service.\n",
    "\n",
    "**4. Benchmarking**: Evaluating the quality and accuracy of the translation with metrics like BLEU or ROUGE, and refining the results by adjusting parameters throughout the process.\n",
    "\n",
    "This guide also clarifies the distinction between \"language\" and \"script,\" which are often used interchangeably but have specific meanings critical to the translation task. Language refers to the spoken or written system of communication, while script refers to the characters used to write the language. Understanding this distinction is key to effectively translating and dubbing content.\n",
    "\n",
    "By using the techniques outlined in this cookbook, you can translate and dub a wide range of content—such as podcasts, training videos, tutorials, and even full-length films—into multiple languages. This method can be applied across various industries, from entertainment and education to business and global communication efforts, enabling creators to expand their reach to diverse linguistic audiences."
   ],
   "id": "38177e0ed7ca3ed6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
