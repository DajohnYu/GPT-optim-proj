{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Voice Translation of Audio Files into Different Languages Using Gpt-4o\n",
    "\n",
    "Have you ever wanted to translate a podcast into your native language? Translating and dubbing audio content can make it more accessible to audiences worldwide. With GPT-4o's new audio-in and audio-out modality, this process is now easier than ever.\n",
    "\n",
    "This guide will walk you through translating an English audio file into Hindi using OpenAI's GPT-4o audio modality API.\n",
    "\n",
    "GPT-4o simplifies the dubbing process for audio content. Previously, you had to convert the audio to text and then translate the text into the target language before converting it back into audio. Now, with GPT-4o’s voice-to-voice capability, you can achieve this in a single step with audio input and output.  \n",
    "\n",
    "A note on semantics used in this Cookbook regarding **Language** and written **Script**. These words are generally used interchangeably, though it's important to understand the distinction, given the task at hand. \n",
    "- **Language** refers to the spoken or written system of communication. For instance, Hindi and Marathi are different languages, but both use the Devanagari script. Similarly, English and French are different languages, but are written in Latin script. \n",
    "- **Script** refers to the set of characters or symbols used to write the language. For example, Serbian language traditionally written in Cyrillic Script, is also written in Latin script.\n",
    "\n",
    "\n",
    "GPT-4o audio in audio out modality makes it easier to bud the audio on one API call.  \n",
    "1. **Dub and Transcribe** the audio file from one language directly to the target langauge  \n",
    "2. **Translation benchmarking** (BLEU or ROUGE) \n",
    "3. **Interpret and improve** scores by adjusting prompting parameters in steps 1-3 as needed  \n",
    "\n",
    "\n",
    "Before we get started, make sure you have the `openai` library installed, and your OpenAI API key is configured as an environment variable. "
   ],
   "id": "4265ef1326248608"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 1: Use GPT-4o model to dub and transcribe the audio content \n",
    "\n",
    "Let's start by creating a function that sends an audio file to OpenAI's GPT-4o API for processing, using the chat completions API endpoint.\n",
    "\n",
    "The function `process_audio_with_gpt_4o` takes three inputs:\n",
    "\n",
    "1. A base64-encoded audio file (base64_encoded_audio) that will be sent to the GPT-4o model.\n",
    "2. Desired output modalities (such as text, or both text and audio). \n",
    "3. A system prompt that instructs the model on how to process the input.\n",
    "\n",
    "The function sends an API request to OpenAI's chat/completions endpoint. The request headers include the API key for authorization. The data payload contains the model type (`gpt-4o-audio-preview`), the selected output modalities, and audio details, such as the voice type and format (in this case, \"alloy\" and \"wav\"). It also includes the system prompt and the base64-encoded audio file as part of the \"user\" message. If the API request is successful (HTTP status 200), the response is returned as JSON. If an error occurs (non-200 status), it prints the error code and message.\n",
    "\n",
    "This function enables audio processing through OpenAI's GPT-4o API, allowing tasks like dubbing, transcription, or translation to be performed based on the input provided."
   ],
   "id": "d84f1356379946f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T00:26:19.179368Z",
     "start_time": "2024-10-22T00:26:19.173763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import os\n",
    "import base64\n",
    "import json \n",
    "\n",
    "# Load the API key from the environment variable\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def process_audio_with_gpt_4o(base64_encoded_audio, output_modalities, system_prompt):\n",
    "    # Chat Completions API end point \n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "    # Set the headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    # Construct the request data\n",
    "    data = {\n",
    "        \"model\": \"gpt-4o-audio-preview\",\n",
    "        \"modalities\": output_modalities,\n",
    "        \"audio\": {\n",
    "            \"voice\": \"alloy\",\n",
    "            \"format\": \"wav\"\n",
    "        },\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"input_audio\",\n",
    "                        \"input_audio\": {\n",
    "                            \"data\": base64_encoded_audio,\n",
    "                            \"format\": \"wav\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    request_response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if request_response.status_code == 200:\n",
    "        return request_response.json()\n",
    "    else:  \n",
    "        print(f\"Error {request_response.status_code}: {request_response.text}\")\n",
    "        return"
   ],
   "id": "21d1697dffd5df48",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Using the function `process_audio_with_gpt_4o`, we will first get an English Transcription of the audio. You can skip this step if you already have a transcription in the source language. \n",
    "\n",
    "In this step, we: \n",
    "1. Read the WAV file and convert it into base64 encoding.\n",
    "2. Set the output modality to [\"text\"], as we only need a text transcription.\n",
    "3. Provide a system prompt to instruct the model to focus on transcribing the speech and to ignore background noises like applause.\n",
    "4. Call the process_audio_with_gpt_4o function to process the audio and return the transcription."
   ],
   "id": "285d64818744965"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T00:26:41.943941Z",
     "start_time": "2024-10-22T00:26:21.170507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio_wav_path = \"./sounds/keynote_recap.wav\"\n",
    "\n",
    "# Read the WAV file and encode it to base64\n",
    "with open(audio_wav_path, \"rb\") as audio_file:\n",
    "    audio_bytes = audio_file.read()\n",
    "    english_audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')\n",
    "\n",
    "modalities = [\"text\"]  # Can be modified as needed\n",
    "prompt = \"The user will provide an audio file in English. Transcribe the audio to English text. Only provide the language transcription, do not include background noises such as applause. \"\n",
    "\n",
    "\n",
    "response_json = process_audio_with_gpt_4o(english_audio_base64, modalities, prompt)\n",
    "\n",
    "english_transcript = response_json['choices'][0]['message']['content']\n",
    "\n",
    "print(english_transcript)"
   ],
   "id": "4bda39cb6d193c0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to our first-ever OpenAI DevDay. Today we are launching a new model, GPT-4 Turbo. GPT-4 Turbo supports up to 128,000 tokens of context. We have a new feature called JSON mode, which ensures the model will respond with valid JSON. You can now call many functions at once, and it'll do better at following instructions in general. You want these models to be able to access better knowledge about the world? So do we. So we're launching retrieval in the platform. You can bring knowledge from outside documents or databases into whatever you're building. GPT-4 Turbo has knowledge about the world up to April of 2023, and we will continue to improve that over time. DALL-E 3, GPT-4 Turbo with Vision, and the new text-to-speech model are all going into the API today. Today we're launching a new program called Custom Models. With Custom Models, our researchers will work closely with a company to help them make a great custom model, especially for them and their use case using our tools. Higher rate limits: we’re doubling the tokens per minute for all of our established GPT-4 customers, so that it’s easier to do more, and you'll be able to request changes to further rate limits and quotas directly in your API account settings. And GPT-4 Turbo is considerably cheaper than GPT-4 by a factor of 3x for prompt tokens and 2x for completion tokens starting today.\n",
      "\n",
      "We’re thrilled to introduce GPTs. GPTs are tailored versions of ChatGPT for a specific purpose. And because they combine instructions, expanded knowledge, and actions, they can be more helpful to you, they can work better in many contexts, and they can give you better control. We know that many people who want to build a GPT don't know how to code. We’ve made it so that you can program the GPT just by having a conversation. You can make private GPTs. You can share your creation publicly with a link for anyone to use. Or, if you're on ChatGPT Enterprise, you can make GPTs just for your company. And later this month, we’re gonna launch the GPT Store. So, those are GPTs and we can't wait to see what you'll build. We’re bringing the same concept to the API. The Assistants API includes persistent threads so they don't have to figure out how to deal with long conversation history, built-in retrieval, Code Interpreter, a working Python interpreter in a sandbox environment, and of course, the improved function calling.\n",
      "\n",
      "As intelligence gets integrated everywhere, we will all have superpowers on demand. We’re excited to see what you all will do with this technology and to discover the new future that we’re all going to architect together. We hope that you’ll come back next year: what we launch today is going to look very quaint relative to what we're busy creating for you now. Thank you for all you do. Thanks for coming here today.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This English transcript will serve as our ground truth as we benchmark the Hindi language dubbing of the audio.   ",
   "id": "7e0f32d513b40911"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With GPT-4o, we can directly dub the audio file from English to Hindi, and get the Hindi transcription of the audio in one API call. For this we set the output modality to `[\"text\", \"audio\"] `",
   "id": "34d396b380fe6d83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T00:27:46.582383Z",
     "start_time": "2024-10-22T00:26:51.293362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "glossary_of_terms_to_keep_in_original_language = \"Turbo, OpenAI, token, GPT, Dall-e, Python\"\n",
    "\n",
    "modalities = [\"text\", \"audio\"] \n",
    "system_prompt = f\"The user will provide an audio file in English. Translate the complete audio in Hindi. You make keep certain words in English for which a direct translation in Hindi does not exist such as ${glossary_of_terms_to_keep_in_original_language}\"\n",
    "\n",
    "\n",
    "response_json = process_audio_with_gpt_4o(english_audio_base64, modalities, system_prompt)\n",
    "\n",
    "message = response_json['choices'][0]['message']"
   ],
   "id": "789059b64086e8c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the following code snippet, we will retrieve both the Hindi transcription and the dubbed audio from the GPT-4o response. Previously, this would have been a multistep process, involving several API calls to first transcribe, then translate, and finally produce the audio in the target language. With GPT-4o, we can now accomplish this in a single API call.",
   "id": "7f2efc0de932d755"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T00:28:10.698023Z",
     "start_time": "2024-10-22T00:28:10.695279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "from pydub.playback import play\n",
    "\n",
    "# Get the transcript from the model. This will vary depending on the modality you are using. \n",
    "hindi_transcript = message['audio']['transcript']\n",
    "\n",
    "print(hindi_transcript)"
   ],
   "id": "1cb29298d8b7ba15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "स्वागत है हमारे पहले OpenAI DevDay में। आज हम एक नया model लॉन्च कर रहे हैं, GPT-4 Turbo। GPT-4 Turbo 128,000 tokens of context को सपोर्ट करता है। हमारे पास एक नया feature है जिसे JSON mode कहते हैं, जो ये सुनिश्चित करता है कि model valid JSON के साथ उत्तर देगा। आप कई functions को एक साथ कॉल कर सकते हैं। और ये निर्देशों का पालन करने में भी बेहतर होगा। यदि आप चाहते हैं कि ये models दुनिया के बारे में बेहतर ज्ञान प्राप्त करें, तो हम भी। इसलिए हम platform में retrieval लॉन्च कर रहे हैं। आप जो कुछ भी बना रहे हैं, उसमें बाहरी documents या databases से जानकारी ला सकते हैं। GPT-4 Turbo की 2023 की अपडेट तक दुनिया के बारे में जानकारी है, और हम इसे समय के साथ बेहतर बनाएंगे। Dall-e 3, GPT-4 Turbo with vision, और नया text-to-speech model आज API में जा रहे हैं। आज हम एक नया प्रोग्राम लॉन्च कर रहे हैं जिसे custom models कहते हैं। Custom models के साथ, हमारे researchers किसी company के साथ मिलकर काम करेंगे ताकि हमारे tool का उपयोग करके उनके लिए एक great custom model तैयार किया जा सके। High rate limits: हमारे सभी established GPT-4 customers के लिए हम tokens per minute को दुगना कर रहे हैं, ताकि और आसानी से अधिक कार्य किया जा सके, और आप अपने API account settings में सीधे rate limit और quotas में परिवर्तन के लिए अनुरोध कर सकते हैं। और GPT-4 Turbo, GPT-4 की तुलना में काफी सस्ता है, Prompt tokens के लिए 3x और completion tokens के लिए 2x सस्ता है, आज से।\n",
      "\n",
      "हम आपको GPTस से मिलवाने के लिए रोमांचित हैं। GPTs, ChatGPT के tailored versions हैं, किसी विशिष्ट उद्देश्य के लिए। और क्योंकि ये निर््देशों, विस्तारित knowledge, और actions को मिलाते हैं, ये आपके लिए अधिक सहायक हो सकते हैं। ये कई context में बेहतर काम कर सकते हैं, और ये आपको बेहतर कंट्रोल प्रदान कर सकते हैं। हम जानते हैं कि कई लोग जो GPT बनाना चाहते हैं, उन्हें coding नहीं आती। हमने इसे इस तरह से बनाया है कि आप सिर्फ बातचीत करके GPT बना सकें। आप private GPTs बना सकते हैं। आप अपनी creation को publicly share कर सकते हैं, एक link के माध्यम से जिससे कोई भी उपयोग कर सके। या, यदि आप ChatGPT Enterprise पर हैं, आप अगरसिर्फ अपनी company के लिए GPTs बना सकते हैं। और बाद में इस महीने, हम GPT स्टोर लॉन्च करेंगे। तो ये हैं GPTs और हमें इंतजार है देखने के लिए कि आप क्या करेंगे।\n",
      "\n",
      "हम वही संकल्प API में ला रहे हैं। Assistant API में persistent threads शामिल हैं ताकि उन्हें लंबी conversation history से निपटने की ज़रूरत न पड़े, built-in retrieval, code interpreter, एक working Python interpreter एक sandbox environment में, और निश्चित रूप से, improved function calling।\n",
      "\n",
      "जैसा कि intelligence हर जगह एकीकृत होता है, हम सब superpowers का उपयोग कर सकेंगे। हम उत्साहित हैं यह देखने के लिए कि आप इस technology के साथ क्या करेंगे, और उस नए future को खोजने के लिए जो हम सभी एक साथ तैयार करेंगे। हमें उम्मीद है कि आप अगले वर्ष वापस आएंगे। धन्यवाद जो कुछ भी आप करते हैं। यहाँ आने के लिए धन्यवाद।\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The transcribed text is a combination of Hindi and English, represented in their respective scripts: Devanagari for Hindi and Latin for English. This approach ensures more natural-sounding speech with the correct pronunciation of both languages' words.\n",
    "\n",
    "We will use the `pydub` module to play the audio. "
   ],
   "id": "d7d71ed42afacb8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T00:32:00.749178Z",
     "start_time": "2024-10-22T00:28:28.428394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio_data_base64 = message['audio']['data']\n",
    "audio_data_bytes = base64.b64decode(audio_data_base64)\n",
    "\n",
    "audio_segment = AudioSegment.from_file(BytesIO(audio_data_bytes), format=\"wav\")\n",
    "play(audio_segment)"
   ],
   "id": "84c60b14af4e0e98",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2. Translation benchmarking (e.g., BLEU or ROUGE) \n",
    "\n",
    "We can assess the quality of the translated text by comparing it to a reference translation using evaluation metrics like BLEU and ROUGE. \n",
    "\n",
    "**BLEU (Bilingual Evaluation Understudy)**: Measures the overlap of n-grams between the candidate and reference translations. Scores range from 0 to 100, with higher scores indicating better quality.\n",
    "\n",
    "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Commonly used for summarization evaluation. Measures the overlap of n-grams and the longest common subsequence between the candidate and reference texts.\n",
    "\n",
    "Ideally, a reference translation (a human-translated version) of the original text is needed for an accurate evaluation. However, developing such evaluations can be challenging, as it requires time and effort from bilingual humans proficient in both languages.\n",
    "\n",
    "An alternative is to transcribe the output audio file from the target language back into the original language to assess the quality of the translation using GPT-4o. "
   ],
   "id": "9f25e38cbf8dbd26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T00:32:13.344350Z",
     "start_time": "2024-10-22T00:32:00.754126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Translate the audio output file generated by the model back into English and compare with the reference text \n",
    "modalities = [\"text\"]  \n",
    "system_prompt = (\"The user will provide an audio file in Hindi. Transcribe the audio to English text. \"\n",
    "                 \"Only provide the language transcription, do not include background noises such as applause. \")\n",
    "\n",
    "\n",
    "response_json = process_audio_with_gpt_4o(audio_data_base64, modalities, system_prompt)\n",
    "\n",
    "re_translated_english_text = response_json['choices'][0]['message']['content']\n",
    "\n",
    "print(re_translated_english_text)"
   ],
   "id": "d662ac09494780a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to our first OpenAI DevDay. Today we are launching a new model, GPT-4 Turbo. GPT-4 Turbo supports 128,000 tokens of context. We have a new feature called JSON mode which ensures that the model responds with valid JSON. You can call multiple functions at the same time and it will be better at following instructions. If you want these models to gain better knowledge about the world, so do we. That's why we are launching retrieval on the platform. You can bring in information from external documents or databases. GPT-4 Turbo is updated with information from the year 2023 and we will continue to improve it over time. DALL-E 3, GPT-4 Turbo with Vision, and the new text-to-speech model are being rolled out to the API. Today we are launching a new program called Custom Models. With Custom Models, our researchers will work with a company to create a great custom model for them using our tools. Higher rate limits: For all our established GPT-4 customers, we are doubling the tokens per minute so that more work can be done with ease and requests can be made for changes in rate limits and quotas. GPT-4 Turbo is significantly more affordable than GPT-4 at 3x cheaper for prompt tokens and 2x cheaper for completion tokens, starting today. We are excited to introduce you to GPTs. GPTs are tailored versions of ChatGPT for a specific purpose. And because they combine extensive knowledge and actions, they can be more helpful for you. They can work better in many contexts and provide you with better control. We know that many people who want to create GPTs don't know how to code. We've designed it so you can create GPTs. You can create private GPTs, share your creations publicly via a link for anyone to use, or if you are on ChatGPT Enterprise, you can make GPTs just for your company. Later this month, we will be launching the GPTs Store. So, these are GPTs, and we look forward to seeing what you will create. We are bringing the same concept to the API with assistant threads included so that they don't need to deal with long conversation histories, built-in retrieval, a code interpreter, a working Python interpreter in a sandbox environment, and definitely improved function calling. As intelligence is integrated everywhere, we can all use superpowers. We are excited to see what you will do with this technology and to explore that new future which we are creating together. We hope you will come back next year. Thank you for everything you do. Thank you for coming here.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With the text re-translated into English from the Hindi audio, we can run the evaluation metrics by comparing it to the original English transcription.",
   "id": "fd673dbf860cdf62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T00:32:17.022228Z",
     "start_time": "2024-10-22T00:32:13.346110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# We'll use the original english transcription as the reference text \n",
    "reference_text = english_transcript\n",
    "\n",
    "candidate_text = re_translated_english_text\n",
    "\n",
    "# BLEU Score Evaluation\n",
    "bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "print(f\"BLEU Score: {bleu.score}\")\n",
    "\n",
    "# ROUGE Score Evaluation\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference_text, candidate_text)\n",
    "print(f\"ROUGE-1 Score: {scores['rouge1'].fmeasure}\")\n",
    "print(f\"ROUGE-L Score: {scores['rougeL'].fmeasure}\")"
   ],
   "id": "b661a404001c660d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 43.66499203298644\n",
      "ROUGE-1 Score: 0.8110882956878849\n",
      "ROUGE-L Score: 0.7330595482546203\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3. Interpret and improve scores by adjusting prompting parameters in steps 1-3 as needed\n",
    "\n",
    "In this example, both BLEU and ROUGE scores indicate that the quality of the voice translation is between very good and excellent.\n",
    "\n",
    "**Interpreting BLEU Scores:** While there is no universally accepted scale, some interpretations suggest:\n",
    "\n",
    "0 to 10: Poor quality translation; significant errors and lack of fluency.\n",
    "\n",
    "10 to 20: Low quality; understandable in parts but contains many errors.\n",
    "\n",
    "20 to 30: Fair quality; conveys the general meaning but lacks precision and fluency.\n",
    "\n",
    "30 to 40: Good quality; understandable and relatively accurate with minor errors.\n",
    "\n",
    "40 to 50: Very good quality; accurate and fluent with very few errors.\n",
    "\n",
    "50 and above: Excellent quality; closely resembles human translation.\n",
    "\n",
    "**Interpreting ROUGE scores:** The interpretation of a \"good\" ROUGE score can vary depending on the task, dataset, and domain. The following guidelines indicate a good outcome:\n",
    "\n",
    "ROUGE-1 (unigram overlap): Scores between 0.5 to 0.6 are generally considered good for abstractive summarization tasks.\n",
    "\n",
    "ROUGE-L (Longest Common Subsequence): Scores around 0.4 to 0.5 are often regarded as good, reflecting the model's ability to capture the structure of the reference text.\n",
    "\n",
    "If the score for your translation is unsatisfactory, consider the following questions:\n",
    "\n",
    "#### 1. Is the source audio accurately transcribed? \n",
    "You could define a parameters `glossary_of_transcription_errors` and add text that has incorrect transcriptions in your system prompt in step 1, if you find the model has incorrectly transcribed certain terms. \n",
    "\n",
    "#### 2. Is the transcribed text free of grammatical errors? \n",
    "Consider using a post-processing step with the GPT model to refine the transcription by removing grammatical mistakes and adding appropriate punctuation.\n",
    "\n",
    "#### 3. Are there words that make sense to keep in the original language?  \n",
    "There may be new terms or concepts for which a translation in target language may not exist or is not universally understood. Revisit `glossary_of_terms_to_keep_in_original_language` and add such terms."
   ],
   "id": "cbbe2e0cafb384fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "In summary, this cookbook offers a clear, step-by-step process for translating and dubbing audio, making content more accessible to a global audience. Using GPT-4o’s audio input and output capabilities, translating and dubbing audio files from one language to another becomes much simpler. Our example focused on translating an audio file from English to Hindi.\n",
    "\n",
    "The process can be broken down into the following steps:\n",
    "\n",
    "**1. Dub and Transcribe:** Directly dub the audio file into the target language using GPT-4o's audio features.  \n",
    "\n",
    "**2. Benchmark Translation Quality:** Evaluate the translation’s accuracy using BLEU or ROUGE scores compared to reference text.\n",
    "\n",
    "**3. Optimize the Process:** If needed, adjust the prompting parameters to improve the transcription and dubbing results.  \n",
    "\n",
    "This guide also highlights the crucial distinction between \"language\" and \"script\"—terms that are often confused but are essential in translation work. Language refers to the system of communication, either spoken or written, while script is the set of characters used to write a language. Grasping this difference is vital for effective translation and dubbing.\n",
    "\n",
    "By following the techniques in this cookbook, you can translate and dub a wide range of content—from podcasts and training videos to full-length films—into multiple languages. This method applies across industries such as entertainment, education, business, and global communication, empowering creators to extend their reach to diverse linguistic audiences."
   ],
   "id": "38177e0ed7ca3ed6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
