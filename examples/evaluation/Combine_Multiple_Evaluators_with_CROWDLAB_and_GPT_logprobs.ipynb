{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Multiple Evaluators (Human or LLM-as-Judge) with CROWDLAB and GPT token probabilities\n",
    "\n",
    "In this notebook we delve into the problem of measuring the performance of evaluators (Whether human or LLM-as-Judge) for complex tasks. \n",
    "\n",
    "No labeling strategy is perfect. The quality of LLM-as-Judge varies highly depending on problem context ([Bavaresco et al., 2024](https://arxiv.org/abs/2406.18403v1)). Using expert human annotators to provide ground-truth labels is expensive and time-consuming. In addition, human annotators are fallible and may provide annotations at a lower quality than state-of-the-art LLMs like GPT-4.\n",
    "\n",
    "We showcase two methods, simple consensus, and an advanced opens-source algorithm (CROWDLAB) to produce a single label from multiple evaluators and estimate the reliability of of evaluators. \n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installing the necessary packages for the evaluation\n",
    "# datasets: for importing the reference datasets\n",
    "# openai: To interact with OpenAI's API\n",
    "# cleanlab: Provides an implementation of CROWDLAB algorithm\n",
    "# pandas: For data manipulation\n",
    "# numpy: For numerical computations\n",
    "\n",
    "!pip install datasets --quiet\n",
    "!pip install openai --quiet\n",
    "!pip install cleanlab --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install numpy --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example task: Evaluating LLM Responses in MT-Bench\n",
    "\n",
    "For the purpose of this notebook, we consider MT-Bench, a suite of pairwise comparison tasks used to benchmark LLM-as-a-Judge ([Zheng et al., 2024](https://arxiv.org/abs/2306.05685)). The MT-Bench dataset consists of 80 unique writing tasks executed by LLMs, with multiple humans as well as an LLM-as-judge (GPT-4) evaluating the performance of the tasks using pair-wise comparisons between two executions.\n",
    "\n",
    "Here is an example task from the MT-Bench dataset:\n",
    "\n",
    "| Task | Model A Response | Model B Response\n",
    "| --- | --- | --- |\n",
    "| \"Compose an engaging travel blog post about a recent trip to Hawaii\" | \"I recently had the pleasure of visiting Hawaii and it quickly and it quickly became one of my favorite places...\" | \"Aloha! I recently had the pleasure of embarking on a trip...\" |\n",
    "\n",
    "\n",
    "We'll load up the MT-Bench dataset and transform it into a format that can be used for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"lmsys/mt_bench_human_judgments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gpt4_pair', 'human'])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset has both \"human\" and \"gpt4\"-graded entries, which we can combine\n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_graded_df = dataset[\"gpt4_pair\"].to_pandas()\n",
    "human_graded_df = dataset[\"human\"].to_pandas()\n",
    "combined_df = pd.concat([gpt4_graded_df, human_graded_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original MT-Bench problems are \"multi-turn\" (That is, they involve multiple turns of interaction between the model and the evaluator). For simplicity, we will consider a \"single-turn\" version of the task, and use the evaluator ratings for the first turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'content': 'Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.', 'role': 'user'},\n",
       "       {'content': 'I recently had the pleasure of visiting Hawaii and it quickly became one of my favorite places. From the stunning beaches to the lush mountains, this place has it all. The people are incredibly friendly and the culture is alive and well. One of the highlights of my trip was visiting the Polynesian Cultural Center. Here, I was able to learn about the culture of the native Hawaiian people and try my hand at traditional crafts and activities. I also had a chance to explore some of the natural wonders of the island, including the breathtaking Hanauma Bay and the majestic Waimea Canyon. Whether you’re looking for a relaxing beach vacation or an adventure filled with culture and nature, Hawaii is the perfect destination.', 'role': 'assistant'}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example multi-turn conversation:\n",
    "combined_df['conversation_a'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate to single-turn:\n",
    "combined_df['conversation_a'] = combined_df['conversation_a'].apply(lambda array: array[:2])\n",
    "combined_df['conversation_b'] = combined_df['conversation_b'].apply(lambda array: array[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit rows to those judging the first turn of conversation:\n",
    "combined_df = combined_df[combined_df.turn == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "judge\n",
       "gpt4_pair    1200\n",
       "expert_24     103\n",
       "author_4      102\n",
       "author_0       92\n",
       "expert_0       74\n",
       "             ... \n",
       "expert_18       5\n",
       "expert_54       5\n",
       "expert_30       3\n",
       "author_1        3\n",
       "expert_52       2\n",
       "Name: count, Length: 66, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.judge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#integer-ize winner labels\n",
    "mapping_dict = dict(model_a=0, model_b=1)\n",
    "reverse_mapping = {v: k for k, v in mapping_dict.items()}\n",
    "combined_df.loc[:, 'winner_binary'] = combined_df['winner'].apply(lambda s: mapping_dict.get(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we examine the distribution of judges-per-example in the MT-Bench dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_wide = combined_df[combined_df.turn==1].pivot_table(\n",
    "    index=['question_id', 'model_a', 'model_b'],\n",
    "    columns='judge',\n",
    "    values=['winner_binary'],\n",
    "    aggfunc='first'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    882\n",
       "2    411\n",
       "3    124\n",
       "4     17\n",
       "6      2\n",
       "5      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df_wide.count(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that each evaluation has between one and five evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1: Generating simple consensus results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the absence of any other method, a simple way to aggregate multiple reviewers is to take consensus votes. This produces an answer but does not take into account the quality of the reviewers, or utilize the fact that we have multiple reviewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "consensus = combined_df_wide.mode(axis=1)\n",
    "consensus_labels = consensus.iloc[:, 0]\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'winner': np.where(consensus_labels, combined_df_wide.index.get_level_values('model_b'), \n",
    "                       combined_df_wide.index.get_level_values('model_a')),\n",
    "    'loser': np.where(consensus_labels, combined_df_wide.index.get_level_values('model_a'), \n",
    "                      combined_df_wide.index.get_level_values('model_b'))\n",
    "})\n",
    "\n",
    "\n",
    "wins = results_df['winner'].value_counts()\n",
    "appearances = pd.concat([results_df['winner'], results_df['loser']]).value_counts()\n",
    "win_rates = (wins / appearances).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us the following ranked win rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. gpt-4: Win Rate = 0.84 (408 wins out of 486 appearances)\n",
      "2. claude-v1: Win Rate = 0.72 (319 wins out of 443 appearances)\n",
      "3. gpt-3.5-turbo: Win Rate = 0.66 (325 wins out of 496 appearances)\n",
      "4. vicuna-13b-v1.2: Win Rate = 0.52 (240 wins out of 458 appearances)\n",
      "5. alpaca-13b: Win Rate = 0.20 (98 wins out of 493 appearances)\n",
      "6. llama-13b: Win Rate = 0.10 (48 wins out of 500 appearances)\n"
     ]
    }
   ],
   "source": [
    "for rank, (model, win_rate) in enumerate(win_rates.items(), 1):\n",
    "    print(f\"{rank}. {model}: Win Rate = {win_rate:.2f} ({wins[model]} wins out of {appearances[model]} appearances)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also measure judges by their level of agreement with the consensus. Understanding consensus is useful for understanding the quality of the judges, but high consensus doesn't necessarily indicate high quality evaluations. For example, if all judges are low quality, they may all agree on the wrong answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_binary_df = combined_df_wide['winner_binary']\n",
    "vote_counts_row = winner_binary_df.notna().sum(axis=1)\n",
    "vote_counts_judge = winner_binary_df.notna().sum()\n",
    "majority_vote = winner_binary_df[vote_counts_row > 1].mode(axis=1).iloc[:, 0]\n",
    "\n",
    "judge_agreement = {judge: {'agree': 0, 'total': 0} for judge in winner_binary_df.columns}\n",
    "for judge in winner_binary_df.columns:\n",
    "    judge_votes = winner_binary_df[judge]\n",
    "    valid_votes = judge_votes[vote_counts_row > 1]\n",
    "    agree_counts = (valid_votes == majority_vote[valid_votes.index]).sum()\n",
    "    total_counts = valid_votes.notna().sum()\n",
    "    judge_agreement[judge]['agree'] = agree_counts\n",
    "    judge_agreement[judge]['total'] = total_counts\n",
    "\n",
    "agreement_percentages = {judge: data['agree'] / data['total'] if data['total'] > 0 else 0 \n",
    "                         for judge, data in judge_agreement.items()}\n",
    "judge_metrics = pd.DataFrame({\n",
    "    'Evaluations': vote_counts_judge,\n",
    "    'Agreement': agreement_percentages\n",
    "})\n",
    "\n",
    "ranked_judges = judge_metrics[judge_metrics['Evaluations'] >= 10].sort_values('Evaluations', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Judge Summary:\n",
      "gpt4_pair: 882 evaluations, 88.15% agreement\n",
      "author_4: 71 evaluations, 94.34% agreement\n",
      "author_0: 65 evaluations, 100.00% agreement\n",
      "expert_0: 58 evaluations, 92.31% agreement\n",
      "expert_24: 58 evaluations, 97.50% agreement\n",
      "author_3: 36 evaluations, 95.83% agreement\n",
      "author_2: 33 evaluations, 96.00% agreement\n",
      "expert_9: 30 evaluations, 100.00% agreement\n",
      "expert_50: 24 evaluations, 80.00% agreement\n",
      "expert_51: 22 evaluations, 100.00% agreement\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nJudge Summary:\")\n",
    "for judge, row in ranked_judges[:10].iterrows():\n",
    "    print(f\"{judge}: {int(row['Evaluations'])} evaluations, {row['Agreement']*100:.2f}% agreement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2: Using CROWDLAB with GPT-4-o mini logprobs \n",
    "\n",
    "The above simple consensus methods do not take into account the quality of the judges. We can use the CROWDLAB algorithm to estimate the quality of the judges and the true answer to the problem. The CROWDLAB algorithm uses a probabilistic model to estimate the quality of the judges and the true answer to the problem. \n",
    "\n",
    "The CROWDLAB algorithm requires two inputs:\n",
    "1. Judgements from Human or AI evaluators, which we already have. \n",
    "2. A quantitative model score. We'll use GPT-4o mini to construct that now!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a probabilistic model with GPT logprobs\n",
    "\n",
    "Here, we'll put use the underlying probabilities from GPT to construct a numerical model score for each response. We'll start by creating a prompt that compares the two responses in MT-Bench:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_to_text(conversation_obj_list, assistant_label):\n",
    "    result_txt = \"\"\n",
    "    for conv_obj in conversation_obj_list:\n",
    "        result_txt += f\"{conv_obj['role'].upper()} {assistant_label.upper() if conv_obj['role'] == 'assistant' else ''}: {conv_obj['content']} \\n\"\n",
    "    return result_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "def produce_prompt_for_llm_evaluation(conversation_a, conversation_b):\n",
    "    prompt_preamble = f\"\"\"\n",
    "    You are a logical and accurate converation reading and grading AI system.\n",
    "    You will be shown two conversations between USER and ASSISTANT.\n",
    "    Read each conversation carefully and decide which one better complies with the USER's instructions\n",
    "    Please output ONLY \"A\" if the ASSISTANT in conversation A better complies with the USER's demands, and output only \"B\" if the ASSISTANT\n",
    "    in conversation B better complies with the USER's demands\n",
    "\n",
    "    <Answer A>\n",
    "    {conversation_to_text(conversation_a, \"a\")}\n",
    "    </Conversation A>\n",
    "\n",
    "    That was conversation A, here is conversation B:\n",
    "\n",
    "    <Conversation B>\n",
    "    {conversation_to_text(conversation_b, \"b\")}\n",
    "    </Conversation B>\n",
    "\n",
    "    Please respond with \"A\" if Assistant A was better and \"B\" if Assistant B was better. ONLY RETURN \"A\" OR \"B\"\n",
    "    \"\"\"\n",
    "    return dedent(prompt_preamble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = produce_prompt_for_llm_evaluation(combined_df['conversation_a'].iloc[0], combined_df['conversation_b'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a logical and accurate converation reading and grading AI system.\n",
      "    You will be shown two conversations between USER and ASSISTANT.\n",
      "    Read each conversation carefully and decide which one better complies with the USER's instructions\n",
      "    Please output ONLY \"A\" if the ASSISTANT in conversation A better complies with the USER's demands, and output only \"B\" if the ASSISTANT\n",
      "    in conversation B better complies with the USER's demands\n",
      "\n",
      "    <Answer A>\n",
      "    USER : Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions. \n",
      "ASSISTANT A: I recently had the pleasure of visiting Hawaii and it quickly became one of my favorite places. From the stunning beaches to the lush mountains, this place has it all. The people are incredibly friendly and the culture is alive and well. One of the highlights of my trip was visiting the Polynesian Cultural Center. Here, I was able to learn about the culture of the native Hawaiian people and try my hand at traditional crafts and activities. I also had a chance to explore some of the natural wonders of the island, including the breathtaking Hanauma Bay and the majestic Waimea Canyon. Whether you’re looking for a relaxing beach vacation or an adventure filled with culture and nature, Hawaii is the perfect destination. \n",
      "\n",
      "    </Conversation A>\n",
      "\n",
      "    That was conversation A, here is conversation B:\n",
      "\n",
      "    <Conversation B>\n",
      "    USER : Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions. \n",
      "ASSISTANT B: Here is a draft travel blog post about a recent trip to Hawaii:\n",
      "\n",
      "Aloha from the Hawaiian Islands! I recently had the opportunity to spend 10 days exploring the beautiful Hawaiian archipelago, soaking in the culture, natural scenery, and must-see attractions. Here are some of the highlights from my Hawaiian adventure:\n",
      "\n",
      "Experience Traditional Hawaiian Culture on Oahu\n",
      "\n",
      "No trip to Hawaii is complete without learning about the rich traditional Hawaiian culture. On Oahu, I visited the Polynesian Cultural Center, where I learned to hula dance, watched dramatic fire knife dancing performances, and took a canoe ride along a tropical river. I also toured ‘Iolani Palace, the only official royal residence in the U.S. and a National Historic Landmark. The palace provides insight into Hawaii's monarchy era in the late 19th century.\n",
      "\n",
      "Marvel at Massive Volcanoes on the Big Island \n",
      "\n",
      "The Big Island of Hawaii is a geologic wonderland, home to massive active volcanoes. I toured Hawaii Volcanoes National Park, where I hiked to the rim of the Kilauea caldera and saw the dramatic cliffs and craters of this active volcano. At night, the glow from the caldera illuminated the sky. I also visited massive Mauna Kea, a dormant volcano, where stargazing at the summit visitor center is spectacular due to the clear, high-altitude air. \n",
      "\n",
      "Relax on Pristine Beaches on Maui\n",
      "\n",
      "The Hawaiian islands are home to some of the most stunning beaches in the world, and Maui has a beach for every taste. I enjoyed the black sand beaches and massive sea cliffs of Waianapanapa State Park. For a quintessential Hawaiian beach scene, head to Makena Beach, also known as “Big Beach.” The wide golden sand beach is flanked by swaying palm trees and the turquoise waters of the Pacific. For a quieter beach, check out the red sand beach at Kaihalulu. The dramatic cliffs and sand provide a perfect backdrop for a peaceful beach day.\n",
      "\n",
      "There's so much natural beauty, culture, adventure, and relaxation to experience in Hawaii. I can't wait to return to the islands again! Aloha and mahalo! \n",
      "\n",
      "    </Conversation B>\n",
      "\n",
      "    Please respond with \"A\" if Assistant A was better and \"B\" if Assistant B was better. ONLY RETURN \"A\" OR \"B\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_completion_with_probs(client, prompt, model_name, seed=123, max_tokens=10, temperature=0, top_logprobs=5, **kwargs):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        seed=seed,\n",
    "        logprobs=True,\n",
    "        top_logprobs=top_logprobs,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    prob_dict = {\n",
    "        tlp.token: round(np.exp(tlp.logprob), 2)\n",
    "        for tlp in completion.choices[0].logprobs.content[0].top_logprobs\n",
    "    }\n",
    "    \n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0.95, 'B': 0.05}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=\"Please respond randomly with EITHER the letter A or B, NO OTHER WORDS\"\n",
    "\n",
    "get_completion_with_probs(client=openai_client,\n",
    "                      prompt=prompt,\n",
    "                      model_name=\"gpt-4o-mini\",\n",
    "                      top_logprobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Interestingly, these probabilities vary by model:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': 0.65, 'A': 0.35}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion_with_probs(client=openai_client,\n",
    "                      prompt=prompt,\n",
    "                      model_name=\"gpt-4\",\n",
    "                      top_logprobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0.75, 'B': 0.25}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion_with_probs(client=openai_client,\n",
    "                      prompt=prompt,\n",
    "                      model_name=\"gpt-4o\",\n",
    "                      top_logprobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MT-Bench, many of the examples are judged multiple times, but we only need to score each conversation once, so we'll drop duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_llm_df = combined_df.drop_duplicates(subset=['question_id', 'model_a', 'model_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nr/p0y8_yt54fvbzy49jprx4vzh0000gn/T/ipykernel_77531/932814148.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  for_llm_df.loc[:, 'conversation_prompt_text'] = for_llm_df.apply(\n"
     ]
    }
   ],
   "source": [
    "for_llm_df.loc[:, 'conversation_prompt_text'] = for_llm_df.apply(\n",
    "    lambda s: produce_prompt_for_llm_evaluation(s['conversation_a'], s['conversation_b']),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nr/p0y8_yt54fvbzy49jprx4vzh0000gn/T/ipykernel_77531/1557080612.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  for_llm_df.loc[:,'score_results'] = for_llm_df['conversation_prompt_text'].apply(lambda s: get_completion_with_probs(prompt=s, client=openai_client, model_name=\"gpt-4o\", max_tokens=10, top_logprobs=2))\n"
     ]
    }
   ],
   "source": [
    "for_llm_df.loc[:,'score_results'] = for_llm_df['conversation_prompt_text'].apply(lambda s: get_completion_with_probs(prompt=s, client=openai_client, model_name=\"gpt-4o\", max_tokens=10, top_logprobs=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract the model results for each conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>score_results</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">81</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">alpaca-13b</th>\n",
       "      <th>claude-v1</th>\n",
       "      <td>{'B': 1.0, 'A': 0.0}</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <td>{'B': 1.0, 'A': 0.0}</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4</th>\n",
       "      <td>{'B': 1.0, '\"B': 0.0}</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b-v1.2</th>\n",
       "      <td>{'B': 1.0, 'A': 0.0}</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <th>claude-v1</th>\n",
       "      <td>{'B': 0.73, 'A': 0.27}</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    score_results     A     B\n",
       "question_id model_a       model_b                                            \n",
       "81          alpaca-13b    claude-v1          {'B': 1.0, 'A': 0.0}  0.00  1.00\n",
       "                          gpt-3.5-turbo      {'B': 1.0, 'A': 0.0}  0.00  1.00\n",
       "                          gpt-4             {'B': 1.0, '\"B': 0.0}  0.00  1.00\n",
       "                          vicuna-13b-v1.2    {'B': 1.0, 'A': 0.0}  0.00  1.00\n",
       "            gpt-3.5-turbo claude-v1        {'B': 0.73, 'A': 0.27}  0.27  0.73"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_results_only = for_llm_df.set_index(['question_id', 'model_a', 'model_b'])[['score_results']]\n",
    "score_results_only['A'] = score_results_only['score_results'].apply(lambda d: d.get('A',0))\n",
    "score_results_only['B'] = score_results_only['score_results'].apply(lambda d: d.get('B',0))\n",
    "score_results_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_results_only = score_results_only[score_results_only.index.isin(combined_df_wide.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And can now feed the results into cleanlab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.multiannotator import get_label_quality_multiannotator\n",
    "\n",
    "results = get_label_quality_multiannotator(combined_df_wide, score_results_only[['A', 'B']].to_numpy(), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_results = results[\"label_quality\"]\n",
    "consensus_results[\"consensus_label\"] = consensus_results[\"consensus_label\"].apply(lambda i: {0:\"A\",1:\"B\"}.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>consensus_label</th>\n",
       "      <th>consensus_quality_score</th>\n",
       "      <th>annotator_agreement</th>\n",
       "      <th>num_annotations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">81</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">alpaca-13b</th>\n",
       "      <th>claude-v1</th>\n",
       "      <td>B</td>\n",
       "      <td>0.916097</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <td>B</td>\n",
       "      <td>0.916097</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4</th>\n",
       "      <td>B</td>\n",
       "      <td>0.916097</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b-v1.2</th>\n",
       "      <td>B</td>\n",
       "      <td>0.916097</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-v1</th>\n",
       "      <th>alpaca-13b</th>\n",
       "      <td>A</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       consensus_label  \\\n",
       "question_id model_a    model_b                           \n",
       "81          alpaca-13b claude-v1                     B   \n",
       "                       gpt-3.5-turbo                 B   \n",
       "                       gpt-4                         B   \n",
       "                       vicuna-13b-v1.2               B   \n",
       "            claude-v1  alpaca-13b                    A   \n",
       "\n",
       "                                        consensus_quality_score  \\\n",
       "question_id model_a    model_b                                    \n",
       "81          alpaca-13b claude-v1                       0.916097   \n",
       "                       gpt-3.5-turbo                   0.916097   \n",
       "                       gpt-4                           0.916097   \n",
       "                       vicuna-13b-v1.2                 0.916097   \n",
       "            claude-v1  alpaca-13b                      0.916095   \n",
       "\n",
       "                                        annotator_agreement  num_annotations  \n",
       "question_id model_a    model_b                                                \n",
       "81          alpaca-13b claude-v1                        1.0                1  \n",
       "                       gpt-3.5-turbo                    1.0                3  \n",
       "                       gpt-4                            1.0                1  \n",
       "                       vicuna-13b-v1.2                  1.0                2  \n",
       "            claude-v1  alpaca-13b                       1.0                1  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consensus_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The produced consensus label here comes with a confidence score, which can be used to understand the reliability of the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>annotator_quality</th>\n",
       "      <th>agreement_with_consensus</th>\n",
       "      <th>worst_class</th>\n",
       "      <th>num_examples_labeled</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>judge</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">winner_binary</th>\n",
       "      <th>gpt4_pair</th>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.982993</td>\n",
       "      <td>A</td>\n",
       "      <td>882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_4</th>\n",
       "      <td>0.943396</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>B</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>B</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_24</th>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>B</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_30</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_54</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_52</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         annotator_quality  agreement_with_consensus  \\\n",
       "              judge                                                    \n",
       "winner_binary gpt4_pair           0.962963                  0.982993   \n",
       "              author_4            0.943396                  0.957746   \n",
       "              author_0            1.000000                  1.000000   \n",
       "              expert_0            1.000000                  1.000000   \n",
       "              expert_24           0.975000                  0.982759   \n",
       "...                                    ...                       ...   \n",
       "              expert_30           1.000000                  1.000000   \n",
       "              expert_54           1.000000                  1.000000   \n",
       "              author_1            0.500000                  0.500000   \n",
       "              expert_18           1.000000                  1.000000   \n",
       "              expert_52           1.000000                  1.000000   \n",
       "\n",
       "                        worst_class  num_examples_labeled  \n",
       "              judge                                        \n",
       "winner_binary gpt4_pair           A                   882  \n",
       "              author_4            B                    71  \n",
       "              author_0            A                    65  \n",
       "              expert_0            B                    58  \n",
       "              expert_24           B                    58  \n",
       "...                             ...                   ...  \n",
       "              expert_30           A                     3  \n",
       "              expert_54           A                     3  \n",
       "              author_1            A                     2  \n",
       "              expert_18           A                     2  \n",
       "              expert_52           A                     1  \n",
       "\n",
       "[66 rows x 4 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"annotator_stats\"][\"worst_class\"] = results[\"annotator_stats\"][\"worst_class\"].apply(lambda i: {0:\"A\",1:\"B\"}.get(i))\n",
    "results[\"annotator_stats\"].sort_values(\"num_examples_labeled\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a more sophisticated algorithm helps us to estimate the quality of the judges and the true answer to the problem.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "The traditional consensus score is a simple and easy-to-understand method for combining multiple evaluators. However, it does not take into account the quality of the judges.\n",
    "\n",
    "The quality of the CROWDLAB algorithm depends on the quality of the model scores. If the model scores are not directionally accurate, or are predisposed towards a certain reviewer, the CROWDLAB algorithm will not be able to accurately estimate the quality of the judges and the true answer to the problem.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated two methods for combining multiple evaluators (human or LLM-as-Judge) with GPT logprobs. We showed that the CROWDLAB algorithm can be used to estimate the quality of the judges and the true answer to the problem. We also showed that the quality of the CROWDLAB algorithm depends on the quality of the model scores. If the model scores are not accurate, the CROWDLAB algorithm will not be able to accurately estimate the quality of the judges and the true answer to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks\n",
    "](https://arxiv.org/abs/2406.18403v1) - Bavaresco et al. Published June 2024\n",
    "- [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) - Zheng, Lianmin, et al. Published December 2024\n",
    "- [CROWDLAB: Supervised learning to infer consensus labels and quality scores for data with multiple annotators](https://arxiv.org/abs/2210.06812) - Goh et al. Published January 2023\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
