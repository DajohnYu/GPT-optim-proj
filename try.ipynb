{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__init__': 'def __init__(\\n        self,\\n\\n        source_df: DataFrame,\\n\\n        spark_context: SparkSession.builder.getOrCreate,\\n\\n        config_path: str,\\n\\n        file_name: str,\\n\\n        src_system: str,\\n\\n    ) -> None:\\n\\n        \"\"\"\\n\\n        A class checking the quality of a source data frame based on the given criteria.\\n\\n        :param source_df (DataFrame): the source data frame to apply the data quality checks on.\\n\\n        :param spark_context (SparkSession.builder.getOrCreate): the spark session to run the data quality check.\\n\\n        :param config_path (str): the path to config csv file.\\n\\n        :param file_name (str): the name of the file to check the data quality check on\\n\\n        :param src_system (str): the source of the file where it comes from (vendor)\\n\\n        :raise KeyError: raise a key error if the columns in the source data frame is not in the config file.\\n\\n        \"\"\"\\n\\n        # Set variables\\n\\n        self.spark = spark_context\\n\\n        self.source_df = source_df\\n\\n\\n\\n        self.error_df = None\\n\\n        self.error_columns = []\\n\\n        self.error_counter = 0\\n\\n        self.schema_dict = {\\n\\n            \"StringType\": StringType,\\n\\n            \"DateType\": DateType,\\n\\n            \"IntegerType\": IntegerType,\\n\\n            \"FloatType\": FloatType,\\n\\n            \"DoubleType\": DoubleType,\\n\\n        }\\n\\n\\n\\n        # Initial configuration\\n\\n        config_content = self.read_s3_file(config_path).decode()\\n\\n        self.config = self.resolve_config(config_path.replace(\"config.json\", \"env.json\"), json.loads(config_content))\\n\\n\\n\\n        dq_rule_path = self.config[src_system][\"dq_rule_path\"]\\n\\n        # dq_rule_content = self.read_s3_file(dq_rule_path)\\n\\n        self.rule_df = pd.read_csv(dq_rule_path, index_col=\"column_name\")\\n\\n        self.file_name = file_name\\n\\n        self.rule_df = self.rule_df[(self.rule_df[\"file_name\"] == self.file_name)]\\n\\n        self.rule_df = self.rule_df.applymap(lambda x: x if x else np.nan)\\n\\n        self.rule_df.sort_index(inplace=True)\\n\\n        self.sns_message = []\\n\\n\\n\\n        self.input_columns = self.source_df.columns\\n\\n        self.output_columns = self.config[src_system][\"sources\"][self.file_name][\"dq_output_columns\"]\\n\\n        for index in range(len(self.input_columns)):\\n\\n            if \".\" in self.input_columns[index]:\\n\\n                self.input_columns[index] = \"`\" + self.input_columns[index] + \"`\"\\n\\n\\n\\n        missed_columns = set(self.input_columns) - set(self.rule_df.index)\\n\\n        if len(missed_columns) > 0:\\n\\n            logger.warning(f\"[{missed_columns}] are not found in the rule file.\")\\n\\n\\n\\n        self.spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\\n\\n        # self.spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\\n\\n        self.spark.conf.set(\"spark.sql.adaptive.enabled\", True)\\n\\n        self.spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\\n\\n\\n\\n    def read_s3_file(self, file_path) -> bytes:\\n\\n        \"\"\"\\n\\n        Read s3 file content and return it in byte format\\n\\n        :param file_path: full s3 object path\\n\\n        :return byte content of the file\\n\\n        \"\"\"\\n\\n        file_res = urlparse(file_path)\\n\\n        try:\\n\\n            file_obj = s3_resource.Object(file_res.netloc, file_res.path.lstrip(\"/\"))\\n\\n            return file_obj.get()[\"Body\"].read()\\n\\n        except ClientError:\\n\\n            raise FileNotFoundError(f\"File cannot be found in S3 given path \\'{file_path}\\'\")\\n\\n\\n\\n    def resolve_config(self, env_path, config_content):\\n\\n        \"\"\"\\n\\n        Read config content and resolve env variables\\n\\n        :param env_path: environment file path\\n\\n        :param config_content: environment agnostic config file\\n\\n        :return environmentally resolved config\\n\\n        \"\"\"\\n\\n        env_content = self.read_s3_file(env_path).decode()\\n\\n        env_sub = json.loads(env_content)[\"subs\"]\\n\\n\\n\\n        config_content_str = (\\n\\n            str(config_content)\\n\\n            .replace(\"<env>\", env_sub[\"<env>\"])\\n\\n            .replace(\"<_env>\", env_sub[\"<_env>\"])\\n\\n            .replace(\"<bucket_name>\", env_sub[\"<bucket_name>\"])\\n\\n            .replace(\"<account>\", env_sub[\"<account>\"])\\n\\n        )\\n\\n\\n\\n        return ast.literal_eval(config_content_str)\\n\\n\\n', 'read_s3_file': 'def read_s3_file(self, file_path) -> bytes:\\n        \"\"\"\\n\\n        Read s3 file content and return it in byte format\\n\\n        :param file_path: full s3 object path\\n\\n        :return byte content of the file\\n\\n        \"\"\"\\n\\n        file_res = urlparse(file_path)\\n\\n        try:\\n\\n            file_obj = s3_resource.Object(file_res.netloc, file_res.path.lstrip(\"/\"))\\n\\n            return file_obj.get()[\"Body\"].read()\\n\\n        except ClientError:\\n\\n            raise FileNotFoundError(f\"File cannot be found in S3 given path \\'{file_path}\\'\")\\n\\n\\n\\n    def resolve_config(self, env_path, config_content):\\n\\n        \"\"\"\\n\\n        Read config content and resolve env variables\\n\\n        :param env_path: environment file path\\n\\n        :param config_content: environment agnostic config file\\n\\n        :return environmentally resolved config\\n\\n        \"\"\"\\n\\n        env_content = self.read_s3_file(env_path).decode()\\n\\n        env_sub = json.loads(env_content)[\"subs\"]\\n\\n\\n\\n        config_content_str = (\\n\\n            str(config_content)\\n\\n            .replace(\"<env>\", env_sub[\"<env>\"])\\n\\n            .replace(\"<_env>\", env_sub[\"<_env>\"])\\n\\n            .replace(\"<bucket_name>\", env_sub[\"<bucket_name>\"])\\n\\n            .replace(\"<account>\", env_sub[\"<account>\"])\\n\\n        )\\n\\n\\n\\n        return ast.literal_eval(config_content_str)\\n\\n\\n', 'resolve_config': 'def resolve_config(self, env_path, config_content):\\n        \"\"\"\\n\\n        Read config content and resolve env variables\\n\\n        :param env_path: environment file path\\n\\n        :param config_content: environment agnostic config file\\n\\n        :return environmentally resolved config\\n\\n        \"\"\"\\n\\n        env_content = self.read_s3_file(env_path).decode()\\n\\n        env_sub = json.loads(env_content)[\"subs\"]\\n\\n\\n\\n        config_content_str = (\\n\\n            str(config_content)\\n\\n            .replace(\"<env>\", env_sub[\"<env>\"])\\n\\n            .replace(\"<_env>\", env_sub[\"<_env>\"])\\n\\n            .replace(\"<bucket_name>\", env_sub[\"<bucket_name>\"])\\n\\n            .replace(\"<account>\", env_sub[\"<account>\"])\\n\\n        )\\n\\n\\n\\n        return ast.literal_eval(config_content_str)\\n\\n\\n', 'is_float': 'def is_float(element) -> bool:\\n        \"\"\"\\n\\n        Check if the given input can be returned as float or not.\\n\\n        :param element: The input which can be anything (_type_).\\n\\n        :return bool: whether it is float (True) or not (False).\\n\\n        \"\"\"\\n\\n        try:\\n\\n            float(element)\\n\\n            return True\\n\\n        except ValueError:\\n\\n            return False\\n\\n\\n\\n    def limit_finder(self, input_col: str, rule_value: Union[str, int, float]) -> Union[float, Column, None]:\\n\\n        \"\"\"\\n\\n        Finds the limit based on the given column. If it is a number it returns the number or if it is a column it returns the f.col.\\n\\n        :param input_col: the column to check this condition on\\n\\n        :param rule_value: value of the limit no matter the datatype\\n\\n        :return Union[str, Column, None]: whether a float or a column in order to check for range check.\\n\\n        :raise KeyError: if the input_col needs other columns that is not in the dataset it will raise an error.\\n\\n        \"\"\"\\n\\n        if self.is_float(rule_value):\\n\\n            rule_value = float(rule_value)\\n\\n            if math.isnan(rule_value):\\n\\n                return None\\n\\n            else:\\n\\n                return rule_value\\n\\n        elif type(rule_value) == str:\\n\\n            if rule_value not in self.input_columns:\\n\\n                print(rule_value)\\n\\n                self.sns_message.append(\\n\\n                    f\"column {rule_value} is not in report {self.file_name} while it is {input_col} needed for range check\"\\n\\n                )\\n\\n                return None\\n\\n            return f.col(rule_value)\\n\\n\\n\\n    def columns_to_check(self, criteria: str) -> Index:\\n\\n        \"\"\"\\n\\n        Returns the indexes to be used while working on the check rules.\\n\\n        :param criteria: whether it is data type, nullable, etc.\\n\\n        :return Index: the index of the columns that this condition should be met.\\n\\n        \"\"\"\\n\\n        return self.rule_df[(self.rule_df[criteria]).notna()].index\\n\\n\\n\\n    def add_error_col(self, error_msg: str, condition: Column, error_col_name: str) -> None:\\n\\n        \"\"\"\\n\\n        Add an error column based on the condition to filter and the error message\\n\\n        :param error_msg: the error message to be added if the condition is met\\n\\n        :param condition: the condition of the error to be met in order to return the error column\\n\\n        :param error_col_name: the name of the error column\\n\\n        \"\"\"\\n\\n        if condition is not None and error_col_name and error_msg:\\n\\n            col_condition = f.when(condition, f.lit(error_msg)).otherwise(f.lit(None))\\n\\n            error_col_name = error_col_name + str(self.error_counter)\\n\\n            self.source_df = self.source_df.withColumn(error_col_name, col_condition)\\n\\n            self.error_columns.append(f.col(error_col_name))\\n\\n            self.error_counter += 1\\n\\n\\n\\n    def data_type_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks the data type of all columns and give an error column for each column\\n\\n        :param input_col: the column to apply this check.\\n\\n        \"\"\"\\n\\n        print(\"start data type check\")\\n\\n        dtype_key = self.rule_df.loc[input_col, \"type\"]\\n\\n        if dtype_key == \"DateType\":\\n\\n            date_format = self.rule_df.loc[input_col, \"date_format\"]\\n\\n            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.to_date(f.col(input_col), date_format))\\n\\n            # self.source_df.select(input_col + \\' schema\\').cache()\\n\\n        else:\\n\\n            dtype = self.schema_dict[dtype_key]()\\n\\n            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.col(input_col).cast(dtype))\\n\\n            # self.source_df.select(input_col + \\' schema\\').cache()\\n\\n        # dtype_cond should check if the given input_col is not null and the one with schema is null.\\n\\n        dtype_cond = ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")) & (\\n\\n            f.col(input_col + \" schema\").isNull()\\n\\n        )\\n\\n        type_error_msg = f\"data_type_FAIL: Column [{input_col}] should be {dtype_key}\"\\n\\n        self.add_error_col(error_msg=type_error_msg, condition=dtype_cond, error_col_name=input_col + \" type_check\")\\n\\n        logger.info(f\"[{input_col}] dtype check is done.\")\\n\\n\\n\\n    def null_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The condition for a null check.\\n\\n        :param input_col: the column to apply the check on.\\n\\n        :raise Column: The not null condition column.\\n\\n        \"\"\"\\n\\n        return (f.col(input_col) == \"\") | (f.col(input_col).isNull())\\n\\n\\n\\n    def null_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks not nullable columns and give an error column for input_col\\n\\n        :param input_col: The column to apply the null check.\\n\\n        \"\"\"\\n\\n        print(\"start null_check\")\\n\\n        if not math.isnan(self.rule_df.loc[input_col, \"nullable\"]):\\n\\n            return\\n\\n        null_condition = self.null_cond_syntax(input_col)\\n\\n        null_error_msg = f\"null_FAIL: Column [{input_col}] cannot be null\"\\n\\n        self.add_error_col(error_msg=null_error_msg, condition=null_condition, error_col_name=input_col + \" null_check\")\\n\\n        logger.info(f\"[{input_col}] null check is done.\")\\n\\n\\n\\n    def sum_check_syntax(self, input_col1: str, input_col2: str, syntax_value: float) -> Column:\\n\\n        \"\"\"\\n\\n        The syntax to check for sum of 2 columns to equal a syntax value, to be used in conditional checks.\\n\\n        :param input_col1: column 1 to check\\n\\n        :param input_col2: column 2 to check\\n\\n        :param syntax_value: column value that column 1 and 2 should equal to.\\n\\n        :return Column: The sum_check Column condition.\\n\\n        \"\"\"\\n\\n        return ~(f.col(input_col1) + f.col(input_col2) != syntax_value)\\n\\n\\n\\n    def conditional_cond_syntax(self, input_col: str, condition_column: str, conditional_variables: str) -> Column:\\n\\n        \"\"\"\\n\\n        Generates a Column condition given input column and the condition column.\\n\\n        :param input_col: The column in which the current syntax is applied on\\n\\n        :param condition_column: the second column which might be used in this conditional check.\\n\\n        :param conditional_variables: The variables of the check. It could be __NOT__NULL__, a string of comma separated variables or a single number.\\n\\n        :return Column: The conditional column condition.\\n\\n        \"\"\"\\n\\n        not_category = []\\n\\n        if conditional_variables == \"__NOT__NULL__\":\\n\\n            category_cond = ~self.null_cond_syntax(input_col)\\n\\n            conditional_msg = f\"{input_col} is not null,\"\\n\\n            return category_cond, conditional_msg\\n\\n        elif self.is_float(conditional_variables):\\n\\n            category_cond = self.sum_check_syntax(\\n\\n                input_col + \" schema\", condition_column + \" schema\", conditional_variables\\n\\n            )\\n\\n            conditional_msg = f\"[{input_col}] and [{condition_column}] sum is not equal to {conditional_variables}\"\\n\\n            return category_cond, conditional_msg\\n\\n        else:\\n\\n            category_list = conditional_variables.split(\",\")\\n\\n            for ind, value in enumerate(category_list):\\n\\n                if \"__NOT__\" == value[: len(\"__NOT__\")]:\\n\\n                    not_category.append(category_list.pop(ind)[len(\"__NOT__\") :])\\n\\n\\n\\n            category_cond = f.col(input_col).isin(category_list) == True\\n\\n            if not_category:\\n\\n                category_cond = category_cond & (f.col(input_col).isin(not_category) == False)\\n\\n            conditional_msg = f\"[{input_col}] is in category ({category_list}),)\"\\n\\n            return category_cond, conditional_msg\\n\\n\\n\\n    def conditional_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks all the conditional columns in a single row in a config csv file.\\n\\n        :param input_col: The column to apply the check on.\\n\\n        \"\"\"\\n\\n        multiple_conditional_list = self.rule_df.loc[input_col, \"conditional_columns\"].split(\";\")\\n\\n        for cond_ind, condition_columns in enumerate(multiple_conditional_list):\\n\\n            current_additional_cond_value = self.rule_df.loc[input_col, \"conditional_column_value\"].split(\";\")[cond_ind]\\n\\n            current_conditional_valuelist = self.rule_df.loc[input_col, \"conditional_valuelist\"].split(\";\")[cond_ind]\\n\\n\\n\\n            first_col_cond, first_col_msg = self.conditional_cond_syntax(\\n\\n                input_col=input_col,\\n\\n                condition_column=condition_columns,\\n\\n                conditional_variables=current_conditional_valuelist,\\n\\n            )\\n\\n            for condition_column in condition_columns.split(\",\"):\\n\\n                if condition_column in self.input_columns:\\n\\n                    second_cal_cond, second_col_msg = self.conditional_cond_syntax(\\n\\n                        input_col=condition_column,\\n\\n                        condition_column=input_col,\\n\\n                        conditional_variables=current_additional_cond_value,\\n\\n                    )\\n\\n                    conditional_cond = first_col_cond & (~second_cal_cond)\\n\\n                    conditional_error_msg = f\"Cond_fail: {first_col_msg}+{second_col_msg}\"\\n\\n                    self.add_error_col(\\n\\n                        error_msg=conditional_error_msg,\\n\\n                        condition=conditional_cond,\\n\\n                        error_col_name=input_col + \" conditional_check\",\\n\\n                    )\\n\\n                else:\\n\\n                    self.sns_message.append(\\n\\n                        f\"Column {condition_column} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                    )\\n\\n        logger.info(f\"[{input_col}] conditional check is done.\")\\n\\n\\n\\n    def range_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The range check column condition\\n\\n        :param input_col: The column to apply the check on\\n\\n        :return Column: The range check column condition.\\n\\n        \"\"\"\\n\\n        schema_col = input_col + \" schema\"\\n\\n        output_cond = None\\n\\n\\n\\n        min_str = self.rule_df.loc[input_col, \"min\"]\\n\\n        min_value = self.limit_finder(input_col, min_str)\\n\\n        if min_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) < min_value)\\n\\n        max_str = self.rule_df.loc[input_col, \"max\"]\\n\\n        max_value = self.limit_finder(input_col, max_str)\\n\\n        if max_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) > max_value)\\n\\n        return min_str, max_str, output_cond\\n\\n\\n\\n    def range_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with a range check and add an error column to self.source_df.\\n\\n        Args:\\n\\n            input_col (str): the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start range_check\")\\n\\n        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\\n\\n        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] range check is done.\")\\n\\n\\n\\n    def file_check(self, input_col):\\n\\n        # finding source side columns\\n\\n        source_df_columns_list = [input_col]\\n\\n        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\\n\\n        if type(reference_columns_str) == str:\\n\\n            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\\n\\n            source_df_columns_list.extend(additional_columns_list)\\n\\n\\n\\n        # Find reference side columns\\n\\n        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\\n\\n        print(\"file check type\", file_check_type)\\n\\n        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\\n\\n        reference_columns = current_file_config[\"reference_columns\"]\\n\\n        print(\"ref cols\", reference_columns)\\n\\n\\n\\n        # Read reference file\\n\\n        file_path = current_file_config[\"path\"]\\n\\n        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\\n\\n        # changing columns names on reference to be the same as source\\n\\n        print(\"src df cols list\", source_df_columns_list)\\n\\n        for col_ind, ref_col in enumerate(reference_columns):\\n\\n            source_col = source_df_columns_list[col_ind]\\n\\n            try:\\n\\n                file_df = file_df.withColumnRenamed(ref_col, source_col)\\n\\n                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                if \"Postal Code\".upper() in source_col.upper():\\n\\n                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\\n\\n            except AnalysisException:\\n\\n                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\\n\\n                return None, None\\n\\n\\n\\n        pre_join_columns = set(self.source_df.columns)\\n\\n        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\\n\\n        post_join_columns = set(self.source_df.columns)\\n\\n        join_col = tuple(post_join_columns - pre_join_columns)[0]\\n\\n        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        file_error_msg = (\\n\\n            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\\n\\n        )\\n\\n        return file_cond, file_error_msg\\n\\n\\n\\n    def category_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'limit_finder': 'def limit_finder(self, input_col: str, rule_value: Union[str, int, float]) -> Union[float, Column, None]:\\n        \"\"\"\\n\\n        Finds the limit based on the given column. If it is a number it returns the number or if it is a column it returns the f.col.\\n\\n        :param input_col: the column to check this condition on\\n\\n        :param rule_value: value of the limit no matter the datatype\\n\\n        :return Union[str, Column, None]: whether a float or a column in order to check for range check.\\n\\n        :raise KeyError: if the input_col needs other columns that is not in the dataset it will raise an error.\\n\\n        \"\"\"\\n\\n        if self.is_float(rule_value):\\n\\n            rule_value = float(rule_value)\\n\\n            if math.isnan(rule_value):\\n\\n                return None\\n\\n            else:\\n\\n                return rule_value\\n\\n        elif type(rule_value) == str:\\n\\n            if rule_value not in self.input_columns:\\n\\n                print(rule_value)\\n\\n                self.sns_message.append(\\n\\n                    f\"column {rule_value} is not in report {self.file_name} while it is {input_col} needed for range check\"\\n\\n                )\\n\\n                return None\\n\\n            return f.col(rule_value)\\n\\n\\n\\n    def columns_to_check(self, criteria: str) -> Index:\\n\\n        \"\"\"\\n\\n        Returns the indexes to be used while working on the check rules.\\n\\n        :param criteria: whether it is data type, nullable, etc.\\n\\n        :return Index: the index of the columns that this condition should be met.\\n\\n        \"\"\"\\n\\n        return self.rule_df[(self.rule_df[criteria]).notna()].index\\n\\n\\n\\n    def add_error_col(self, error_msg: str, condition: Column, error_col_name: str) -> None:\\n\\n        \"\"\"\\n\\n        Add an error column based on the condition to filter and the error message\\n\\n        :param error_msg: the error message to be added if the condition is met\\n\\n        :param condition: the condition of the error to be met in order to return the error column\\n\\n        :param error_col_name: the name of the error column\\n\\n        \"\"\"\\n\\n        if condition is not None and error_col_name and error_msg:\\n\\n            col_condition = f.when(condition, f.lit(error_msg)).otherwise(f.lit(None))\\n\\n            error_col_name = error_col_name + str(self.error_counter)\\n\\n            self.source_df = self.source_df.withColumn(error_col_name, col_condition)\\n\\n            self.error_columns.append(f.col(error_col_name))\\n\\n            self.error_counter += 1\\n\\n\\n\\n    def data_type_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks the data type of all columns and give an error column for each column\\n\\n        :param input_col: the column to apply this check.\\n\\n        \"\"\"\\n\\n        print(\"start data type check\")\\n\\n        dtype_key = self.rule_df.loc[input_col, \"type\"]\\n\\n        if dtype_key == \"DateType\":\\n\\n            date_format = self.rule_df.loc[input_col, \"date_format\"]\\n\\n            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.to_date(f.col(input_col), date_format))\\n\\n            # self.source_df.select(input_col + \\' schema\\').cache()\\n\\n        else:\\n\\n            dtype = self.schema_dict[dtype_key]()\\n\\n            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.col(input_col).cast(dtype))\\n\\n            # self.source_df.select(input_col + \\' schema\\').cache()\\n\\n        # dtype_cond should check if the given input_col is not null and the one with schema is null.\\n\\n        dtype_cond = ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")) & (\\n\\n            f.col(input_col + \" schema\").isNull()\\n\\n        )\\n\\n        type_error_msg = f\"data_type_FAIL: Column [{input_col}] should be {dtype_key}\"\\n\\n        self.add_error_col(error_msg=type_error_msg, condition=dtype_cond, error_col_name=input_col + \" type_check\")\\n\\n        logger.info(f\"[{input_col}] dtype check is done.\")\\n\\n\\n\\n    def null_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The condition for a null check.\\n\\n        :param input_col: the column to apply the check on.\\n\\n        :raise Column: The not null condition column.\\n\\n        \"\"\"\\n\\n        return (f.col(input_col) == \"\") | (f.col(input_col).isNull())\\n\\n\\n\\n    def null_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks not nullable columns and give an error column for input_col\\n\\n        :param input_col: The column to apply the null check.\\n\\n        \"\"\"\\n\\n        print(\"start null_check\")\\n\\n        if not math.isnan(self.rule_df.loc[input_col, \"nullable\"]):\\n\\n            return\\n\\n        null_condition = self.null_cond_syntax(input_col)\\n\\n        null_error_msg = f\"null_FAIL: Column [{input_col}] cannot be null\"\\n\\n        self.add_error_col(error_msg=null_error_msg, condition=null_condition, error_col_name=input_col + \" null_check\")\\n\\n        logger.info(f\"[{input_col}] null check is done.\")\\n\\n\\n\\n    def sum_check_syntax(self, input_col1: str, input_col2: str, syntax_value: float) -> Column:\\n\\n        \"\"\"\\n\\n        The syntax to check for sum of 2 columns to equal a syntax value, to be used in conditional checks.\\n\\n        :param input_col1: column 1 to check\\n\\n        :param input_col2: column 2 to check\\n\\n        :param syntax_value: column value that column 1 and 2 should equal to.\\n\\n        :return Column: The sum_check Column condition.\\n\\n        \"\"\"\\n\\n        return ~(f.col(input_col1) + f.col(input_col2) != syntax_value)\\n\\n\\n\\n    def conditional_cond_syntax(self, input_col: str, condition_column: str, conditional_variables: str) -> Column:\\n\\n        \"\"\"\\n\\n        Generates a Column condition given input column and the condition column.\\n\\n        :param input_col: The column in which the current syntax is applied on\\n\\n        :param condition_column: the second column which might be used in this conditional check.\\n\\n        :param conditional_variables: The variables of the check. It could be __NOT__NULL__, a string of comma separated variables or a single number.\\n\\n        :return Column: The conditional column condition.\\n\\n        \"\"\"\\n\\n        not_category = []\\n\\n        if conditional_variables == \"__NOT__NULL__\":\\n\\n            category_cond = ~self.null_cond_syntax(input_col)\\n\\n            conditional_msg = f\"{input_col} is not null,\"\\n\\n            return category_cond, conditional_msg\\n\\n        elif self.is_float(conditional_variables):\\n\\n            category_cond = self.sum_check_syntax(\\n\\n                input_col + \" schema\", condition_column + \" schema\", conditional_variables\\n\\n            )\\n\\n            conditional_msg = f\"[{input_col}] and [{condition_column}] sum is not equal to {conditional_variables}\"\\n\\n            return category_cond, conditional_msg\\n\\n        else:\\n\\n            category_list = conditional_variables.split(\",\")\\n\\n            for ind, value in enumerate(category_list):\\n\\n                if \"__NOT__\" == value[: len(\"__NOT__\")]:\\n\\n                    not_category.append(category_list.pop(ind)[len(\"__NOT__\") :])\\n\\n\\n\\n            category_cond = f.col(input_col).isin(category_list) == True\\n\\n            if not_category:\\n\\n                category_cond = category_cond & (f.col(input_col).isin(not_category) == False)\\n\\n            conditional_msg = f\"[{input_col}] is in category ({category_list}),)\"\\n\\n            return category_cond, conditional_msg\\n\\n\\n\\n    def conditional_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks all the conditional columns in a single row in a config csv file.\\n\\n        :param input_col: The column to apply the check on.\\n\\n        \"\"\"\\n\\n        multiple_conditional_list = self.rule_df.loc[input_col, \"conditional_columns\"].split(\";\")\\n\\n        for cond_ind, condition_columns in enumerate(multiple_conditional_list):\\n\\n            current_additional_cond_value = self.rule_df.loc[input_col, \"conditional_column_value\"].split(\";\")[cond_ind]\\n\\n            current_conditional_valuelist = self.rule_df.loc[input_col, \"conditional_valuelist\"].split(\";\")[cond_ind]\\n\\n\\n\\n            first_col_cond, first_col_msg = self.conditional_cond_syntax(\\n\\n                input_col=input_col,\\n\\n                condition_column=condition_columns,\\n\\n                conditional_variables=current_conditional_valuelist,\\n\\n            )\\n\\n            for condition_column in condition_columns.split(\",\"):\\n\\n                if condition_column in self.input_columns:\\n\\n                    second_cal_cond, second_col_msg = self.conditional_cond_syntax(\\n\\n                        input_col=condition_column,\\n\\n                        condition_column=input_col,\\n\\n                        conditional_variables=current_additional_cond_value,\\n\\n                    )\\n\\n                    conditional_cond = first_col_cond & (~second_cal_cond)\\n\\n                    conditional_error_msg = f\"Cond_fail: {first_col_msg}+{second_col_msg}\"\\n\\n                    self.add_error_col(\\n\\n                        error_msg=conditional_error_msg,\\n\\n                        condition=conditional_cond,\\n\\n                        error_col_name=input_col + \" conditional_check\",\\n\\n                    )\\n\\n                else:\\n\\n                    self.sns_message.append(\\n\\n                        f\"Column {condition_column} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                    )\\n\\n        logger.info(f\"[{input_col}] conditional check is done.\")\\n\\n\\n\\n    def range_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The range check column condition\\n\\n        :param input_col: The column to apply the check on\\n\\n        :return Column: The range check column condition.\\n\\n        \"\"\"\\n\\n        schema_col = input_col + \" schema\"\\n\\n        output_cond = None\\n\\n\\n\\n        min_str = self.rule_df.loc[input_col, \"min\"]\\n\\n        min_value = self.limit_finder(input_col, min_str)\\n\\n        if min_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) < min_value)\\n\\n        max_str = self.rule_df.loc[input_col, \"max\"]\\n\\n        max_value = self.limit_finder(input_col, max_str)\\n\\n        if max_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) > max_value)\\n\\n        return min_str, max_str, output_cond\\n\\n\\n\\n    def range_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with a range check and add an error column to self.source_df.\\n\\n        Args:\\n\\n            input_col (str): the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start range_check\")\\n\\n        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\\n\\n        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] range check is done.\")\\n\\n\\n\\n    def file_check(self, input_col):\\n\\n        # finding source side columns\\n\\n        source_df_columns_list = [input_col]\\n\\n        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\\n\\n        if type(reference_columns_str) == str:\\n\\n            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\\n\\n            source_df_columns_list.extend(additional_columns_list)\\n\\n\\n\\n        # Find reference side columns\\n\\n        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\\n\\n        print(\"file check type\", file_check_type)\\n\\n        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\\n\\n        reference_columns = current_file_config[\"reference_columns\"]\\n\\n        print(\"ref cols\", reference_columns)\\n\\n\\n\\n        # Read reference file\\n\\n        file_path = current_file_config[\"path\"]\\n\\n        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\\n\\n        # changing columns names on reference to be the same as source\\n\\n        print(\"src df cols list\", source_df_columns_list)\\n\\n        for col_ind, ref_col in enumerate(reference_columns):\\n\\n            source_col = source_df_columns_list[col_ind]\\n\\n            try:\\n\\n                file_df = file_df.withColumnRenamed(ref_col, source_col)\\n\\n                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                if \"Postal Code\".upper() in source_col.upper():\\n\\n                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\\n\\n            except AnalysisException:\\n\\n                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\\n\\n                return None, None\\n\\n\\n\\n        pre_join_columns = set(self.source_df.columns)\\n\\n        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\\n\\n        post_join_columns = set(self.source_df.columns)\\n\\n        join_col = tuple(post_join_columns - pre_join_columns)[0]\\n\\n        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        file_error_msg = (\\n\\n            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\\n\\n        )\\n\\n        return file_cond, file_error_msg\\n\\n\\n\\n    def category_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'columns_to_check': 'def columns_to_check(self, criteria: str) -> Index:\\n        \"\"\"\\n\\n        Returns the indexes to be used while working on the check rules.\\n\\n        :param criteria: whether it is data type, nullable, etc.\\n\\n        :return Index: the index of the columns that this condition should be met.\\n\\n        \"\"\"\\n\\n        return self.rule_df[(self.rule_df[criteria]).notna()].index\\n\\n\\n\\n    def add_error_col(self, error_msg: str, condition: Column, error_col_name: str) -> None:\\n\\n        \"\"\"\\n\\n        Add an error column based on the condition to filter and the error message\\n\\n        :param error_msg: the error message to be added if the condition is met\\n\\n        :param condition: the condition of the error to be met in order to return the error column\\n\\n        :param error_col_name: the name of the error column\\n\\n        \"\"\"\\n\\n        if condition is not None and error_col_name and error_msg:\\n\\n            col_condition = f.when(condition, f.lit(error_msg)).otherwise(f.lit(None))\\n\\n            error_col_name = error_col_name + str(self.error_counter)\\n\\n            self.source_df = self.source_df.withColumn(error_col_name, col_condition)\\n\\n            self.error_columns.append(f.col(error_col_name))\\n\\n            self.error_counter += 1\\n\\n\\n\\n    def data_type_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks the data type of all columns and give an error column for each column\\n\\n        :param input_col: the column to apply this check.\\n\\n        \"\"\"\\n\\n        print(\"start data type check\")\\n\\n        dtype_key = self.rule_df.loc[input_col, \"type\"]\\n\\n        if dtype_key == \"DateType\":\\n\\n            date_format = self.rule_df.loc[input_col, \"date_format\"]\\n\\n            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.to_date(f.col(input_col), date_format))\\n\\n            # self.source_df.select(input_col + \\' schema\\').cache()\\n\\n        else:\\n\\n            dtype = self.schema_dict[dtype_key]()\\n\\n            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.col(input_col).cast(dtype))\\n\\n            # self.source_df.select(input_col + \\' schema\\').cache()\\n\\n        # dtype_cond should check if the given input_col is not null and the one with schema is null.\\n\\n        dtype_cond = ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")) & (\\n\\n            f.col(input_col + \" schema\").isNull()\\n\\n        )\\n\\n        type_error_msg = f\"data_type_FAIL: Column [{input_col}] should be {dtype_key}\"\\n\\n        self.add_error_col(error_msg=type_error_msg, condition=dtype_cond, error_col_name=input_col + \" type_check\")\\n\\n        logger.info(f\"[{input_col}] dtype check is done.\")\\n\\n\\n\\n    def null_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The condition for a null check.\\n\\n        :param input_col: the column to apply the check on.\\n\\n        :raise Column: The not null condition column.\\n\\n        \"\"\"\\n\\n        return (f.col(input_col) == \"\") | (f.col(input_col).isNull())\\n\\n\\n\\n    def null_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks not nullable columns and give an error column for input_col\\n\\n        :param input_col: The column to apply the null check.\\n\\n        \"\"\"\\n\\n        print(\"start null_check\")\\n\\n        if not math.isnan(self.rule_df.loc[input_col, \"nullable\"]):\\n\\n            return\\n\\n        null_condition = self.null_cond_syntax(input_col)\\n\\n        null_error_msg = f\"null_FAIL: Column [{input_col}] cannot be null\"\\n\\n        self.add_error_col(error_msg=null_error_msg, condition=null_condition, error_col_name=input_col + \" null_check\")\\n\\n        logger.info(f\"[{input_col}] null check is done.\")\\n\\n\\n\\n    def sum_check_syntax(self, input_col1: str, input_col2: str, syntax_value: float) -> Column:\\n\\n        \"\"\"\\n\\n        The syntax to check for sum of 2 columns to equal a syntax value, to be used in conditional checks.\\n\\n        :param input_col1: column 1 to check\\n\\n        :param input_col2: column 2 to check\\n\\n        :param syntax_value: column value that column 1 and 2 should equal to.\\n\\n        :return Column: The sum_check Column condition.\\n\\n        \"\"\"\\n\\n        return ~(f.col(input_col1) + f.col(input_col2) != syntax_value)\\n\\n\\n\\n    def conditional_cond_syntax(self, input_col: str, condition_column: str, conditional_variables: str) -> Column:\\n\\n        \"\"\"\\n\\n        Generates a Column condition given input column and the condition column.\\n\\n        :param input_col: The column in which the current syntax is applied on\\n\\n        :param condition_column: the second column which might be used in this conditional check.\\n\\n        :param conditional_variables: The variables of the check. It could be __NOT__NULL__, a string of comma separated variables or a single number.\\n\\n        :return Column: The conditional column condition.\\n\\n        \"\"\"\\n\\n        not_category = []\\n\\n        if conditional_variables == \"__NOT__NULL__\":\\n\\n            category_cond = ~self.null_cond_syntax(input_col)\\n\\n            conditional_msg = f\"{input_col} is not null,\"\\n\\n            return category_cond, conditional_msg\\n\\n        elif self.is_float(conditional_variables):\\n\\n            category_cond = self.sum_check_syntax(\\n\\n                input_col + \" schema\", condition_column + \" schema\", conditional_variables\\n\\n            )\\n\\n            conditional_msg = f\"[{input_col}] and [{condition_column}] sum is not equal to {conditional_variables}\"\\n\\n            return category_cond, conditional_msg\\n\\n        else:\\n\\n            category_list = conditional_variables.split(\",\")\\n\\n            for ind, value in enumerate(category_list):\\n\\n                if \"__NOT__\" == value[: len(\"__NOT__\")]:\\n\\n                    not_category.append(category_list.pop(ind)[len(\"__NOT__\") :])\\n\\n\\n\\n            category_cond = f.col(input_col).isin(category_list) == True\\n\\n            if not_category:\\n\\n                category_cond = category_cond & (f.col(input_col).isin(not_category) == False)\\n\\n            conditional_msg = f\"[{input_col}] is in category ({category_list}),)\"\\n\\n            return category_cond, conditional_msg\\n\\n\\n\\n    def conditional_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks all the conditional columns in a single row in a config csv file.\\n\\n        :param input_col: The column to apply the check on.\\n\\n        \"\"\"\\n\\n        multiple_conditional_list = self.rule_df.loc[input_col, \"conditional_columns\"].split(\";\")\\n\\n        for cond_ind, condition_columns in enumerate(multiple_conditional_list):\\n\\n            current_additional_cond_value = self.rule_df.loc[input_col, \"conditional_column_value\"].split(\";\")[cond_ind]\\n\\n            current_conditional_valuelist = self.rule_df.loc[input_col, \"conditional_valuelist\"].split(\";\")[cond_ind]\\n\\n\\n\\n            first_col_cond, first_col_msg = self.conditional_cond_syntax(\\n\\n                input_col=input_col,\\n\\n                condition_column=condition_columns,\\n\\n                conditional_variables=current_conditional_valuelist,\\n\\n            )\\n\\n            for condition_column in condition_columns.split(\",\"):\\n\\n                if condition_column in self.input_columns:\\n\\n                    second_cal_cond, second_col_msg = self.conditional_cond_syntax(\\n\\n                        input_col=condition_column,\\n\\n                        condition_column=input_col,\\n\\n                        conditional_variables=current_additional_cond_value,\\n\\n                    )\\n\\n                    conditional_cond = first_col_cond & (~second_cal_cond)\\n\\n                    conditional_error_msg = f\"Cond_fail: {first_col_msg}+{second_col_msg}\"\\n\\n                    self.add_error_col(\\n\\n                        error_msg=conditional_error_msg,\\n\\n                        condition=conditional_cond,\\n\\n                        error_col_name=input_col + \" conditional_check\",\\n\\n                    )\\n\\n                else:\\n\\n                    self.sns_message.append(\\n\\n                        f\"Column {condition_column} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                    )\\n\\n        logger.info(f\"[{input_col}] conditional check is done.\")\\n\\n\\n\\n    def range_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The range check column condition\\n\\n        :param input_col: The column to apply the check on\\n\\n        :return Column: The range check column condition.\\n\\n        \"\"\"\\n\\n        schema_col = input_col + \" schema\"\\n\\n        output_cond = None\\n\\n\\n\\n        min_str = self.rule_df.loc[input_col, \"min\"]\\n\\n        min_value = self.limit_finder(input_col, min_str)\\n\\n        if min_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) < min_value)\\n\\n        max_str = self.rule_df.loc[input_col, \"max\"]\\n\\n        max_value = self.limit_finder(input_col, max_str)\\n\\n        if max_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) > max_value)\\n\\n        return min_str, max_str, output_cond\\n\\n\\n\\n    def range_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with a range check and add an error column to self.source_df.\\n\\n        Args:\\n\\n            input_col (str): the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start range_check\")\\n\\n        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\\n\\n        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] range check is done.\")\\n\\n\\n\\n    def file_check(self, input_col):\\n\\n        # finding source side columns\\n\\n        source_df_columns_list = [input_col]\\n\\n        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\\n\\n        if type(reference_columns_str) == str:\\n\\n            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\\n\\n            source_df_columns_list.extend(additional_columns_list)\\n\\n\\n\\n        # Find reference side columns\\n\\n        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\\n\\n        print(\"file check type\", file_check_type)\\n\\n        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\\n\\n        reference_columns = current_file_config[\"reference_columns\"]\\n\\n        print(\"ref cols\", reference_columns)\\n\\n\\n\\n        # Read reference file\\n\\n        file_path = current_file_config[\"path\"]\\n\\n        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\\n\\n        # changing columns names on reference to be the same as source\\n\\n        print(\"src df cols list\", source_df_columns_list)\\n\\n        for col_ind, ref_col in enumerate(reference_columns):\\n\\n            source_col = source_df_columns_list[col_ind]\\n\\n            try:\\n\\n                file_df = file_df.withColumnRenamed(ref_col, source_col)\\n\\n                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                if \"Postal Code\".upper() in source_col.upper():\\n\\n                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\\n\\n            except AnalysisException:\\n\\n                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\\n\\n                return None, None\\n\\n\\n\\n        pre_join_columns = set(self.source_df.columns)\\n\\n        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\\n\\n        post_join_columns = set(self.source_df.columns)\\n\\n        join_col = tuple(post_join_columns - pre_join_columns)[0]\\n\\n        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        file_error_msg = (\\n\\n            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\\n\\n        )\\n\\n        return file_cond, file_error_msg\\n\\n\\n\\n    def category_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'add_error_col': 'def add_error_col(self, error_msg: str, condition: Column, error_col_name: str) -> None:\\n        \"\"\"\\n\\n        Add an error column based on the condition to filter and the error message\\n\\n        :param error_msg: the error message to be added if the condition is met\\n\\n        :param condition: the condition of the error to be met in order to return the error column\\n\\n        :param error_col_name: the name of the error column\\n\\n        \"\"\"\\n\\n        if condition is not None and error_col_name and error_msg:\\n\\n            col_condition = f.when(condition, f.lit(error_msg)).otherwise(f.lit(None))\\n\\n            error_col_name = error_col_name + str(self.error_counter)\\n\\n            self.source_df = self.source_df.withColumn(error_col_name, col_condition)\\n\\n            self.error_columns.append(f.col(error_col_name))\\n\\n            self.error_counter += 1\\n\\n\\n\\n    def data_type_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks the data type of all columns and give an error column for each column\\n\\n        :param input_col: the column to apply this check.\\n\\n        \"\"\"\\n\\n        print(\"start data type check\")\\n\\n        dtype_key = self.rule_df.loc[input_col, \"type\"]\\n\\n        if dtype_key == \"DateType\":\\n\\n            date_format = self.rule_df.loc[input_col, \"date_format\"]\\n\\n            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.to_date(f.col(input_col), date_format))\\n\\n            # self.source_df.select(input_col + \\' schema\\').cache()\\n\\n        else:\\n\\n            dtype = self.schema_dict[dtype_key]()\\n\\n            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.col(input_col).cast(dtype))\\n\\n            # self.source_df.select(input_col + \\' schema\\').cache()\\n\\n        # dtype_cond should check if the given input_col is not null and the one with schema is null.\\n\\n        dtype_cond = ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")) & (\\n\\n            f.col(input_col + \" schema\").isNull()\\n\\n        )\\n\\n        type_error_msg = f\"data_type_FAIL: Column [{input_col}] should be {dtype_key}\"\\n\\n        self.add_error_col(error_msg=type_error_msg, condition=dtype_cond, error_col_name=input_col + \" type_check\")\\n\\n        logger.info(f\"[{input_col}] dtype check is done.\")\\n\\n\\n\\n    def null_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The condition for a null check.\\n\\n        :param input_col: the column to apply the check on.\\n\\n        :raise Column: The not null condition column.\\n\\n        \"\"\"\\n\\n        return (f.col(input_col) == \"\") | (f.col(input_col).isNull())\\n\\n\\n\\n    def null_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks not nullable columns and give an error column for input_col\\n\\n        :param input_col: The column to apply the null check.\\n\\n        \"\"\"\\n\\n        print(\"start null_check\")\\n\\n        if not math.isnan(self.rule_df.loc[input_col, \"nullable\"]):\\n\\n            return\\n\\n        null_condition = self.null_cond_syntax(input_col)\\n\\n        null_error_msg = f\"null_FAIL: Column [{input_col}] cannot be null\"\\n\\n        self.add_error_col(error_msg=null_error_msg, condition=null_condition, error_col_name=input_col + \" null_check\")\\n\\n        logger.info(f\"[{input_col}] null check is done.\")\\n\\n\\n\\n    def sum_check_syntax(self, input_col1: str, input_col2: str, syntax_value: float) -> Column:\\n\\n        \"\"\"\\n\\n        The syntax to check for sum of 2 columns to equal a syntax value, to be used in conditional checks.\\n\\n        :param input_col1: column 1 to check\\n\\n        :param input_col2: column 2 to check\\n\\n        :param syntax_value: column value that column 1 and 2 should equal to.\\n\\n        :return Column: The sum_check Column condition.\\n\\n        \"\"\"\\n\\n        return ~(f.col(input_col1) + f.col(input_col2) != syntax_value)\\n\\n\\n\\n    def conditional_cond_syntax(self, input_col: str, condition_column: str, conditional_variables: str) -> Column:\\n\\n        \"\"\"\\n\\n        Generates a Column condition given input column and the condition column.\\n\\n        :param input_col: The column in which the current syntax is applied on\\n\\n        :param condition_column: the second column which might be used in this conditional check.\\n\\n        :param conditional_variables: The variables of the check. It could be __NOT__NULL__, a string of comma separated variables or a single number.\\n\\n        :return Column: The conditional column condition.\\n\\n        \"\"\"\\n\\n        not_category = []\\n\\n        if conditional_variables == \"__NOT__NULL__\":\\n\\n            category_cond = ~self.null_cond_syntax(input_col)\\n\\n            conditional_msg = f\"{input_col} is not null,\"\\n\\n            return category_cond, conditional_msg\\n\\n        elif self.is_float(conditional_variables):\\n\\n            category_cond = self.sum_check_syntax(\\n\\n                input_col + \" schema\", condition_column + \" schema\", conditional_variables\\n\\n            )\\n\\n            conditional_msg = f\"[{input_col}] and [{condition_column}] sum is not equal to {conditional_variables}\"\\n\\n            return category_cond, conditional_msg\\n\\n        else:\\n\\n            category_list = conditional_variables.split(\",\")\\n\\n            for ind, value in enumerate(category_list):\\n\\n                if \"__NOT__\" == value[: len(\"__NOT__\")]:\\n\\n                    not_category.append(category_list.pop(ind)[len(\"__NOT__\") :])\\n\\n\\n\\n            category_cond = f.col(input_col).isin(category_list) == True\\n\\n            if not_category:\\n\\n                category_cond = category_cond & (f.col(input_col).isin(not_category) == False)\\n\\n            conditional_msg = f\"[{input_col}] is in category ({category_list}),)\"\\n\\n            return category_cond, conditional_msg\\n\\n\\n\\n    def conditional_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks all the conditional columns in a single row in a config csv file.\\n\\n        :param input_col: The column to apply the check on.\\n\\n        \"\"\"\\n\\n        multiple_conditional_list = self.rule_df.loc[input_col, \"conditional_columns\"].split(\";\")\\n\\n        for cond_ind, condition_columns in enumerate(multiple_conditional_list):\\n\\n            current_additional_cond_value = self.rule_df.loc[input_col, \"conditional_column_value\"].split(\";\")[cond_ind]\\n\\n            current_conditional_valuelist = self.rule_df.loc[input_col, \"conditional_valuelist\"].split(\";\")[cond_ind]\\n\\n\\n\\n            first_col_cond, first_col_msg = self.conditional_cond_syntax(\\n\\n                input_col=input_col,\\n\\n                condition_column=condition_columns,\\n\\n                conditional_variables=current_conditional_valuelist,\\n\\n            )\\n\\n            for condition_column in condition_columns.split(\",\"):\\n\\n                if condition_column in self.input_columns:\\n\\n                    second_cal_cond, second_col_msg = self.conditional_cond_syntax(\\n\\n                        input_col=condition_column,\\n\\n                        condition_column=input_col,\\n\\n                        conditional_variables=current_additional_cond_value,\\n\\n                    )\\n\\n                    conditional_cond = first_col_cond & (~second_cal_cond)\\n\\n                    conditional_error_msg = f\"Cond_fail: {first_col_msg}+{second_col_msg}\"\\n\\n                    self.add_error_col(\\n\\n                        error_msg=conditional_error_msg,\\n\\n                        condition=conditional_cond,\\n\\n                        error_col_name=input_col + \" conditional_check\",\\n\\n                    )\\n\\n                else:\\n\\n                    self.sns_message.append(\\n\\n                        f\"Column {condition_column} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                    )\\n\\n        logger.info(f\"[{input_col}] conditional check is done.\")\\n\\n\\n\\n    def range_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The range check column condition\\n\\n        :param input_col: The column to apply the check on\\n\\n        :return Column: The range check column condition.\\n\\n        \"\"\"\\n\\n        schema_col = input_col + \" schema\"\\n\\n        output_cond = None\\n\\n\\n\\n        min_str = self.rule_df.loc[input_col, \"min\"]\\n\\n        min_value = self.limit_finder(input_col, min_str)\\n\\n        if min_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) < min_value)\\n\\n        max_str = self.rule_df.loc[input_col, \"max\"]\\n\\n        max_value = self.limit_finder(input_col, max_str)\\n\\n        if max_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) > max_value)\\n\\n        return min_str, max_str, output_cond\\n\\n\\n\\n    def range_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with a range check and add an error column to self.source_df.\\n\\n        Args:\\n\\n            input_col (str): the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start range_check\")\\n\\n        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\\n\\n        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] range check is done.\")\\n\\n\\n\\n    def file_check(self, input_col):\\n\\n        # finding source side columns\\n\\n        source_df_columns_list = [input_col]\\n\\n        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\\n\\n        if type(reference_columns_str) == str:\\n\\n            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\\n\\n            source_df_columns_list.extend(additional_columns_list)\\n\\n\\n\\n        # Find reference side columns\\n\\n        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\\n\\n        print(\"file check type\", file_check_type)\\n\\n        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\\n\\n        reference_columns = current_file_config[\"reference_columns\"]\\n\\n        print(\"ref cols\", reference_columns)\\n\\n\\n\\n        # Read reference file\\n\\n        file_path = current_file_config[\"path\"]\\n\\n        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\\n\\n        # changing columns names on reference to be the same as source\\n\\n        print(\"src df cols list\", source_df_columns_list)\\n\\n        for col_ind, ref_col in enumerate(reference_columns):\\n\\n            source_col = source_df_columns_list[col_ind]\\n\\n            try:\\n\\n                file_df = file_df.withColumnRenamed(ref_col, source_col)\\n\\n                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                if \"Postal Code\".upper() in source_col.upper():\\n\\n                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\\n\\n            except AnalysisException:\\n\\n                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\\n\\n                return None, None\\n\\n\\n\\n        pre_join_columns = set(self.source_df.columns)\\n\\n        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\\n\\n        post_join_columns = set(self.source_df.columns)\\n\\n        join_col = tuple(post_join_columns - pre_join_columns)[0]\\n\\n        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        file_error_msg = (\\n\\n            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\\n\\n        )\\n\\n        return file_cond, file_error_msg\\n\\n\\n\\n    def category_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'data_type_check': 'def data_type_check(self, input_col: str) -> None:\\n        \"\"\"\\n\\n        Checks the data type of all columns and give an error column for each column\\n\\n        :param input_col: the column to apply this check.\\n\\n        \"\"\"\\n\\n        print(\"start data type check\")\\n\\n        dtype_key = self.rule_df.loc[input_col, \"type\"]\\n\\n        if dtype_key == \"DateType\":\\n\\n            date_format = self.rule_df.loc[input_col, \"date_format\"]\\n\\n            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.to_date(f.col(input_col), date_format))\\n\\n            # self.source_df.select(input_col + \\' schema\\').cache()\\n\\n        else:\\n\\n            dtype = self.schema_dict[dtype_key]()\\n\\n            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.col(input_col).cast(dtype))\\n\\n            # self.source_df.select(input_col + \\' schema\\').cache()\\n\\n        # dtype_cond should check if the given input_col is not null and the one with schema is null.\\n\\n        dtype_cond = ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")) & (\\n\\n            f.col(input_col + \" schema\").isNull()\\n\\n        )\\n\\n        type_error_msg = f\"data_type_FAIL: Column [{input_col}] should be {dtype_key}\"\\n\\n        self.add_error_col(error_msg=type_error_msg, condition=dtype_cond, error_col_name=input_col + \" type_check\")\\n\\n        logger.info(f\"[{input_col}] dtype check is done.\")\\n\\n\\n\\n    def null_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The condition for a null check.\\n\\n        :param input_col: the column to apply the check on.\\n\\n        :raise Column: The not null condition column.\\n\\n        \"\"\"\\n\\n        return (f.col(input_col) == \"\") | (f.col(input_col).isNull())\\n\\n\\n\\n    def null_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks not nullable columns and give an error column for input_col\\n\\n        :param input_col: The column to apply the null check.\\n\\n        \"\"\"\\n\\n        print(\"start null_check\")\\n\\n        if not math.isnan(self.rule_df.loc[input_col, \"nullable\"]):\\n\\n            return\\n\\n        null_condition = self.null_cond_syntax(input_col)\\n\\n        null_error_msg = f\"null_FAIL: Column [{input_col}] cannot be null\"\\n\\n        self.add_error_col(error_msg=null_error_msg, condition=null_condition, error_col_name=input_col + \" null_check\")\\n\\n        logger.info(f\"[{input_col}] null check is done.\")\\n\\n\\n\\n    def sum_check_syntax(self, input_col1: str, input_col2: str, syntax_value: float) -> Column:\\n\\n        \"\"\"\\n\\n        The syntax to check for sum of 2 columns to equal a syntax value, to be used in conditional checks.\\n\\n        :param input_col1: column 1 to check\\n\\n        :param input_col2: column 2 to check\\n\\n        :param syntax_value: column value that column 1 and 2 should equal to.\\n\\n        :return Column: The sum_check Column condition.\\n\\n        \"\"\"\\n\\n        return ~(f.col(input_col1) + f.col(input_col2) != syntax_value)\\n\\n\\n\\n    def conditional_cond_syntax(self, input_col: str, condition_column: str, conditional_variables: str) -> Column:\\n\\n        \"\"\"\\n\\n        Generates a Column condition given input column and the condition column.\\n\\n        :param input_col: The column in which the current syntax is applied on\\n\\n        :param condition_column: the second column which might be used in this conditional check.\\n\\n        :param conditional_variables: The variables of the check. It could be __NOT__NULL__, a string of comma separated variables or a single number.\\n\\n        :return Column: The conditional column condition.\\n\\n        \"\"\"\\n\\n        not_category = []\\n\\n        if conditional_variables == \"__NOT__NULL__\":\\n\\n            category_cond = ~self.null_cond_syntax(input_col)\\n\\n            conditional_msg = f\"{input_col} is not null,\"\\n\\n            return category_cond, conditional_msg\\n\\n        elif self.is_float(conditional_variables):\\n\\n            category_cond = self.sum_check_syntax(\\n\\n                input_col + \" schema\", condition_column + \" schema\", conditional_variables\\n\\n            )\\n\\n            conditional_msg = f\"[{input_col}] and [{condition_column}] sum is not equal to {conditional_variables}\"\\n\\n            return category_cond, conditional_msg\\n\\n        else:\\n\\n            category_list = conditional_variables.split(\",\")\\n\\n            for ind, value in enumerate(category_list):\\n\\n                if \"__NOT__\" == value[: len(\"__NOT__\")]:\\n\\n                    not_category.append(category_list.pop(ind)[len(\"__NOT__\") :])\\n\\n\\n\\n            category_cond = f.col(input_col).isin(category_list) == True\\n\\n            if not_category:\\n\\n                category_cond = category_cond & (f.col(input_col).isin(not_category) == False)\\n\\n            conditional_msg = f\"[{input_col}] is in category ({category_list}),)\"\\n\\n            return category_cond, conditional_msg\\n\\n\\n\\n    def conditional_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks all the conditional columns in a single row in a config csv file.\\n\\n        :param input_col: The column to apply the check on.\\n\\n        \"\"\"\\n\\n        multiple_conditional_list = self.rule_df.loc[input_col, \"conditional_columns\"].split(\";\")\\n\\n        for cond_ind, condition_columns in enumerate(multiple_conditional_list):\\n\\n            current_additional_cond_value = self.rule_df.loc[input_col, \"conditional_column_value\"].split(\";\")[cond_ind]\\n\\n            current_conditional_valuelist = self.rule_df.loc[input_col, \"conditional_valuelist\"].split(\";\")[cond_ind]\\n\\n\\n\\n            first_col_cond, first_col_msg = self.conditional_cond_syntax(\\n\\n                input_col=input_col,\\n\\n                condition_column=condition_columns,\\n\\n                conditional_variables=current_conditional_valuelist,\\n\\n            )\\n\\n            for condition_column in condition_columns.split(\",\"):\\n\\n                if condition_column in self.input_columns:\\n\\n                    second_cal_cond, second_col_msg = self.conditional_cond_syntax(\\n\\n                        input_col=condition_column,\\n\\n                        condition_column=input_col,\\n\\n                        conditional_variables=current_additional_cond_value,\\n\\n                    )\\n\\n                    conditional_cond = first_col_cond & (~second_cal_cond)\\n\\n                    conditional_error_msg = f\"Cond_fail: {first_col_msg}+{second_col_msg}\"\\n\\n                    self.add_error_col(\\n\\n                        error_msg=conditional_error_msg,\\n\\n                        condition=conditional_cond,\\n\\n                        error_col_name=input_col + \" conditional_check\",\\n\\n                    )\\n\\n                else:\\n\\n                    self.sns_message.append(\\n\\n                        f\"Column {condition_column} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                    )\\n\\n        logger.info(f\"[{input_col}] conditional check is done.\")\\n\\n\\n\\n    def range_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The range check column condition\\n\\n        :param input_col: The column to apply the check on\\n\\n        :return Column: The range check column condition.\\n\\n        \"\"\"\\n\\n        schema_col = input_col + \" schema\"\\n\\n        output_cond = None\\n\\n\\n\\n        min_str = self.rule_df.loc[input_col, \"min\"]\\n\\n        min_value = self.limit_finder(input_col, min_str)\\n\\n        if min_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) < min_value)\\n\\n        max_str = self.rule_df.loc[input_col, \"max\"]\\n\\n        max_value = self.limit_finder(input_col, max_str)\\n\\n        if max_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) > max_value)\\n\\n        return min_str, max_str, output_cond\\n\\n\\n\\n    def range_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with a range check and add an error column to self.source_df.\\n\\n        Args:\\n\\n            input_col (str): the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start range_check\")\\n\\n        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\\n\\n        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] range check is done.\")\\n\\n\\n\\n    def file_check(self, input_col):\\n\\n        # finding source side columns\\n\\n        source_df_columns_list = [input_col]\\n\\n        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\\n\\n        if type(reference_columns_str) == str:\\n\\n            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\\n\\n            source_df_columns_list.extend(additional_columns_list)\\n\\n\\n\\n        # Find reference side columns\\n\\n        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\\n\\n        print(\"file check type\", file_check_type)\\n\\n        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\\n\\n        reference_columns = current_file_config[\"reference_columns\"]\\n\\n        print(\"ref cols\", reference_columns)\\n\\n\\n\\n        # Read reference file\\n\\n        file_path = current_file_config[\"path\"]\\n\\n        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\\n\\n        # changing columns names on reference to be the same as source\\n\\n        print(\"src df cols list\", source_df_columns_list)\\n\\n        for col_ind, ref_col in enumerate(reference_columns):\\n\\n            source_col = source_df_columns_list[col_ind]\\n\\n            try:\\n\\n                file_df = file_df.withColumnRenamed(ref_col, source_col)\\n\\n                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                if \"Postal Code\".upper() in source_col.upper():\\n\\n                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\\n\\n            except AnalysisException:\\n\\n                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\\n\\n                return None, None\\n\\n\\n\\n        pre_join_columns = set(self.source_df.columns)\\n\\n        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\\n\\n        post_join_columns = set(self.source_df.columns)\\n\\n        join_col = tuple(post_join_columns - pre_join_columns)[0]\\n\\n        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        file_error_msg = (\\n\\n            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\\n\\n        )\\n\\n        return file_cond, file_error_msg\\n\\n\\n\\n    def category_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'null_cond_syntax': 'def null_cond_syntax(self, input_col: str) -> Column:\\n        \"\"\"\\n\\n        The condition for a null check.\\n\\n        :param input_col: the column to apply the check on.\\n\\n        :raise Column: The not null condition column.\\n\\n        \"\"\"\\n\\n        return (f.col(input_col) == \"\") | (f.col(input_col).isNull())\\n\\n\\n\\n    def null_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks not nullable columns and give an error column for input_col\\n\\n        :param input_col: The column to apply the null check.\\n\\n        \"\"\"\\n\\n        print(\"start null_check\")\\n\\n        if not math.isnan(self.rule_df.loc[input_col, \"nullable\"]):\\n\\n            return\\n\\n        null_condition = self.null_cond_syntax(input_col)\\n\\n        null_error_msg = f\"null_FAIL: Column [{input_col}] cannot be null\"\\n\\n        self.add_error_col(error_msg=null_error_msg, condition=null_condition, error_col_name=input_col + \" null_check\")\\n\\n        logger.info(f\"[{input_col}] null check is done.\")\\n\\n\\n\\n    def sum_check_syntax(self, input_col1: str, input_col2: str, syntax_value: float) -> Column:\\n\\n        \"\"\"\\n\\n        The syntax to check for sum of 2 columns to equal a syntax value, to be used in conditional checks.\\n\\n        :param input_col1: column 1 to check\\n\\n        :param input_col2: column 2 to check\\n\\n        :param syntax_value: column value that column 1 and 2 should equal to.\\n\\n        :return Column: The sum_check Column condition.\\n\\n        \"\"\"\\n\\n        return ~(f.col(input_col1) + f.col(input_col2) != syntax_value)\\n\\n\\n\\n    def conditional_cond_syntax(self, input_col: str, condition_column: str, conditional_variables: str) -> Column:\\n\\n        \"\"\"\\n\\n        Generates a Column condition given input column and the condition column.\\n\\n        :param input_col: The column in which the current syntax is applied on\\n\\n        :param condition_column: the second column which might be used in this conditional check.\\n\\n        :param conditional_variables: The variables of the check. It could be __NOT__NULL__, a string of comma separated variables or a single number.\\n\\n        :return Column: The conditional column condition.\\n\\n        \"\"\"\\n\\n        not_category = []\\n\\n        if conditional_variables == \"__NOT__NULL__\":\\n\\n            category_cond = ~self.null_cond_syntax(input_col)\\n\\n            conditional_msg = f\"{input_col} is not null,\"\\n\\n            return category_cond, conditional_msg\\n\\n        elif self.is_float(conditional_variables):\\n\\n            category_cond = self.sum_check_syntax(\\n\\n                input_col + \" schema\", condition_column + \" schema\", conditional_variables\\n\\n            )\\n\\n            conditional_msg = f\"[{input_col}] and [{condition_column}] sum is not equal to {conditional_variables}\"\\n\\n            return category_cond, conditional_msg\\n\\n        else:\\n\\n            category_list = conditional_variables.split(\",\")\\n\\n            for ind, value in enumerate(category_list):\\n\\n                if \"__NOT__\" == value[: len(\"__NOT__\")]:\\n\\n                    not_category.append(category_list.pop(ind)[len(\"__NOT__\") :])\\n\\n\\n\\n            category_cond = f.col(input_col).isin(category_list) == True\\n\\n            if not_category:\\n\\n                category_cond = category_cond & (f.col(input_col).isin(not_category) == False)\\n\\n            conditional_msg = f\"[{input_col}] is in category ({category_list}),)\"\\n\\n            return category_cond, conditional_msg\\n\\n\\n\\n    def conditional_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks all the conditional columns in a single row in a config csv file.\\n\\n        :param input_col: The column to apply the check on.\\n\\n        \"\"\"\\n\\n        multiple_conditional_list = self.rule_df.loc[input_col, \"conditional_columns\"].split(\";\")\\n\\n        for cond_ind, condition_columns in enumerate(multiple_conditional_list):\\n\\n            current_additional_cond_value = self.rule_df.loc[input_col, \"conditional_column_value\"].split(\";\")[cond_ind]\\n\\n            current_conditional_valuelist = self.rule_df.loc[input_col, \"conditional_valuelist\"].split(\";\")[cond_ind]\\n\\n\\n\\n            first_col_cond, first_col_msg = self.conditional_cond_syntax(\\n\\n                input_col=input_col,\\n\\n                condition_column=condition_columns,\\n\\n                conditional_variables=current_conditional_valuelist,\\n\\n            )\\n\\n            for condition_column in condition_columns.split(\",\"):\\n\\n                if condition_column in self.input_columns:\\n\\n                    second_cal_cond, second_col_msg = self.conditional_cond_syntax(\\n\\n                        input_col=condition_column,\\n\\n                        condition_column=input_col,\\n\\n                        conditional_variables=current_additional_cond_value,\\n\\n                    )\\n\\n                    conditional_cond = first_col_cond & (~second_cal_cond)\\n\\n                    conditional_error_msg = f\"Cond_fail: {first_col_msg}+{second_col_msg}\"\\n\\n                    self.add_error_col(\\n\\n                        error_msg=conditional_error_msg,\\n\\n                        condition=conditional_cond,\\n\\n                        error_col_name=input_col + \" conditional_check\",\\n\\n                    )\\n\\n                else:\\n\\n                    self.sns_message.append(\\n\\n                        f\"Column {condition_column} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                    )\\n\\n        logger.info(f\"[{input_col}] conditional check is done.\")\\n\\n\\n\\n    def range_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The range check column condition\\n\\n        :param input_col: The column to apply the check on\\n\\n        :return Column: The range check column condition.\\n\\n        \"\"\"\\n\\n        schema_col = input_col + \" schema\"\\n\\n        output_cond = None\\n\\n\\n\\n        min_str = self.rule_df.loc[input_col, \"min\"]\\n\\n        min_value = self.limit_finder(input_col, min_str)\\n\\n        if min_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) < min_value)\\n\\n        max_str = self.rule_df.loc[input_col, \"max\"]\\n\\n        max_value = self.limit_finder(input_col, max_str)\\n\\n        if max_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) > max_value)\\n\\n        return min_str, max_str, output_cond\\n\\n\\n\\n    def range_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with a range check and add an error column to self.source_df.\\n\\n        Args:\\n\\n            input_col (str): the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start range_check\")\\n\\n        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\\n\\n        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] range check is done.\")\\n\\n\\n\\n    def file_check(self, input_col):\\n\\n        # finding source side columns\\n\\n        source_df_columns_list = [input_col]\\n\\n        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\\n\\n        if type(reference_columns_str) == str:\\n\\n            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\\n\\n            source_df_columns_list.extend(additional_columns_list)\\n\\n\\n\\n        # Find reference side columns\\n\\n        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\\n\\n        print(\"file check type\", file_check_type)\\n\\n        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\\n\\n        reference_columns = current_file_config[\"reference_columns\"]\\n\\n        print(\"ref cols\", reference_columns)\\n\\n\\n\\n        # Read reference file\\n\\n        file_path = current_file_config[\"path\"]\\n\\n        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\\n\\n        # changing columns names on reference to be the same as source\\n\\n        print(\"src df cols list\", source_df_columns_list)\\n\\n        for col_ind, ref_col in enumerate(reference_columns):\\n\\n            source_col = source_df_columns_list[col_ind]\\n\\n            try:\\n\\n                file_df = file_df.withColumnRenamed(ref_col, source_col)\\n\\n                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                if \"Postal Code\".upper() in source_col.upper():\\n\\n                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\\n\\n            except AnalysisException:\\n\\n                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\\n\\n                return None, None\\n\\n\\n\\n        pre_join_columns = set(self.source_df.columns)\\n\\n        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\\n\\n        post_join_columns = set(self.source_df.columns)\\n\\n        join_col = tuple(post_join_columns - pre_join_columns)[0]\\n\\n        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        file_error_msg = (\\n\\n            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\\n\\n        )\\n\\n        return file_cond, file_error_msg\\n\\n\\n\\n    def category_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'null_check': 'def null_check(self, input_col: str) -> None:\\n        \"\"\"\\n\\n        Checks not nullable columns and give an error column for input_col\\n\\n        :param input_col: The column to apply the null check.\\n\\n        \"\"\"\\n\\n        print(\"start null_check\")\\n\\n        if not math.isnan(self.rule_df.loc[input_col, \"nullable\"]):\\n\\n            return\\n\\n        null_condition = self.null_cond_syntax(input_col)\\n\\n        null_error_msg = f\"null_FAIL: Column [{input_col}] cannot be null\"\\n\\n        self.add_error_col(error_msg=null_error_msg, condition=null_condition, error_col_name=input_col + \" null_check\")\\n\\n        logger.info(f\"[{input_col}] null check is done.\")\\n\\n\\n\\n    def sum_check_syntax(self, input_col1: str, input_col2: str, syntax_value: float) -> Column:\\n\\n        \"\"\"\\n\\n        The syntax to check for sum of 2 columns to equal a syntax value, to be used in conditional checks.\\n\\n        :param input_col1: column 1 to check\\n\\n        :param input_col2: column 2 to check\\n\\n        :param syntax_value: column value that column 1 and 2 should equal to.\\n\\n        :return Column: The sum_check Column condition.\\n\\n        \"\"\"\\n\\n        return ~(f.col(input_col1) + f.col(input_col2) != syntax_value)\\n\\n\\n\\n    def conditional_cond_syntax(self, input_col: str, condition_column: str, conditional_variables: str) -> Column:\\n\\n        \"\"\"\\n\\n        Generates a Column condition given input column and the condition column.\\n\\n        :param input_col: The column in which the current syntax is applied on\\n\\n        :param condition_column: the second column which might be used in this conditional check.\\n\\n        :param conditional_variables: The variables of the check. It could be __NOT__NULL__, a string of comma separated variables or a single number.\\n\\n        :return Column: The conditional column condition.\\n\\n        \"\"\"\\n\\n        not_category = []\\n\\n        if conditional_variables == \"__NOT__NULL__\":\\n\\n            category_cond = ~self.null_cond_syntax(input_col)\\n\\n            conditional_msg = f\"{input_col} is not null,\"\\n\\n            return category_cond, conditional_msg\\n\\n        elif self.is_float(conditional_variables):\\n\\n            category_cond = self.sum_check_syntax(\\n\\n                input_col + \" schema\", condition_column + \" schema\", conditional_variables\\n\\n            )\\n\\n            conditional_msg = f\"[{input_col}] and [{condition_column}] sum is not equal to {conditional_variables}\"\\n\\n            return category_cond, conditional_msg\\n\\n        else:\\n\\n            category_list = conditional_variables.split(\",\")\\n\\n            for ind, value in enumerate(category_list):\\n\\n                if \"__NOT__\" == value[: len(\"__NOT__\")]:\\n\\n                    not_category.append(category_list.pop(ind)[len(\"__NOT__\") :])\\n\\n\\n\\n            category_cond = f.col(input_col).isin(category_list) == True\\n\\n            if not_category:\\n\\n                category_cond = category_cond & (f.col(input_col).isin(not_category) == False)\\n\\n            conditional_msg = f\"[{input_col}] is in category ({category_list}),)\"\\n\\n            return category_cond, conditional_msg\\n\\n\\n\\n    def conditional_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks all the conditional columns in a single row in a config csv file.\\n\\n        :param input_col: The column to apply the check on.\\n\\n        \"\"\"\\n\\n        multiple_conditional_list = self.rule_df.loc[input_col, \"conditional_columns\"].split(\";\")\\n\\n        for cond_ind, condition_columns in enumerate(multiple_conditional_list):\\n\\n            current_additional_cond_value = self.rule_df.loc[input_col, \"conditional_column_value\"].split(\";\")[cond_ind]\\n\\n            current_conditional_valuelist = self.rule_df.loc[input_col, \"conditional_valuelist\"].split(\";\")[cond_ind]\\n\\n\\n\\n            first_col_cond, first_col_msg = self.conditional_cond_syntax(\\n\\n                input_col=input_col,\\n\\n                condition_column=condition_columns,\\n\\n                conditional_variables=current_conditional_valuelist,\\n\\n            )\\n\\n            for condition_column in condition_columns.split(\",\"):\\n\\n                if condition_column in self.input_columns:\\n\\n                    second_cal_cond, second_col_msg = self.conditional_cond_syntax(\\n\\n                        input_col=condition_column,\\n\\n                        condition_column=input_col,\\n\\n                        conditional_variables=current_additional_cond_value,\\n\\n                    )\\n\\n                    conditional_cond = first_col_cond & (~second_cal_cond)\\n\\n                    conditional_error_msg = f\"Cond_fail: {first_col_msg}+{second_col_msg}\"\\n\\n                    self.add_error_col(\\n\\n                        error_msg=conditional_error_msg,\\n\\n                        condition=conditional_cond,\\n\\n                        error_col_name=input_col + \" conditional_check\",\\n\\n                    )\\n\\n                else:\\n\\n                    self.sns_message.append(\\n\\n                        f\"Column {condition_column} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                    )\\n\\n        logger.info(f\"[{input_col}] conditional check is done.\")\\n\\n\\n\\n    def range_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The range check column condition\\n\\n        :param input_col: The column to apply the check on\\n\\n        :return Column: The range check column condition.\\n\\n        \"\"\"\\n\\n        schema_col = input_col + \" schema\"\\n\\n        output_cond = None\\n\\n\\n\\n        min_str = self.rule_df.loc[input_col, \"min\"]\\n\\n        min_value = self.limit_finder(input_col, min_str)\\n\\n        if min_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) < min_value)\\n\\n        max_str = self.rule_df.loc[input_col, \"max\"]\\n\\n        max_value = self.limit_finder(input_col, max_str)\\n\\n        if max_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) > max_value)\\n\\n        return min_str, max_str, output_cond\\n\\n\\n\\n    def range_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with a range check and add an error column to self.source_df.\\n\\n        Args:\\n\\n            input_col (str): the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start range_check\")\\n\\n        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\\n\\n        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] range check is done.\")\\n\\n\\n\\n    def file_check(self, input_col):\\n\\n        # finding source side columns\\n\\n        source_df_columns_list = [input_col]\\n\\n        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\\n\\n        if type(reference_columns_str) == str:\\n\\n            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\\n\\n            source_df_columns_list.extend(additional_columns_list)\\n\\n\\n\\n        # Find reference side columns\\n\\n        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\\n\\n        print(\"file check type\", file_check_type)\\n\\n        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\\n\\n        reference_columns = current_file_config[\"reference_columns\"]\\n\\n        print(\"ref cols\", reference_columns)\\n\\n\\n\\n        # Read reference file\\n\\n        file_path = current_file_config[\"path\"]\\n\\n        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\\n\\n        # changing columns names on reference to be the same as source\\n\\n        print(\"src df cols list\", source_df_columns_list)\\n\\n        for col_ind, ref_col in enumerate(reference_columns):\\n\\n            source_col = source_df_columns_list[col_ind]\\n\\n            try:\\n\\n                file_df = file_df.withColumnRenamed(ref_col, source_col)\\n\\n                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                if \"Postal Code\".upper() in source_col.upper():\\n\\n                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\\n\\n            except AnalysisException:\\n\\n                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\\n\\n                return None, None\\n\\n\\n\\n        pre_join_columns = set(self.source_df.columns)\\n\\n        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\\n\\n        post_join_columns = set(self.source_df.columns)\\n\\n        join_col = tuple(post_join_columns - pre_join_columns)[0]\\n\\n        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        file_error_msg = (\\n\\n            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\\n\\n        )\\n\\n        return file_cond, file_error_msg\\n\\n\\n\\n    def category_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'sum_check_syntax': 'def sum_check_syntax(self, input_col1: str, input_col2: str, syntax_value: float) -> Column:\\n        \"\"\"\\n\\n        The syntax to check for sum of 2 columns to equal a syntax value, to be used in conditional checks.\\n\\n        :param input_col1: column 1 to check\\n\\n        :param input_col2: column 2 to check\\n\\n        :param syntax_value: column value that column 1 and 2 should equal to.\\n\\n        :return Column: The sum_check Column condition.\\n\\n        \"\"\"\\n\\n        return ~(f.col(input_col1) + f.col(input_col2) != syntax_value)\\n\\n\\n\\n    def conditional_cond_syntax(self, input_col: str, condition_column: str, conditional_variables: str) -> Column:\\n\\n        \"\"\"\\n\\n        Generates a Column condition given input column and the condition column.\\n\\n        :param input_col: The column in which the current syntax is applied on\\n\\n        :param condition_column: the second column which might be used in this conditional check.\\n\\n        :param conditional_variables: The variables of the check. It could be __NOT__NULL__, a string of comma separated variables or a single number.\\n\\n        :return Column: The conditional column condition.\\n\\n        \"\"\"\\n\\n        not_category = []\\n\\n        if conditional_variables == \"__NOT__NULL__\":\\n\\n            category_cond = ~self.null_cond_syntax(input_col)\\n\\n            conditional_msg = f\"{input_col} is not null,\"\\n\\n            return category_cond, conditional_msg\\n\\n        elif self.is_float(conditional_variables):\\n\\n            category_cond = self.sum_check_syntax(\\n\\n                input_col + \" schema\", condition_column + \" schema\", conditional_variables\\n\\n            )\\n\\n            conditional_msg = f\"[{input_col}] and [{condition_column}] sum is not equal to {conditional_variables}\"\\n\\n            return category_cond, conditional_msg\\n\\n        else:\\n\\n            category_list = conditional_variables.split(\",\")\\n\\n            for ind, value in enumerate(category_list):\\n\\n                if \"__NOT__\" == value[: len(\"__NOT__\")]:\\n\\n                    not_category.append(category_list.pop(ind)[len(\"__NOT__\") :])\\n\\n\\n\\n            category_cond = f.col(input_col).isin(category_list) == True\\n\\n            if not_category:\\n\\n                category_cond = category_cond & (f.col(input_col).isin(not_category) == False)\\n\\n            conditional_msg = f\"[{input_col}] is in category ({category_list}),)\"\\n\\n            return category_cond, conditional_msg\\n\\n\\n\\n    def conditional_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks all the conditional columns in a single row in a config csv file.\\n\\n        :param input_col: The column to apply the check on.\\n\\n        \"\"\"\\n\\n        multiple_conditional_list = self.rule_df.loc[input_col, \"conditional_columns\"].split(\";\")\\n\\n        for cond_ind, condition_columns in enumerate(multiple_conditional_list):\\n\\n            current_additional_cond_value = self.rule_df.loc[input_col, \"conditional_column_value\"].split(\";\")[cond_ind]\\n\\n            current_conditional_valuelist = self.rule_df.loc[input_col, \"conditional_valuelist\"].split(\";\")[cond_ind]\\n\\n\\n\\n            first_col_cond, first_col_msg = self.conditional_cond_syntax(\\n\\n                input_col=input_col,\\n\\n                condition_column=condition_columns,\\n\\n                conditional_variables=current_conditional_valuelist,\\n\\n            )\\n\\n            for condition_column in condition_columns.split(\",\"):\\n\\n                if condition_column in self.input_columns:\\n\\n                    second_cal_cond, second_col_msg = self.conditional_cond_syntax(\\n\\n                        input_col=condition_column,\\n\\n                        condition_column=input_col,\\n\\n                        conditional_variables=current_additional_cond_value,\\n\\n                    )\\n\\n                    conditional_cond = first_col_cond & (~second_cal_cond)\\n\\n                    conditional_error_msg = f\"Cond_fail: {first_col_msg}+{second_col_msg}\"\\n\\n                    self.add_error_col(\\n\\n                        error_msg=conditional_error_msg,\\n\\n                        condition=conditional_cond,\\n\\n                        error_col_name=input_col + \" conditional_check\",\\n\\n                    )\\n\\n                else:\\n\\n                    self.sns_message.append(\\n\\n                        f\"Column {condition_column} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                    )\\n\\n        logger.info(f\"[{input_col}] conditional check is done.\")\\n\\n\\n\\n    def range_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The range check column condition\\n\\n        :param input_col: The column to apply the check on\\n\\n        :return Column: The range check column condition.\\n\\n        \"\"\"\\n\\n        schema_col = input_col + \" schema\"\\n\\n        output_cond = None\\n\\n\\n\\n        min_str = self.rule_df.loc[input_col, \"min\"]\\n\\n        min_value = self.limit_finder(input_col, min_str)\\n\\n        if min_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) < min_value)\\n\\n        max_str = self.rule_df.loc[input_col, \"max\"]\\n\\n        max_value = self.limit_finder(input_col, max_str)\\n\\n        if max_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) > max_value)\\n\\n        return min_str, max_str, output_cond\\n\\n\\n\\n    def range_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with a range check and add an error column to self.source_df.\\n\\n        Args:\\n\\n            input_col (str): the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start range_check\")\\n\\n        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\\n\\n        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] range check is done.\")\\n\\n\\n\\n    def file_check(self, input_col):\\n\\n        # finding source side columns\\n\\n        source_df_columns_list = [input_col]\\n\\n        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\\n\\n        if type(reference_columns_str) == str:\\n\\n            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\\n\\n            source_df_columns_list.extend(additional_columns_list)\\n\\n\\n\\n        # Find reference side columns\\n\\n        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\\n\\n        print(\"file check type\", file_check_type)\\n\\n        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\\n\\n        reference_columns = current_file_config[\"reference_columns\"]\\n\\n        print(\"ref cols\", reference_columns)\\n\\n\\n\\n        # Read reference file\\n\\n        file_path = current_file_config[\"path\"]\\n\\n        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\\n\\n        # changing columns names on reference to be the same as source\\n\\n        print(\"src df cols list\", source_df_columns_list)\\n\\n        for col_ind, ref_col in enumerate(reference_columns):\\n\\n            source_col = source_df_columns_list[col_ind]\\n\\n            try:\\n\\n                file_df = file_df.withColumnRenamed(ref_col, source_col)\\n\\n                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                if \"Postal Code\".upper() in source_col.upper():\\n\\n                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\\n\\n            except AnalysisException:\\n\\n                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\\n\\n                return None, None\\n\\n\\n\\n        pre_join_columns = set(self.source_df.columns)\\n\\n        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\\n\\n        post_join_columns = set(self.source_df.columns)\\n\\n        join_col = tuple(post_join_columns - pre_join_columns)[0]\\n\\n        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        file_error_msg = (\\n\\n            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\\n\\n        )\\n\\n        return file_cond, file_error_msg\\n\\n\\n\\n    def category_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'conditional_cond_syntax': 'def conditional_cond_syntax(self, input_col: str, condition_column: str, conditional_variables: str) -> Column:\\n        \"\"\"\\n\\n        Generates a Column condition given input column and the condition column.\\n\\n        :param input_col: The column in which the current syntax is applied on\\n\\n        :param condition_column: the second column which might be used in this conditional check.\\n\\n        :param conditional_variables: The variables of the check. It could be __NOT__NULL__, a string of comma separated variables or a single number.\\n\\n        :return Column: The conditional column condition.\\n\\n        \"\"\"\\n\\n        not_category = []\\n\\n        if conditional_variables == \"__NOT__NULL__\":\\n\\n            category_cond = ~self.null_cond_syntax(input_col)\\n\\n            conditional_msg = f\"{input_col} is not null,\"\\n\\n            return category_cond, conditional_msg\\n\\n        elif self.is_float(conditional_variables):\\n\\n            category_cond = self.sum_check_syntax(\\n\\n                input_col + \" schema\", condition_column + \" schema\", conditional_variables\\n\\n            )\\n\\n            conditional_msg = f\"[{input_col}] and [{condition_column}] sum is not equal to {conditional_variables}\"\\n\\n            return category_cond, conditional_msg\\n\\n        else:\\n\\n            category_list = conditional_variables.split(\",\")\\n\\n            for ind, value in enumerate(category_list):\\n\\n                if \"__NOT__\" == value[: len(\"__NOT__\")]:\\n\\n                    not_category.append(category_list.pop(ind)[len(\"__NOT__\") :])\\n\\n\\n\\n            category_cond = f.col(input_col).isin(category_list) == True\\n\\n            if not_category:\\n\\n                category_cond = category_cond & (f.col(input_col).isin(not_category) == False)\\n\\n            conditional_msg = f\"[{input_col}] is in category ({category_list}),)\"\\n\\n            return category_cond, conditional_msg\\n\\n\\n\\n    def conditional_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Checks all the conditional columns in a single row in a config csv file.\\n\\n        :param input_col: The column to apply the check on.\\n\\n        \"\"\"\\n\\n        multiple_conditional_list = self.rule_df.loc[input_col, \"conditional_columns\"].split(\";\")\\n\\n        for cond_ind, condition_columns in enumerate(multiple_conditional_list):\\n\\n            current_additional_cond_value = self.rule_df.loc[input_col, \"conditional_column_value\"].split(\";\")[cond_ind]\\n\\n            current_conditional_valuelist = self.rule_df.loc[input_col, \"conditional_valuelist\"].split(\";\")[cond_ind]\\n\\n\\n\\n            first_col_cond, first_col_msg = self.conditional_cond_syntax(\\n\\n                input_col=input_col,\\n\\n                condition_column=condition_columns,\\n\\n                conditional_variables=current_conditional_valuelist,\\n\\n            )\\n\\n            for condition_column in condition_columns.split(\",\"):\\n\\n                if condition_column in self.input_columns:\\n\\n                    second_cal_cond, second_col_msg = self.conditional_cond_syntax(\\n\\n                        input_col=condition_column,\\n\\n                        condition_column=input_col,\\n\\n                        conditional_variables=current_additional_cond_value,\\n\\n                    )\\n\\n                    conditional_cond = first_col_cond & (~second_cal_cond)\\n\\n                    conditional_error_msg = f\"Cond_fail: {first_col_msg}+{second_col_msg}\"\\n\\n                    self.add_error_col(\\n\\n                        error_msg=conditional_error_msg,\\n\\n                        condition=conditional_cond,\\n\\n                        error_col_name=input_col + \" conditional_check\",\\n\\n                    )\\n\\n                else:\\n\\n                    self.sns_message.append(\\n\\n                        f\"Column {condition_column} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                    )\\n\\n        logger.info(f\"[{input_col}] conditional check is done.\")\\n\\n\\n\\n    def range_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The range check column condition\\n\\n        :param input_col: The column to apply the check on\\n\\n        :return Column: The range check column condition.\\n\\n        \"\"\"\\n\\n        schema_col = input_col + \" schema\"\\n\\n        output_cond = None\\n\\n\\n\\n        min_str = self.rule_df.loc[input_col, \"min\"]\\n\\n        min_value = self.limit_finder(input_col, min_str)\\n\\n        if min_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) < min_value)\\n\\n        max_str = self.rule_df.loc[input_col, \"max\"]\\n\\n        max_value = self.limit_finder(input_col, max_str)\\n\\n        if max_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) > max_value)\\n\\n        return min_str, max_str, output_cond\\n\\n\\n\\n    def range_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with a range check and add an error column to self.source_df.\\n\\n        Args:\\n\\n            input_col (str): the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start range_check\")\\n\\n        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\\n\\n        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] range check is done.\")\\n\\n\\n\\n    def file_check(self, input_col):\\n\\n        # finding source side columns\\n\\n        source_df_columns_list = [input_col]\\n\\n        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\\n\\n        if type(reference_columns_str) == str:\\n\\n            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\\n\\n            source_df_columns_list.extend(additional_columns_list)\\n\\n\\n\\n        # Find reference side columns\\n\\n        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\\n\\n        print(\"file check type\", file_check_type)\\n\\n        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\\n\\n        reference_columns = current_file_config[\"reference_columns\"]\\n\\n        print(\"ref cols\", reference_columns)\\n\\n\\n\\n        # Read reference file\\n\\n        file_path = current_file_config[\"path\"]\\n\\n        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\\n\\n        # changing columns names on reference to be the same as source\\n\\n        print(\"src df cols list\", source_df_columns_list)\\n\\n        for col_ind, ref_col in enumerate(reference_columns):\\n\\n            source_col = source_df_columns_list[col_ind]\\n\\n            try:\\n\\n                file_df = file_df.withColumnRenamed(ref_col, source_col)\\n\\n                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                if \"Postal Code\".upper() in source_col.upper():\\n\\n                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\\n\\n            except AnalysisException:\\n\\n                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\\n\\n                return None, None\\n\\n\\n\\n        pre_join_columns = set(self.source_df.columns)\\n\\n        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\\n\\n        post_join_columns = set(self.source_df.columns)\\n\\n        join_col = tuple(post_join_columns - pre_join_columns)[0]\\n\\n        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        file_error_msg = (\\n\\n            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\\n\\n        )\\n\\n        return file_cond, file_error_msg\\n\\n\\n\\n    def category_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'conditional_check': 'def conditional_check(self, input_col: str) -> None:\\n        \"\"\"\\n\\n        Checks all the conditional columns in a single row in a config csv file.\\n\\n        :param input_col: The column to apply the check on.\\n\\n        \"\"\"\\n\\n        multiple_conditional_list = self.rule_df.loc[input_col, \"conditional_columns\"].split(\";\")\\n\\n        for cond_ind, condition_columns in enumerate(multiple_conditional_list):\\n\\n            current_additional_cond_value = self.rule_df.loc[input_col, \"conditional_column_value\"].split(\";\")[cond_ind]\\n\\n            current_conditional_valuelist = self.rule_df.loc[input_col, \"conditional_valuelist\"].split(\";\")[cond_ind]\\n\\n\\n\\n            first_col_cond, first_col_msg = self.conditional_cond_syntax(\\n\\n                input_col=input_col,\\n\\n                condition_column=condition_columns,\\n\\n                conditional_variables=current_conditional_valuelist,\\n\\n            )\\n\\n            for condition_column in condition_columns.split(\",\"):\\n\\n                if condition_column in self.input_columns:\\n\\n                    second_cal_cond, second_col_msg = self.conditional_cond_syntax(\\n\\n                        input_col=condition_column,\\n\\n                        condition_column=input_col,\\n\\n                        conditional_variables=current_additional_cond_value,\\n\\n                    )\\n\\n                    conditional_cond = first_col_cond & (~second_cal_cond)\\n\\n                    conditional_error_msg = f\"Cond_fail: {first_col_msg}+{second_col_msg}\"\\n\\n                    self.add_error_col(\\n\\n                        error_msg=conditional_error_msg,\\n\\n                        condition=conditional_cond,\\n\\n                        error_col_name=input_col + \" conditional_check\",\\n\\n                    )\\n\\n                else:\\n\\n                    self.sns_message.append(\\n\\n                        f\"Column {condition_column} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                    )\\n\\n        logger.info(f\"[{input_col}] conditional check is done.\")\\n\\n\\n\\n    def range_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The range check column condition\\n\\n        :param input_col: The column to apply the check on\\n\\n        :return Column: The range check column condition.\\n\\n        \"\"\"\\n\\n        schema_col = input_col + \" schema\"\\n\\n        output_cond = None\\n\\n\\n\\n        min_str = self.rule_df.loc[input_col, \"min\"]\\n\\n        min_value = self.limit_finder(input_col, min_str)\\n\\n        if min_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) < min_value)\\n\\n        max_str = self.rule_df.loc[input_col, \"max\"]\\n\\n        max_value = self.limit_finder(input_col, max_str)\\n\\n        if max_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) > max_value)\\n\\n        return min_str, max_str, output_cond\\n\\n\\n\\n    def range_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with a range check and add an error column to self.source_df.\\n\\n        Args:\\n\\n            input_col (str): the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start range_check\")\\n\\n        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\\n\\n        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] range check is done.\")\\n\\n\\n\\n    def file_check(self, input_col):\\n\\n        # finding source side columns\\n\\n        source_df_columns_list = [input_col]\\n\\n        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\\n\\n        if type(reference_columns_str) == str:\\n\\n            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\\n\\n            source_df_columns_list.extend(additional_columns_list)\\n\\n\\n\\n        # Find reference side columns\\n\\n        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\\n\\n        print(\"file check type\", file_check_type)\\n\\n        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\\n\\n        reference_columns = current_file_config[\"reference_columns\"]\\n\\n        print(\"ref cols\", reference_columns)\\n\\n\\n\\n        # Read reference file\\n\\n        file_path = current_file_config[\"path\"]\\n\\n        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\\n\\n        # changing columns names on reference to be the same as source\\n\\n        print(\"src df cols list\", source_df_columns_list)\\n\\n        for col_ind, ref_col in enumerate(reference_columns):\\n\\n            source_col = source_df_columns_list[col_ind]\\n\\n            try:\\n\\n                file_df = file_df.withColumnRenamed(ref_col, source_col)\\n\\n                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                if \"Postal Code\".upper() in source_col.upper():\\n\\n                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\\n\\n            except AnalysisException:\\n\\n                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\\n\\n                return None, None\\n\\n\\n\\n        pre_join_columns = set(self.source_df.columns)\\n\\n        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\\n\\n        post_join_columns = set(self.source_df.columns)\\n\\n        join_col = tuple(post_join_columns - pre_join_columns)[0]\\n\\n        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        file_error_msg = (\\n\\n            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\\n\\n        )\\n\\n        return file_cond, file_error_msg\\n\\n\\n\\n    def category_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'range_cond_syntax': 'def range_cond_syntax(self, input_col: str) -> Column:\\n        \"\"\"\\n\\n        The range check column condition\\n\\n        :param input_col: The column to apply the check on\\n\\n        :return Column: The range check column condition.\\n\\n        \"\"\"\\n\\n        schema_col = input_col + \" schema\"\\n\\n        output_cond = None\\n\\n\\n\\n        min_str = self.rule_df.loc[input_col, \"min\"]\\n\\n        min_value = self.limit_finder(input_col, min_str)\\n\\n        if min_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) < min_value)\\n\\n        max_str = self.rule_df.loc[input_col, \"max\"]\\n\\n        max_value = self.limit_finder(input_col, max_str)\\n\\n        if max_value is not None:\\n\\n            output_cond = output_cond | (f.col(schema_col) > max_value)\\n\\n        return min_str, max_str, output_cond\\n\\n\\n\\n    def range_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with a range check and add an error column to self.source_df.\\n\\n        Args:\\n\\n            input_col (str): the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start range_check\")\\n\\n        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\\n\\n        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] range check is done.\")\\n\\n\\n\\n    def file_check(self, input_col):\\n\\n        # finding source side columns\\n\\n        source_df_columns_list = [input_col]\\n\\n        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\\n\\n        if type(reference_columns_str) == str:\\n\\n            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\\n\\n            source_df_columns_list.extend(additional_columns_list)\\n\\n\\n\\n        # Find reference side columns\\n\\n        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\\n\\n        print(\"file check type\", file_check_type)\\n\\n        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\\n\\n        reference_columns = current_file_config[\"reference_columns\"]\\n\\n        print(\"ref cols\", reference_columns)\\n\\n\\n\\n        # Read reference file\\n\\n        file_path = current_file_config[\"path\"]\\n\\n        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\\n\\n        # changing columns names on reference to be the same as source\\n\\n        print(\"src df cols list\", source_df_columns_list)\\n\\n        for col_ind, ref_col in enumerate(reference_columns):\\n\\n            source_col = source_df_columns_list[col_ind]\\n\\n            try:\\n\\n                file_df = file_df.withColumnRenamed(ref_col, source_col)\\n\\n                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                if \"Postal Code\".upper() in source_col.upper():\\n\\n                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\\n\\n            except AnalysisException:\\n\\n                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\\n\\n                return None, None\\n\\n\\n\\n        pre_join_columns = set(self.source_df.columns)\\n\\n        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\\n\\n        post_join_columns = set(self.source_df.columns)\\n\\n        join_col = tuple(post_join_columns - pre_join_columns)[0]\\n\\n        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        file_error_msg = (\\n\\n            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\\n\\n        )\\n\\n        return file_cond, file_error_msg\\n\\n\\n\\n    def category_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'range_check': 'def range_check(self, input_col: str) -> None:\\n        \"\"\"\\n\\n        This method checks input_col with a range check and add an error column to self.source_df.\\n\\n        Args:\\n\\n            input_col (str): the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start range_check\")\\n\\n        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\\n\\n        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] range check is done.\")\\n\\n\\n\\n    def file_check(self, input_col):\\n\\n        # finding source side columns\\n\\n        source_df_columns_list = [input_col]\\n\\n        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\\n\\n        if type(reference_columns_str) == str:\\n\\n            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\\n\\n            source_df_columns_list.extend(additional_columns_list)\\n\\n\\n\\n        # Find reference side columns\\n\\n        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\\n\\n        print(\"file check type\", file_check_type)\\n\\n        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\\n\\n        reference_columns = current_file_config[\"reference_columns\"]\\n\\n        print(\"ref cols\", reference_columns)\\n\\n\\n\\n        # Read reference file\\n\\n        file_path = current_file_config[\"path\"]\\n\\n        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\\n\\n        # changing columns names on reference to be the same as source\\n\\n        print(\"src df cols list\", source_df_columns_list)\\n\\n        for col_ind, ref_col in enumerate(reference_columns):\\n\\n            source_col = source_df_columns_list[col_ind]\\n\\n            try:\\n\\n                file_df = file_df.withColumnRenamed(ref_col, source_col)\\n\\n                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                if \"Postal Code\".upper() in source_col.upper():\\n\\n                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\\n\\n            except AnalysisException:\\n\\n                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\\n\\n                return None, None\\n\\n\\n\\n        pre_join_columns = set(self.source_df.columns)\\n\\n        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\\n\\n        post_join_columns = set(self.source_df.columns)\\n\\n        join_col = tuple(post_join_columns - pre_join_columns)[0]\\n\\n        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        file_error_msg = (\\n\\n            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\\n\\n        )\\n\\n        return file_cond, file_error_msg\\n\\n\\n\\n    def category_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'file_check': 'def file_check(self, input_col):\\n        # finding source side columns\\n\\n        source_df_columns_list = [input_col]\\n\\n        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\\n\\n        if type(reference_columns_str) == str:\\n\\n            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\\n\\n            source_df_columns_list.extend(additional_columns_list)\\n\\n\\n\\n        # Find reference side columns\\n\\n        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\\n\\n        print(\"file check type\", file_check_type)\\n\\n        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\\n\\n        reference_columns = current_file_config[\"reference_columns\"]\\n\\n        print(\"ref cols\", reference_columns)\\n\\n\\n\\n        # Read reference file\\n\\n        file_path = current_file_config[\"path\"]\\n\\n        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\\n\\n        # changing columns names on reference to be the same as source\\n\\n        print(\"src df cols list\", source_df_columns_list)\\n\\n        for col_ind, ref_col in enumerate(reference_columns):\\n\\n            source_col = source_df_columns_list[col_ind]\\n\\n            try:\\n\\n                file_df = file_df.withColumnRenamed(ref_col, source_col)\\n\\n                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\\n\\n                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\\n\\n                if \"Postal Code\".upper() in source_col.upper():\\n\\n                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\\n\\n            except AnalysisException:\\n\\n                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\\n\\n                return None, None\\n\\n\\n\\n        pre_join_columns = set(self.source_df.columns)\\n\\n        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\\n\\n        post_join_columns = set(self.source_df.columns)\\n\\n        join_col = tuple(post_join_columns - pre_join_columns)[0]\\n\\n        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\\n\\n        file_error_msg = (\\n\\n            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\\n\\n        )\\n\\n        return file_cond, file_error_msg\\n\\n\\n\\n    def category_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'category_check': 'def category_check(self, input_col: str) -> None:\\n        \"\"\"\\n\\n        Method checks input_col with a category and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start category check\")\\n\\n        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\\n\\n        if valuelist_type[0:2] == \"__\":\\n\\n            category_cond, category_error_msg = self.file_check(input_col)\\n\\n        else:\\n\\n            category_list = valuelist_type.split(\",\")\\n\\n            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\\n\\n            category_cond = (f.col(input_col).isin(category_list) == False) & (\\n\\n                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n            )\\n\\n            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\\n\\n        )\\n\\n        logger.info(f\"[{input_col}] category check is done.\")\\n\\n\\n\\n    def duplicate_cond_syntax(self, input_col: str) -> Column:\\n\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'duplicate_cond_syntax': 'def duplicate_cond_syntax(self, input_col: str) -> Column:\\n        \"\"\"\\n\\n        The duplicate check column condition\\n\\n        :param input_col: The column to apply the check on.\\n\\n        :raise Column: the duplicate check column condition.\\n\\n        \"\"\"\\n\\n        self.source_df = self.source_df.join(\\n\\n            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\\n\\n            on=input_col,\\n\\n            how=\"inner\",\\n\\n        )\\n\\n        return f.col(\"Duplicate_indicator\") > 1\\n\\n\\n\\n    def duplicate_check(self, input_col: str) -> None:\\n\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'duplicate_check': 'def duplicate_check(self, input_col: str) -> None:\\n        \"\"\"\\n\\n        This method checks input_col with should be unique and add an error column to self.source_df.\\n\\n        :param input_col: the column to check.\\n\\n        \"\"\"\\n\\n        print(\"start duplicate_check\")\\n\\n        schema_col = input_col + \" schema\"\\n\\n        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\\n\\n            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\\n\\n        )\\n\\n        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\\n\\n        self.add_error_col(\\n\\n            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\\n\\n        )\\n\\n        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\\n\\n        logger.info(f\"[{input_col}] duplicate check is done.\")\\n\\n\\n\\n    def main_pipeline(self) -> DataFrame:\\n\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n', 'main_pipeline': 'def main_pipeline(self) -> DataFrame:\\n        \"\"\"\\n\\n        The main pipeline to do all the checks on the given data frame.\\n\\n        :return DataFrame: The consolidated error dataframe.\\n\\n        \"\"\"\\n\\n\\n\\n        columns_to_check_dict = {}\\n\\n        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\\n\\n        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\\n\\n        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\\n\\n        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\\n\\n        columns_to_check_dict[self.range_check] = list(\\n\\n            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\\n\\n        )\\n\\n\\n\\n        for index in range(len(self.input_columns)):\\n\\n            print(\"inpul col\", self.input_columns[index])\\n\\n            for check_type in columns_to_check_dict.keys():\\n\\n                if self.input_columns[index] in columns_to_check_dict[check_type]:\\n\\n                    check_type(self.input_columns[index])\\n\\n\\n\\n        # conditional column in a different loop since they rely on schema of the multiple columns.\\n\\n        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\\n\\n        for conditional_col in columns_to_check_dict[self.conditional_check]:\\n\\n            if conditional_col in self.input_columns:\\n\\n                self.conditional_check(conditional_col)\\n\\n            else:\\n\\n                self.sns_message.append(\\n\\n                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\\n\\n                )\\n\\n\\n\\n        # combining all error columns to one column.\\n\\n        self.source_df = (\\n\\n            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\\n\\n            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\\n\\n            .drop(\"temp\")\\n\\n        )\\n\\n\\n\\n        # exploding the error column into multiple rows.\\n\\n        print(\"exploding error df\")\\n\\n        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\\n\\n            f.col(\"Error\").isNotNull()\\n\\n        )\\n\\n\\n\\n        if self.error_df and self.error_df.rdd.isEmpty():\\n\\n            self.error_df = None\\n\\n        return self.error_df, self.sns_message\\n'}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "import inspect\n",
    "\n",
    "def get_functions_dict(script_path):\n",
    "    functions_dict = {}\n",
    "    with open(script_path) as f:\n",
    "        lines = f.readlines()\n",
    "    for index,line in enumerate(lines):\n",
    "        if line.strip().startswith('def '):\n",
    "            line_space = len(line) - len(line.lstrip()) \n",
    "            # Get the function name\n",
    "            func_name = line.split()[1].split('(')[0]\n",
    "            # Get the function definition\n",
    "            func_def = line.strip()\n",
    "            i=1\n",
    "            next_line = lines[index+i]\n",
    "            next_line_space = len(next_line)- len(next_line.lstrip())\n",
    "            while next_line_space>line_space or next_line.endswith(':\\n') or next_line==\"\\n\":\n",
    "                \n",
    "                func_def += f'\\n{next_line}'\n",
    "                i+=1\n",
    "                try:\n",
    "                    next_line = lines[index+i]\n",
    "                except IndexError:\n",
    "                    break\n",
    "                next_line_space = len(next_line)- len(next_line.lstrip())\n",
    "            functions_dict[func_name] = func_def\n",
    "            # Add the function to the dictionary\n",
    "                \n",
    "    return functions_dict\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = \"example_script.py\"\n",
    "\n",
    "# Example usage\n",
    "file_path = \"dq_check.py\"\n",
    "functions_dict = get_functions_dict(file_path)\n",
    "print(functions_dict)\n",
    "# finally:\n",
    "#     f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def __init__(\n",
      "        self,\n",
      "\n",
      "        source_df: DataFrame,\n",
      "\n",
      "        spark_context: SparkSession.builder.getOrCreate,\n",
      "\n",
      "        config_path: str,\n",
      "\n",
      "        file_name: str,\n",
      "\n",
      "        src_system: str,\n",
      "\n",
      "    ) -> None:\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        A class checking the quality of a source data frame based on the given criteria.\n",
      "\n",
      "        :param source_df (DataFrame): the source data frame to apply the data quality checks on.\n",
      "\n",
      "        :param spark_context (SparkSession.builder.getOrCreate): the spark session to run the data quality check.\n",
      "\n",
      "        :param config_path (str): the path to config csv file.\n",
      "\n",
      "        :param file_name (str): the name of the file to check the data quality check on\n",
      "\n",
      "        :param src_system (str): the source of the file where it comes from (vendor)\n",
      "\n",
      "        :raise KeyError: raise a key error if the columns in the source data frame is not in the config file.\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Set variables\n",
      "\n",
      "        self.spark = spark_context\n",
      "\n",
      "        self.source_df = source_df\n",
      "\n",
      "\n",
      "\n",
      "        self.error_df = None\n",
      "\n",
      "        self.error_columns = []\n",
      "\n",
      "        self.error_counter = 0\n",
      "\n",
      "        self.schema_dict = {\n",
      "\n",
      "            \"StringType\": StringType,\n",
      "\n",
      "            \"DateType\": DateType,\n",
      "\n",
      "            \"IntegerType\": IntegerType,\n",
      "\n",
      "            \"FloatType\": FloatType,\n",
      "\n",
      "            \"DoubleType\": DoubleType,\n",
      "\n",
      "        }\n",
      "\n",
      "\n",
      "\n",
      "        # Initial configuration\n",
      "\n",
      "        config_content = self.read_s3_file(config_path).decode()\n",
      "\n",
      "        self.config = self.resolve_config(config_path.replace(\"config.json\", \"env.json\"), json.loads(config_content))\n",
      "\n",
      "\n",
      "\n",
      "        dq_rule_path = self.config[src_system][\"dq_rule_path\"]\n",
      "\n",
      "        # dq_rule_content = self.read_s3_file(dq_rule_path)\n",
      "\n",
      "        self.rule_df = pd.read_csv(dq_rule_path, index_col=\"column_name\")\n",
      "\n",
      "        self.file_name = file_name\n",
      "\n",
      "        self.rule_df = self.rule_df[(self.rule_df[\"file_name\"] == self.file_name)]\n",
      "\n",
      "        self.rule_df = self.rule_df.applymap(lambda x: x if x else np.nan)\n",
      "\n",
      "        self.rule_df.sort_index(inplace=True)\n",
      "\n",
      "        self.sns_message = []\n",
      "\n",
      "\n",
      "\n",
      "        self.input_columns = self.source_df.columns\n",
      "\n",
      "        self.output_columns = self.config[src_system][\"sources\"][self.file_name][\"dq_output_columns\"]\n",
      "\n",
      "        for index in range(len(self.input_columns)):\n",
      "\n",
      "            if \".\" in self.input_columns[index]:\n",
      "\n",
      "                self.input_columns[index] = \"`\" + self.input_columns[index] + \"`\"\n",
      "\n",
      "\n",
      "\n",
      "        missed_columns = set(self.input_columns) - set(self.rule_df.index)\n",
      "\n",
      "        if len(missed_columns) > 0:\n",
      "\n",
      "            logger.warning(f\"[{missed_columns}] are not found in the rule file.\")\n",
      "\n",
      "\n",
      "\n",
      "        self.spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
      "\n",
      "        # self.spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\n",
      "\n",
      "        self.spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
      "\n",
      "        self.spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
      "\n",
      "\n",
      "\n",
      "    def read_s3_file(self, file_path) -> bytes:\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        Read s3 file content and return it in byte format\n",
      "\n",
      "        :param file_path: full s3 object path\n",
      "\n",
      "        :return byte content of the file\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        file_res = urlparse(file_path)\n",
      "\n",
      "        try:\n",
      "\n",
      "            file_obj = s3_resource.Object(file_res.netloc, file_res.path.lstrip(\"/\"))\n",
      "\n",
      "            return file_obj.get()[\"Body\"].read()\n",
      "\n",
      "        except ClientError:\n",
      "\n",
      "            raise FileNotFoundError(f\"File cannot be found in S3 given path '{file_path}'\")\n",
      "\n",
      "\n",
      "\n",
      "    def resolve_config(self, env_path, config_content):\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        Read config content and resolve env variables\n",
      "\n",
      "        :param env_path: environment file path\n",
      "\n",
      "        :param config_content: environment agnostic config file\n",
      "\n",
      "        :return environmentally resolved config\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        env_content = self.read_s3_file(env_path).decode()\n",
      "\n",
      "        env_sub = json.loads(env_content)[\"subs\"]\n",
      "\n",
      "\n",
      "\n",
      "        config_content_str = (\n",
      "\n",
      "            str(config_content)\n",
      "\n",
      "            .replace(\"<env>\", env_sub[\"<env>\"])\n",
      "\n",
      "            .replace(\"<_env>\", env_sub[\"<_env>\"])\n",
      "\n",
      "            .replace(\"<bucket_name>\", env_sub[\"<bucket_name>\"])\n",
      "\n",
      "            .replace(\"<account>\", env_sub[\"<account>\"])\n",
      "\n",
      "        )\n",
      "\n",
      "\n",
      "\n",
      "        return ast.literal_eval(config_content_str)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(functions_dict['__init__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "import inspect\n",
    "\n",
    "def get_functions_dict(script_path):\n",
    "    functions_dict = {}\n",
    "    with open(script_path) as f:\n",
    "        lines = f.readlines()\n",
    "    for index,line in enumerate(lines):\n",
    "        if line.strip().startswith('def '):\n",
    "            # Get the function name\n",
    "            func_name = line.split()[1].split('(')[0]\n",
    "            # Add the function to the dictionary\n",
    "                \n",
    "    return functions_dict\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = \"example_script.py\"\n",
    "\n",
    "# Example usage\n",
    "file_path = \"dq_check.py\"\n",
    "functions_dict = get_functions_dict(file_path)\n",
    "print(functions_dict)\n",
    "# finally:\n",
    "#     f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "my_str = \"    Hello,    world!\"\n",
    "count = len(my_str) - len(my_str.lstrip())   # subtract 1 to exclude the first non-space character\n",
    "print(count)  # output: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getmembers, isfunction\n",
    "from test_code.to_test.dq_utility import DataCheck\n",
    "    \n",
    "functions_list = getmembers(DataCheck, isfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def __init__(\n",
      "        self,\n",
      "        source_df: DataFrame,\n",
      "        spark_context: SparkSession.builder.getOrCreate,\n",
      "        config_path: str,\n",
      "        file_name: str,\n",
      "        src_system: str,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        A class checking the quality of a source data frame based on the given criteria.\n",
      "        :param source_df (DataFrame): the source data frame to apply the data quality checks on.\n",
      "        :param spark_context (SparkSession.builder.getOrCreate): the spark session to run the data quality check.\n",
      "        :param config_path (str): the path to config csv file.\n",
      "        :param file_name (str): the name of the file to check the data quality check on\n",
      "        :param src_system (str): the source of the file where it comes from (vendor)\n",
      "        :raise KeyError: raise a key error if the columns in the source data frame is not in the config file.\n",
      "        \"\"\"\n",
      "        # Set variables\n",
      "        self.spark = spark_context\n",
      "        self.source_df = source_df\n",
      "\n",
      "        self.error_df = None\n",
      "        self.error_columns = []\n",
      "        self.error_counter = 0\n",
      "        self.schema_dict = {\n",
      "            \"StringType\": StringType,\n",
      "            \"DateType\": DateType,\n",
      "            \"IntegerType\": IntegerType,\n",
      "            \"FloatType\": FloatType,\n",
      "            \"DoubleType\": DoubleType,\n",
      "        }\n",
      "\n",
      "        # Initial configuration\n",
      "        config_content = self.read_s3_file(config_path).decode()\n",
      "        self.config = self.resolve_config(config_path.replace(\"config.json\", \"env.json\"), json.loads(config_content))\n",
      "\n",
      "        dq_rule_path = self.config[src_system][\"dq_rule_path\"]\n",
      "        # dq_rule_content = self.read_s3_file(dq_rule_path)\n",
      "        self.rule_df = pd.read_csv(dq_rule_path, index_col=\"column_name\")\n",
      "        self.file_name = file_name\n",
      "        self.rule_df = self.rule_df[(self.rule_df[\"file_name\"] == self.file_name)]\n",
      "        self.rule_df = self.rule_df.applymap(lambda x: x if x else np.nan)\n",
      "        self.rule_df.sort_index(inplace=True)\n",
      "        self.sns_message = []\n",
      "\n",
      "        self.input_columns = self.source_df.columns\n",
      "        self.output_columns = self.config[src_system][\"sources\"][self.file_name][\"dq_output_columns\"]\n",
      "        for index in range(len(self.input_columns)):\n",
      "            if \".\" in self.input_columns[index]:\n",
      "                self.input_columns[index] = \"`\" + self.input_columns[index] + \"`\"\n",
      "\n",
      "        missed_columns = set(self.input_columns) - set(self.rule_df.index)\n",
      "        if len(missed_columns) > 0:\n",
      "            logger.warning(f\"[{missed_columns}] are not found in the rule file.\")\n",
      "\n",
      "        self.spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
      "        # self.spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\n",
      "        self.spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
      "        self.spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(functions_list[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['main.py', 'dq_check.py', 'test_dq_check_short.py', 'dq_check_short.py']\n",
      "unit_test_from_function\n",
      "broadcast\n",
      "urlparse\n",
      "broadcast\n",
      "urlparse\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import os\n",
    "\n",
    "# Set the path to the directory containing the Python files\n",
    "directory_path = \".\"\n",
    "\n",
    "# Get a list of all the Python files in the directory\n",
    "py_files = [f for f in os.listdir(directory_path) if f.endswith('.py')]\n",
    "print(py_files)\n",
    "# Iterate over each file and extract the function names\n",
    "for file in py_files:\n",
    "    # Import the module\n",
    "    module = __import__(file[:-3])\n",
    "    \n",
    "    # Iterate over each function in the module and print the name\n",
    "    for name, obj in inspect.getmembers(module):\n",
    "        if inspect.isfunction(obj):\n",
    "            print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unit_test_from_function', 'broadcast', 'urlparse', 'broadcast', 'urlparse']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import inspect\n",
    "import importlib.util\n",
    "\n",
    "def get_functions(directory):\n",
    "    functions = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.py'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            spec = importlib.util.spec_from_file_location(filename, filepath)\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(module)\n",
    "            for name, obj in inspect.getmembers(module):\n",
    "                if inspect.isfunction(obj):\n",
    "                    functions.append(name)\n",
    "    return functions\n",
    "get_functions('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dq_check'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getmodulename('/home/ubuntu/openai-cookbook/test_code/dq_check.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib.util\n",
    "import inspect\n",
    "\n",
    "def get_functions_and_classes_in_dir(dir_path):\n",
    "    for filename in os.listdir(dir_path):\n",
    "        obj_dict = {}\n",
    "        if filename.endswith(\".py\"):\n",
    "            module_name = filename[:-3]\n",
    "            spec = importlib.util.spec_from_file_location(module_name, os.path.join(dir_path, filename))\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(module)\n",
    "            for name, obj in inspect.getmembers(module):\n",
    "                if inspect.isfunction(obj) or inspect.isclass(obj):\n",
    "                    obj_dict[name]=obj\n",
    "    return obj_dict\n",
    "get_functions_and_classes_in_dir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function name: unit_test_from_function\n",
      "Function definition: def unit_test_from_function(\n",
      "    function_to_test: str,  # Python function to test, as a string\n",
      "    unit_test_package: str = \"pytest\",  # unit testing package; use the name as it appears in the import statement\n",
      "    approx_min_cases_to_cover: int = 7,  # minimum number of test case categories to cover (approximate)\n",
      "    print_text: bool = False,  # optionally prints text; helpful for understanding the function & debugging\n",
      "    engine: str = \"GPT4\",  # engine used to generate text plans in steps 1, 2, and 2b\n",
      "    max_tokens: int = 1000,  # can set this high, as generations should be stopped earlier by stop sequences\n",
      "    temperature: float = 0.4,  # temperature = 0 can sometimes get stuck in repetitive loops, so we use 0.4\n",
      "    reruns_if_fail: int = 1,  # if the output code cannot be parsed, this will re-run the function up to N times\n",
      ") -> str:\n",
      "    \"\"\"Outputs a unit test for a given Python function, using a 3-step GPT-3 prompt.\"\"\"\n",
      "\n",
      "    # Step 1: Generate an explanation of the function\n",
      "\n",
      "    # create a markdown-formatted prompt that asks GPT-3 to complete an explanation of the function, formatted as a bullet list\n",
      "    prompt_to_explain_the_function = f\"\"\"# How to write great unit tests with {unit_test_package}\n",
      "\n",
      "In this advanced tutorial for experts, we'll use Python 3.9 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function.\n",
      "```python\n",
      "{function_to_test}\n",
      "```\n",
      "\n",
      "Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
      "- First,\"\"\"\n",
      "    if print_text:\n",
      "        text_color_prefix = \"\\033[30m\"  # black; if you read against a dark background \\033[97m is white\n",
      "        print(\n",
      "            text_color_prefix + prompt_to_explain_the_function, end=\"\"\n",
      "        )  # end='' prevents a newline from being printed\n",
      "\n",
      "    # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n",
      "    explanation_response = openai.ChatCompletion.create(\n",
      "        engine=engine,\n",
      "        messages=[{\n",
      "            \"role\":\"system\",\n",
      "            \"content\": prompt_to_explain_the_function\n",
      "        }],\n",
      "        # stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n",
      "        # max_tokens=max_tokens,\n",
      "        # temperature=temperature,\n",
      "        # stream=True,\n",
      "    )\n",
      "    # explanation_completion = \"\"\n",
      "    if print_text:\n",
      "        completion_color_prefix = \"\\033[92m\"  # green\n",
      "        print(completion_color_prefix, end=\"\")\n",
      "    explanation_completion = explanation_response.choices[0].message.content\n",
      "    if print_text:\n",
      "        print(explanation_completion, end=\"\")\n",
      "\n",
      "    # Step 2: Generate a plan to write a unit test\n",
      "\n",
      "    # create a markdown-formatted prompt that asks GPT-3 to complete a plan for writing unit tests, formatted as a bullet list\n",
      "    prompt_to_explain_a_plan = f\"\"\"\n",
      "\n",
      "A good unit test suite should aim to:\n",
      "- Test the function's behavior for a wide range of possible inputs\n",
      "- Test edge cases that the author may not have foreseen\n",
      "- Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain\n",
      "- Be easy to read and understand, with clean code and descriptive names\n",
      "- Be deterministic, so that the tests always pass or fail in the same way\n",
      "\n",
      "`{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n",
      "\n",
      "For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n",
      "-\"\"\"\n",
      "    if print_text:\n",
      "        print(text_color_prefix + prompt_to_explain_a_plan, end=\"\")\n",
      "\n",
      "    # append this planning prompt to the results from step 1\n",
      "    prior_text = prompt_to_explain_the_function + explanation_completion\n",
      "    full_plan_prompt = prior_text + prompt_to_explain_a_plan\n",
      "\n",
      "    # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n",
      "    plan_response = openai.ChatCompletion.create(\n",
      "        engine=engine,\n",
      "        messages=[{\n",
      "            \"role\":\"system\",\n",
      "            \"content\": full_plan_prompt\n",
      "        }],\n",
      "        # stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n",
      "        # max_tokens=max_tokens,\n",
      "        # temperature=temperature,\n",
      "        # stream=True,\n",
      "    )\n",
      "    # plan_completion = \"\"\n",
      "    if print_text:\n",
      "        print(completion_color_prefix, end=\"\")\n",
      "    plan_completion = plan_response.choices[0].message.content\n",
      "    if print_text:\n",
      "        print(plan_completion, end=\"\")\n",
      "\n",
      "    # Step 2b: If the plan is short, ask GPT-3 to elaborate further\n",
      "    # this counts top-level bullets (e.g., categories), but not sub-bullets (e.g., test cases)\n",
      "    elaboration_needed = (\n",
      "        plan_completion.count(\"\\n-\") + 1 < approx_min_cases_to_cover\n",
      "    )  # adds 1 because the first bullet is not counted\n",
      "    if elaboration_needed:\n",
      "        prompt_to_elaborate_on_the_plan = f\"\"\"\n",
      "\n",
      "In addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):\n",
      "-\"\"\"\n",
      "        if print_text:\n",
      "            print(text_color_prefix + prompt_to_elaborate_on_the_plan, end=\"\")\n",
      "\n",
      "        # append this elaboration prompt to the results from step 2\n",
      "        prior_text = full_plan_prompt + plan_completion\n",
      "        full_elaboration_prompt = prior_text + prompt_to_elaborate_on_the_plan\n",
      "\n",
      "        # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n",
      "        elaboration_response = openai.ChatCompletion.create(\n",
      "            engine=engine,\n",
      "            messages=[{\n",
      "                \"role\":\"system\",\n",
      "                \"content\": full_elaboration_prompt\n",
      "            }],\n",
      "            # stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n",
      "            # max_tokens=max_tokens,\n",
      "            # temperature=temperature,\n",
      "            # stream=True,\n",
      "        )\n",
      "        # elaboration_completion = \"\"\n",
      "        if print_text:\n",
      "            print(completion_color_prefix, end=\"\")\n",
      "        elaboration_completion = elaboration_response.choices[0].message.content\n",
      "        if print_text:\n",
      "            print(elaboration_completion, end=\"\")\n",
      "\n",
      "    # Step 3: Generate the unit test\n",
      "\n",
      "    # create a markdown-formatted prompt that asks GPT-3 to complete a unit test\n",
      "    starter_comment = \"\"\n",
      "    if unit_test_package == \"pytest\":\n",
      "        starter_comment = (\n",
      "            \"Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\"\n",
      "        )\n",
      "    prompt_to_generate_the_unit_test = f\"\"\"\n",
      "\n",
      "Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n",
      "```python\n",
      "import {unit_test_package}  # used for our unit tests\n",
      "\n",
      "{function_to_test}\n",
      "\n",
      "#{starter_comment}\"\"\"\n",
      "    if print_text:\n",
      "        print(text_color_prefix + prompt_to_generate_the_unit_test, end=\"\")\n",
      "\n",
      "    # append this unit test prompt to the results from step 3\n",
      "    if elaboration_needed:\n",
      "        prior_text = full_elaboration_prompt + elaboration_completion\n",
      "    else:\n",
      "        prior_text = full_plan_prompt + plan_completion\n",
      "    full_unit_test_prompt = prior_text + prompt_to_generate_the_unit_test\n",
      "\n",
      "    # send the prompt to the API, using ``` as a stop sequence to stop at the end of the code block\n",
      "    unit_test_response = openai.ChatCompletion.create(\n",
      "        engine=engine,\n",
      "        messages=[{\n",
      "            \"role\":\"system\",\n",
      "            \"content\": full_unit_test_prompt\n",
      "        }],\n",
      "        # stop=\"```\",\n",
      "        # max_tokens=max_tokens,\n",
      "        # temperature=temperature,\n",
      "        # stream=True,\n",
      "    )\n",
      "    # unit_test_completion = \"\"\n",
      "    if print_text:\n",
      "        print(completion_color_prefix, end=\"\")\n",
      "    unit_test_completion = unit_test_response.choices[0].message.content\n",
      "    if print_text:\n",
      "        print(unit_test_completion, end=\"\")\n",
      "\n",
      "    # check the output for errors\n",
      "    code_start_index = prompt_to_generate_the_unit_test.find(\"```python\\n\") + len(\"```python\\n\")\n",
      "    code_output = prompt_to_generate_the_unit_test[code_start_index:] + unit_test_completion\n",
      "    try:\n",
      "        ast.parse(code_output)\n",
      "    except SyntaxError as e:\n",
      "        print(f\"Syntax error in generated code: {e}\")\n",
      "        if reruns_if_fail > 0:\n",
      "            print(\"Rerunning...\")\n",
      "            return unit_test_from_function(\n",
      "                function_to_test=function_to_test,\n",
      "                unit_test_package=unit_test_package,\n",
      "                approx_min_cases_to_cover=approx_min_cases_to_cover,\n",
      "                print_text=print_text,\n",
      "                engine=engine,\n",
      "                max_tokens=max_tokens,\n",
      "                temperature=temperature,\n",
      "                reruns_if_fail=reruns_if_fail - 1,  # decrement rerun counter when calling again\n",
      "            )\n",
      "\n",
      "    # return the unit test as a string\n",
      "    return unit_test_completion\n",
      "\n",
      "------------\n",
      "Function name: broadcast\n",
      "Function definition: @since(1.6)\n",
      "def broadcast(df: DataFrame) -> DataFrame:\n",
      "    \"\"\"Marks a DataFrame as small enough for use in broadcast joins.\"\"\"\n",
      "\n",
      "    sc = SparkContext._active_spark_context\n",
      "    assert sc is not None and sc._jvm is not None\n",
      "    return DataFrame(sc._jvm.functions.broadcast(df._jdf), df.sparkSession)\n",
      "\n",
      "------------\n",
      "Function name: urlparse\n",
      "Function definition: def urlparse(url, scheme='', allow_fragments=True):\n",
      "    \"\"\"Parse a URL into 6 components:\n",
      "    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n",
      "\n",
      "    The result is a named 6-tuple with fields corresponding to the\n",
      "    above. It is either a ParseResult or ParseResultBytes object,\n",
      "    depending on the type of the url parameter.\n",
      "\n",
      "    The username, password, hostname, and port sub-components of netloc\n",
      "    can also be accessed as attributes of the returned object.\n",
      "\n",
      "    The scheme argument provides the default value of the scheme\n",
      "    component when no scheme is found in url.\n",
      "\n",
      "    If allow_fragments is False, no attempt is made to separate the\n",
      "    fragment component from the previous component, which can be either\n",
      "    path or query.\n",
      "\n",
      "    Note that % escapes are not expanded.\n",
      "    \"\"\"\n",
      "    url, scheme, _coerce_result = _coerce_args(url, scheme)\n",
      "    splitresult = urlsplit(url, scheme, allow_fragments)\n",
      "    scheme, netloc, url, query, fragment = splitresult\n",
      "    if scheme in uses_params and ';' in url:\n",
      "        url, params = _splitparams(url)\n",
      "    else:\n",
      "        params = ''\n",
      "    result = ParseResult(scheme, netloc, url, params, query, fragment)\n",
      "    return _coerce_result(result)\n",
      "\n",
      "------------\n",
      "Method name: __init__\n",
      "Method definition:     def __init__(\n",
      "        self,\n",
      "        desc: Optional[str] = None,\n",
      "        stackTrace: Optional[str] = None,\n",
      "        cause: Optional[Py4JJavaError] = None,\n",
      "        origin: Optional[Py4JJavaError] = None,\n",
      "    ):\n",
      "        # desc & stackTrace vs origin are mutually exclusive.\n",
      "        # cause is optional.\n",
      "        assert (origin is not None and desc is None and stackTrace is None) or (\n",
      "            origin is None and desc is not None and stackTrace is not None\n",
      "        )\n",
      "\n",
      "        self.desc = desc if desc is not None else cast(Py4JJavaError, origin).getMessage()\n",
      "        assert SparkContext._jvm is not None\n",
      "        self.stackTrace = (\n",
      "            stackTrace\n",
      "            if stackTrace is not None\n",
      "            else (SparkContext._jvm.org.apache.spark.util.Utils.exceptionString(origin))\n",
      "        )\n",
      "        self.cause = convert_exception(cause) if cause is not None else None\n",
      "        if self.cause is None and origin is not None and origin.getCause() is not None:\n",
      "            self.cause = convert_exception(origin.getCause())\n",
      "        self._origin = origin\n",
      "\n",
      "------------\n",
      "Method name: __str__\n",
      "Method definition:     def __str__(self) -> str:\n",
      "        assert SparkContext._jvm is not None\n",
      "\n",
      "        jvm = SparkContext._jvm\n",
      "        sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()\n",
      "        debug_enabled = sql_conf.pysparkJVMStacktraceEnabled()\n",
      "        desc = self.desc\n",
      "        if debug_enabled:\n",
      "            desc = desc + \"\\n\\nJVM stacktrace:\\n%s\" % self.stackTrace\n",
      "        return str(desc)\n",
      "\n",
      "------------\n",
      "Method name: getErrorClass\n",
      "Method definition:     def getErrorClass(self) -> Optional[str]:\n",
      "        assert SparkContext._gateway is not None\n",
      "\n",
      "        gw = SparkContext._gateway\n",
      "        if self._origin is not None and is_instance_of(\n",
      "            gw, self._origin, \"org.apache.spark.SparkThrowable\"\n",
      "        ):\n",
      "            return self._origin.getErrorClass()\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "------------\n",
      "Method name: getSqlState\n",
      "Method definition:     def getSqlState(self) -> Optional[str]:\n",
      "        assert SparkContext._gateway is not None\n",
      "\n",
      "        gw = SparkContext._gateway\n",
      "        if self._origin is not None and is_instance_of(\n",
      "            gw, self._origin, \"org.apache.spark.SparkThrowable\"\n",
      "        ):\n",
      "            return self._origin.getSqlState()\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "------------\n",
      "Method name: __init__\n",
      "Method definition:     def __init__(self, error_response, operation_name):\n",
      "        retry_info = self._get_retry_info(error_response)\n",
      "        error = error_response.get('Error', {})\n",
      "        msg = self.MSG_TEMPLATE.format(\n",
      "            error_code=error.get('Code', 'Unknown'),\n",
      "            error_message=error.get('Message', 'Unknown'),\n",
      "            operation_name=operation_name,\n",
      "            retry_info=retry_info,\n",
      "        )\n",
      "        super().__init__(msg)\n",
      "        self.response = error_response\n",
      "        self.operation_name = operation_name\n",
      "\n",
      "------------\n",
      "Method name: __reduce__\n",
      "Method definition:     def __reduce__(self):\n",
      "        # Subclasses of ClientError's are dynamically generated and\n",
      "        # cannot be pickled unless they are attributes of a\n",
      "        # module. So at the very least return a ClientError back.\n",
      "        return ClientError, (self.response, self.operation_name)\n",
      "\n",
      "------------\n",
      "Method name: _get_retry_info\n",
      "Method definition:     def _get_retry_info(self, response):\n",
      "        retry_info = ''\n",
      "        if 'ResponseMetadata' in response:\n",
      "            metadata = response['ResponseMetadata']\n",
      "            if metadata.get('MaxAttemptsReached', False):\n",
      "                if 'RetryAttempts' in metadata:\n",
      "                    retry_info = (\n",
      "                        f\" (reached max retries: {metadata['RetryAttempts']})\"\n",
      "                    )\n",
      "        return retry_info\n",
      "\n",
      "------------\n",
      "Method name: __add__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __and__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __bool__\n",
      "Method definition:     def __nonzero__(self) -> None:\n",
      "        raise ValueError(\n",
      "            \"Cannot convert column into bool: please use '&' for 'and', '|' for 'or', \"\n",
      "            \"'~' for 'not' when building DataFrame boolean expressions.\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: __contains__\n",
      "Method definition:     def __contains__(self, item: Any) -> None:\n",
      "        raise ValueError(\n",
      "            \"Cannot apply 'in' operator against a column: please use 'contains' \"\n",
      "            \"in a string column or 'array_contains' function for an array column.\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: __div__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     def __eq__(  # type: ignore[override]\n",
      "        self,\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        \"\"\"binary function\"\"\"\n",
      "        return _bin_op(\"equalTo\")(self, other)\n",
      "\n",
      "------------\n",
      "Method name: __ge__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __getattr__\n",
      "Method definition:     def __getattr__(self, item: Any) -> \"Column\":\n",
      "        if item.startswith(\"__\"):\n",
      "            raise AttributeError(item)\n",
      "        return self[item]\n",
      "\n",
      "------------\n",
      "Method name: __getitem__\n",
      "Method definition:     def __getitem__(self, k: Any) -> \"Column\":\n",
      "        if isinstance(k, slice):\n",
      "            if k.step is not None:\n",
      "                raise ValueError(\"slice with step is not supported.\")\n",
      "            return self.substr(k.start, k.stop)\n",
      "        else:\n",
      "            return _bin_op(\"apply\")(self, k)\n",
      "\n",
      "------------\n",
      "Method name: __gt__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __init__\n",
      "Method definition:     def __init__(self, jc: JavaObject) -> None:\n",
      "        self._jc = jc\n",
      "\n",
      "------------\n",
      "Method name: __invert__\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None and sc._jvm is not None\n",
      "        jc = getattr(sc._jvm.functions, name)(self._jc)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __iter__\n",
      "Method definition:     def __iter__(self) -> None:\n",
      "        raise TypeError(\"Column is not iterable\")\n",
      "\n",
      "------------\n",
      "Method name: __le__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __lt__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __mod__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __mul__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     def __ne__(  # type: ignore[override]\n",
      "        self,\n",
      "        other: Any,\n",
      "    ) -> \"Column\":\n",
      "        \"\"\"binary function\"\"\"\n",
      "        return _bin_op(\"notEqual\")(self, other)\n",
      "\n",
      "------------\n",
      "Method name: __neg__\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None and sc._jvm is not None\n",
      "        jc = getattr(sc._jvm.functions, name)(self._jc)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __nonzero__\n",
      "Method definition:     def __nonzero__(self) -> None:\n",
      "        raise ValueError(\n",
      "            \"Cannot convert column into bool: please use '&' for 'and', '|' for 'or', \"\n",
      "            \"'~' for 'not' when building DataFrame boolean expressions.\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: __or__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __pow__\n",
      "Method definition:     def _(self: \"Column\", other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\"]) -> \"Column\":\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None and sc._jvm is not None\n",
      "        fn = getattr(sc._jvm.functions, name)\n",
      "        jc = other._jc if isinstance(other, Column) else _create_column_from_literal(other)\n",
      "        njc = fn(self._jc, jc) if not reverse else fn(jc, self._jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __radd__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __rand__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __rdiv__\n",
      "Method definition:     def _(self: \"Column\", other: Union[\"LiteralType\", \"DecimalLiteral\"]) -> \"Column\":\n",
      "        jother = _create_column_from_literal(other)\n",
      "        jc = getattr(jother, name)(self._jc)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        return \"Column<'%s'>\" % self._jc.toString()\n",
      "\n",
      "------------\n",
      "Method name: __rmod__\n",
      "Method definition:     def _(self: \"Column\", other: Union[\"LiteralType\", \"DecimalLiteral\"]) -> \"Column\":\n",
      "        jother = _create_column_from_literal(other)\n",
      "        jc = getattr(jother, name)(self._jc)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __rmul__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __ror__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __rpow__\n",
      "Method definition:     def _(self: \"Column\", other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\"]) -> \"Column\":\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None and sc._jvm is not None\n",
      "        fn = getattr(sc._jvm.functions, name)\n",
      "        jc = other._jc if isinstance(other, Column) else _create_column_from_literal(other)\n",
      "        njc = fn(self._jc, jc) if not reverse else fn(jc, self._jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __rsub__\n",
      "Method definition:     def _(self: \"Column\", other: Union[\"LiteralType\", \"DecimalLiteral\"]) -> \"Column\":\n",
      "        jother = _create_column_from_literal(other)\n",
      "        jc = getattr(jother, name)(self._jc)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __rtruediv__\n",
      "Method definition:     def _(self: \"Column\", other: Union[\"LiteralType\", \"DecimalLiteral\"]) -> \"Column\":\n",
      "        jother = _create_column_from_literal(other)\n",
      "        jc = getattr(jother, name)(self._jc)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __sub__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __truediv__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: alias\n",
      "Method definition:     def alias(self, *alias: str, **kwargs: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Returns this column aliased with a new name or names (in the case of expressions that\n",
      "        return more than one column, such as explode).\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        alias : str\n",
      "            desired column names (collects all positional arguments passed)\n",
      "\n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        metadata: dict\n",
      "            a dict of information to be stored in ``metadata`` attribute of the\n",
      "            corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      "            only argument)\n",
      "\n",
      "            .. versionchanged:: 2.2.0\n",
      "               Added optional ``metadata`` argument.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.age.alias(\"age2\")).collect()\n",
      "        [Row(age2=2), Row(age2=5)]\n",
      "        >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      "        99\n",
      "        \"\"\"\n",
      "\n",
      "        metadata = kwargs.pop(\"metadata\", None)\n",
      "        assert not kwargs, \"Unexpected kwargs where passed: %s\" % kwargs\n",
      "\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None\n",
      "        if len(alias) == 1:\n",
      "            if metadata:\n",
      "                assert sc._jvm is not None\n",
      "                jmeta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(json.dumps(metadata))\n",
      "                return Column(getattr(self._jc, \"as\")(alias[0], jmeta))\n",
      "            else:\n",
      "                return Column(getattr(self._jc, \"as\")(alias[0]))\n",
      "        else:\n",
      "            if metadata:\n",
      "                raise ValueError(\"metadata can only be provided for a single column\")\n",
      "            return Column(getattr(self._jc, \"as\")(_to_seq(sc, list(alias))))\n",
      "\n",
      "------------\n",
      "Method name: asc\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: asc_nulls_first\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: asc_nulls_last\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: astype\n",
      "Method definition:     def cast(self, dataType: Union[DataType, str]) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Casts the column into type ``dataType``.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      "        [Row(ages='2'), Row(ages='5')]\n",
      "        >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      "        [Row(ages='2'), Row(ages='5')]\n",
      "        \"\"\"\n",
      "        if isinstance(dataType, str):\n",
      "            jc = self._jc.cast(dataType)\n",
      "        elif isinstance(dataType, DataType):\n",
      "            from pyspark.sql import SparkSession\n",
      "\n",
      "            spark = SparkSession._getActiveSessionOrCreate()\n",
      "            jdt = spark._jsparkSession.parseDataType(dataType.json())\n",
      "            jc = self._jc.cast(jdt)\n",
      "        else:\n",
      "            raise TypeError(\"unexpected type: %s\" % type(dataType))\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: between\n",
      "Method definition:     def between(\n",
      "        self,\n",
      "        lowerBound: Union[\"Column\", \"LiteralType\", \"DateTimeLiteral\", \"DecimalLiteral\"],\n",
      "        upperBound: Union[\"Column\", \"LiteralType\", \"DateTimeLiteral\", \"DecimalLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        \"\"\"\n",
      "        True if the current column is between the lower bound and upper bound, inclusive.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.name, df.age.between(2, 4)).show()\n",
      "        +-----+---------------------------+\n",
      "        | name|((age >= 2) AND (age <= 4))|\n",
      "        +-----+---------------------------+\n",
      "        |Alice|                       true|\n",
      "        |  Bob|                      false|\n",
      "        +-----+---------------------------+\n",
      "        \"\"\"\n",
      "        return (self >= lowerBound) & (self <= upperBound)\n",
      "\n",
      "------------\n",
      "Method name: bitwiseAND\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: bitwiseOR\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: bitwiseXOR\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: cast\n",
      "Method definition:     def cast(self, dataType: Union[DataType, str]) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Casts the column into type ``dataType``.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      "        [Row(ages='2'), Row(ages='5')]\n",
      "        >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      "        [Row(ages='2'), Row(ages='5')]\n",
      "        \"\"\"\n",
      "        if isinstance(dataType, str):\n",
      "            jc = self._jc.cast(dataType)\n",
      "        elif isinstance(dataType, DataType):\n",
      "            from pyspark.sql import SparkSession\n",
      "\n",
      "            spark = SparkSession._getActiveSessionOrCreate()\n",
      "            jdt = spark._jsparkSession.parseDataType(dataType.json())\n",
      "            jc = self._jc.cast(jdt)\n",
      "        else:\n",
      "            raise TypeError(\"unexpected type: %s\" % type(dataType))\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: contains\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: desc\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: desc_nulls_first\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: desc_nulls_last\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: dropFields\n",
      "Method definition:     def dropFields(self, *fieldNames: str) -> \"Column\":\n",
      "        \"\"\"\n",
      "        An expression that drops fields in :class:`StructType` by name.\n",
      "        This is a no-op if schema doesn't contain field name(s).\n",
      "\n",
      "        .. versionadded:: 3.1.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> from pyspark.sql.functions import col, lit\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
      "        >>> df.withColumn('a', df['a'].dropFields('b')).show()\n",
      "        +-----------------+\n",
      "        |                a|\n",
      "        +-----------------+\n",
      "        |{2, 3, {4, 5, 6}}|\n",
      "        +-----------------+\n",
      "\n",
      "        >>> df.withColumn('a', df['a'].dropFields('b', 'c')).show()\n",
      "        +--------------+\n",
      "        |             a|\n",
      "        +--------------+\n",
      "        |{3, {4, 5, 6}}|\n",
      "        +--------------+\n",
      "\n",
      "        This method supports dropping multiple nested fields directly e.g.\n",
      "\n",
      "        >>> df.withColumn(\"a\", col(\"a\").dropFields(\"e.g\", \"e.h\")).show()\n",
      "        +--------------+\n",
      "        |             a|\n",
      "        +--------------+\n",
      "        |{1, 2, 3, {4}}|\n",
      "        +--------------+\n",
      "\n",
      "        However, if you are going to add/replace multiple nested fields,\n",
      "        it is preferred to extract out the nested struct before\n",
      "        adding/replacing multiple fields e.g.\n",
      "\n",
      "        >>> df.select(col(\"a\").withField(\n",
      "        ...     \"e\", col(\"a.e\").dropFields(\"g\", \"h\")).alias(\"a\")\n",
      "        ... ).show()\n",
      "        +--------------+\n",
      "        |             a|\n",
      "        +--------------+\n",
      "        |{1, 2, 3, {4}}|\n",
      "        +--------------+\n",
      "\n",
      "        \"\"\"\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None\n",
      "        jc = self._jc.dropFields(_to_seq(sc, fieldNames))\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: endswith\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: eqNullSafe\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: getField\n",
      "Method definition:     def getField(self, name: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        An expression that gets a field by name in a :class:`StructType`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
      "        >>> df.select(df.r.getField(\"b\")).show()\n",
      "        +---+\n",
      "        |r.b|\n",
      "        +---+\n",
      "        |  b|\n",
      "        +---+\n",
      "        >>> df.select(df.r.a).show()\n",
      "        +---+\n",
      "        |r.a|\n",
      "        +---+\n",
      "        |  1|\n",
      "        +---+\n",
      "        \"\"\"\n",
      "        if isinstance(name, Column):\n",
      "            warnings.warn(\n",
      "                \"A column as 'name' in getField is deprecated as of Spark 3.0, and will not \"\n",
      "                \"be supported in the future release. Use `column[name]` or `column.name` syntax \"\n",
      "                \"instead.\",\n",
      "                FutureWarning,\n",
      "            )\n",
      "        return self[name]\n",
      "\n",
      "------------\n",
      "Method name: getItem\n",
      "Method definition:     def getItem(self, key: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        An expression that gets an item at position ``ordinal`` out of a list,\n",
      "        or gets an item by key out of a dict.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      "        >>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n",
      "        +----+------+\n",
      "        |l[0]|d[key]|\n",
      "        +----+------+\n",
      "        |   1| value|\n",
      "        +----+------+\n",
      "        \"\"\"\n",
      "        if isinstance(key, Column):\n",
      "            warnings.warn(\n",
      "                \"A column as 'key' in getItem is deprecated as of Spark 3.0, and will not \"\n",
      "                \"be supported in the future release. Use `column[key]` or `column.key` syntax \"\n",
      "                \"instead.\",\n",
      "                FutureWarning,\n",
      "            )\n",
      "        return self[key]\n",
      "\n",
      "------------\n",
      "Method name: ilike\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: isNotNull\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: isNull\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: isin\n",
      "Method definition:     def isin(self, *cols: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        A boolean expression that is evaluated to true if the value of this\n",
      "        expression is contained by the evaluated values of the arguments.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        >>> df[df.age.isin([1, 2, 3])].collect()\n",
      "        [Row(age=2, name='Alice')]\n",
      "        \"\"\"\n",
      "        if len(cols) == 1 and isinstance(cols[0], (list, set)):\n",
      "            cols = cast(Tuple, cols[0])\n",
      "        cols = cast(\n",
      "            Tuple,\n",
      "            [c._jc if isinstance(c, Column) else _create_column_from_literal(c) for c in cols],\n",
      "        )\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None\n",
      "        jc = getattr(self._jc, \"isin\")(_to_seq(sc, cols))\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: like\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: name\n",
      "Method definition:     def alias(self, *alias: str, **kwargs: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Returns this column aliased with a new name or names (in the case of expressions that\n",
      "        return more than one column, such as explode).\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        alias : str\n",
      "            desired column names (collects all positional arguments passed)\n",
      "\n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        metadata: dict\n",
      "            a dict of information to be stored in ``metadata`` attribute of the\n",
      "            corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      "            only argument)\n",
      "\n",
      "            .. versionchanged:: 2.2.0\n",
      "               Added optional ``metadata`` argument.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.age.alias(\"age2\")).collect()\n",
      "        [Row(age2=2), Row(age2=5)]\n",
      "        >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      "        99\n",
      "        \"\"\"\n",
      "\n",
      "        metadata = kwargs.pop(\"metadata\", None)\n",
      "        assert not kwargs, \"Unexpected kwargs where passed: %s\" % kwargs\n",
      "\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None\n",
      "        if len(alias) == 1:\n",
      "            if metadata:\n",
      "                assert sc._jvm is not None\n",
      "                jmeta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(json.dumps(metadata))\n",
      "                return Column(getattr(self._jc, \"as\")(alias[0], jmeta))\n",
      "            else:\n",
      "                return Column(getattr(self._jc, \"as\")(alias[0]))\n",
      "        else:\n",
      "            if metadata:\n",
      "                raise ValueError(\"metadata can only be provided for a single column\")\n",
      "            return Column(getattr(self._jc, \"as\")(_to_seq(sc, list(alias))))\n",
      "\n",
      "------------\n",
      "Method name: otherwise\n",
      "Method definition:     def otherwise(self, value: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        value\n",
      "            a literal value, or a :class:`Column` expression.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import functions as F\n",
      "        >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n",
      "        +-----+-------------------------------------+\n",
      "        | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      "        +-----+-------------------------------------+\n",
      "        |Alice|                                    0|\n",
      "        |  Bob|                                    1|\n",
      "        +-----+-------------------------------------+\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        pyspark.sql.functions.when\n",
      "        \"\"\"\n",
      "        v = value._jc if isinstance(value, Column) else value\n",
      "        jc = self._jc.otherwise(v)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: over\n",
      "Method definition:     def over(self, window: \"WindowSpec\") -> \"Column\":\n",
      "        \"\"\"\n",
      "        Define a windowing column.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        window : :class:`WindowSpec`\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`Column`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> window = Window.partitionBy(\"name\").orderBy(\"age\") \\\n",
      "                .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      "        >>> from pyspark.sql.functions import rank, min\n",
      "        >>> from pyspark.sql.functions import desc\n",
      "        >>> df.withColumn(\"rank\", rank().over(window)) \\\n",
      "                .withColumn(\"min\", min('age').over(window)).sort(desc(\"age\")).show()\n",
      "        +---+-----+----+---+\n",
      "        |age| name|rank|min|\n",
      "        +---+-----+----+---+\n",
      "        |  5|  Bob|   1|  5|\n",
      "        |  2|Alice|   1|  2|\n",
      "        +---+-----+----+---+\n",
      "        \"\"\"\n",
      "        from pyspark.sql.window import WindowSpec\n",
      "\n",
      "        if not isinstance(window, WindowSpec):\n",
      "            raise TypeError(\"window should be WindowSpec\")\n",
      "        jc = self._jc.over(window._jspec)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: rlike\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: startswith\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: substr\n",
      "Method definition:     def substr(self, startPos: Union[int, \"Column\"], length: Union[int, \"Column\"]) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Return a :class:`Column` which is a substring of the column.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        startPos : :class:`Column` or int\n",
      "            start position\n",
      "        length : :class:`Column` or int\n",
      "            length of the substring\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
      "        [Row(col='Ali'), Row(col='Bob')]\n",
      "        \"\"\"\n",
      "        if type(startPos) != type(length):\n",
      "            raise TypeError(\n",
      "                \"startPos and length must be the same type. \"\n",
      "                \"Got {startPos_t} and {length_t}, respectively.\".format(\n",
      "                    startPos_t=type(startPos),\n",
      "                    length_t=type(length),\n",
      "                )\n",
      "            )\n",
      "        if isinstance(startPos, int):\n",
      "            jc = self._jc.substr(startPos, length)\n",
      "        elif isinstance(startPos, Column):\n",
      "            jc = self._jc.substr(startPos._jc, cast(\"Column\", length)._jc)\n",
      "        else:\n",
      "            raise TypeError(\"Unexpected type: %s\" % type(startPos))\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: when\n",
      "Method definition:     def when(self, condition: \"Column\", value: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        condition : :class:`Column`\n",
      "            a boolean :class:`Column` expression.\n",
      "        value\n",
      "            a literal value, or a :class:`Column` expression.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import functions as F\n",
      "        >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n",
      "        +-----+------------------------------------------------------------+\n",
      "        | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      "        +-----+------------------------------------------------------------+\n",
      "        |Alice|                                                          -1|\n",
      "        |  Bob|                                                           1|\n",
      "        +-----+------------------------------------------------------------+\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        pyspark.sql.functions.when\n",
      "        \"\"\"\n",
      "        if not isinstance(condition, Column):\n",
      "            raise TypeError(\"condition should be a Column\")\n",
      "        v = value._jc if isinstance(value, Column) else value\n",
      "        jc = self._jc.when(condition._jc, v)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: withField\n",
      "Method definition:     def withField(self, fieldName: str, col: \"Column\") -> \"Column\":\n",
      "        \"\"\"\n",
      "        An expression that adds/replaces a field in :class:`StructType` by name.\n",
      "\n",
      "        .. versionadded:: 3.1.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> from pyspark.sql.functions import lit\n",
      "        >>> df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n",
      "        >>> df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n",
      "        +---+\n",
      "        |  b|\n",
      "        +---+\n",
      "        |  3|\n",
      "        +---+\n",
      "        >>> df.withColumn('a', df['a'].withField('d', lit(4))).select('a.d').show()\n",
      "        +---+\n",
      "        |  d|\n",
      "        +---+\n",
      "        |  4|\n",
      "        +---+\n",
      "        \"\"\"\n",
      "        if not isinstance(fieldName, str):\n",
      "            raise TypeError(\"fieldName should be a string\")\n",
      "\n",
      "        if not isinstance(col, Column):\n",
      "            raise TypeError(\"col should be a Column\")\n",
      "\n",
      "        return Column(self._jc.withField(fieldName, col._jc))\n",
      "\n",
      "------------\n",
      "Method name: __init__\n",
      "Method definition:     def __init__(\n",
      "        self,\n",
      "        source_df: DataFrame,\n",
      "        spark_context: SparkSession.builder.getOrCreate,\n",
      "        config_path: str,\n",
      "        file_name: str,\n",
      "        src_system: str,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        A class checking the quality of a source data frame based on the given criteria.\n",
      "        :param source_df (DataFrame): the source data frame to apply the data quality checks on.\n",
      "        :param spark_context (SparkSession.builder.getOrCreate): the spark session to run the data quality check.\n",
      "        :param config_path (str): the path to config csv file.\n",
      "        :param file_name (str): the name of the file to check the data quality check on\n",
      "        :param src_system (str): the source of the file where it comes from (vendor)\n",
      "        :raise KeyError: raise a key error if the columns in the source data frame is not in the config file.\n",
      "        \"\"\"\n",
      "        # Set variables\n",
      "        self.spark = spark_context\n",
      "        self.source_df = source_df\n",
      "\n",
      "        self.error_df = None\n",
      "        self.error_columns = []\n",
      "        self.error_counter = 0\n",
      "        self.schema_dict = {\n",
      "            \"StringType\": StringType,\n",
      "            \"DateType\": DateType,\n",
      "            \"IntegerType\": IntegerType,\n",
      "            \"FloatType\": FloatType,\n",
      "            \"DoubleType\": DoubleType,\n",
      "        }\n",
      "\n",
      "        # Initial configuration\n",
      "        config_content = self.read_s3_file(config_path).decode()\n",
      "        self.config = self.resolve_config(config_path.replace(\"config.json\", \"env.json\"), json.loads(config_content))\n",
      "\n",
      "        dq_rule_path = self.config[src_system][\"dq_rule_path\"]\n",
      "        # dq_rule_content = self.read_s3_file(dq_rule_path)\n",
      "        self.rule_df = pd.read_csv(dq_rule_path, index_col=\"column_name\")\n",
      "        self.file_name = file_name\n",
      "        self.rule_df = self.rule_df[(self.rule_df[\"file_name\"] == self.file_name)]\n",
      "        self.rule_df = self.rule_df.applymap(lambda x: x if x else np.nan)\n",
      "        self.rule_df.sort_index(inplace=True)\n",
      "        self.sns_message = []\n",
      "\n",
      "        self.input_columns = self.source_df.columns\n",
      "        self.output_columns = self.config[src_system][\"sources\"][self.file_name][\"dq_output_columns\"]\n",
      "        for index in range(len(self.input_columns)):\n",
      "            if \".\" in self.input_columns[index]:\n",
      "                self.input_columns[index] = \"`\" + self.input_columns[index] + \"`\"\n",
      "\n",
      "        missed_columns = set(self.input_columns) - set(self.rule_df.index)\n",
      "        if len(missed_columns) > 0:\n",
      "            logger.warning(f\"[{missed_columns}] are not found in the rule file.\")\n",
      "\n",
      "        self.spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
      "        # self.spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\n",
      "        self.spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
      "        self.spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
      "\n",
      "------------\n",
      "Method name: add_error_col\n",
      "Method definition:     def add_error_col(self, error_msg: str, condition: Column, error_col_name: str) -> None:\n",
      "        \"\"\"\n",
      "        Add an error column based on the condition to filter and the error message\n",
      "        :param error_msg: the error message to be added if the condition is met\n",
      "        :param condition: the condition of the error to be met in order to return the error column\n",
      "        :param error_col_name: the name of the error column\n",
      "        \"\"\"\n",
      "        if condition is not None and error_col_name and error_msg:\n",
      "            col_condition = f.when(condition, f.lit(error_msg)).otherwise(f.lit(None))\n",
      "            error_col_name = error_col_name + str(self.error_counter)\n",
      "            self.source_df = self.source_df.withColumn(error_col_name, col_condition)\n",
      "            self.error_columns.append(f.col(error_col_name))\n",
      "            self.error_counter += 1\n",
      "\n",
      "------------\n",
      "Method name: category_check\n",
      "Method definition:     def category_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        Method checks input_col with a category and add an error column to self.source_df.\n",
      "        :param input_col: the column to check.\n",
      "        \"\"\"\n",
      "        print(\"start category check\")\n",
      "        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\n",
      "        if valuelist_type[0:2] == \"__\":\n",
      "            category_cond, category_error_msg = self.file_check(input_col)\n",
      "        else:\n",
      "            category_list = valuelist_type.split(\",\")\n",
      "            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\n",
      "            category_cond = (f.col(input_col).isin(category_list) == False) & (\n",
      "                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\n",
      "            )\n",
      "            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\n",
      "        self.add_error_col(\n",
      "            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\n",
      "        )\n",
      "        logger.info(f\"[{input_col}] category check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: columns_to_check\n",
      "Method definition:     def columns_to_check(self, criteria: str) -> Index:\n",
      "        \"\"\"\n",
      "        Returns the indexes to be used while working on the check rules.\n",
      "        :param criteria: whether it is data type, nullable, etc.\n",
      "        :return Index: the index of the columns that this condition should be met.\n",
      "        \"\"\"\n",
      "        return self.rule_df[(self.rule_df[criteria]).notna()].index\n",
      "\n",
      "------------\n",
      "Method name: conditional_check\n",
      "Method definition:     def conditional_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        Checks all the conditional columns in a single row in a config csv file.\n",
      "        :param input_col: The column to apply the check on.\n",
      "        \"\"\"\n",
      "        multiple_conditional_list = self.rule_df.loc[input_col, \"conditional_columns\"].split(\";\")\n",
      "        for cond_ind, condition_columns in enumerate(multiple_conditional_list):\n",
      "            current_additional_cond_value = self.rule_df.loc[input_col, \"conditional_column_value\"].split(\";\")[cond_ind]\n",
      "            current_conditional_valuelist = self.rule_df.loc[input_col, \"conditional_valuelist\"].split(\";\")[cond_ind]\n",
      "\n",
      "            first_col_cond, first_col_msg = self.conditional_cond_syntax(\n",
      "                input_col=input_col,\n",
      "                condition_column=condition_columns,\n",
      "                conditional_variables=current_conditional_valuelist,\n",
      "            )\n",
      "            for condition_column in condition_columns.split(\",\"):\n",
      "                if condition_column in self.input_columns:\n",
      "                    second_cal_cond, second_col_msg = self.conditional_cond_syntax(\n",
      "                        input_col=condition_column,\n",
      "                        condition_column=input_col,\n",
      "                        conditional_variables=current_additional_cond_value,\n",
      "                    )\n",
      "                    conditional_cond = first_col_cond & (~second_cal_cond)\n",
      "                    conditional_error_msg = f\"Cond_fail: {first_col_msg}+{second_col_msg}\"\n",
      "                    self.add_error_col(\n",
      "                        error_msg=conditional_error_msg,\n",
      "                        condition=conditional_cond,\n",
      "                        error_col_name=input_col + \" conditional_check\",\n",
      "                    )\n",
      "                else:\n",
      "                    self.sns_message.append(\n",
      "                        f\"Column {condition_column} is not in report {self.file_name} while it is needed for conditional check\"\n",
      "                    )\n",
      "        logger.info(f\"[{input_col}] conditional check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: conditional_cond_syntax\n",
      "Method definition:     def conditional_cond_syntax(self, input_col: str, condition_column: str, conditional_variables: str) -> Column:\n",
      "        \"\"\"\n",
      "        Generates a Column condition given input column and the condition column.\n",
      "        :param input_col: The column in which the current syntax is applied on\n",
      "        :param condition_column: the second column which might be used in this conditional check.\n",
      "        :param conditional_variables: The variables of the check. It could be __NOT__NULL__, a string of comma separated variables or a single number.\n",
      "        :return Column: The conditional column condition.\n",
      "        \"\"\"\n",
      "        not_category = []\n",
      "        if conditional_variables == \"__NOT__NULL__\":\n",
      "            category_cond = ~self.null_cond_syntax(input_col)\n",
      "            conditional_msg = f\"{input_col} is not null,\"\n",
      "            return category_cond, conditional_msg\n",
      "        elif self.is_float(conditional_variables):\n",
      "            category_cond = self.sum_check_syntax(\n",
      "                input_col + \" schema\", condition_column + \" schema\", conditional_variables\n",
      "            )\n",
      "            conditional_msg = f\"[{input_col}] and [{condition_column}] sum is not equal to {conditional_variables}\"\n",
      "            return category_cond, conditional_msg\n",
      "        else:\n",
      "            category_list = conditional_variables.split(\",\")\n",
      "            for ind, value in enumerate(category_list):\n",
      "                if \"__NOT__\" == value[: len(\"__NOT__\")]:\n",
      "                    not_category.append(category_list.pop(ind)[len(\"__NOT__\") :])\n",
      "\n",
      "            category_cond = f.col(input_col).isin(category_list) == True\n",
      "            if not_category:\n",
      "                category_cond = category_cond & (f.col(input_col).isin(not_category) == False)\n",
      "            conditional_msg = f\"[{input_col}] is in category ({category_list}),)\"\n",
      "            return category_cond, conditional_msg\n",
      "\n",
      "------------\n",
      "Method name: data_type_check\n",
      "Method definition:     def data_type_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        Checks the data type of all columns and give an error column for each column\n",
      "        :param input_col: the column to apply this check.\n",
      "        \"\"\"\n",
      "        print(\"start data type check\")\n",
      "        dtype_key = self.rule_df.loc[input_col, \"type\"]\n",
      "        if dtype_key == \"DateType\":\n",
      "            date_format = self.rule_df.loc[input_col, \"date_format\"]\n",
      "            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.to_date(f.col(input_col), date_format))\n",
      "            # self.source_df.select(input_col + ' schema').cache()\n",
      "        else:\n",
      "            dtype = self.schema_dict[dtype_key]()\n",
      "            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.col(input_col).cast(dtype))\n",
      "            # self.source_df.select(input_col + ' schema').cache()\n",
      "        # dtype_cond should check if the given input_col is not null and the one with schema is null.\n",
      "        dtype_cond = ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")) & (\n",
      "            f.col(input_col + \" schema\").isNull()\n",
      "        )\n",
      "        type_error_msg = f\"data_type_FAIL: Column [{input_col}] should be {dtype_key}\"\n",
      "        self.add_error_col(error_msg=type_error_msg, condition=dtype_cond, error_col_name=input_col + \" type_check\")\n",
      "        logger.info(f\"[{input_col}] dtype check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: duplicate_check\n",
      "Method definition:     def duplicate_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        This method checks input_col with should be unique and add an error column to self.source_df.\n",
      "        :param input_col: the column to check.\n",
      "        \"\"\"\n",
      "        print(\"start duplicate_check\")\n",
      "        schema_col = input_col + \" schema\"\n",
      "        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\n",
      "            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\n",
      "        )\n",
      "        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\n",
      "        self.add_error_col(\n",
      "            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\n",
      "        )\n",
      "        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\n",
      "        logger.info(f\"[{input_col}] duplicate check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: duplicate_cond_syntax\n",
      "Method definition:     def duplicate_cond_syntax(self, input_col: str) -> Column:\n",
      "        \"\"\"\n",
      "        The duplicate check column condition\n",
      "        :param input_col: The column to apply the check on.\n",
      "        :raise Column: the duplicate check column condition.\n",
      "        \"\"\"\n",
      "        self.source_df = self.source_df.join(\n",
      "            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\n",
      "            on=input_col,\n",
      "            how=\"inner\",\n",
      "        )\n",
      "        return f.col(\"Duplicate_indicator\") > 1\n",
      "\n",
      "------------\n",
      "Method name: file_check\n",
      "Method definition:     def file_check(self, input_col):\n",
      "        # finding source side columns\n",
      "        source_df_columns_list = [input_col]\n",
      "        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\n",
      "        if type(reference_columns_str) == str:\n",
      "            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\n",
      "            source_df_columns_list.extend(additional_columns_list)\n",
      "\n",
      "        # Find reference side columns\n",
      "        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\n",
      "        print(\"file check type\", file_check_type)\n",
      "        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\n",
      "        reference_columns = current_file_config[\"reference_columns\"]\n",
      "        print(\"ref cols\", reference_columns)\n",
      "\n",
      "        # Read reference file\n",
      "        file_path = current_file_config[\"path\"]\n",
      "        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\n",
      "        # changing columns names on reference to be the same as source\n",
      "        print(\"src df cols list\", source_df_columns_list)\n",
      "        for col_ind, ref_col in enumerate(reference_columns):\n",
      "            source_col = source_df_columns_list[col_ind]\n",
      "            try:\n",
      "                file_df = file_df.withColumnRenamed(ref_col, source_col)\n",
      "                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\n",
      "                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\n",
      "                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\n",
      "                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\n",
      "                if \"Postal Code\".upper() in source_col.upper():\n",
      "                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\n",
      "            except AnalysisException:\n",
      "                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\n",
      "                return None, None\n",
      "\n",
      "        pre_join_columns = set(self.source_df.columns)\n",
      "        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\n",
      "        post_join_columns = set(self.source_df.columns)\n",
      "        join_col = tuple(post_join_columns - pre_join_columns)[0]\n",
      "        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\n",
      "        file_error_msg = (\n",
      "            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\n",
      "        )\n",
      "        return file_cond, file_error_msg\n",
      "\n",
      "------------\n",
      "Method name: is_float\n",
      "Method definition:     @staticmethod\n",
      "    def is_float(element) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the given input can be returned as float or not.\n",
      "        :param element: The input which can be anything (_type_).\n",
      "        :return bool: whether it is float (True) or not (False).\n",
      "        \"\"\"\n",
      "        try:\n",
      "            float(element)\n",
      "            return True\n",
      "        except ValueError:\n",
      "            return False\n",
      "\n",
      "------------\n",
      "Method name: limit_finder\n",
      "Method definition:     def limit_finder(self, input_col: str, rule_value: Union[str, int, float]) -> Union[float, Column, None]:\n",
      "        \"\"\"\n",
      "        Finds the limit based on the given column. If it is a number it returns the number or if it is a column it returns the f.col.\n",
      "        :param input_col: the column to check this condition on\n",
      "        :param rule_value: value of the limit no matter the datatype\n",
      "        :return Union[str, Column, None]: whether a float or a column in order to check for range check.\n",
      "        :raise KeyError: if the input_col needs other columns that is not in the dataset it will raise an error.\n",
      "        \"\"\"\n",
      "        if self.is_float(rule_value):\n",
      "            rule_value = float(rule_value)\n",
      "            if math.isnan(rule_value):\n",
      "                return None\n",
      "            else:\n",
      "                return rule_value\n",
      "        elif type(rule_value) == str:\n",
      "            if rule_value not in self.input_columns:\n",
      "                print(rule_value)\n",
      "                self.sns_message.append(\n",
      "                    f\"column {rule_value} is not in report {self.file_name} while it is {input_col} needed for range check\"\n",
      "                )\n",
      "                return None\n",
      "            return f.col(rule_value)\n",
      "\n",
      "------------\n",
      "Method name: main_pipeline\n",
      "Method definition:     def main_pipeline(self) -> DataFrame:\n",
      "        \"\"\"\n",
      "        The main pipeline to do all the checks on the given data frame.\n",
      "        :return DataFrame: The consolidated error dataframe.\n",
      "        \"\"\"\n",
      "\n",
      "        columns_to_check_dict = {}\n",
      "        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\n",
      "        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\n",
      "        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\n",
      "        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\n",
      "        columns_to_check_dict[self.range_check] = list(\n",
      "            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\n",
      "        )\n",
      "\n",
      "        for index in range(len(self.input_columns)):\n",
      "            print(\"inpul col\", self.input_columns[index])\n",
      "            for check_type in columns_to_check_dict.keys():\n",
      "                if self.input_columns[index] in columns_to_check_dict[check_type]:\n",
      "                    check_type(self.input_columns[index])\n",
      "\n",
      "        # conditional column in a different loop since they rely on schema of the multiple columns.\n",
      "        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\n",
      "        for conditional_col in columns_to_check_dict[self.conditional_check]:\n",
      "            if conditional_col in self.input_columns:\n",
      "                self.conditional_check(conditional_col)\n",
      "            else:\n",
      "                self.sns_message.append(\n",
      "                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\n",
      "                )\n",
      "\n",
      "        # combining all error columns to one column.\n",
      "        self.source_df = (\n",
      "            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\n",
      "            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\n",
      "            .drop(\"temp\")\n",
      "        )\n",
      "\n",
      "        # exploding the error column into multiple rows.\n",
      "        print(\"exploding error df\")\n",
      "        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\n",
      "            f.col(\"Error\").isNotNull()\n",
      "        )\n",
      "\n",
      "        if self.error_df and self.error_df.rdd.isEmpty():\n",
      "            self.error_df = None\n",
      "        return self.error_df, self.sns_message\n",
      "\n",
      "------------\n",
      "Method name: null_check\n",
      "Method definition:     def null_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        Checks not nullable columns and give an error column for input_col\n",
      "        :param input_col: The column to apply the null check.\n",
      "        \"\"\"\n",
      "        print(\"start null_check\")\n",
      "        if not math.isnan(self.rule_df.loc[input_col, \"nullable\"]):\n",
      "            return\n",
      "        null_condition = self.null_cond_syntax(input_col)\n",
      "        null_error_msg = f\"null_FAIL: Column [{input_col}] cannot be null\"\n",
      "        self.add_error_col(error_msg=null_error_msg, condition=null_condition, error_col_name=input_col + \" null_check\")\n",
      "        logger.info(f\"[{input_col}] null check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: null_cond_syntax\n",
      "Method definition:     def null_cond_syntax(self, input_col: str) -> Column:\n",
      "        \"\"\"\n",
      "        The condition for a null check.\n",
      "        :param input_col: the column to apply the check on.\n",
      "        :raise Column: The not null condition column.\n",
      "        \"\"\"\n",
      "        return (f.col(input_col) == \"\") | (f.col(input_col).isNull())\n",
      "\n",
      "------------\n",
      "Method name: range_check\n",
      "Method definition:     def range_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        This method checks input_col with a range check and add an error column to self.source_df.\n",
      "        Args:\n",
      "            input_col (str): the column to check.\n",
      "        \"\"\"\n",
      "        print(\"start range_check\")\n",
      "        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\n",
      "        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\n",
      "        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\n",
      "        self.add_error_col(\n",
      "            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\n",
      "        )\n",
      "        logger.info(f\"[{input_col}] range check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: range_cond_syntax\n",
      "Method definition:     def range_cond_syntax(self, input_col: str) -> Column:\n",
      "        \"\"\"\n",
      "        The range check column condition\n",
      "        :param input_col: The column to apply the check on\n",
      "        :return Column: The range check column condition.\n",
      "        \"\"\"\n",
      "        schema_col = input_col + \" schema\"\n",
      "        output_cond = None\n",
      "\n",
      "        min_str = self.rule_df.loc[input_col, \"min\"]\n",
      "        min_value = self.limit_finder(input_col, min_str)\n",
      "        if min_value is not None:\n",
      "            output_cond = output_cond | (f.col(schema_col) < min_value)\n",
      "        max_str = self.rule_df.loc[input_col, \"max\"]\n",
      "        max_value = self.limit_finder(input_col, max_str)\n",
      "        if max_value is not None:\n",
      "            output_cond = output_cond | (f.col(schema_col) > max_value)\n",
      "        return min_str, max_str, output_cond\n",
      "\n",
      "------------\n",
      "Method name: read_s3_file\n",
      "Method definition:     def read_s3_file(self, file_path) -> bytes:\n",
      "        \"\"\"\n",
      "        Read s3 file content and return it in byte format\n",
      "        :param file_path: full s3 object path\n",
      "        :return byte content of the file\n",
      "        \"\"\"\n",
      "        file_res = urlparse(file_path)\n",
      "        try:\n",
      "            file_obj = s3_resource.Object(file_res.netloc, file_res.path.lstrip(\"/\"))\n",
      "            return file_obj.get()[\"Body\"].read()\n",
      "        except ClientError:\n",
      "            raise FileNotFoundError(f\"File cannot be found in S3 given path '{file_path}'\")\n",
      "\n",
      "------------\n",
      "Method name: resolve_config\n",
      "Method definition:     def resolve_config(self, env_path, config_content):\n",
      "        \"\"\"\n",
      "        Read config content and resolve env variables\n",
      "        :param env_path: environment file path\n",
      "        :param config_content: environment agnostic config file\n",
      "        :return environmentally resolved config\n",
      "        \"\"\"\n",
      "        env_content = self.read_s3_file(env_path).decode()\n",
      "        env_sub = json.loads(env_content)[\"subs\"]\n",
      "\n",
      "        config_content_str = (\n",
      "            str(config_content)\n",
      "            .replace(\"<env>\", env_sub[\"<env>\"])\n",
      "            .replace(\"<_env>\", env_sub[\"<_env>\"])\n",
      "            .replace(\"<bucket_name>\", env_sub[\"<bucket_name>\"])\n",
      "            .replace(\"<account>\", env_sub[\"<account>\"])\n",
      "        )\n",
      "\n",
      "        return ast.literal_eval(config_content_str)\n",
      "\n",
      "------------\n",
      "Method name: sum_check_syntax\n",
      "Method definition:     def sum_check_syntax(self, input_col1: str, input_col2: str, syntax_value: float) -> Column:\n",
      "        \"\"\"\n",
      "        The syntax to check for sum of 2 columns to equal a syntax value, to be used in conditional checks.\n",
      "        :param input_col1: column 1 to check\n",
      "        :param input_col2: column 2 to check\n",
      "        :param syntax_value: column value that column 1 and 2 should equal to.\n",
      "        :return Column: The sum_check Column condition.\n",
      "        \"\"\"\n",
      "        return ~(f.col(input_col1) + f.col(input_col2) != syntax_value)\n",
      "\n",
      "------------\n",
      "Method name: __getattr__\n",
      "Method definition:     def __getattr__(self, name: str) -> Column:\n",
      "        \"\"\"Returns the :class:`Column` denoted by ``name``.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.age).collect()\n",
      "        [Row(age=2), Row(age=5)]\n",
      "        \"\"\"\n",
      "        if name not in self.columns:\n",
      "            raise AttributeError(\n",
      "                \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name)\n",
      "            )\n",
      "        jc = self._jdf.apply(name)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __getitem__\n",
      "Method definition:     def __getitem__(self, item: Union[int, str, Column, List, Tuple]) -> Union[Column, \"DataFrame\"]:\n",
      "        \"\"\"Returns the column as a :class:`Column`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df['age']).collect()\n",
      "        [Row(age=2), Row(age=5)]\n",
      "        >>> df[ [\"name\", \"age\"]].collect()\n",
      "        [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "        >>> df[ df.age > 3 ].collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        >>> df[df[0] > 3].collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        \"\"\"\n",
      "        if isinstance(item, str):\n",
      "            jc = self._jdf.apply(item)\n",
      "            return Column(jc)\n",
      "        elif isinstance(item, Column):\n",
      "            return self.filter(item)\n",
      "        elif isinstance(item, (list, tuple)):\n",
      "            return self.select(*item)\n",
      "        elif isinstance(item, int):\n",
      "            jc = self._jdf.apply(self.columns[item])\n",
      "            return Column(jc)\n",
      "        else:\n",
      "            raise TypeError(\"unexpected item type: %s\" % type(item))\n",
      "\n",
      "------------\n",
      "Method name: __init__\n",
      "Method definition:     def __init__(\n",
      "        self,\n",
      "        jdf: JavaObject,\n",
      "        sql_ctx: Union[\"SQLContext\", \"SparkSession\"],\n",
      "    ):\n",
      "        from pyspark.sql.context import SQLContext\n",
      "\n",
      "        self._sql_ctx: Optional[\"SQLContext\"] = None\n",
      "\n",
      "        if isinstance(sql_ctx, SQLContext):\n",
      "            assert not os.environ.get(\"SPARK_TESTING\")  # Sanity check for our internal usage.\n",
      "            assert isinstance(sql_ctx, SQLContext)\n",
      "            # We should remove this if-else branch in the future release, and rename\n",
      "            # sql_ctx to session in the constructor. This is an internal code path but\n",
      "            # was kept with an warning because it's used intensively by third-party libraries.\n",
      "            warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "            self._sql_ctx = sql_ctx\n",
      "            session = sql_ctx.sparkSession\n",
      "        else:\n",
      "            session = sql_ctx\n",
      "        self._session: \"SparkSession\" = session\n",
      "\n",
      "        self._sc: SparkContext = sql_ctx._sc\n",
      "        self._jdf: JavaObject = jdf\n",
      "        self.is_cached = False\n",
      "        # initialized lazily\n",
      "        self._schema: Optional[StructType] = None\n",
      "        self._lazy_rdd: Optional[RDD[Row]] = None\n",
      "        # Check whether _repr_html is supported or not, we use it to avoid calling _jdf twice\n",
      "        # by __repr__ and _repr_html_ while eager evaluation opened.\n",
      "        self._support_repr_html = False\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        if not self._support_repr_html and self.sparkSession._jconf.isReplEagerEvalEnabled():\n",
      "            vertical = False\n",
      "            return self._jdf.showString(\n",
      "                self.sparkSession._jconf.replEagerEvalMaxNumRows(),\n",
      "                self.sparkSession._jconf.replEagerEvalTruncate(),\n",
      "                vertical,\n",
      "            )\n",
      "        else:\n",
      "            return \"DataFrame[%s]\" % (\", \".join(\"%s: %s\" % c for c in self.dtypes))\n",
      "\n",
      "------------\n",
      "Method name: _collect_as_arrow\n",
      "Method definition:     def _collect_as_arrow(self, split_batches: bool = False) -> List[\"pa.RecordBatch\"]:\n",
      "        \"\"\"\n",
      "        Returns all records as a list of ArrowRecordBatches, pyarrow must be installed\n",
      "        and available on driver and worker Python environments.\n",
      "        This is an experimental feature.\n",
      "\n",
      "        :param split_batches: split batches such that each column is in its own allocation, so\n",
      "            that the selfDestruct optimization is effective; default False.\n",
      "\n",
      "        .. note:: Experimental.\n",
      "        \"\"\"\n",
      "        from pyspark.sql.dataframe import DataFrame\n",
      "\n",
      "        assert isinstance(self, DataFrame)\n",
      "\n",
      "        with SCCallSiteSync(self._sc):\n",
      "            (\n",
      "                port,\n",
      "                auth_secret,\n",
      "                jsocket_auth_server,\n",
      "            ) = self._jdf.collectAsArrowToPython()\n",
      "\n",
      "        # Collect list of un-ordered batches where last element is a list of correct order indices\n",
      "        try:\n",
      "            batch_stream = _load_from_socket((port, auth_secret), ArrowCollectSerializer())\n",
      "            if split_batches:\n",
      "                # When spark.sql.execution.arrow.pyspark.selfDestruct.enabled, ensure\n",
      "                # each column in each record batch is contained in its own allocation.\n",
      "                # Otherwise, selfDestruct does nothing; it frees each column as its\n",
      "                # converted, but each column will actually be a list of slices of record\n",
      "                # batches, and so no memory is actually freed until all columns are\n",
      "                # converted.\n",
      "                import pyarrow as pa\n",
      "\n",
      "                results = []\n",
      "                for batch_or_indices in batch_stream:\n",
      "                    if isinstance(batch_or_indices, pa.RecordBatch):\n",
      "                        batch_or_indices = pa.RecordBatch.from_arrays(\n",
      "                            [\n",
      "                                # This call actually reallocates the array\n",
      "                                pa.concat_arrays([array])\n",
      "                                for array in batch_or_indices\n",
      "                            ],\n",
      "                            schema=batch_or_indices.schema,\n",
      "                        )\n",
      "                    results.append(batch_or_indices)\n",
      "            else:\n",
      "                results = list(batch_stream)\n",
      "        finally:\n",
      "            # Join serving thread and raise any exceptions from collectAsArrowToPython\n",
      "            jsocket_auth_server.getResult()\n",
      "\n",
      "        # Separate RecordBatches from batch order indices in results\n",
      "        batches = results[:-1]\n",
      "        batch_order = results[-1]\n",
      "\n",
      "        # Re-order the batch list using the correct order\n",
      "        return [batches[i] for i in batch_order]\n",
      "\n",
      "------------\n",
      "Method name: _jcols\n",
      "Method definition:     def _jcols(self, *cols: \"ColumnOrName\") -> JavaObject:\n",
      "        \"\"\"Return a JVM Seq of Columns from a list of Column or column names\n",
      "\n",
      "        If `cols` has only one list in it, cols[0] will be used as the list.\n",
      "        \"\"\"\n",
      "        if len(cols) == 1 and isinstance(cols[0], list):\n",
      "            cols = cols[0]\n",
      "        return self._jseq(cols, _to_java_column)\n",
      "\n",
      "------------\n",
      "Method name: _jmap\n",
      "Method definition:     def _jmap(self, jm: Dict) -> JavaObject:\n",
      "        \"\"\"Return a JVM Scala Map from a dict\"\"\"\n",
      "        return _to_scala_map(self.sparkSession._sc, jm)\n",
      "\n",
      "------------\n",
      "Method name: _joinAsOf\n",
      "Method definition:     def _joinAsOf(\n",
      "        self,\n",
      "        other: \"DataFrame\",\n",
      "        leftAsOfColumn: Union[str, Column],\n",
      "        rightAsOfColumn: Union[str, Column],\n",
      "        on: Optional[Union[str, List[str], Column, List[Column]]] = None,\n",
      "        how: Optional[str] = None,\n",
      "        *,\n",
      "        tolerance: Optional[Column] = None,\n",
      "        allowExactMatches: bool = True,\n",
      "        direction: str = \"backward\",\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Perform an as-of join.\n",
      "\n",
      "        This is similar to a left-join except that we match on nearest\n",
      "        key rather than equal keys.\n",
      "\n",
      "        .. versionadded:: 3.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : :class:`DataFrame`\n",
      "            Right side of the join\n",
      "        leftAsOfColumn : str or :class:`Column`\n",
      "            a string for the as-of join column name, or a Column\n",
      "        rightAsOfColumn : str or :class:`Column`\n",
      "            a string for the as-of join column name, or a Column\n",
      "        on : str, list or :class:`Column`, optional\n",
      "            a string for the join column name, a list of column names,\n",
      "            a join expression (Column), or a list of Columns.\n",
      "            If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "            the column(s) must exist on both sides, and this performs an equi-join.\n",
      "        how : str, optional\n",
      "            default ``inner``. Must be one of: ``inner`` and ``left``.\n",
      "        tolerance : :class:`Column`, optional\n",
      "            an asof tolerance within this range; must be compatible\n",
      "            with the merge index.\n",
      "        allowExactMatches : bool, optional\n",
      "            default ``True``.\n",
      "        direction : str, optional\n",
      "            default ``backward``. Must be one of: ``backward``, ``forward``, and ``nearest``.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The following performs an as-of join between ``left`` and ``right``.\n",
      "\n",
      "        >>> left = spark.createDataFrame([(1, \"a\"), (5, \"b\"), (10,  \"c\")], [\"a\", \"left_val\"])\n",
      "        >>> right = spark.createDataFrame([(1, 1), (2, 2), (3, 3), (6, 6), (7, 7)],\n",
      "        ...                               [\"a\", \"right_val\"])\n",
      "        >>> left._joinAsOf(\n",
      "        ...     right, leftAsOfColumn=\"a\", rightAsOfColumn=\"a\"\n",
      "        ... ).select(left.a, 'left_val', 'right_val').sort(\"a\").collect()\n",
      "        [Row(a=1, left_val='a', right_val=1),\n",
      "         Row(a=5, left_val='b', right_val=3),\n",
      "         Row(a=10, left_val='c', right_val=7)]\n",
      "\n",
      "        >>> from pyspark.sql import functions as F\n",
      "        >>> left._joinAsOf(\n",
      "        ...     right, leftAsOfColumn=\"a\", rightAsOfColumn=\"a\", tolerance=F.lit(1)\n",
      "        ... ).select(left.a, 'left_val', 'right_val').sort(\"a\").collect()\n",
      "        [Row(a=1, left_val='a', right_val=1)]\n",
      "\n",
      "        >>> left._joinAsOf(\n",
      "        ...     right, leftAsOfColumn=\"a\", rightAsOfColumn=\"a\", how=\"left\", tolerance=F.lit(1)\n",
      "        ... ).select(left.a, 'left_val', 'right_val').sort(\"a\").collect()\n",
      "        [Row(a=1, left_val='a', right_val=1),\n",
      "         Row(a=5, left_val='b', right_val=None),\n",
      "         Row(a=10, left_val='c', right_val=None)]\n",
      "\n",
      "        >>> left._joinAsOf(\n",
      "        ...     right, leftAsOfColumn=\"a\", rightAsOfColumn=\"a\", allowExactMatches=False\n",
      "        ... ).select(left.a, 'left_val', 'right_val').sort(\"a\").collect()\n",
      "        [Row(a=5, left_val='b', right_val=3),\n",
      "         Row(a=10, left_val='c', right_val=7)]\n",
      "\n",
      "        >>> left._joinAsOf(\n",
      "        ...     right, leftAsOfColumn=\"a\", rightAsOfColumn=\"a\", direction=\"forward\"\n",
      "        ... ).select(left.a, 'left_val', 'right_val').sort(\"a\").collect()\n",
      "        [Row(a=1, left_val='a', right_val=1),\n",
      "         Row(a=5, left_val='b', right_val=6)]\n",
      "        \"\"\"\n",
      "        if isinstance(leftAsOfColumn, str):\n",
      "            leftAsOfColumn = self[leftAsOfColumn]\n",
      "        left_as_of_jcol = leftAsOfColumn._jc\n",
      "        if isinstance(rightAsOfColumn, str):\n",
      "            rightAsOfColumn = other[rightAsOfColumn]\n",
      "        right_as_of_jcol = rightAsOfColumn._jc\n",
      "\n",
      "        if on is not None and not isinstance(on, list):\n",
      "            on = [on]  # type: ignore[assignment]\n",
      "\n",
      "        if on is not None:\n",
      "            if isinstance(on[0], str):\n",
      "                on = self._jseq(cast(List[str], on))\n",
      "            else:\n",
      "                assert isinstance(on[0], Column), \"on should be Column or list of Column\"\n",
      "                on = reduce(lambda x, y: x.__and__(y), cast(List[Column], on))\n",
      "                on = on._jc\n",
      "\n",
      "        if how is None:\n",
      "            how = \"inner\"\n",
      "        assert isinstance(how, str), \"how should be a string\"\n",
      "\n",
      "        if tolerance is not None:\n",
      "            assert isinstance(tolerance, Column), \"tolerance should be Column\"\n",
      "            tolerance = tolerance._jc\n",
      "\n",
      "        jdf = self._jdf.joinAsOf(\n",
      "            other._jdf,\n",
      "            left_as_of_jcol,\n",
      "            right_as_of_jcol,\n",
      "            on,\n",
      "            how,\n",
      "            tolerance,\n",
      "            allowExactMatches,\n",
      "            direction,\n",
      "        )\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: _jseq\n",
      "Method definition:     def _jseq(\n",
      "        self,\n",
      "        cols: Sequence,\n",
      "        converter: Optional[Callable[..., Union[\"PrimitiveType\", JavaObject]]] = None,\n",
      "    ) -> JavaObject:\n",
      "        \"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\n",
      "        return _to_seq(self.sparkSession._sc, cols, converter)\n",
      "\n",
      "------------\n",
      "Method name: _repr_html_\n",
      "Method definition:     def _repr_html_(self) -> Optional[str]:\n",
      "        \"\"\"Returns a :class:`DataFrame` with html code when you enabled eager evaluation\n",
      "        by 'spark.sql.repl.eagerEval.enabled', this only called by REPL you are\n",
      "        using support eager evaluation with HTML.\n",
      "        \"\"\"\n",
      "        if not self._support_repr_html:\n",
      "            self._support_repr_html = True\n",
      "        if self.sparkSession._jconf.isReplEagerEvalEnabled():\n",
      "            max_num_rows = max(self.sparkSession._jconf.replEagerEvalMaxNumRows(), 0)\n",
      "            sock_info = self._jdf.getRowsToPython(\n",
      "                max_num_rows,\n",
      "                self.sparkSession._jconf.replEagerEvalTruncate(),\n",
      "            )\n",
      "            rows = list(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "            head = rows[0]\n",
      "            row_data = rows[1:]\n",
      "            has_more_data = len(row_data) > max_num_rows\n",
      "            row_data = row_data[:max_num_rows]\n",
      "\n",
      "            html = \"<table border='1'>\\n\"\n",
      "            # generate table head\n",
      "            html += \"<tr><th>%s</th></tr>\\n\" % \"</th><th>\".join(map(lambda x: html_escape(x), head))\n",
      "            # generate table rows\n",
      "            for row in row_data:\n",
      "                html += \"<tr><td>%s</td></tr>\\n\" % \"</td><td>\".join(\n",
      "                    map(lambda x: html_escape(x), row)\n",
      "                )\n",
      "            html += \"</table>\\n\"\n",
      "            if has_more_data:\n",
      "                html += \"only showing top %d %s\\n\" % (\n",
      "                    max_num_rows,\n",
      "                    \"row\" if max_num_rows == 1 else \"rows\",\n",
      "                )\n",
      "            return html\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "------------\n",
      "Method name: _sort_cols\n",
      "Method definition:     def _sort_cols(\n",
      "        self, cols: Sequence[Union[str, Column, List[Union[str, Column]]]], kwargs: Dict[str, Any]\n",
      "    ) -> JavaObject:\n",
      "        \"\"\"Return a JVM Seq of Columns that describes the sort order\"\"\"\n",
      "        if not cols:\n",
      "            raise ValueError(\"should sort by at least one column\")\n",
      "        if len(cols) == 1 and isinstance(cols[0], list):\n",
      "            cols = cols[0]\n",
      "        jcols = [_to_java_column(cast(\"ColumnOrName\", c)) for c in cols]\n",
      "        ascending = kwargs.get(\"ascending\", True)\n",
      "        if isinstance(ascending, (bool, int)):\n",
      "            if not ascending:\n",
      "                jcols = [jc.desc() for jc in jcols]\n",
      "        elif isinstance(ascending, list):\n",
      "            jcols = [jc if asc else jc.desc() for asc, jc in zip(ascending, jcols)]\n",
      "        else:\n",
      "            raise TypeError(\"ascending can only be boolean or list, but got %s\" % type(ascending))\n",
      "        return self._jseq(jcols)\n",
      "\n",
      "------------\n",
      "Method name: _to_corrected_pandas_type\n",
      "Method definition:     @staticmethod\n",
      "    def _to_corrected_pandas_type(dt: DataType) -> Optional[Type]:\n",
      "        \"\"\"\n",
      "        When converting Spark SQL records to Pandas `pandas.DataFrame`, the inferred data type\n",
      "        may be wrong. This method gets the corrected data type for Pandas if that type may be\n",
      "        inferred incorrectly.\n",
      "        \"\"\"\n",
      "        import numpy as np\n",
      "\n",
      "        if type(dt) == ByteType:\n",
      "            return np.int8\n",
      "        elif type(dt) == ShortType:\n",
      "            return np.int16\n",
      "        elif type(dt) == IntegerType:\n",
      "            return np.int32\n",
      "        elif type(dt) == LongType:\n",
      "            return np.int64\n",
      "        elif type(dt) == FloatType:\n",
      "            return np.float32\n",
      "        elif type(dt) == DoubleType:\n",
      "            return np.float64\n",
      "        elif type(dt) == BooleanType:\n",
      "            return np.bool  # type: ignore[attr-defined]\n",
      "        elif type(dt) == TimestampType:\n",
      "            return np.datetime64\n",
      "        elif type(dt) == TimestampNTZType:\n",
      "            return np.datetime64\n",
      "        elif type(dt) == DayTimeIntervalType:\n",
      "            return np.timedelta64\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "------------\n",
      "Method name: agg\n",
      "Method definition:     def agg(self, *exprs: Union[Column, Dict[str, str]]) -> \"DataFrame\":\n",
      "        \"\"\"Aggregate on the entire :class:`DataFrame` without groups\n",
      "        (shorthand for ``df.groupBy().agg()``).\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.agg({\"age\": \"max\"}).collect()\n",
      "        [Row(max(age)=5)]\n",
      "        >>> from pyspark.sql import functions as F\n",
      "        >>> df.agg(F.min(df.age)).collect()\n",
      "        [Row(min(age)=2)]\n",
      "        \"\"\"\n",
      "        return self.groupBy().agg(*exprs)  # type: ignore[arg-type]\n",
      "\n",
      "------------\n",
      "Method name: alias\n",
      "Method definition:     def alias(self, alias: str) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` with an alias set.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        alias : str\n",
      "            an alias name to be set for the :class:`DataFrame`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import *\n",
      "        >>> df_as1 = df.alias(\"df_as1\")\n",
      "        >>> df_as2 = df.alias(\"df_as2\")\n",
      "        >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      "        >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\") \\\n",
      "                .sort(desc(\"df_as1.name\")).collect()\n",
      "        [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n",
      "        \"\"\"\n",
      "        assert isinstance(alias, str), \"alias should be a string\"\n",
      "        return DataFrame(getattr(self._jdf, \"as\")(alias), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: approxQuantile\n",
      "Method definition:     def approxQuantile(\n",
      "        self,\n",
      "        col: Union[str, List[str], Tuple[str]],\n",
      "        probabilities: Union[List[float], Tuple[float]],\n",
      "        relativeError: float,\n",
      "    ) -> Union[List[float], List[List[float]]]:\n",
      "        \"\"\"\n",
      "        Calculates the approximate quantiles of numerical columns of a\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        The result of this algorithm has the following deterministic bound:\n",
      "        If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      "        probability `p` up to error `err`, then the algorithm will return\n",
      "        a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      "        close to (p * N). More precisely,\n",
      "\n",
      "          floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      "\n",
      "        This method implements a variation of the Greenwald-Khanna\n",
      "        algorithm (with some speed optimizations). The algorithm was first\n",
      "        present in [[https://doi.org/10.1145/375663.375670\n",
      "        Space-efficient Online Computation of Quantile Summaries]]\n",
      "        by Greenwald and Khanna.\n",
      "\n",
      "        Note that null values will be ignored in numerical columns before calculation.\n",
      "        For columns only containing null values, an empty list is returned.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        col: str, tuple or list\n",
      "            Can be a single column name, or a list of names for multiple columns.\n",
      "\n",
      "            .. versionchanged:: 2.2\n",
      "               Added support for multiple columns.\n",
      "        probabilities : list or tuple\n",
      "            a list of quantile probabilities\n",
      "            Each number must belong to [0, 1].\n",
      "            For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      "        relativeError : float\n",
      "            The relative target precision to achieve\n",
      "            (>= 0). If set to zero, the exact quantiles are computed, which\n",
      "            could be very expensive. Note that values greater than 1 are\n",
      "            accepted but give the same result as 1.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        list\n",
      "            the approximate quantiles at the given probabilities. If\n",
      "            the input `col` is a string, the output is a list of floats. If the\n",
      "            input `col` is a list or tuple of strings, the output is also a\n",
      "            list, but each element in it is a list of floats, i.e., the output\n",
      "            is a list of list of floats.\n",
      "        \"\"\"\n",
      "\n",
      "        if not isinstance(col, (str, list, tuple)):\n",
      "            raise TypeError(\"col should be a string, list or tuple, but got %r\" % type(col))\n",
      "\n",
      "        isStr = isinstance(col, str)\n",
      "\n",
      "        if isinstance(col, tuple):\n",
      "            col = list(col)\n",
      "        elif isStr:\n",
      "            col = [cast(str, col)]\n",
      "\n",
      "        for c in col:\n",
      "            if not isinstance(c, str):\n",
      "                raise TypeError(\"columns should be strings, but got %r\" % type(c))\n",
      "        col = _to_list(self._sc, cast(List[\"ColumnOrName\"], col))\n",
      "\n",
      "        if not isinstance(probabilities, (list, tuple)):\n",
      "            raise TypeError(\"probabilities should be a list or tuple\")\n",
      "        if isinstance(probabilities, tuple):\n",
      "            probabilities = list(probabilities)\n",
      "        for p in probabilities:\n",
      "            if not isinstance(p, (float, int)) or p < 0 or p > 1:\n",
      "                raise ValueError(\"probabilities should be numerical (float, int) in [0,1].\")\n",
      "        probabilities = _to_list(self._sc, cast(List[\"ColumnOrName\"], probabilities))\n",
      "\n",
      "        if not isinstance(relativeError, (float, int)):\n",
      "            raise TypeError(\"relativeError should be numerical (float, int)\")\n",
      "        if relativeError < 0:\n",
      "            raise ValueError(\"relativeError should be >= 0.\")\n",
      "        relativeError = float(relativeError)\n",
      "\n",
      "        jaq = self._jdf.stat().approxQuantile(col, probabilities, relativeError)\n",
      "        jaq_list = [list(j) for j in jaq]\n",
      "        return jaq_list[0] if isStr else jaq_list\n",
      "\n",
      "------------\n",
      "Method name: cache\n",
      "Method definition:     def cache(self) -> \"DataFrame\":\n",
      "        \"\"\"Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      "        \"\"\"\n",
      "        self.is_cached = True\n",
      "        self._jdf.cache()\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: checkpoint\n",
      "Method definition:     def checkpoint(self, eager: bool = True) -> \"DataFrame\":\n",
      "        \"\"\"Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\n",
      "        truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      "        iterative algorithms where the plan may grow exponentially. It will be saved to files\n",
      "        inside the checkpoint directory set with :meth:`SparkContext.setCheckpointDir`.\n",
      "\n",
      "        .. versionadded:: 2.1.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        eager : bool, optional\n",
      "            Whether to checkpoint this :class:`DataFrame` immediately\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This API is experimental.\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.checkpoint(eager)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: coalesce\n",
      "Method definition:     def coalesce(self, numPartitions: int) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      "\n",
      "        Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      "        narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      "        there will not be a shuffle, instead each of the 100 new partitions will\n",
      "        claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      "        it will stay at the current number of partitions.\n",
      "\n",
      "        However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      "        this may result in your computation taking place on fewer nodes than\n",
      "        you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      "        you can call repartition(). This will add a shuffle step, but means the\n",
      "        current upstream partitions will be executed in parallel (per whatever\n",
      "        the current partitioning is).\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        numPartitions : int\n",
      "            specify the target number of partitions\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.coalesce(1).rdd.getNumPartitions()\n",
      "        1\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.coalesce(numPartitions), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: colRegex\n",
      "Method definition:     def colRegex(self, colName: str) -> Column:\n",
      "        \"\"\"\n",
      "        Selects column based on the column name specified as a regex and returns it\n",
      "        as :class:`Column`.\n",
      "\n",
      "        .. versionadded:: 2.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        colName : str\n",
      "            string, column name specified as a regex.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      "        >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      "        +----+\n",
      "        |Col2|\n",
      "        +----+\n",
      "        |   1|\n",
      "        |   2|\n",
      "        |   3|\n",
      "        +----+\n",
      "        \"\"\"\n",
      "        if not isinstance(colName, str):\n",
      "            raise TypeError(\"colName should be provided as string\")\n",
      "        jc = self._jdf.colRegex(colName)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: collect\n",
      "Method definition:     def collect(self) -> List[Row]:\n",
      "        \"\"\"Returns all the records as a list of :class:`Row`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.collect()\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        \"\"\"\n",
      "        with SCCallSiteSync(self._sc):\n",
      "            sock_info = self._jdf.collectToPython()\n",
      "        return list(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\n",
      "------------\n",
      "Method name: corr\n",
      "Method definition:     def corr(self, col1: str, col2: str, method: Optional[str] = None) -> float:\n",
      "        \"\"\"\n",
      "        Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      "        Currently only supports the Pearson Correlation Coefficient.\n",
      "        :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str\n",
      "            The name of the first column\n",
      "        col2 : str\n",
      "            The name of the second column\n",
      "        method : str, optional\n",
      "            The correlation method. Currently only supports \"pearson\"\n",
      "        \"\"\"\n",
      "        if not isinstance(col1, str):\n",
      "            raise TypeError(\"col1 should be a string.\")\n",
      "        if not isinstance(col2, str):\n",
      "            raise TypeError(\"col2 should be a string.\")\n",
      "        if not method:\n",
      "            method = \"pearson\"\n",
      "        if not method == \"pearson\":\n",
      "            raise ValueError(\n",
      "                \"Currently only the calculation of the Pearson Correlation \"\n",
      "                + \"coefficient is supported.\"\n",
      "            )\n",
      "        return self._jdf.stat().corr(col1, col2, method)\n",
      "\n",
      "------------\n",
      "Method name: count\n",
      "Method definition:     def count(self) -> int:\n",
      "        \"\"\"Returns the number of rows in this :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.count()\n",
      "        2\n",
      "        \"\"\"\n",
      "        return int(self._jdf.count())\n",
      "\n",
      "------------\n",
      "Method name: cov\n",
      "Method definition:     def cov(self, col1: str, col2: str) -> float:\n",
      "        \"\"\"\n",
      "        Calculate the sample covariance for the given columns, specified by their names, as a\n",
      "        double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str\n",
      "            The name of the first column\n",
      "        col2 : str\n",
      "            The name of the second column\n",
      "        \"\"\"\n",
      "        if not isinstance(col1, str):\n",
      "            raise TypeError(\"col1 should be a string.\")\n",
      "        if not isinstance(col2, str):\n",
      "            raise TypeError(\"col2 should be a string.\")\n",
      "        return self._jdf.stat().cov(col1, col2)\n",
      "\n",
      "------------\n",
      "Method name: createGlobalTempView\n",
      "Method definition:     def createGlobalTempView(self, name: str) -> None:\n",
      "        \"\"\"Creates a global temporary view with this :class:`DataFrame`.\n",
      "\n",
      "        The lifetime of this temporary view is tied to this Spark application.\n",
      "        throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      "        catalog.\n",
      "\n",
      "        .. versionadded:: 2.1.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.createGlobalTempView(\"people\")\n",
      "        >>> df2 = spark.sql(\"select * from global_temp.people\")\n",
      "        >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "        True\n",
      "        >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      "        >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      "        True\n",
      "\n",
      "        \"\"\"\n",
      "        self._jdf.createGlobalTempView(name)\n",
      "\n",
      "------------\n",
      "Method name: createOrReplaceGlobalTempView\n",
      "Method definition:     def createOrReplaceGlobalTempView(self, name: str) -> None:\n",
      "        \"\"\"Creates or replaces a global temporary view using the given name.\n",
      "\n",
      "        The lifetime of this temporary view is tied to this Spark application.\n",
      "\n",
      "        .. versionadded:: 2.2.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      "        >>> df2 = df.filter(df.age > 3)\n",
      "        >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      "        >>> df3 = spark.sql(\"select * from global_temp.people\")\n",
      "        >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      "        True\n",
      "        >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      "        True\n",
      "\n",
      "        \"\"\"\n",
      "        self._jdf.createOrReplaceGlobalTempView(name)\n",
      "\n",
      "------------\n",
      "Method name: createOrReplaceTempView\n",
      "Method definition:     def createOrReplaceTempView(self, name: str) -> None:\n",
      "        \"\"\"Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      "\n",
      "        The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "        that was used to create this :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.createOrReplaceTempView(\"people\")\n",
      "        >>> df2 = df.filter(df.age > 3)\n",
      "        >>> df2.createOrReplaceTempView(\"people\")\n",
      "        >>> df3 = spark.sql(\"select * from people\")\n",
      "        >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      "        True\n",
      "        >>> spark.catalog.dropTempView(\"people\")\n",
      "        True\n",
      "\n",
      "        \"\"\"\n",
      "        self._jdf.createOrReplaceTempView(name)\n",
      "\n",
      "------------\n",
      "Method name: createTempView\n",
      "Method definition:     def createTempView(self, name: str) -> None:\n",
      "        \"\"\"Creates a local temporary view with this :class:`DataFrame`.\n",
      "\n",
      "        The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "        that was used to create this :class:`DataFrame`.\n",
      "        throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      "        catalog.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.createTempView(\"people\")\n",
      "        >>> df2 = spark.sql(\"select * from people\")\n",
      "        >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "        True\n",
      "        >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      "        >>> spark.catalog.dropTempView(\"people\")\n",
      "        True\n",
      "\n",
      "        \"\"\"\n",
      "        self._jdf.createTempView(name)\n",
      "\n",
      "------------\n",
      "Method name: crossJoin\n",
      "Method definition:     def crossJoin(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Returns the cartesian product with another :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 2.1.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : :class:`DataFrame`\n",
      "            Right side of the cartesian product.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(\"age\", \"name\").collect()\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        >>> df2.select(\"name\", \"height\").collect()\n",
      "        [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n",
      "        >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n",
      "        [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n",
      "         Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n",
      "        \"\"\"\n",
      "\n",
      "        jdf = self._jdf.crossJoin(other._jdf)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: crosstab\n",
      "Method definition:     def crosstab(self, col1: str, col2: str) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      "        table. The number of distinct values for each column should be less than 1e4. At most 1e6\n",
      "        non-zero pair frequencies will be returned.\n",
      "        The first column of each row will be the distinct values of `col1` and the column names\n",
      "        will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      "        Pairs that have no occurrences will have zero as their counts.\n",
      "        :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str\n",
      "            The name of the first column. Distinct items will make the first item of\n",
      "            each row.\n",
      "        col2 : str\n",
      "            The name of the second column. Distinct items will make the column names\n",
      "            of the :class:`DataFrame`.\n",
      "        \"\"\"\n",
      "        if not isinstance(col1, str):\n",
      "            raise TypeError(\"col1 should be a string.\")\n",
      "        if not isinstance(col2, str):\n",
      "            raise TypeError(\"col2 should be a string.\")\n",
      "        return DataFrame(self._jdf.stat().crosstab(col1, col2), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: cube\n",
      "Method definition:     def cube(self, *cols: \"ColumnOrName\") -> \"GroupedData\":  # type: ignore[misc]\n",
      "        \"\"\"\n",
      "        Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      "        the specified columns, so we can run aggregations on them.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      "        +-----+----+-----+\n",
      "        | name| age|count|\n",
      "        +-----+----+-----+\n",
      "        | null|null|    2|\n",
      "        | null|   2|    1|\n",
      "        | null|   5|    1|\n",
      "        |Alice|null|    1|\n",
      "        |Alice|   2|    1|\n",
      "        |  Bob|null|    1|\n",
      "        |  Bob|   5|    1|\n",
      "        +-----+----+-----+\n",
      "        \"\"\"\n",
      "        jgd = self._jdf.cube(self._jcols(*cols))\n",
      "        from pyspark.sql.group import GroupedData\n",
      "\n",
      "        return GroupedData(jgd, self)\n",
      "\n",
      "------------\n",
      "Method name: describe\n",
      "Method definition:     def describe(self, *cols: Union[str, List[str]]) -> \"DataFrame\":\n",
      "        \"\"\"Computes basic statistics for numeric and string columns.\n",
      "\n",
      "        .. versionadded:: 1.3.1\n",
      "\n",
      "        This include count, mean, stddev, min, and max. If no columns are\n",
      "        given, this function computes statistics for all numerical or string columns.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This function is meant for exploratory data analysis, as we make no\n",
      "        guarantee about the backward compatibility of the schema of the resulting\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        Use summary for expanded statistics and control over which statistics to compute.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      "        ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      "        ... )\n",
      "        >>> df.describe(['age']).show()\n",
      "        +-------+----+\n",
      "        |summary| age|\n",
      "        +-------+----+\n",
      "        |  count|   3|\n",
      "        |   mean|12.0|\n",
      "        | stddev| 1.0|\n",
      "        |    min|  11|\n",
      "        |    max|  13|\n",
      "        +-------+----+\n",
      "\n",
      "        >>> df.describe(['age', 'weight', 'height']).show()\n",
      "        +-------+----+------------------+-----------------+\n",
      "        |summary| age|            weight|           height|\n",
      "        +-------+----+------------------+-----------------+\n",
      "        |  count|   3|                 3|                3|\n",
      "        |   mean|12.0| 40.73333333333333|            145.0|\n",
      "        | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      "        |    min|  11|              37.8|            142.2|\n",
      "        |    max|  13|              44.1|            150.5|\n",
      "        +-------+----+------------------+-----------------+\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        DataFrame.summary\n",
      "        \"\"\"\n",
      "        if len(cols) == 1 and isinstance(cols[0], list):\n",
      "            cols = cols[0]  # type: ignore[assignment]\n",
      "        jdf = self._jdf.describe(self._jseq(cols))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: distinct\n",
      "Method definition:     def distinct(self) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.distinct().count()\n",
      "        2\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.distinct(), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: drop\n",
      "Method definition:     def drop(self, *cols: \"ColumnOrName\") -> \"DataFrame\":  # type: ignore[misc]\n",
      "        \"\"\"Returns a new :class:`DataFrame` that drops the specified column.\n",
      "        This is a no-op if schema doesn't contain the given column name(s).\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols: str or :class:`Column`\n",
      "            a name of the column, or the :class:`Column` to drop\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.drop('age').collect()\n",
      "        [Row(name='Alice'), Row(name='Bob')]\n",
      "\n",
      "        >>> df.drop(df.age).collect()\n",
      "        [Row(name='Alice'), Row(name='Bob')]\n",
      "\n",
      "        >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      "        [Row(age=5, height=85, name='Bob')]\n",
      "\n",
      "        >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      "        [Row(age=5, name='Bob', height=85)]\n",
      "\n",
      "        >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      "        [Row(name='Bob')]\n",
      "        \"\"\"\n",
      "        if len(cols) == 1:\n",
      "            col = cols[0]\n",
      "            if isinstance(col, str):\n",
      "                jdf = self._jdf.drop(col)\n",
      "            elif isinstance(col, Column):\n",
      "                jdf = self._jdf.drop(col._jc)\n",
      "            else:\n",
      "                raise TypeError(\"col should be a string or a Column\")\n",
      "        else:\n",
      "            for col in cols:\n",
      "                if not isinstance(col, str):\n",
      "                    raise TypeError(\"each col in the param list should be a string\")\n",
      "            jdf = self._jdf.drop(self._jseq(cols))\n",
      "\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: dropDuplicates\n",
      "Method definition:     def dropDuplicates(self, subset: Optional[List[str]] = None) -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "        optionally only considering certain columns.\n",
      "\n",
      "        For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      "        :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      "        duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      "        be and system will accordingly limit the state. In addition, too late data older than\n",
      "        watermark will be dropped to avoid any possibility of duplicates.\n",
      "\n",
      "        :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = sc.parallelize([ \\\\\n",
      "        ...     Row(name='Alice', age=5, height=80), \\\\\n",
      "        ...     Row(name='Alice', age=5, height=80), \\\\\n",
      "        ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      "        >>> df.dropDuplicates().show()\n",
      "        +-----+---+------+\n",
      "        | name|age|height|\n",
      "        +-----+---+------+\n",
      "        |Alice|  5|    80|\n",
      "        |Alice| 10|    80|\n",
      "        +-----+---+------+\n",
      "\n",
      "        >>> df.dropDuplicates(['name', 'height']).show()\n",
      "        +-----+---+------+\n",
      "        | name|age|height|\n",
      "        +-----+---+------+\n",
      "        |Alice|  5|    80|\n",
      "        +-----+---+------+\n",
      "        \"\"\"\n",
      "        if subset is not None and (not isinstance(subset, Iterable) or isinstance(subset, str)):\n",
      "            raise TypeError(\"Parameter 'subset' must be a list of columns\")\n",
      "\n",
      "        if subset is None:\n",
      "            jdf = self._jdf.dropDuplicates()\n",
      "        else:\n",
      "            jdf = self._jdf.dropDuplicates(self._jseq(subset))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: drop_duplicates\n",
      "Method definition:     def dropDuplicates(self, subset: Optional[List[str]] = None) -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "        optionally only considering certain columns.\n",
      "\n",
      "        For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      "        :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      "        duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      "        be and system will accordingly limit the state. In addition, too late data older than\n",
      "        watermark will be dropped to avoid any possibility of duplicates.\n",
      "\n",
      "        :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = sc.parallelize([ \\\\\n",
      "        ...     Row(name='Alice', age=5, height=80), \\\\\n",
      "        ...     Row(name='Alice', age=5, height=80), \\\\\n",
      "        ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      "        >>> df.dropDuplicates().show()\n",
      "        +-----+---+------+\n",
      "        | name|age|height|\n",
      "        +-----+---+------+\n",
      "        |Alice|  5|    80|\n",
      "        |Alice| 10|    80|\n",
      "        +-----+---+------+\n",
      "\n",
      "        >>> df.dropDuplicates(['name', 'height']).show()\n",
      "        +-----+---+------+\n",
      "        | name|age|height|\n",
      "        +-----+---+------+\n",
      "        |Alice|  5|    80|\n",
      "        +-----+---+------+\n",
      "        \"\"\"\n",
      "        if subset is not None and (not isinstance(subset, Iterable) or isinstance(subset, str)):\n",
      "            raise TypeError(\"Parameter 'subset' must be a list of columns\")\n",
      "\n",
      "        if subset is None:\n",
      "            jdf = self._jdf.dropDuplicates()\n",
      "        else:\n",
      "            jdf = self._jdf.dropDuplicates(self._jseq(subset))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: dropna\n",
      "Method definition:     def dropna(\n",
      "        self,\n",
      "        how: str = \"any\",\n",
      "        thresh: Optional[int] = None,\n",
      "        subset: Optional[Union[str, Tuple[str, ...], List[str]]] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` omitting rows with null values.\n",
      "        :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      "\n",
      "        .. versionadded:: 1.3.1\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        how : str, optional\n",
      "            'any' or 'all'.\n",
      "            If 'any', drop a row if it contains any nulls.\n",
      "            If 'all', drop a row only if all its values are null.\n",
      "        thresh: int, optional\n",
      "            default None\n",
      "            If specified, drop rows that have less than `thresh` non-null values.\n",
      "            This overwrites the `how` parameter.\n",
      "        subset : str, tuple or list, optional\n",
      "            optional list of column names to consider.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df4.na.drop().show()\n",
      "        +---+------+-----+\n",
      "        |age|height| name|\n",
      "        +---+------+-----+\n",
      "        | 10|    80|Alice|\n",
      "        +---+------+-----+\n",
      "        \"\"\"\n",
      "        if how is not None and how not in [\"any\", \"all\"]:\n",
      "            raise ValueError(\"how ('\" + how + \"') should be 'any' or 'all'\")\n",
      "\n",
      "        if subset is None:\n",
      "            subset = self.columns\n",
      "        elif isinstance(subset, str):\n",
      "            subset = [subset]\n",
      "        elif not isinstance(subset, (list, tuple)):\n",
      "            raise TypeError(\"subset should be a list or tuple of column names\")\n",
      "\n",
      "        if thresh is None:\n",
      "            thresh = len(subset) if how == \"any\" else 1\n",
      "\n",
      "        return DataFrame(self._jdf.na().drop(thresh, self._jseq(subset)), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: exceptAll\n",
      "Method definition:     def exceptAll(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      "        not in another :class:`DataFrame` while preserving duplicates.\n",
      "\n",
      "        This is equivalent to `EXCEPT ALL` in SQL.\n",
      "        As standard in SQL, this function resolves columns by position (not by name).\n",
      "\n",
      "        .. versionadded:: 2.4.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame(\n",
      "        ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "        >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "\n",
      "        >>> df1.exceptAll(df2).show()\n",
      "        +---+---+\n",
      "        | C1| C2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  c|  4|\n",
      "        +---+---+\n",
      "\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.exceptAll(other._jdf), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: explain\n",
      "Method definition:     def explain(\n",
      "        self, extended: Optional[Union[bool, str]] = None, mode: Optional[str] = None\n",
      "    ) -> None:\n",
      "        \"\"\"Prints the (logical and physical) plans to the console for debugging purpose.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        extended : bool, optional\n",
      "            default ``False``. If ``False``, prints only the physical plan.\n",
      "            When this is a string without specifying the ``mode``, it works as the mode is\n",
      "            specified.\n",
      "        mode : str, optional\n",
      "            specifies the expected output format of plans.\n",
      "\n",
      "            * ``simple``: Print only a physical plan.\n",
      "            * ``extended``: Print both logical and physical plans.\n",
      "            * ``codegen``: Print a physical plan and generated codes if they are available.\n",
      "            * ``cost``: Print a logical plan and statistics if they are available.\n",
      "            * ``formatted``: Split explain output into two sections: a physical plan outline \\\n",
      "                and node details.\n",
      "\n",
      "            .. versionchanged:: 3.0.0\n",
      "               Added optional argument `mode` to specify the expected output format of plans.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.explain()\n",
      "        == Physical Plan ==\n",
      "        *(1) Scan ExistingRDD[age#0,name#1]\n",
      "\n",
      "        >>> df.explain(True)\n",
      "        == Parsed Logical Plan ==\n",
      "        ...\n",
      "        == Analyzed Logical Plan ==\n",
      "        ...\n",
      "        == Optimized Logical Plan ==\n",
      "        ...\n",
      "        == Physical Plan ==\n",
      "        ...\n",
      "\n",
      "        >>> df.explain(mode=\"formatted\")\n",
      "        == Physical Plan ==\n",
      "        * Scan ExistingRDD (1)\n",
      "        (1) Scan ExistingRDD [codegen id : 1]\n",
      "        Output [2]: [age#0, name#1]\n",
      "        ...\n",
      "\n",
      "        >>> df.explain(\"cost\")\n",
      "        == Optimized Logical Plan ==\n",
      "        ...Statistics...\n",
      "        ...\n",
      "        \"\"\"\n",
      "\n",
      "        if extended is not None and mode is not None:\n",
      "            raise ValueError(\"extended and mode should not be set together.\")\n",
      "\n",
      "        # For the no argument case: df.explain()\n",
      "        is_no_argument = extended is None and mode is None\n",
      "\n",
      "        # For the cases below:\n",
      "        #   explain(True)\n",
      "        #   explain(extended=False)\n",
      "        is_extended_case = isinstance(extended, bool) and mode is None\n",
      "\n",
      "        # For the case when extended is mode:\n",
      "        #   df.explain(\"formatted\")\n",
      "        is_extended_as_mode = isinstance(extended, str) and mode is None\n",
      "\n",
      "        # For the mode specified:\n",
      "        #   df.explain(mode=\"formatted\")\n",
      "        is_mode_case = extended is None and isinstance(mode, str)\n",
      "\n",
      "        if not (is_no_argument or is_extended_case or is_extended_as_mode or is_mode_case):\n",
      "            argtypes = [str(type(arg)) for arg in [extended, mode] if arg is not None]\n",
      "            raise TypeError(\n",
      "                \"extended (optional) and mode (optional) should be a string \"\n",
      "                \"and bool; however, got [%s].\" % \", \".join(argtypes)\n",
      "            )\n",
      "\n",
      "        # Sets an explain mode depending on a given argument\n",
      "        if is_no_argument:\n",
      "            explain_mode = \"simple\"\n",
      "        elif is_extended_case:\n",
      "            explain_mode = \"extended\" if extended else \"simple\"\n",
      "        elif is_mode_case:\n",
      "            explain_mode = cast(str, mode)\n",
      "        elif is_extended_as_mode:\n",
      "            explain_mode = cast(str, extended)\n",
      "        assert self._sc._jvm is not None\n",
      "        print(self._sc._jvm.PythonSQLUtils.explainString(self._jdf.queryExecution(), explain_mode))\n",
      "\n",
      "------------\n",
      "Method name: fillna\n",
      "Method definition:     def fillna(\n",
      "        self,\n",
      "        value: Union[\"LiteralType\", Dict[str, \"LiteralType\"]],\n",
      "        subset: Optional[Union[str, Tuple[str, ...], List[str]]] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Replace null values, alias for ``na.fill()``.\n",
      "        :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      "\n",
      "        .. versionadded:: 1.3.1\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        value : int, float, string, bool or dict\n",
      "            Value to replace null values with.\n",
      "            If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      "            from column name (string) to replacement value. The replacement value must be\n",
      "            an int, float, boolean, or string.\n",
      "        subset : str, tuple or list, optional\n",
      "            optional list of column names to consider.\n",
      "            Columns specified in subset that do not have matching data type are ignored.\n",
      "            For example, if `value` is a string, and subset contains a non-string column,\n",
      "            then the non-string column is simply ignored.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df4.na.fill(50).show()\n",
      "        +---+------+-----+\n",
      "        |age|height| name|\n",
      "        +---+------+-----+\n",
      "        | 10|    80|Alice|\n",
      "        |  5|    50|  Bob|\n",
      "        | 50|    50|  Tom|\n",
      "        | 50|    50| null|\n",
      "        +---+------+-----+\n",
      "\n",
      "        >>> df5.na.fill(False).show()\n",
      "        +----+-------+-----+\n",
      "        | age|   name|  spy|\n",
      "        +----+-------+-----+\n",
      "        |  10|  Alice|false|\n",
      "        |   5|    Bob|false|\n",
      "        |null|Mallory| true|\n",
      "        +----+-------+-----+\n",
      "\n",
      "        >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      "        +---+------+-------+\n",
      "        |age|height|   name|\n",
      "        +---+------+-------+\n",
      "        | 10|    80|  Alice|\n",
      "        |  5|  null|    Bob|\n",
      "        | 50|  null|    Tom|\n",
      "        | 50|  null|unknown|\n",
      "        +---+------+-------+\n",
      "        \"\"\"\n",
      "        if not isinstance(value, (float, int, str, bool, dict)):\n",
      "            raise TypeError(\"value should be a float, int, string, bool or dict\")\n",
      "\n",
      "        # Note that bool validates isinstance(int), but we don't want to\n",
      "        # convert bools to floats\n",
      "\n",
      "        if not isinstance(value, bool) and isinstance(value, int):\n",
      "            value = float(value)\n",
      "\n",
      "        if isinstance(value, dict):\n",
      "            return DataFrame(self._jdf.na().fill(value), self.sparkSession)\n",
      "        elif subset is None:\n",
      "            return DataFrame(self._jdf.na().fill(value), self.sparkSession)\n",
      "        else:\n",
      "            if isinstance(subset, str):\n",
      "                subset = [subset]\n",
      "            elif not isinstance(subset, (list, tuple)):\n",
      "                raise TypeError(\"subset should be a list or tuple of column names\")\n",
      "\n",
      "            return DataFrame(self._jdf.na().fill(value, self._jseq(subset)), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: filter\n",
      "Method definition:     def filter(self, condition: \"ColumnOrName\") -> \"DataFrame\":\n",
      "        \"\"\"Filters rows using the given condition.\n",
      "\n",
      "        :func:`where` is an alias for :func:`filter`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        condition : :class:`Column` or str\n",
      "            a :class:`Column` of :class:`types.BooleanType`\n",
      "            or a string of SQL expression.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.filter(df.age > 3).collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        >>> df.where(df.age == 2).collect()\n",
      "        [Row(age=2, name='Alice')]\n",
      "\n",
      "        >>> df.filter(\"age > 3\").collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        >>> df.where(\"age = 2\").collect()\n",
      "        [Row(age=2, name='Alice')]\n",
      "        \"\"\"\n",
      "        if isinstance(condition, str):\n",
      "            jdf = self._jdf.filter(condition)\n",
      "        elif isinstance(condition, Column):\n",
      "            jdf = self._jdf.filter(condition._jc)\n",
      "        else:\n",
      "            raise TypeError(\"condition should be string or Column\")\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: first\n",
      "Method definition:     def first(self) -> Optional[Row]:\n",
      "        \"\"\"Returns the first row as a :class:`Row`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.first()\n",
      "        Row(age=2, name='Alice')\n",
      "        \"\"\"\n",
      "        return self.head()\n",
      "\n",
      "------------\n",
      "Method name: foreach\n",
      "Method definition:     def foreach(self, f: Callable[[Row], None]) -> None:\n",
      "        \"\"\"Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      "\n",
      "        This is a shorthand for ``df.rdd.foreach()``.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(person):\n",
      "        ...     print(person.name)\n",
      "        >>> df.foreach(f)\n",
      "        \"\"\"\n",
      "        self.rdd.foreach(f)\n",
      "\n",
      "------------\n",
      "Method name: foreachPartition\n",
      "Method definition:     def foreachPartition(self, f: Callable[[Iterator[Row]], None]) -> None:\n",
      "        \"\"\"Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      "\n",
      "        This a shorthand for ``df.rdd.foreachPartition()``.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(people):\n",
      "        ...     for person in people:\n",
      "        ...         print(person.name)\n",
      "        >>> df.foreachPartition(f)\n",
      "        \"\"\"\n",
      "        self.rdd.foreachPartition(f)  # type: ignore[arg-type]\n",
      "\n",
      "------------\n",
      "Method name: freqItems\n",
      "Method definition:     def freqItems(\n",
      "        self, cols: Union[List[str], Tuple[str]], support: Optional[float] = None\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Finding frequent items for columns, possibly with false positives. Using the\n",
      "        frequent element count algorithm described in\n",
      "        \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      "        :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : list or tuple\n",
      "            Names of the columns to calculate frequent items for as a list or tuple of\n",
      "            strings.\n",
      "        support : float, optional\n",
      "            The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      "            The support must be greater than 1e-4.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This function is meant for exploratory data analysis, as we make no\n",
      "        guarantee about the backward compatibility of the schema of the resulting\n",
      "        :class:`DataFrame`.\n",
      "        \"\"\"\n",
      "        if isinstance(cols, tuple):\n",
      "            cols = list(cols)\n",
      "        if not isinstance(cols, list):\n",
      "            raise TypeError(\"cols must be a list or tuple of column names as strings.\")\n",
      "        if not support:\n",
      "            support = 0.01\n",
      "        return DataFrame(\n",
      "            self._jdf.stat().freqItems(_to_seq(self._sc, cols), support), self.sparkSession\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: groupBy\n",
      "Method definition:     def groupBy(self, *cols: \"ColumnOrName\") -> \"GroupedData\":  # type: ignore[misc]\n",
      "        \"\"\"Groups the :class:`DataFrame` using the specified columns,\n",
      "        so we can run aggregation on them. See :class:`GroupedData`\n",
      "        for all the available aggregate functions.\n",
      "\n",
      "        :func:`groupby` is an alias for :func:`groupBy`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : list, str or :class:`Column`\n",
      "            columns to group by.\n",
      "            Each element should be a column name (string) or an expression (:class:`Column`).\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.groupBy().avg().collect()\n",
      "        [Row(avg(age)=3.5)]\n",
      "        >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      "        [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "        >>> sorted(df.groupBy(df.name).avg().collect())\n",
      "        [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "        >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      "        [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      "        \"\"\"\n",
      "        jgd = self._jdf.groupBy(self._jcols(*cols))\n",
      "        from pyspark.sql.group import GroupedData\n",
      "\n",
      "        return GroupedData(jgd, self)\n",
      "\n",
      "------------\n",
      "Method name: groupby\n",
      "Method definition:     def groupBy(self, *cols: \"ColumnOrName\") -> \"GroupedData\":  # type: ignore[misc]\n",
      "        \"\"\"Groups the :class:`DataFrame` using the specified columns,\n",
      "        so we can run aggregation on them. See :class:`GroupedData`\n",
      "        for all the available aggregate functions.\n",
      "\n",
      "        :func:`groupby` is an alias for :func:`groupBy`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : list, str or :class:`Column`\n",
      "            columns to group by.\n",
      "            Each element should be a column name (string) or an expression (:class:`Column`).\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.groupBy().avg().collect()\n",
      "        [Row(avg(age)=3.5)]\n",
      "        >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      "        [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "        >>> sorted(df.groupBy(df.name).avg().collect())\n",
      "        [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "        >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      "        [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      "        \"\"\"\n",
      "        jgd = self._jdf.groupBy(self._jcols(*cols))\n",
      "        from pyspark.sql.group import GroupedData\n",
      "\n",
      "        return GroupedData(jgd, self)\n",
      "\n",
      "------------\n",
      "Method name: head\n",
      "Method definition:     def head(self, n: Optional[int] = None) -> Union[Optional[Row], List[Row]]:\n",
      "        \"\"\"Returns the first ``n`` rows.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This method should only be used if the resulting array is expected\n",
      "        to be small, as all the data is loaded into the driver's memory.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int, optional\n",
      "            default 1. Number of rows to return.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        If n is greater than 1, return a list of :class:`Row`.\n",
      "        If n is 1, return a single Row.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.head()\n",
      "        Row(age=2, name='Alice')\n",
      "        >>> df.head(1)\n",
      "        [Row(age=2, name='Alice')]\n",
      "        \"\"\"\n",
      "        if n is None:\n",
      "            rs = self.head(1)\n",
      "            return rs[0] if rs else None\n",
      "        return self.take(n)\n",
      "\n",
      "------------\n",
      "Method name: hint\n",
      "Method definition:     def hint(\n",
      "        self, name: str, *parameters: Union[\"PrimitiveType\", List[\"PrimitiveType\"]]\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Specifies some hint on the current :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 2.2.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        name : str\n",
      "            A name of the hint.\n",
      "        parameters : str, list, float or int\n",
      "            Optional parameters.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`DataFrame`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n",
      "        +----+---+------+\n",
      "        |name|age|height|\n",
      "        +----+---+------+\n",
      "        | Bob|  5|    85|\n",
      "        +----+---+------+\n",
      "        \"\"\"\n",
      "        if len(parameters) == 1 and isinstance(parameters[0], list):\n",
      "            parameters = parameters[0]  # type: ignore[assignment]\n",
      "\n",
      "        if not isinstance(name, str):\n",
      "            raise TypeError(\"name should be provided as str, got {0}\".format(type(name)))\n",
      "\n",
      "        allowed_types = (str, list, float, int)\n",
      "        for p in parameters:\n",
      "            if not isinstance(p, allowed_types):\n",
      "                raise TypeError(\n",
      "                    \"all parameters should be in {0}, got {1} of type {2}\".format(\n",
      "                        allowed_types, p, type(p)\n",
      "                    )\n",
      "                )\n",
      "\n",
      "        jdf = self._jdf.hint(name, self._jseq(parameters))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: inputFiles\n",
      "Method definition:     def inputFiles(self) -> List[str]:\n",
      "        \"\"\"\n",
      "        Returns a best-effort snapshot of the files that compose this :class:`DataFrame`.\n",
      "        This method simply asks each constituent BaseRelation for its respective files and\n",
      "        takes the union of all results. Depending on the source relations, this may not find\n",
      "        all input files. Duplicates are removed.\n",
      "\n",
      "        .. versionadded:: 3.1.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
      "        >>> len(df.inputFiles())\n",
      "        1\n",
      "        \"\"\"\n",
      "        return list(self._jdf.inputFiles())\n",
      "\n",
      "------------\n",
      "Method name: intersect\n",
      "Method definition:     @since(1.3)\n",
      "    def intersect(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` containing rows only in\n",
      "        both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      "\n",
      "        This is equivalent to `INTERSECT` in SQL.\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.intersect(other._jdf), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: intersectAll\n",
      "Method definition:     def intersectAll(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
      "        and another :class:`DataFrame` while preserving duplicates.\n",
      "\n",
      "        This is equivalent to `INTERSECT ALL` in SQL. As standard in SQL, this function\n",
      "        resolves columns by position (not by name).\n",
      "\n",
      "        .. versionadded:: 2.4.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "        >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "\n",
      "        >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      "        +---+---+\n",
      "        | C1| C2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  1|\n",
      "        |  b|  3|\n",
      "        +---+---+\n",
      "\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.intersectAll(other._jdf), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: isEmpty\n",
      "Method definition:     def isEmpty(self) -> bool:\n",
      "        \"\"\"Returns ``True`` if this :class:`DataFrame` is empty.\n",
      "\n",
      "        .. versionadded:: 3.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df_empty = spark.createDataFrame([], 'a STRING')\n",
      "        >>> df_non_empty = spark.createDataFrame([(\"a\")], 'STRING')\n",
      "        >>> df_empty.isEmpty()\n",
      "        True\n",
      "        >>> df_non_empty.isEmpty()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self._jdf.isEmpty()\n",
      "\n",
      "------------\n",
      "Method name: isLocal\n",
      "Method definition:     @since(1.3)\n",
      "    def isLocal(self) -> bool:\n",
      "        \"\"\"Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      "        (without any Spark executors).\n",
      "        \"\"\"\n",
      "        return self._jdf.isLocal()\n",
      "\n",
      "------------\n",
      "Method name: join\n",
      "Method definition:     def join(\n",
      "        self,\n",
      "        other: \"DataFrame\",\n",
      "        on: Optional[Union[str, List[str], Column, List[Column]]] = None,\n",
      "        how: Optional[str] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Joins with another :class:`DataFrame`, using the given join expression.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : :class:`DataFrame`\n",
      "            Right side of the join\n",
      "        on : str, list or :class:`Column`, optional\n",
      "            a string for the join column name, a list of column names,\n",
      "            a join expression (Column), or a list of Columns.\n",
      "            If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "            the column(s) must exist on both sides, and this performs an equi-join.\n",
      "        how : str, optional\n",
      "            default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "            ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      "            ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      "            ``anti``, ``leftanti`` and ``left_anti``.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The following performs a full outer join between ``df1`` and ``df2``.\n",
      "\n",
      "        >>> from pyspark.sql.functions import desc\n",
      "        >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height) \\\n",
      "                .sort(desc(\"name\")).collect()\n",
      "        [Row(name='Bob', height=85), Row(name='Alice', height=None), Row(name=None, height=80)]\n",
      "\n",
      "        >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).collect()\n",
      "        [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      "\n",
      "        >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      "        >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      "        [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "\n",
      "        >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      "        [Row(name='Bob', height=85)]\n",
      "\n",
      "        >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      "        [Row(name='Bob', age=5)]\n",
      "        \"\"\"\n",
      "\n",
      "        if on is not None and not isinstance(on, list):\n",
      "            on = [on]  # type: ignore[assignment]\n",
      "\n",
      "        if on is not None:\n",
      "            if isinstance(on[0], str):\n",
      "                on = self._jseq(cast(List[str], on))\n",
      "            else:\n",
      "                assert isinstance(on[0], Column), \"on should be Column or list of Column\"\n",
      "                on = reduce(lambda x, y: x.__and__(y), cast(List[Column], on))\n",
      "                on = on._jc\n",
      "\n",
      "        if on is None and how is None:\n",
      "            jdf = self._jdf.join(other._jdf)\n",
      "        else:\n",
      "            if how is None:\n",
      "                how = \"inner\"\n",
      "            if on is None:\n",
      "                on = self._jseq([])\n",
      "            assert isinstance(how, str), \"how should be a string\"\n",
      "            jdf = self._jdf.join(other._jdf, on, how)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: limit\n",
      "Method definition:     def limit(self, num: int) -> \"DataFrame\":\n",
      "        \"\"\"Limits the result count to the number specified.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.limit(1).collect()\n",
      "        [Row(age=2, name='Alice')]\n",
      "        >>> df.limit(0).collect()\n",
      "        []\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.limit(num)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: localCheckpoint\n",
      "Method definition:     def localCheckpoint(self, eager: bool = True) -> \"DataFrame\":\n",
      "        \"\"\"Returns a locally checkpointed version of this :class:`DataFrame`. Checkpointing can be\n",
      "        used to truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      "        iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
      "        stored in the executors using the caching subsystem and therefore they are not reliable.\n",
      "\n",
      "        .. versionadded:: 2.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        eager : bool, optional\n",
      "            Whether to checkpoint this :class:`DataFrame` immediately\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This API is experimental.\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.localCheckpoint(eager)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: mapInArrow\n",
      "Method definition:     def mapInArrow(\n",
      "        self, func: \"ArrowMapIterFunction\", schema: Union[StructType, str]\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      "        function that takes and outputs a PyArrow's `RecordBatch`, and returns the result as a\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        The function should take an iterator of `pyarrow.RecordBatch`\\\\s and return\n",
      "        another iterator of `pyarrow.RecordBatch`\\\\s. All columns are passed\n",
      "        together as an iterator of `pyarrow.RecordBatch`\\\\s to the function and the\n",
      "        returned iterator of `pyarrow.RecordBatch`\\\\s are combined as a :class:`DataFrame`.\n",
      "        Each `pyarrow.RecordBatch` size can be controlled by\n",
      "        `spark.sql.execution.arrow.maxRecordsPerBatch`.\n",
      "\n",
      "        .. versionadded:: 3.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            a Python native function that takes an iterator of `pyarrow.RecordBatch`\\\\s, and\n",
      "            outputs an iterator of `pyarrow.RecordBatch`\\\\s.\n",
      "        schema : :class:`pyspark.sql.types.DataType` or str\n",
      "            the return type of the `func` in PySpark. The value can be either a\n",
      "            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyarrow  # doctest: +SKIP\n",
      "        >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      "        >>> def filter_func(iterator):\n",
      "        ...     for batch in iterator:\n",
      "        ...         pdf = batch.to_pandas()\n",
      "        ...         yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])\n",
      "        >>> df.mapInArrow(filter_func, df.schema).show()  # doctest: +SKIP\n",
      "        +---+---+\n",
      "        | id|age|\n",
      "        +---+---+\n",
      "        |  1| 21|\n",
      "        +---+---+\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This API is unstable, and for developers.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        pyspark.sql.functions.pandas_udf\n",
      "        pyspark.sql.DataFrame.mapInPandas\n",
      "        \"\"\"\n",
      "        from pyspark.sql import DataFrame\n",
      "        from pyspark.sql.pandas.functions import pandas_udf\n",
      "\n",
      "        assert isinstance(self, DataFrame)\n",
      "\n",
      "        # The usage of the pandas_udf is internal so type checking is disabled.\n",
      "        udf = pandas_udf(\n",
      "            func, returnType=schema, functionType=PythonEvalType.SQL_MAP_ARROW_ITER_UDF\n",
      "        )  # type: ignore[call-overload]\n",
      "        udf_column = udf(*[self[col] for col in self.columns])\n",
      "        jdf = self._jdf.pythonMapInArrow(udf_column._jc.expr())\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: mapInPandas\n",
      "Method definition:     def mapInPandas(\n",
      "        self, func: \"PandasMapIterFunction\", schema: Union[StructType, str]\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      "        function that takes and outputs a pandas DataFrame, and returns the result as a\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        The function should take an iterator of `pandas.DataFrame`\\\\s and return\n",
      "        another iterator of `pandas.DataFrame`\\\\s. All columns are passed\n",
      "        together as an iterator of `pandas.DataFrame`\\\\s to the function and the\n",
      "        returned iterator of `pandas.DataFrame`\\\\s are combined as a :class:`DataFrame`.\n",
      "        Each `pandas.DataFrame` size can be controlled by\n",
      "        `spark.sql.execution.arrow.maxRecordsPerBatch`.\n",
      "\n",
      "        .. versionadded:: 3.0.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            a Python native function that takes an iterator of `pandas.DataFrame`\\\\s, and\n",
      "            outputs an iterator of `pandas.DataFrame`\\\\s.\n",
      "        schema : :class:`pyspark.sql.types.DataType` or str\n",
      "            the return type of the `func` in PySpark. The value can be either a\n",
      "            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import pandas_udf\n",
      "        >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      "        >>> def filter_func(iterator):\n",
      "        ...     for pdf in iterator:\n",
      "        ...         yield pdf[pdf.id == 1]\n",
      "        >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
      "        +---+---+\n",
      "        | id|age|\n",
      "        +---+---+\n",
      "        |  1| 21|\n",
      "        +---+---+\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This API is experimental\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        pyspark.sql.functions.pandas_udf\n",
      "        \"\"\"\n",
      "        from pyspark.sql import DataFrame\n",
      "        from pyspark.sql.pandas.functions import pandas_udf\n",
      "\n",
      "        assert isinstance(self, DataFrame)\n",
      "\n",
      "        # The usage of the pandas_udf is internal so type checking is disabled.\n",
      "        udf = pandas_udf(\n",
      "            func, returnType=schema, functionType=PythonEvalType.SQL_MAP_PANDAS_ITER_UDF\n",
      "        )  # type: ignore[call-overload]\n",
      "        udf_column = udf(*[self[col] for col in self.columns])\n",
      "        jdf = self._jdf.mapInPandas(udf_column._jc.expr())\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: observe\n",
      "Method definition:     @since(3.3)\n",
      "    def observe(self, observation: \"Observation\", *exprs: Column) -> \"DataFrame\":\n",
      "        \"\"\"Observe (named) metrics through an :class:`Observation` instance.\n",
      "\n",
      "        A user can retrieve the metrics by accessing `Observation.get`.\n",
      "\n",
      "        .. versionadded:: 3.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        observation : :class:`Observation`\n",
      "            an :class:`Observation` instance to obtain the metric.\n",
      "        exprs : list of :class:`Column`\n",
      "            column expressions (:class:`Column`).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`DataFrame`\n",
      "            the observed :class:`DataFrame`.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This method does not support streaming datasets.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import col, count, lit, max\n",
      "        >>> from pyspark.sql import Observation\n",
      "        >>> observation = Observation(\"my metrics\")\n",
      "        >>> observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n",
      "        >>> observed_df.count()\n",
      "        2\n",
      "        >>> observation.get\n",
      "        {'count': 2, 'max(age)': 5}\n",
      "        \"\"\"\n",
      "        from pyspark.sql import Observation\n",
      "\n",
      "        assert isinstance(observation, Observation), \"observation should be Observation\"\n",
      "        return observation._on(self, *exprs)\n",
      "\n",
      "------------\n",
      "Method name: orderBy\n",
      "Method definition:     def sort(\n",
      "        self, *cols: Union[str, Column, List[Union[str, Column]]], **kwargs: Any\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : str, list, or :class:`Column`, optional\n",
      "             list of :class:`Column` or column names to sort by.\n",
      "\n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        ascending : bool or list, optional\n",
      "            boolean or list of boolean (default ``True``).\n",
      "            Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "            If a list is specified, length of the list must equal length of the `cols`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.sort(df.age.desc()).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> df.sort(\"age\", ascending=False).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> df.orderBy(df.age.desc()).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> from pyspark.sql.functions import *\n",
      "        >>> df.sort(asc(\"age\")).collect()\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.sort(self._sort_cols(cols, kwargs))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: pandas_api\n",
      "Method definition:     def pandas_api(\n",
      "        self, index_col: Optional[Union[str, List[str]]] = None\n",
      "    ) -> \"PandasOnSparkDataFrame\":\n",
      "        \"\"\"\n",
      "        Converts the existing DataFrame into a pandas-on-Spark DataFrame.\n",
      "\n",
      "        If a pandas-on-Spark DataFrame is converted to a Spark DataFrame and then back\n",
      "        to pandas-on-Spark, it will lose the index information and the original index\n",
      "        will be turned into a normal column.\n",
      "\n",
      "        This is only available if Pandas is installed and available.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        index_col: str or list of str, optional, default: None\n",
      "            Index column of table in Spark.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        pyspark.pandas.frame.DataFrame.to_spark\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.show()  # doctest: +SKIP\n",
      "        +----+----+\n",
      "        |Col1|Col2|\n",
      "        +----+----+\n",
      "        |   a|   1|\n",
      "        |   b|   2|\n",
      "        |   c|   3|\n",
      "        +----+----+\n",
      "\n",
      "        >>> df.pandas_api()  # doctest: +SKIP\n",
      "          Col1  Col2\n",
      "        0    a     1\n",
      "        1    b     2\n",
      "        2    c     3\n",
      "\n",
      "        We can specify the index columns.\n",
      "\n",
      "        >>> df.pandas_api(index_col=\"Col1\"): # doctest: +SKIP\n",
      "              Col2\n",
      "        Col1\n",
      "        a        1\n",
      "        b        2\n",
      "        c        3\n",
      "        \"\"\"\n",
      "        from pyspark.pandas.namespace import _get_index_map\n",
      "        from pyspark.pandas.frame import DataFrame as PandasOnSparkDataFrame\n",
      "        from pyspark.pandas.internal import InternalFrame\n",
      "\n",
      "        index_spark_columns, index_names = _get_index_map(self, index_col)\n",
      "        internal = InternalFrame(\n",
      "            spark_frame=self,\n",
      "            index_spark_columns=index_spark_columns,\n",
      "            index_names=index_names,  # type: ignore[arg-type]\n",
      "        )\n",
      "        return PandasOnSparkDataFrame(internal)\n",
      "\n",
      "------------\n",
      "Method name: persist\n",
      "Method definition:     def persist(\n",
      "        self,\n",
      "        storageLevel: StorageLevel = (StorageLevel.MEMORY_AND_DISK_DESER),\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      "        operations after the first time it is computed. This can only be used to assign\n",
      "        a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      "        If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
      "        \"\"\"\n",
      "        self.is_cached = True\n",
      "        javaStorageLevel = self._sc._getJavaStorageLevel(storageLevel)\n",
      "        self._jdf.persist(javaStorageLevel)\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: printSchema\n",
      "Method definition:     def printSchema(self) -> None:\n",
      "        \"\"\"Prints out the schema in the tree format.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- age: integer (nullable = true)\n",
      "         |-- name: string (nullable = true)\n",
      "        <BLANKLINE>\n",
      "        \"\"\"\n",
      "        print(self._jdf.schema().treeString())\n",
      "\n",
      "------------\n",
      "Method name: randomSplit\n",
      "Method definition:     def randomSplit(self, weights: List[float], seed: Optional[int] = None) -> List[\"DataFrame\"]:\n",
      "        \"\"\"Randomly splits this :class:`DataFrame` with the provided weights.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        weights : list\n",
      "            list of doubles as weights with which to split the :class:`DataFrame`.\n",
      "            Weights will be normalized if they don't sum up to 1.0.\n",
      "        seed : int, optional\n",
      "            The seed for sampling.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> splits = df4.randomSplit([1.0, 2.0], 24)\n",
      "        >>> splits[0].count()\n",
      "        2\n",
      "\n",
      "        >>> splits[1].count()\n",
      "        2\n",
      "        \"\"\"\n",
      "        for w in weights:\n",
      "            if w < 0.0:\n",
      "                raise ValueError(\"Weights must be positive. Found weight value: %s\" % w)\n",
      "        seed = seed if seed is not None else random.randint(0, sys.maxsize)\n",
      "        df_array = self._jdf.randomSplit(\n",
      "            _to_list(self.sparkSession._sc, cast(List[\"ColumnOrName\"], weights)), int(seed)\n",
      "        )\n",
      "        return [DataFrame(df, self.sparkSession) for df in df_array]\n",
      "\n",
      "------------\n",
      "Method name: registerTempTable\n",
      "Method definition:     def registerTempTable(self, name: str) -> None:\n",
      "        \"\"\"Registers this :class:`DataFrame` as a temporary table using the given name.\n",
      "\n",
      "        The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "        that was used to create this :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "            Use :meth:`DataFrame.createOrReplaceTempView` instead.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.registerTempTable(\"people\")\n",
      "        >>> df2 = spark.sql(\"select * from people\")\n",
      "        >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "        True\n",
      "        >>> spark.catalog.dropTempView(\"people\")\n",
      "        True\n",
      "\n",
      "        \"\"\"\n",
      "        warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n",
      "        self._jdf.createOrReplaceTempView(name)\n",
      "\n",
      "------------\n",
      "Method name: repartition\n",
      "Method definition:     def repartition(  # type: ignore[misc]\n",
      "        self, numPartitions: Union[int, \"ColumnOrName\"], *cols: \"ColumnOrName\"\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      "        resulting :class:`DataFrame` is hash partitioned.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        numPartitions : int\n",
      "            can be an int to specify the target number of partitions or a Column.\n",
      "            If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      "            the default number of partitions is used.\n",
      "        cols : str or :class:`Column`\n",
      "            partitioning columns.\n",
      "\n",
      "            .. versionchanged:: 1.6\n",
      "               Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      "               optional if partitioning columns are specified.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.repartition(10).rdd.getNumPartitions()\n",
      "        10\n",
      "        >>> data = df.union(df).repartition(\"age\")\n",
      "        >>> data.show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        +---+-----+\n",
      "        >>> data = data.repartition(7, \"age\")\n",
      "        >>> data.show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        +---+-----+\n",
      "        >>> data.rdd.getNumPartitions()\n",
      "        7\n",
      "        >>> data = data.repartition(3, \"name\", \"age\")\n",
      "        >>> data.show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  5|  Bob|\n",
      "        |  5|  Bob|\n",
      "        |  2|Alice|\n",
      "        |  2|Alice|\n",
      "        +---+-----+\n",
      "        \"\"\"\n",
      "        if isinstance(numPartitions, int):\n",
      "            if len(cols) == 0:\n",
      "                return DataFrame(self._jdf.repartition(numPartitions), self.sparkSession)\n",
      "            else:\n",
      "                return DataFrame(\n",
      "                    self._jdf.repartition(numPartitions, self._jcols(*cols)),\n",
      "                    self.sparkSession,\n",
      "                )\n",
      "        elif isinstance(numPartitions, (str, Column)):\n",
      "            cols = (numPartitions,) + cols\n",
      "            return DataFrame(self._jdf.repartition(self._jcols(*cols)), self.sparkSession)\n",
      "        else:\n",
      "            raise TypeError(\"numPartitions should be an int or Column\")\n",
      "\n",
      "------------\n",
      "Method name: repartitionByRange\n",
      "Method definition:     def repartitionByRange(  # type: ignore[misc]\n",
      "        self, numPartitions: Union[int, \"ColumnOrName\"], *cols: \"ColumnOrName\"\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      "        resulting :class:`DataFrame` is range partitioned.\n",
      "\n",
      "        At least one partition-by expression must be specified.\n",
      "        When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      "\n",
      "        .. versionadded:: 2.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        numPartitions : int\n",
      "            can be an int to specify the target number of partitions or a Column.\n",
      "            If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      "            the default number of partitions is used.\n",
      "        cols : str or :class:`Column`\n",
      "            partitioning columns.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Due to performance reasons this method uses sampling to estimate the ranges.\n",
      "        Hence, the output may not be consistent, since sampling can return different values.\n",
      "        The sample size can be controlled by the config\n",
      "        `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      "        2\n",
      "        >>> df.show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        +---+-----+\n",
      "        >>> df.repartitionByRange(1, \"age\").rdd.getNumPartitions()\n",
      "        1\n",
      "        >>> data = df.repartitionByRange(\"age\")\n",
      "        >>> df.show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        +---+-----+\n",
      "        \"\"\"\n",
      "        if isinstance(numPartitions, int):\n",
      "            if len(cols) == 0:\n",
      "                raise ValueError(\"At least one partition-by expression must be specified.\")\n",
      "            else:\n",
      "                return DataFrame(\n",
      "                    self._jdf.repartitionByRange(numPartitions, self._jcols(*cols)),\n",
      "                    self.sparkSession,\n",
      "                )\n",
      "        elif isinstance(numPartitions, (str, Column)):\n",
      "            cols = (numPartitions,) + cols\n",
      "            return DataFrame(self._jdf.repartitionByRange(self._jcols(*cols)), self.sparkSession)\n",
      "        else:\n",
      "            raise TypeError(\"numPartitions should be an int, string or Column\")\n",
      "\n",
      "------------\n",
      "Method name: replace\n",
      "Method definition:     def replace(  # type: ignore[misc]\n",
      "        self,\n",
      "        to_replace: Union[\n",
      "            \"LiteralType\", List[\"LiteralType\"], Dict[\"LiteralType\", \"OptionalPrimitiveType\"]\n",
      "        ],\n",
      "        value: Optional[\n",
      "            Union[\"OptionalPrimitiveType\", List[\"OptionalPrimitiveType\"], _NoValueType]\n",
      "        ] = _NoValue,\n",
      "        subset: Optional[List[str]] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` replacing a value with another value.\n",
      "        :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      "        aliases of each other.\n",
      "        Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      "        or strings. Value can have None. When replacing, the new value will be cast\n",
      "        to the type of the existing column.\n",
      "        For numeric replacements all values to be replaced should have unique\n",
      "        floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      "        and arbitrary replacement will be used.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        to_replace : bool, int, float, string, list or dict\n",
      "            Value to be replaced.\n",
      "            If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      "            must be a mapping between a value and a replacement.\n",
      "        value : bool, int, float, string or None, optional\n",
      "            The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      "            list, `value` should be of the same length and type as `to_replace`.\n",
      "            If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      "            used as a replacement for each item in `to_replace`.\n",
      "        subset : list, optional\n",
      "            optional list of column names to consider.\n",
      "            Columns specified in subset that do not have matching data type are ignored.\n",
      "            For example, if `value` is a string, and subset contains a non-string column,\n",
      "            then the non-string column is simply ignored.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df4.na.replace(10, 20).show()\n",
      "        +----+------+-----+\n",
      "        | age|height| name|\n",
      "        +----+------+-----+\n",
      "        |  20|    80|Alice|\n",
      "        |   5|  null|  Bob|\n",
      "        |null|  null|  Tom|\n",
      "        |null|  null| null|\n",
      "        +----+------+-----+\n",
      "\n",
      "        >>> df4.na.replace('Alice', None).show()\n",
      "        +----+------+----+\n",
      "        | age|height|name|\n",
      "        +----+------+----+\n",
      "        |  10|    80|null|\n",
      "        |   5|  null| Bob|\n",
      "        |null|  null| Tom|\n",
      "        |null|  null|null|\n",
      "        +----+------+----+\n",
      "\n",
      "        >>> df4.na.replace({'Alice': None}).show()\n",
      "        +----+------+----+\n",
      "        | age|height|name|\n",
      "        +----+------+----+\n",
      "        |  10|    80|null|\n",
      "        |   5|  null| Bob|\n",
      "        |null|  null| Tom|\n",
      "        |null|  null|null|\n",
      "        +----+------+----+\n",
      "\n",
      "        >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      "        +----+------+----+\n",
      "        | age|height|name|\n",
      "        +----+------+----+\n",
      "        |  10|    80|   A|\n",
      "        |   5|  null|   B|\n",
      "        |null|  null| Tom|\n",
      "        |null|  null|null|\n",
      "        +----+------+----+\n",
      "        \"\"\"\n",
      "        if value is _NoValue:\n",
      "            if isinstance(to_replace, dict):\n",
      "                value = None\n",
      "            else:\n",
      "                raise TypeError(\"value argument is required when to_replace is not a dictionary.\")\n",
      "\n",
      "        # Helper functions\n",
      "        def all_of(types: Union[Type, Tuple[Type, ...]]) -> Callable[[Iterable], bool]:\n",
      "            \"\"\"Given a type or tuple of types and a sequence of xs\n",
      "            check if each x is instance of type(s)\n",
      "\n",
      "            >>> all_of(bool)([True, False])\n",
      "            True\n",
      "            >>> all_of(str)([\"a\", 1])\n",
      "            False\n",
      "            \"\"\"\n",
      "\n",
      "            def all_of_(xs: Iterable) -> bool:\n",
      "                return all(isinstance(x, types) for x in xs)\n",
      "\n",
      "            return all_of_\n",
      "\n",
      "        all_of_bool = all_of(bool)\n",
      "        all_of_str = all_of(str)\n",
      "        all_of_numeric = all_of((float, int))\n",
      "\n",
      "        # Validate input types\n",
      "        valid_types = (bool, float, int, str, list, tuple)\n",
      "        if not isinstance(to_replace, valid_types + (dict,)):\n",
      "            raise TypeError(\n",
      "                \"to_replace should be a bool, float, int, string, list, tuple, or dict. \"\n",
      "                \"Got {0}\".format(type(to_replace))\n",
      "            )\n",
      "\n",
      "        if (\n",
      "            not isinstance(value, valid_types)\n",
      "            and value is not None\n",
      "            and not isinstance(to_replace, dict)\n",
      "        ):\n",
      "            raise TypeError(\n",
      "                \"If to_replace is not a dict, value should be \"\n",
      "                \"a bool, float, int, string, list, tuple or None. \"\n",
      "                \"Got {0}\".format(type(value))\n",
      "            )\n",
      "\n",
      "        if isinstance(to_replace, (list, tuple)) and isinstance(value, (list, tuple)):\n",
      "            if len(to_replace) != len(value):\n",
      "                raise ValueError(\n",
      "                    \"to_replace and value lists should be of the same length. \"\n",
      "                    \"Got {0} and {1}\".format(len(to_replace), len(value))\n",
      "                )\n",
      "\n",
      "        if not (subset is None or isinstance(subset, (list, tuple, str))):\n",
      "            raise TypeError(\n",
      "                \"subset should be a list or tuple of column names, \"\n",
      "                \"column name or None. Got {0}\".format(type(subset))\n",
      "            )\n",
      "\n",
      "        # Reshape input arguments if necessary\n",
      "        if isinstance(to_replace, (float, int, str)):\n",
      "            to_replace = [to_replace]\n",
      "\n",
      "        if isinstance(to_replace, dict):\n",
      "            rep_dict = to_replace\n",
      "            if value is not None:\n",
      "                warnings.warn(\"to_replace is a dict and value is not None. value will be ignored.\")\n",
      "        else:\n",
      "            if isinstance(value, (float, int, str)) or value is None:\n",
      "                value = [value for _ in range(len(to_replace))]\n",
      "            rep_dict = dict(zip(to_replace, cast(\"Iterable[Optional[Union[float, str]]]\", value)))\n",
      "\n",
      "        if isinstance(subset, str):\n",
      "            subset = [subset]\n",
      "\n",
      "        # Verify we were not passed in mixed type generics.\n",
      "        if not any(\n",
      "            all_of_type(rep_dict.keys())\n",
      "            and all_of_type(x for x in rep_dict.values() if x is not None)\n",
      "            for all_of_type in [all_of_bool, all_of_str, all_of_numeric]\n",
      "        ):\n",
      "            raise ValueError(\"Mixed type replacements are not supported\")\n",
      "\n",
      "        if subset is None:\n",
      "            return DataFrame(self._jdf.na().replace(\"*\", rep_dict), self.sparkSession)\n",
      "        else:\n",
      "            return DataFrame(\n",
      "                self._jdf.na().replace(self._jseq(subset), self._jmap(rep_dict)),\n",
      "                self.sparkSession,\n",
      "            )\n",
      "\n",
      "------------\n",
      "Method name: rollup\n",
      "Method definition:     def rollup(self, *cols: \"ColumnOrName\") -> \"GroupedData\":  # type: ignore[misc]\n",
      "        \"\"\"\n",
      "        Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      "        the specified columns, so we can run aggregation on them.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      "        +-----+----+-----+\n",
      "        | name| age|count|\n",
      "        +-----+----+-----+\n",
      "        | null|null|    2|\n",
      "        |Alice|null|    1|\n",
      "        |Alice|   2|    1|\n",
      "        |  Bob|null|    1|\n",
      "        |  Bob|   5|    1|\n",
      "        +-----+----+-----+\n",
      "        \"\"\"\n",
      "        jgd = self._jdf.rollup(self._jcols(*cols))\n",
      "        from pyspark.sql.group import GroupedData\n",
      "\n",
      "        return GroupedData(jgd, self)\n",
      "\n",
      "------------\n",
      "Method name: sameSemantics\n",
      "Method definition:     def sameSemantics(self, other: \"DataFrame\") -> bool:\n",
      "        \"\"\"\n",
      "        Returns `True` when the logical query plans inside both :class:`DataFrame`\\\\s are equal and\n",
      "        therefore return same results.\n",
      "\n",
      "        .. versionadded:: 3.1.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The equality comparison here is simplified by tolerating the cosmetic differences\n",
      "        such as attribute names.\n",
      "\n",
      "        This API can compare both :class:`DataFrame`\\\\s very fast but can still return\n",
      "        `False` on the :class:`DataFrame` that return the same results, for instance, from\n",
      "        different plans. Such false negative semantic can be useful when caching as an example.\n",
      "\n",
      "        This API is a developer API.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.range(10)\n",
      "        >>> df2 = spark.range(10)\n",
      "        >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\n",
      "        True\n",
      "        >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\n",
      "        False\n",
      "        >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n",
      "        True\n",
      "        \"\"\"\n",
      "        if not isinstance(other, DataFrame):\n",
      "            raise TypeError(\"other parameter should be of DataFrame; however, got %s\" % type(other))\n",
      "        return self._jdf.sameSemantics(other._jdf)\n",
      "\n",
      "------------\n",
      "Method name: sample\n",
      "Method definition:     def sample(  # type: ignore[misc]\n",
      "        self,\n",
      "        withReplacement: Optional[Union[float, bool]] = None,\n",
      "        fraction: Optional[Union[int, float]] = None,\n",
      "        seed: Optional[int] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Returns a sampled subset of this :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        withReplacement : bool, optional\n",
      "            Sample with replacement or not (default ``False``).\n",
      "        fraction : float, optional\n",
      "            Fraction of rows to generate, range [0.0, 1.0].\n",
      "        seed : int, optional\n",
      "            Seed for sampling (default a random seed).\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This is not guaranteed to provide exactly the fraction specified of the total\n",
      "        count of the given :class:`DataFrame`.\n",
      "\n",
      "        `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.sample(0.5, 3).count()\n",
      "        7\n",
      "        >>> df.sample(fraction=0.5, seed=3).count()\n",
      "        7\n",
      "        >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      "        1\n",
      "        >>> df.sample(1.0).count()\n",
      "        10\n",
      "        >>> df.sample(fraction=1.0).count()\n",
      "        10\n",
      "        >>> df.sample(False, fraction=1.0).count()\n",
      "        10\n",
      "        \"\"\"\n",
      "\n",
      "        # For the cases below:\n",
      "        #   sample(True, 0.5 [, seed])\n",
      "        #   sample(True, fraction=0.5 [, seed])\n",
      "        #   sample(withReplacement=False, fraction=0.5 [, seed])\n",
      "        is_withReplacement_set = type(withReplacement) == bool and isinstance(fraction, float)\n",
      "\n",
      "        # For the case below:\n",
      "        #   sample(faction=0.5 [, seed])\n",
      "        is_withReplacement_omitted_kwargs = withReplacement is None and isinstance(fraction, float)\n",
      "\n",
      "        # For the case below:\n",
      "        #   sample(0.5 [, seed])\n",
      "        is_withReplacement_omitted_args = isinstance(withReplacement, float)\n",
      "\n",
      "        if not (\n",
      "            is_withReplacement_set\n",
      "            or is_withReplacement_omitted_kwargs\n",
      "            or is_withReplacement_omitted_args\n",
      "        ):\n",
      "            argtypes = [\n",
      "                str(type(arg)) for arg in [withReplacement, fraction, seed] if arg is not None\n",
      "            ]\n",
      "            raise TypeError(\n",
      "                \"withReplacement (optional), fraction (required) and seed (optional)\"\n",
      "                \" should be a bool, float and number; however, \"\n",
      "                \"got [%s].\" % \", \".join(argtypes)\n",
      "            )\n",
      "\n",
      "        if is_withReplacement_omitted_args:\n",
      "            if fraction is not None:\n",
      "                seed = cast(int, fraction)\n",
      "            fraction = withReplacement\n",
      "            withReplacement = None\n",
      "\n",
      "        seed = int(seed) if seed is not None else None\n",
      "        args = [arg for arg in [withReplacement, fraction, seed] if arg is not None]\n",
      "        jdf = self._jdf.sample(*args)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: sampleBy\n",
      "Method definition:     def sampleBy(\n",
      "        self, col: \"ColumnOrName\", fractions: Dict[Any, float], seed: Optional[int] = None\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Returns a stratified sample without replacement based on the\n",
      "        fraction given on each stratum.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`Column` or str\n",
      "            column that defines strata\n",
      "\n",
      "            .. versionchanged:: 3.0\n",
      "               Added sampling by a column of :class:`Column`\n",
      "        fractions : dict\n",
      "            sampling fraction for each stratum. If a stratum is not\n",
      "            specified, we treat its fraction as zero.\n",
      "        seed : int, optional\n",
      "            random seed\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        a new :class:`DataFrame` that represents the stratified sample\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import col\n",
      "        >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      "        >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      "        >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      "        +---+-----+\n",
      "        |key|count|\n",
      "        +---+-----+\n",
      "        |  0|    3|\n",
      "        |  1|    6|\n",
      "        +---+-----+\n",
      "        >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      "        33\n",
      "        \"\"\"\n",
      "        if isinstance(col, str):\n",
      "            col = Column(col)\n",
      "        elif not isinstance(col, Column):\n",
      "            raise TypeError(\"col must be a string or a column, but got %r\" % type(col))\n",
      "        if not isinstance(fractions, dict):\n",
      "            raise TypeError(\"fractions must be a dict but got %r\" % type(fractions))\n",
      "        for k, v in fractions.items():\n",
      "            if not isinstance(k, (float, int, str)):\n",
      "                raise TypeError(\"key must be float, int, or string, but got %r\" % type(k))\n",
      "            fractions[k] = float(v)\n",
      "        col = col._jc\n",
      "        seed = seed if seed is not None else random.randint(0, sys.maxsize)\n",
      "        return DataFrame(\n",
      "            self._jdf.stat().sampleBy(col, self._jmap(fractions), seed), self.sparkSession\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: select\n",
      "Method definition:     def select(self, *cols: \"ColumnOrName\") -> \"DataFrame\":  # type: ignore[misc]\n",
      "        \"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : str, :class:`Column`, or list\n",
      "            column names (string) or expressions (:class:`Column`).\n",
      "            If one of the column names is '*', that column is expanded to include all columns\n",
      "            in the current :class:`DataFrame`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select('*').collect()\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        >>> df.select('name', 'age').collect()\n",
      "        [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "        >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n",
      "        [Row(name='Alice', age=12), Row(name='Bob', age=15)]\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.select(self._jcols(*cols))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: selectExpr\n",
      "Method definition:     def selectExpr(self, *expr: Union[str, List[str]]) -> \"DataFrame\":\n",
      "        \"\"\"Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      "\n",
      "        This is a variant of :func:`select` that accepts SQL expressions.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      "        [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      "        \"\"\"\n",
      "        if len(expr) == 1 and isinstance(expr[0], list):\n",
      "            expr = expr[0]  # type: ignore[assignment]\n",
      "        jdf = self._jdf.selectExpr(self._jseq(expr))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: semanticHash\n",
      "Method definition:     def semanticHash(self) -> int:\n",
      "        \"\"\"\n",
      "        Returns a hash code of the logical query plan against this :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 3.1.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Unlike the standard hash code, the hash is calculated against the query plan\n",
      "        simplified by tolerating the cosmetic differences such as attribute names.\n",
      "\n",
      "        This API is a developer API.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  # doctest: +SKIP\n",
      "        1855039936\n",
      "        >>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  # doctest: +SKIP\n",
      "        1855039936\n",
      "        \"\"\"\n",
      "        return self._jdf.semanticHash()\n",
      "\n",
      "------------\n",
      "Method name: show\n",
      "Method definition:     def show(self, n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -> None:\n",
      "        \"\"\"Prints the first ``n`` rows to the console.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int, optional\n",
      "            Number of rows to show.\n",
      "        truncate : bool or int, optional\n",
      "            If set to ``True``, truncate strings longer than 20 chars by default.\n",
      "            If set to a number greater than one, truncates long strings to length ``truncate``\n",
      "            and align cells right.\n",
      "        vertical : bool, optional\n",
      "            If set to ``True``, print output rows vertically (one line\n",
      "            per column value).\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df\n",
      "        DataFrame[age: int, name: string]\n",
      "        >>> df.show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        +---+-----+\n",
      "        >>> df.show(truncate=3)\n",
      "        +---+----+\n",
      "        |age|name|\n",
      "        +---+----+\n",
      "        |  2| Ali|\n",
      "        |  5| Bob|\n",
      "        +---+----+\n",
      "        >>> df.show(vertical=True)\n",
      "        -RECORD 0-----\n",
      "         age  | 2\n",
      "         name | Alice\n",
      "        -RECORD 1-----\n",
      "         age  | 5\n",
      "         name | Bob\n",
      "        \"\"\"\n",
      "\n",
      "        if not isinstance(n, int) or isinstance(n, bool):\n",
      "            raise TypeError(\"Parameter 'n' (number of rows) must be an int\")\n",
      "\n",
      "        if not isinstance(vertical, bool):\n",
      "            raise TypeError(\"Parameter 'vertical' must be a bool\")\n",
      "\n",
      "        if isinstance(truncate, bool) and truncate:\n",
      "            print(self._jdf.showString(n, 20, vertical))\n",
      "        else:\n",
      "            try:\n",
      "                int_truncate = int(truncate)\n",
      "            except ValueError:\n",
      "                raise TypeError(\n",
      "                    \"Parameter 'truncate={}' should be either bool or int.\".format(truncate)\n",
      "                )\n",
      "\n",
      "            print(self._jdf.showString(n, int_truncate, vertical))\n",
      "\n",
      "------------\n",
      "Method name: sort\n",
      "Method definition:     def sort(\n",
      "        self, *cols: Union[str, Column, List[Union[str, Column]]], **kwargs: Any\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : str, list, or :class:`Column`, optional\n",
      "             list of :class:`Column` or column names to sort by.\n",
      "\n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        ascending : bool or list, optional\n",
      "            boolean or list of boolean (default ``True``).\n",
      "            Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "            If a list is specified, length of the list must equal length of the `cols`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.sort(df.age.desc()).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> df.sort(\"age\", ascending=False).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> df.orderBy(df.age.desc()).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> from pyspark.sql.functions import *\n",
      "        >>> df.sort(asc(\"age\")).collect()\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.sort(self._sort_cols(cols, kwargs))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: sortWithinPartitions\n",
      "Method definition:     def sortWithinPartitions(\n",
      "        self, *cols: Union[str, Column, List[Union[str, Column]]], **kwargs: Any\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : str, list or :class:`Column`, optional\n",
      "            list of :class:`Column` or column names to sort by.\n",
      "\n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        ascending : bool or list, optional\n",
      "            boolean or list of boolean (default ``True``).\n",
      "            Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "            If a list is specified, length of the list must equal length of the `cols`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        +---+-----+\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.sortWithinPartitions(self._sort_cols(cols, kwargs))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: subtract\n",
      "Method definition:     @since(1.3)\n",
      "    def subtract(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      "        but not in another :class:`DataFrame`.\n",
      "\n",
      "        This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      "\n",
      "        \"\"\"\n",
      "        return DataFrame(getattr(self._jdf, \"except\")(other._jdf), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: summary\n",
      "Method definition:     def summary(self, *statistics: str) -> \"DataFrame\":\n",
      "        \"\"\"Computes specified statistics for numeric and string columns. Available statistics are:\n",
      "        - count\n",
      "        - mean\n",
      "        - stddev\n",
      "        - min\n",
      "        - max\n",
      "        - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
      "\n",
      "        If no statistics are given, this function computes count, mean, stddev, min,\n",
      "        approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      "\n",
      "        .. versionadded:: 2.3.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This function is meant for exploratory data analysis, as we make no\n",
      "        guarantee about the backward compatibility of the schema of the resulting\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      "        ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      "        ... )\n",
      "        >>> df.select(\"age\", \"weight\", \"height\").summary().show()\n",
      "        +-------+----+------------------+-----------------+\n",
      "        |summary| age|            weight|           height|\n",
      "        +-------+----+------------------+-----------------+\n",
      "        |  count|   3|                 3|                3|\n",
      "        |   mean|12.0| 40.73333333333333|            145.0|\n",
      "        | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      "        |    min|  11|              37.8|            142.2|\n",
      "        |    25%|  11|              37.8|            142.2|\n",
      "        |    50%|  12|              40.3|            142.3|\n",
      "        |    75%|  13|              44.1|            150.5|\n",
      "        |    max|  13|              44.1|            150.5|\n",
      "        +-------+----+------------------+-----------------+\n",
      "\n",
      "        >>> df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      "        +-------+---+------+------+\n",
      "        |summary|age|weight|height|\n",
      "        +-------+---+------+------+\n",
      "        |  count|  3|     3|     3|\n",
      "        |    min| 11|  37.8| 142.2|\n",
      "        |    25%| 11|  37.8| 142.2|\n",
      "        |    75%| 13|  44.1| 150.5|\n",
      "        |    max| 13|  44.1| 150.5|\n",
      "        +-------+---+------+------+\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        DataFrame.display\n",
      "        \"\"\"\n",
      "        if len(statistics) == 1 and isinstance(statistics[0], list):\n",
      "            statistics = statistics[0]\n",
      "        jdf = self._jdf.summary(self._jseq(statistics))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: tail\n",
      "Method definition:     def tail(self, num: int) -> List[Row]:\n",
      "        \"\"\"\n",
      "        Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "\n",
      "        Running tail requires moving data into the application's driver process, and doing so with\n",
      "        a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      "\n",
      "        .. versionadded:: 3.0.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.tail(1)\n",
      "        [Row(age=5, name='Bob')]\n",
      "        \"\"\"\n",
      "        with SCCallSiteSync(self._sc):\n",
      "            sock_info = self._jdf.tailToPython(num)\n",
      "        return list(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\n",
      "------------\n",
      "Method name: take\n",
      "Method definition:     def take(self, num: int) -> List[Row]:\n",
      "        \"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.take(2)\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        \"\"\"\n",
      "        return self.limit(num).collect()\n",
      "\n",
      "------------\n",
      "Method name: toDF\n",
      "Method definition:     def toDF(self, *cols: \"ColumnOrName\") -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` that with new specified column names\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : str\n",
      "            new column names\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.toDF('f1', 'f2').collect()\n",
      "        [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.toDF(self._jseq(cols))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: toJSON\n",
      "Method definition:     def toJSON(self, use_unicode: bool = True) -> RDD[str]:\n",
      "        \"\"\"Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      "\n",
      "        Each row is turned into a JSON document as one element in the returned RDD.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.toJSON().first()\n",
      "        '{\"age\":2,\"name\":\"Alice\"}'\n",
      "        \"\"\"\n",
      "        rdd = self._jdf.toJSON()\n",
      "        return RDD(rdd.toJavaRDD(), self._sc, UTF8Deserializer(use_unicode))\n",
      "\n",
      "------------\n",
      "Method name: toLocalIterator\n",
      "Method definition:     def toLocalIterator(self, prefetchPartitions: bool = False) -> Iterator[Row]:\n",
      "        \"\"\"\n",
      "        Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      "        The iterator will consume as much memory as the largest partition in this\n",
      "        :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
      "        partitions.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        prefetchPartitions : bool, optional\n",
      "            If Spark should pre-fetch the next partition  before it is needed.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> list(df.toLocalIterator())\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        \"\"\"\n",
      "        with SCCallSiteSync(self._sc):\n",
      "            sock_info = self._jdf.toPythonIterator(prefetchPartitions)\n",
      "        return _local_iterator_from_socket(sock_info, BatchedSerializer(CPickleSerializer()))\n",
      "\n",
      "------------\n",
      "Method name: toPandas\n",
      "Method definition:     def toPandas(self) -> \"PandasDataFrameLike\":\n",
      "        \"\"\"\n",
      "        Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      "\n",
      "        This is only available if Pandas is installed and available.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This method should only be used if the resulting Pandas ``pandas.DataFrame`` is\n",
      "        expected to be small, as all the data is loaded into the driver's memory.\n",
      "\n",
      "        Usage with ``spark.sql.execution.arrow.pyspark.enabled=True`` is experimental.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.toPandas()  # doctest: +SKIP\n",
      "           age   name\n",
      "        0    2  Alice\n",
      "        1    5    Bob\n",
      "        \"\"\"\n",
      "        from pyspark.sql.dataframe import DataFrame\n",
      "\n",
      "        assert isinstance(self, DataFrame)\n",
      "\n",
      "        from pyspark.sql.pandas.utils import require_minimum_pandas_version\n",
      "\n",
      "        require_minimum_pandas_version()\n",
      "\n",
      "        import numpy as np\n",
      "        import pandas as pd\n",
      "        from pandas.core.dtypes.common import is_timedelta64_dtype\n",
      "\n",
      "        jconf = self.sparkSession._jconf\n",
      "        timezone = jconf.sessionLocalTimeZone()\n",
      "\n",
      "        if jconf.arrowPySparkEnabled():\n",
      "            use_arrow = True\n",
      "            try:\n",
      "                from pyspark.sql.pandas.types import to_arrow_schema\n",
      "                from pyspark.sql.pandas.utils import require_minimum_pyarrow_version\n",
      "\n",
      "                require_minimum_pyarrow_version()\n",
      "                to_arrow_schema(self.schema)\n",
      "            except Exception as e:\n",
      "\n",
      "                if jconf.arrowPySparkFallbackEnabled():\n",
      "                    msg = (\n",
      "                        \"toPandas attempted Arrow optimization because \"\n",
      "                        \"'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, \"\n",
      "                        \"failed by the reason below:\\n  %s\\n\"\n",
      "                        \"Attempting non-optimization as \"\n",
      "                        \"'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to \"\n",
      "                        \"true.\" % str(e)\n",
      "                    )\n",
      "                    warn(msg)\n",
      "                    use_arrow = False\n",
      "                else:\n",
      "                    msg = (\n",
      "                        \"toPandas attempted Arrow optimization because \"\n",
      "                        \"'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has \"\n",
      "                        \"reached the error below and will not continue because automatic fallback \"\n",
      "                        \"with 'spark.sql.execution.arrow.pyspark.fallback.enabled' has been set to \"\n",
      "                        \"false.\\n  %s\" % str(e)\n",
      "                    )\n",
      "                    warn(msg)\n",
      "                    raise\n",
      "\n",
      "            # Try to use Arrow optimization when the schema is supported and the required version\n",
      "            # of PyArrow is found, if 'spark.sql.execution.arrow.pyspark.enabled' is enabled.\n",
      "            if use_arrow:\n",
      "                try:\n",
      "                    from pyspark.sql.pandas.types import (\n",
      "                        _check_series_localize_timestamps,\n",
      "                        _convert_map_items_to_dict,\n",
      "                    )\n",
      "                    import pyarrow\n",
      "\n",
      "                    # Rename columns to avoid duplicated column names.\n",
      "                    tmp_column_names = [\"col_{}\".format(i) for i in range(len(self.columns))]\n",
      "                    self_destruct = jconf.arrowPySparkSelfDestructEnabled()\n",
      "                    batches = self.toDF(*tmp_column_names)._collect_as_arrow(\n",
      "                        split_batches=self_destruct\n",
      "                    )\n",
      "                    if len(batches) > 0:\n",
      "                        table = pyarrow.Table.from_batches(batches)\n",
      "                        # Ensure only the table has a reference to the batches, so that\n",
      "                        # self_destruct (if enabled) is effective\n",
      "                        del batches\n",
      "                        # Pandas DataFrame created from PyArrow uses datetime64[ns] for date type\n",
      "                        # values, but we should use datetime.date to match the behavior with when\n",
      "                        # Arrow optimization is disabled.\n",
      "                        pandas_options = {\"date_as_object\": True}\n",
      "                        if self_destruct:\n",
      "                            # Configure PyArrow to use as little memory as possible:\n",
      "                            # self_destruct - free columns as they are converted\n",
      "                            # split_blocks - create a separate Pandas block for each column\n",
      "                            # use_threads - convert one column at a time\n",
      "                            pandas_options.update(\n",
      "                                {\n",
      "                                    \"self_destruct\": True,\n",
      "                                    \"split_blocks\": True,\n",
      "                                    \"use_threads\": False,\n",
      "                                }\n",
      "                            )\n",
      "                        pdf = table.to_pandas(**pandas_options)\n",
      "                        # Rename back to the original column names.\n",
      "                        pdf.columns = self.columns\n",
      "                        for field in self.schema:\n",
      "                            if isinstance(field.dataType, TimestampType):\n",
      "                                pdf[field.name] = _check_series_localize_timestamps(\n",
      "                                    pdf[field.name], timezone\n",
      "                                )\n",
      "                            elif isinstance(field.dataType, MapType):\n",
      "                                pdf[field.name] = _convert_map_items_to_dict(pdf[field.name])\n",
      "                        return pdf\n",
      "                    else:\n",
      "                        corrected_panda_types = {}\n",
      "                        for index, field in enumerate(self.schema):\n",
      "                            pandas_type = PandasConversionMixin._to_corrected_pandas_type(\n",
      "                                field.dataType\n",
      "                            )\n",
      "                            corrected_panda_types[tmp_column_names[index]] = (\n",
      "                                np.object0 if pandas_type is None else pandas_type\n",
      "                            )\n",
      "\n",
      "                        pdf = pd.DataFrame(columns=tmp_column_names).astype(\n",
      "                            dtype=corrected_panda_types\n",
      "                        )\n",
      "                        pdf.columns = self.columns\n",
      "                        return pdf\n",
      "                except Exception as e:\n",
      "                    # We might have to allow fallback here as well but multiple Spark jobs can\n",
      "                    # be executed. So, simply fail in this case for now.\n",
      "                    msg = (\n",
      "                        \"toPandas attempted Arrow optimization because \"\n",
      "                        \"'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has \"\n",
      "                        \"reached the error below and can not continue. Note that \"\n",
      "                        \"'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an \"\n",
      "                        \"effect on failures in the middle of \"\n",
      "                        \"computation.\\n  %s\" % str(e)\n",
      "                    )\n",
      "                    warn(msg)\n",
      "                    raise\n",
      "\n",
      "        # Below is toPandas without Arrow optimization.\n",
      "        pdf = pd.DataFrame.from_records(self.collect(), columns=self.columns)\n",
      "        column_counter = Counter(self.columns)\n",
      "\n",
      "        corrected_dtypes: List[Optional[Type]] = [None] * len(self.schema)\n",
      "        for index, field in enumerate(self.schema):\n",
      "            # We use `iloc` to access columns with duplicate column names.\n",
      "            if column_counter[field.name] > 1:\n",
      "                pandas_col = pdf.iloc[:, index]\n",
      "            else:\n",
      "                pandas_col = pdf[field.name]\n",
      "\n",
      "            pandas_type = PandasConversionMixin._to_corrected_pandas_type(field.dataType)\n",
      "            # SPARK-21766: if an integer field is nullable and has null values, it can be\n",
      "            # inferred by pandas as a float column. If we convert the column with NaN back\n",
      "            # to integer type e.g., np.int16, we will hit an exception. So we use the\n",
      "            # pandas-inferred float type, rather than the corrected type from the schema\n",
      "            # in this case.\n",
      "            if pandas_type is not None and not (\n",
      "                isinstance(field.dataType, IntegralType)\n",
      "                and field.nullable\n",
      "                and pandas_col.isnull().any()\n",
      "            ):\n",
      "                corrected_dtypes[index] = pandas_type\n",
      "            # Ensure we fall back to nullable numpy types.\n",
      "            if isinstance(field.dataType, IntegralType) and pandas_col.isnull().any():\n",
      "                corrected_dtypes[index] = np.float64\n",
      "            if isinstance(field.dataType, BooleanType) and pandas_col.isnull().any():\n",
      "                corrected_dtypes[index] = np.object  # type: ignore[attr-defined]\n",
      "\n",
      "        df = pd.DataFrame()\n",
      "        for index, t in enumerate(corrected_dtypes):\n",
      "            column_name = self.schema[index].name\n",
      "\n",
      "            # We use `iloc` to access columns with duplicate column names.\n",
      "            if column_counter[column_name] > 1:\n",
      "                series = pdf.iloc[:, index]\n",
      "            else:\n",
      "                series = pdf[column_name]\n",
      "\n",
      "            # No need to cast for non-empty series for timedelta. The type is already correct.\n",
      "            should_check_timedelta = is_timedelta64_dtype(t) and len(pdf) == 0\n",
      "\n",
      "            if (t is not None and not is_timedelta64_dtype(t)) or should_check_timedelta:\n",
      "                series = series.astype(t, copy=False)\n",
      "\n",
      "            with catch_warnings():\n",
      "                from pandas.errors import PerformanceWarning\n",
      "\n",
      "                simplefilter(action=\"ignore\", category=PerformanceWarning)\n",
      "                # `insert` API makes copy of data,\n",
      "                # we only do it for Series of duplicate column names.\n",
      "                # `pdf.iloc[:, index] = pdf.iloc[:, index]...` doesn't always work\n",
      "                # because `iloc` could return a view or a copy depending by context.\n",
      "                if column_counter[column_name] > 1:\n",
      "                    df.insert(index, column_name, series, allow_duplicates=True)\n",
      "                else:\n",
      "                    df[column_name] = series\n",
      "\n",
      "        if timezone is None:\n",
      "            return df\n",
      "        else:\n",
      "            from pyspark.sql.pandas.types import _check_series_convert_timestamps_local_tz\n",
      "\n",
      "            for field in self.schema:\n",
      "                # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n",
      "                if isinstance(field.dataType, TimestampType):\n",
      "                    df[field.name] = _check_series_convert_timestamps_local_tz(\n",
      "                        df[field.name], timezone\n",
      "                    )\n",
      "            return df\n",
      "\n",
      "------------\n",
      "Method name: to_koalas\n",
      "Method definition:     def to_koalas(\n",
      "        self, index_col: Optional[Union[str, List[str]]] = None\n",
      "    ) -> \"PandasOnSparkDataFrame\":\n",
      "        return self.pandas_api(index_col)\n",
      "\n",
      "------------\n",
      "Method name: to_pandas_on_spark\n",
      "Method definition:     def to_pandas_on_spark(\n",
      "        self, index_col: Optional[Union[str, List[str]]] = None\n",
      "    ) -> \"PandasOnSparkDataFrame\":\n",
      "        warnings.warn(\n",
      "            \"DataFrame.to_pandas_on_spark is deprecated. Use DataFrame.pandas_api instead.\",\n",
      "            FutureWarning,\n",
      "        )\n",
      "        return self.pandas_api(index_col)\n",
      "\n",
      "------------\n",
      "Method name: transform\n",
      "Method definition:     def transform(self, func: Callable[..., \"DataFrame\"], *args: Any, **kwargs: Any) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
      "\n",
      "        .. versionadded:: 3.0.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            a function that takes and returns a :class:`DataFrame`.\n",
      "        *args\n",
      "            Positional arguments to pass to func.\n",
      "\n",
      "            .. versionadded:: 3.3.0\n",
      "        **kwargs\n",
      "            Keyword arguments to pass to func.\n",
      "\n",
      "            .. versionadded:: 3.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import col\n",
      "        >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
      "        >>> def cast_all_to_int(input_df):\n",
      "        ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
      "        >>> def sort_columns_asc(input_df):\n",
      "        ...     return input_df.select(*sorted(input_df.columns))\n",
      "        >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
      "        +-----+---+\n",
      "        |float|int|\n",
      "        +-----+---+\n",
      "        |    1|  1|\n",
      "        |    2|  2|\n",
      "        +-----+---+\n",
      "        >>> def add_n(input_df, n):\n",
      "        ...     return input_df.select([(col(col_name) + n).alias(col_name)\n",
      "        ...                             for col_name in input_df.columns])\n",
      "        >>> df.transform(add_n, 1).transform(add_n, n=10).show()\n",
      "        +---+-----+\n",
      "        |int|float|\n",
      "        +---+-----+\n",
      "        | 12| 12.0|\n",
      "        | 13| 13.0|\n",
      "        +---+-----+\n",
      "        \"\"\"\n",
      "        result = func(self, *args, **kwargs)\n",
      "        assert isinstance(\n",
      "            result, DataFrame\n",
      "        ), \"Func returned an instance of type [%s], \" \"should have been DataFrame.\" % type(result)\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: union\n",
      "Method definition:     @since(2.0)\n",
      "    def union(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      "        (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      "\n",
      "        Also as standard in SQL, this function resolves columns by position (not by name).\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.union(other._jdf), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: unionAll\n",
      "Method definition:     @since(1.3)\n",
      "    def unionAll(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      "        (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      "\n",
      "        Also as standard in SQL, this function resolves columns by position (not by name).\n",
      "        \"\"\"\n",
      "        return self.union(other)\n",
      "\n",
      "------------\n",
      "Method name: unionByName\n",
      "Method definition:     def unionByName(self, other: \"DataFrame\", allowMissingColumns: bool = False) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n",
      "        union (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      "\n",
      "        .. versionadded:: 2.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The difference between this function and :func:`union` is that this function\n",
      "        resolves columns by name (not by position):\n",
      "\n",
      "        >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      "        >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      "        >>> df1.unionByName(df2).show()\n",
      "        +----+----+----+\n",
      "        |col0|col1|col2|\n",
      "        +----+----+----+\n",
      "        |   1|   2|   3|\n",
      "        |   6|   4|   5|\n",
      "        +----+----+----+\n",
      "\n",
      "        When the parameter `allowMissingColumns` is ``True``, the set of column names\n",
      "        in this and other :class:`DataFrame` can differ; missing columns will be filled with null.\n",
      "        Further, the missing columns of this :class:`DataFrame` will be added at the end\n",
      "        in the schema of the union result:\n",
      "\n",
      "        >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      "        >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
      "        >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      "        +----+----+----+----+\n",
      "        |col0|col1|col2|col3|\n",
      "        +----+----+----+----+\n",
      "        |   1|   2|   3|null|\n",
      "        |null|   4|   5|   6|\n",
      "        +----+----+----+----+\n",
      "\n",
      "        .. versionchanged:: 3.1.0\n",
      "           Added optional argument `allowMissingColumns` to specify whether to allow\n",
      "           missing columns.\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.unionByName(other._jdf, allowMissingColumns), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: unpersist\n",
      "Method definition:     def unpersist(self, blocking: bool = False) -> \"DataFrame\":\n",
      "        \"\"\"Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      "        memory and disk.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      "        \"\"\"\n",
      "        self.is_cached = False\n",
      "        self._jdf.unpersist(blocking)\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: where\n",
      "Method definition:     def filter(self, condition: \"ColumnOrName\") -> \"DataFrame\":\n",
      "        \"\"\"Filters rows using the given condition.\n",
      "\n",
      "        :func:`where` is an alias for :func:`filter`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        condition : :class:`Column` or str\n",
      "            a :class:`Column` of :class:`types.BooleanType`\n",
      "            or a string of SQL expression.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.filter(df.age > 3).collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        >>> df.where(df.age == 2).collect()\n",
      "        [Row(age=2, name='Alice')]\n",
      "\n",
      "        >>> df.filter(\"age > 3\").collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        >>> df.where(\"age = 2\").collect()\n",
      "        [Row(age=2, name='Alice')]\n",
      "        \"\"\"\n",
      "        if isinstance(condition, str):\n",
      "            jdf = self._jdf.filter(condition)\n",
      "        elif isinstance(condition, Column):\n",
      "            jdf = self._jdf.filter(condition._jc)\n",
      "        else:\n",
      "            raise TypeError(\"condition should be string or Column\")\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: withColumn\n",
      "Method definition:     def withColumn(self, colName: str, col: Column) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      "        existing column that has the same name.\n",
      "\n",
      "        The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      "        a column from some other :class:`DataFrame` will raise an error.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        colName : str\n",
      "            string, name of the new column.\n",
      "        col : :class:`Column`\n",
      "            a :class:`Column` expression for the new column.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This method introduces a projection internally. Therefore, calling it multiple\n",
      "        times, for instance, via loops in order to add multiple columns can generate big\n",
      "        plans which can cause performance issues and even `StackOverflowException`.\n",
      "        To avoid this, use :func:`select` with the multiple columns at once.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.withColumn('age2', df.age + 2).collect()\n",
      "        [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      "\n",
      "        \"\"\"\n",
      "        if not isinstance(col, Column):\n",
      "            raise TypeError(\"col should be Column\")\n",
      "        return DataFrame(self._jdf.withColumn(colName, col._jc), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: withColumnRenamed\n",
      "Method definition:     def withColumnRenamed(self, existing: str, new: str) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` by renaming an existing column.\n",
      "        This is a no-op if schema doesn't contain the given column name.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        existing : str\n",
      "            string, name of the existing column to rename.\n",
      "        new : str\n",
      "            string, new name of the column.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.withColumnRenamed('age', 'age2').collect()\n",
      "        [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.withColumnRenamed(existing, new), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: withColumns\n",
      "Method definition:     def withColumns(self, *colsMap: Dict[str, Column]) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n",
      "        existing columns that has the same names.\n",
      "\n",
      "        The colsMap is a map of column name and column, the column must only refer to attributes\n",
      "        supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n",
      "\n",
      "        .. versionadded:: 3.3.0\n",
      "           Added support for multiple columns adding\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        colsMap : dict\n",
      "            a dict of column name and :class:`Column`. Currently, only single map is supported.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).collect()\n",
      "        [Row(age=2, name='Alice', age2=4, age3=5), Row(age=5, name='Bob', age2=7, age3=8)]\n",
      "        \"\"\"\n",
      "        # Below code is to help enable kwargs in future.\n",
      "        assert len(colsMap) == 1\n",
      "        colsMap = colsMap[0]  # type: ignore[assignment]\n",
      "\n",
      "        if not isinstance(colsMap, dict):\n",
      "            raise TypeError(\"colsMap must be dict of column name and column.\")\n",
      "\n",
      "        col_names = list(colsMap.keys())\n",
      "        cols = list(colsMap.values())\n",
      "\n",
      "        return DataFrame(\n",
      "            self._jdf.withColumns(_to_seq(self._sc, col_names), self._jcols(*cols)),\n",
      "            self.sparkSession,\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: withMetadata\n",
      "Method definition:     def withMetadata(self, columnName: str, metadata: Dict[str, Any]) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` by updating an existing column with metadata.\n",
      "\n",
      "        .. versionadded:: 3.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        columnName : str\n",
      "            string, name of the existing column to update the metadata.\n",
      "        metadata : dict\n",
      "            dict, new metadata to be assigned to df.schema[columnName].metadata\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n",
      "        >>> df_meta.schema['age'].metadata\n",
      "        {'foo': 'bar'}\n",
      "        \"\"\"\n",
      "        if not isinstance(metadata, dict):\n",
      "            raise TypeError(\"metadata should be a dict\")\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None and sc._jvm is not None\n",
      "        jmeta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(json.dumps(metadata))\n",
      "        return DataFrame(self._jdf.withMetadata(columnName, jmeta), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: withWatermark\n",
      "Method definition:     def withWatermark(self, eventTime: str, delayThreshold: str) -> \"DataFrame\":\n",
      "        \"\"\"Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      "        in time before which we assume no more late data is going to arrive.\n",
      "\n",
      "        Spark will use this watermark for several purposes:\n",
      "          - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      "            when using output modes that do not allow updates.\n",
      "\n",
      "          - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      "\n",
      "        The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      "        all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      "        of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      "        to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      "        process records that arrive more than `delayThreshold` late.\n",
      "\n",
      "        .. versionadded:: 2.1.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        eventTime : str\n",
      "            the name of the column that contains the event time of the row.\n",
      "        delayThreshold : str\n",
      "            the minimum delay to wait to data to arrive late, relative to the\n",
      "            latest record that has been processed in the form of an interval\n",
      "            (e.g. \"1 minute\" or \"5 hours\").\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This API is evolving.\n",
      "\n",
      "        >>> from pyspark.sql.functions import timestamp_seconds\n",
      "        >>> sdf.select(\n",
      "        ...    'name',\n",
      "        ...    timestamp_seconds(sdf.time).alias('time')).withWatermark('time', '10 minutes')\n",
      "        DataFrame[name: string, time: timestamp]\n",
      "        \"\"\"\n",
      "        if not eventTime or type(eventTime) is not str:\n",
      "            raise TypeError(\"eventTime should be provided as a string\")\n",
      "        if not delayThreshold or type(delayThreshold) is not str:\n",
      "            raise TypeError(\"delayThreshold should be provided as a string interval\")\n",
      "        jdf = self._jdf.withWatermark(eventTime, delayThreshold)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: writeTo\n",
      "Method definition:     def writeTo(self, table: str) -> DataFrameWriterV2:\n",
      "        \"\"\"\n",
      "        Create a write configuration builder for v2 sources.\n",
      "\n",
      "        This builder is used to configure and execute write operations.\n",
      "\n",
      "        For example, to append or create or replace existing tables.\n",
      "\n",
      "        .. versionadded:: 3.1.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n",
      "        >>> df.writeTo(                              # doctest: +SKIP\n",
      "        ...     \"catalog.db.table\"\n",
      "        ... ).partitionedBy(\"col\").createOrReplace()\n",
      "        \"\"\"\n",
      "        return DataFrameWriterV2(self, table)\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     def __eq__(self, other: Any) -> bool:\n",
      "        return isinstance(other, self.__class__) and self.__dict__ == other.__dict__\n",
      "\n",
      "------------\n",
      "Method name: __hash__\n",
      "Method definition:     def __hash__(self) -> int:\n",
      "        return hash(str(self))\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     def __ne__(self, other: Any) -> bool:\n",
      "        return not self.__eq__(other)\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        return self.__class__.__name__ + \"()\"\n",
      "\n",
      "------------\n",
      "Method name: fromInternal\n",
      "Method definition:     def fromInternal(self, v: int) -> datetime.date:\n",
      "        if v is not None:\n",
      "            return datetime.date.fromordinal(v + self.EPOCH_ORDINAL)\n",
      "\n",
      "------------\n",
      "Method name: json\n",
      "Method definition:     def json(self) -> str:\n",
      "        return json.dumps(self.jsonValue(), separators=(\",\", \":\"), sort_keys=True)\n",
      "\n",
      "------------\n",
      "Method name: jsonValue\n",
      "Method definition:     def jsonValue(self) -> Union[str, Dict[str, Any]]:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: needConversion\n",
      "Method definition:     def needConversion(self) -> bool:\n",
      "        return True\n",
      "\n",
      "------------\n",
      "Method name: simpleString\n",
      "Method definition:     def simpleString(self) -> str:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: toInternal\n",
      "Method definition:     def toInternal(self, d: datetime.date) -> int:\n",
      "        if d is not None:\n",
      "            return d.toordinal() - self.EPOCH_ORDINAL\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     def __eq__(self, other: Any) -> bool:\n",
      "        return isinstance(other, self.__class__) and self.__dict__ == other.__dict__\n",
      "\n",
      "------------\n",
      "Method name: __hash__\n",
      "Method definition:     def __hash__(self) -> int:\n",
      "        return hash(str(self))\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     def __ne__(self, other: Any) -> bool:\n",
      "        return not self.__eq__(other)\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        return self.__class__.__name__ + \"()\"\n",
      "\n",
      "------------\n",
      "Method name: fromInternal\n",
      "Method definition:     def fromInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts an internal SQL object into a native Python object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: json\n",
      "Method definition:     def json(self) -> str:\n",
      "        return json.dumps(self.jsonValue(), separators=(\",\", \":\"), sort_keys=True)\n",
      "\n",
      "------------\n",
      "Method name: jsonValue\n",
      "Method definition:     def jsonValue(self) -> Union[str, Dict[str, Any]]:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: needConversion\n",
      "Method definition:     def needConversion(self) -> bool:\n",
      "        \"\"\"\n",
      "        Does this type needs conversion between Python object and internal SQL object.\n",
      "\n",
      "        This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "        \"\"\"\n",
      "        return False\n",
      "\n",
      "------------\n",
      "Method name: simpleString\n",
      "Method definition:     def simpleString(self) -> str:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: toInternal\n",
      "Method definition:     def toInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts a Python object into an internal SQL object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     def __eq__(self, other: Any) -> bool:\n",
      "        return isinstance(other, self.__class__) and self.__dict__ == other.__dict__\n",
      "\n",
      "------------\n",
      "Method name: __hash__\n",
      "Method definition:     def __hash__(self) -> int:\n",
      "        return hash(str(self))\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     def __ne__(self, other: Any) -> bool:\n",
      "        return not self.__eq__(other)\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        return self.__class__.__name__ + \"()\"\n",
      "\n",
      "------------\n",
      "Method name: fromInternal\n",
      "Method definition:     def fromInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts an internal SQL object into a native Python object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: json\n",
      "Method definition:     def json(self) -> str:\n",
      "        return json.dumps(self.jsonValue(), separators=(\",\", \":\"), sort_keys=True)\n",
      "\n",
      "------------\n",
      "Method name: jsonValue\n",
      "Method definition:     def jsonValue(self) -> Union[str, Dict[str, Any]]:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: needConversion\n",
      "Method definition:     def needConversion(self) -> bool:\n",
      "        \"\"\"\n",
      "        Does this type needs conversion between Python object and internal SQL object.\n",
      "\n",
      "        This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "        \"\"\"\n",
      "        return False\n",
      "\n",
      "------------\n",
      "Method name: simpleString\n",
      "Method definition:     def simpleString(self) -> str:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: toInternal\n",
      "Method definition:     def toInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts a Python object into an internal SQL object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: __abs__\n",
      "Method definition:     def __abs__(self) -> Index:\n",
      "        return self._unary_method(operator.abs)\n",
      "\n",
      "------------\n",
      "Method name: __add__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__add__\")\n",
      "    def __add__(self, other):\n",
      "        return self._arith_method(other, operator.add)\n",
      "\n",
      "------------\n",
      "Method name: __and__\n",
      "Method definition:     @final\n",
      "    def __and__(self, other):\n",
      "        warnings.warn(\n",
      "            \"Index.__and__ operating as a set operation is deprecated, \"\n",
      "            \"in the future this will be a logical operation matching \"\n",
      "            \"Series.__and__.  Use index.intersection(other) instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        return self.intersection(other)\n",
      "\n",
      "------------\n",
      "Method name: __array__\n",
      "Method definition:     def __array__(self, dtype=None) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        The array interface, return my values.\n",
      "        \"\"\"\n",
      "        return np.asarray(self._data, dtype=dtype)\n",
      "\n",
      "------------\n",
      "Method name: __array_ufunc__\n",
      "Method definition:     def __array_ufunc__(self, ufunc: np.ufunc, method: str_t, *inputs, **kwargs):\n",
      "        if any(isinstance(other, (ABCSeries, ABCDataFrame)) for other in inputs):\n",
      "            return NotImplemented\n",
      "\n",
      "        # TODO(2.0) the 'and', 'or' and 'xor' dunder methods are currently set\n",
      "        # operations and not logical operations, so don't dispatch\n",
      "        # This is deprecated, so this full 'if' clause can be removed once\n",
      "        # deprecation is enforced in 2.0\n",
      "        if not (\n",
      "            method == \"__call__\"\n",
      "            and ufunc in (np.bitwise_and, np.bitwise_or, np.bitwise_xor)\n",
      "        ):\n",
      "            result = arraylike.maybe_dispatch_ufunc_to_dunder_op(\n",
      "                self, ufunc, method, *inputs, **kwargs\n",
      "            )\n",
      "            if result is not NotImplemented:\n",
      "                return result\n",
      "\n",
      "        if \"out\" in kwargs:\n",
      "            # e.g. test_dti_isub_tdi\n",
      "            return arraylike.dispatch_ufunc_with_out(\n",
      "                self, ufunc, method, *inputs, **kwargs\n",
      "            )\n",
      "\n",
      "        if method == \"reduce\":\n",
      "            result = arraylike.dispatch_reduction_ufunc(\n",
      "                self, ufunc, method, *inputs, **kwargs\n",
      "            )\n",
      "            if result is not NotImplemented:\n",
      "                return result\n",
      "\n",
      "        new_inputs = [x if x is not self else x._values for x in inputs]\n",
      "        result = getattr(ufunc, method)(*new_inputs, **kwargs)\n",
      "        if ufunc.nout == 2:\n",
      "            # i.e. np.divmod, np.modf, np.frexp\n",
      "            return tuple(self.__array_wrap__(x) for x in result)\n",
      "\n",
      "        return self.__array_wrap__(result)\n",
      "\n",
      "------------\n",
      "Method name: __array_wrap__\n",
      "Method definition:     def __array_wrap__(self, result, context=None):\n",
      "        \"\"\"\n",
      "        Gets called after a ufunc and other functions e.g. np.split.\n",
      "        \"\"\"\n",
      "        result = lib.item_from_zerodim(result)\n",
      "        if is_bool_dtype(result) or lib.is_scalar(result) or np.ndim(result) > 1:\n",
      "            return result\n",
      "\n",
      "        return Index(result, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: __bool__\n",
      "Method definition:     @final\n",
      "    def __nonzero__(self) -> NoReturn:\n",
      "        raise ValueError(\n",
      "            f\"The truth value of a {type(self).__name__} is ambiguous. \"\n",
      "            \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: __contains__\n",
      "Method definition:     def __contains__(self, key: Any) -> bool:\n",
      "        \"\"\"\n",
      "        Return a boolean indicating whether the provided key is in the index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        key : label\n",
      "            The key to check if it is present in the index.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether the key search is in the index.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If the key is not hashable.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.isin : Returns an ndarray of boolean dtype indicating whether the\n",
      "            list-like key is in the index.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx\n",
      "        Int64Index([1, 2, 3, 4], dtype='int64')\n",
      "\n",
      "        >>> 2 in idx\n",
      "        True\n",
      "        >>> 6 in idx\n",
      "        False\n",
      "        \"\"\"\n",
      "        hash(key)\n",
      "        try:\n",
      "            return key in self._engine\n",
      "        except (OverflowError, TypeError, ValueError):\n",
      "            return False\n",
      "\n",
      "------------\n",
      "Method name: __copy__\n",
      "Method definition:     @final\n",
      "    def __copy__(self: _IndexT, **kwargs) -> _IndexT:\n",
      "        return self.copy(**kwargs)\n",
      "\n",
      "------------\n",
      "Method name: __deepcopy__\n",
      "Method definition:     @final\n",
      "    def __deepcopy__(self: _IndexT, memo=None) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Parameters\n",
      "        ----------\n",
      "        memo, default None\n",
      "            Standard signature. Unused\n",
      "        \"\"\"\n",
      "        return self.copy(deep=True)\n",
      "\n",
      "------------\n",
      "Method name: __dir__\n",
      "Method definition:     def __dir__(self) -> list[str]:\n",
      "        \"\"\"\n",
      "        Provide method name lookup and completion.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Only provide 'public' methods.\n",
      "        \"\"\"\n",
      "        rv = set(super().__dir__())\n",
      "        rv = (rv - self._dir_deletions()) | self._dir_additions()\n",
      "        return sorted(rv)\n",
      "\n",
      "------------\n",
      "Method name: __divmod__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__divmod__\")\n",
      "    def __divmod__(self, other):\n",
      "        return self._arith_method(other, divmod)\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__eq__\")\n",
      "    def __eq__(self, other):\n",
      "        return self._cmp_method(other, operator.eq)\n",
      "\n",
      "------------\n",
      "Method name: __floordiv__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__floordiv__\")\n",
      "    def __floordiv__(self, other):\n",
      "        return self._arith_method(other, operator.floordiv)\n",
      "\n",
      "------------\n",
      "Method name: __ge__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__ge__\")\n",
      "    def __ge__(self, other):\n",
      "        return self._cmp_method(other, operator.ge)\n",
      "\n",
      "------------\n",
      "Method name: __getitem__\n",
      "Method definition:     def __getitem__(self, key):\n",
      "        \"\"\"\n",
      "        Override numpy.ndarray's __getitem__ method to work as desired.\n",
      "\n",
      "        This function adds lists and Series as valid boolean indexers\n",
      "        (ndarrays only supports ndarray with dtype=bool).\n",
      "\n",
      "        If resulting ndim != 1, plain ndarray is returned instead of\n",
      "        corresponding `Index` subclass.\n",
      "\n",
      "        \"\"\"\n",
      "        getitem = self._data.__getitem__\n",
      "\n",
      "        if is_integer(key) or is_float(key):\n",
      "            # GH#44051 exclude bool, which would return a 2d ndarray\n",
      "            key = com.cast_scalar_indexer(key, warn_float=True)\n",
      "            return getitem(key)\n",
      "\n",
      "        if isinstance(key, slice):\n",
      "            # This case is separated from the conditional above to avoid\n",
      "            # pessimization com.is_bool_indexer and ndim checks.\n",
      "            result = getitem(key)\n",
      "            # Going through simple_new for performance.\n",
      "            return type(self)._simple_new(result, name=self._name)\n",
      "\n",
      "        if com.is_bool_indexer(key):\n",
      "            # if we have list[bools, length=1e5] then doing this check+convert\n",
      "            #  takes 166 µs + 2.1 ms and cuts the ndarray.__getitem__\n",
      "            #  time below from 3.8 ms to 496 µs\n",
      "            # if we already have ndarray[bool], the overhead is 1.4 µs or .25%\n",
      "            if is_extension_array_dtype(getattr(key, \"dtype\", None)):\n",
      "                key = key.to_numpy(dtype=bool, na_value=False)\n",
      "            else:\n",
      "                key = np.asarray(key, dtype=bool)\n",
      "\n",
      "        result = getitem(key)\n",
      "        # Because we ruled out integer above, we always get an arraylike here\n",
      "        if result.ndim > 1:\n",
      "            deprecate_ndim_indexing(result)\n",
      "            if hasattr(result, \"_ndarray\"):\n",
      "                # i.e. NDArrayBackedExtensionArray\n",
      "                # Unpack to ndarray for MPL compat\n",
      "                # error: Item \"ndarray[Any, Any]\" of\n",
      "                # \"Union[ExtensionArray, ndarray[Any, Any]]\"\n",
      "                # has no attribute \"_ndarray\"\n",
      "                return result._ndarray  # type: ignore[union-attr]\n",
      "            return result\n",
      "\n",
      "        # NB: Using _constructor._simple_new would break if MultiIndex\n",
      "        #  didn't override __getitem__\n",
      "        return self._constructor._simple_new(result, name=self._name)\n",
      "\n",
      "------------\n",
      "Method name: __gt__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__gt__\")\n",
      "    def __gt__(self, other):\n",
      "        return self._cmp_method(other, operator.gt)\n",
      "\n",
      "------------\n",
      "Method name: __iadd__\n",
      "Method definition:     def __iadd__(self, other):\n",
      "        # alias for __add__\n",
      "        return self + other\n",
      "\n",
      "------------\n",
      "Method name: __invert__\n",
      "Method definition:     def __invert__(self) -> Index:\n",
      "        # GH#8875\n",
      "        return self._unary_method(operator.inv)\n",
      "\n",
      "------------\n",
      "Method name: __iter__\n",
      "Method definition:     def __iter__(self):\n",
      "        \"\"\"\n",
      "        Return an iterator of the values.\n",
      "\n",
      "        These are each a scalar type, which is a Python scalar\n",
      "        (for str, int, float) or a pandas scalar\n",
      "        (for Timestamp/Timedelta/Interval/Period)\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        iterator\n",
      "        \"\"\"\n",
      "        # We are explicitly making element iterators.\n",
      "        if not isinstance(self._values, np.ndarray):\n",
      "            # Check type instead of dtype to catch DTA/TDA\n",
      "            return iter(self._values)\n",
      "        else:\n",
      "            return map(self._values.item, range(self._values.size))\n",
      "\n",
      "------------\n",
      "Method name: __le__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__le__\")\n",
      "    def __le__(self, other):\n",
      "        return self._cmp_method(other, operator.le)\n",
      "\n",
      "------------\n",
      "Method name: __len__\n",
      "Method definition:     def __len__(self) -> int:\n",
      "        \"\"\"\n",
      "        Return the length of the Index.\n",
      "        \"\"\"\n",
      "        return len(self._data)\n",
      "\n",
      "------------\n",
      "Method name: __lt__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__lt__\")\n",
      "    def __lt__(self, other):\n",
      "        return self._cmp_method(other, operator.lt)\n",
      "\n",
      "------------\n",
      "Method name: __mod__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__mod__\")\n",
      "    def __mod__(self, other):\n",
      "        return self._arith_method(other, operator.mod)\n",
      "\n",
      "------------\n",
      "Method name: __mul__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__mul__\")\n",
      "    def __mul__(self, other):\n",
      "        return self._arith_method(other, operator.mul)\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__ne__\")\n",
      "    def __ne__(self, other):\n",
      "        return self._cmp_method(other, operator.ne)\n",
      "\n",
      "------------\n",
      "Method name: __neg__\n",
      "Method definition:     def __neg__(self) -> Index:\n",
      "        return self._unary_method(operator.neg)\n",
      "\n",
      "------------\n",
      "Method name: __new__\n",
      "Method definition:     def __new__(\n",
      "        cls, data=None, dtype=None, copy=False, name=None, tupleize_cols=True, **kwargs\n",
      "    ) -> Index:\n",
      "\n",
      "        if kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing keywords other than 'data', 'dtype', 'copy', 'name', \"\n",
      "                \"'tupleize_cols' is deprecated and will raise TypeError in a \"\n",
      "                \"future version.  Use the specific Index subclass directly instead.\",\n",
      "                FutureWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "\n",
      "        from pandas.core.arrays import PandasArray\n",
      "        from pandas.core.indexes.range import RangeIndex\n",
      "\n",
      "        name = maybe_extract_name(name, data, cls)\n",
      "\n",
      "        if dtype is not None:\n",
      "            dtype = pandas_dtype(dtype)\n",
      "        if \"tz\" in kwargs:\n",
      "            tz = kwargs.pop(\"tz\")\n",
      "            validate_tz_from_dtype(dtype, tz)\n",
      "            dtype = tz_to_dtype(tz)\n",
      "\n",
      "        if type(data) is PandasArray:\n",
      "            # ensure users don't accidentally put a PandasArray in an index,\n",
      "            #  but don't unpack StringArray\n",
      "            data = data.to_numpy()\n",
      "        if isinstance(dtype, PandasDtype):\n",
      "            dtype = dtype.numpy_dtype\n",
      "\n",
      "        data_dtype = getattr(data, \"dtype\", None)\n",
      "\n",
      "        # range\n",
      "        if isinstance(data, (range, RangeIndex)):\n",
      "            result = RangeIndex(start=data, copy=copy, name=name)\n",
      "            if dtype is not None:\n",
      "                return result.astype(dtype, copy=False)\n",
      "            return result\n",
      "\n",
      "        elif is_ea_or_datetimelike_dtype(dtype):\n",
      "            # non-EA dtype indexes have special casting logic, so we punt here\n",
      "            klass = cls._dtype_to_subclass(dtype)\n",
      "            if klass is not Index:\n",
      "                return klass(data, dtype=dtype, copy=copy, name=name, **kwargs)\n",
      "\n",
      "            ea_cls = dtype.construct_array_type()\n",
      "            data = ea_cls._from_sequence(data, dtype=dtype, copy=copy)\n",
      "            disallow_kwargs(kwargs)\n",
      "            return Index._simple_new(data, name=name)\n",
      "\n",
      "        elif is_ea_or_datetimelike_dtype(data_dtype):\n",
      "            data_dtype = cast(DtypeObj, data_dtype)\n",
      "            klass = cls._dtype_to_subclass(data_dtype)\n",
      "            if klass is not Index:\n",
      "                result = klass(data, copy=copy, name=name, **kwargs)\n",
      "                if dtype is not None:\n",
      "                    return result.astype(dtype, copy=False)\n",
      "                return result\n",
      "            elif dtype is not None:\n",
      "                # GH#45206\n",
      "                data = data.astype(dtype, copy=False)\n",
      "\n",
      "            disallow_kwargs(kwargs)\n",
      "            data = extract_array(data, extract_numpy=True)\n",
      "            return Index._simple_new(data, name=name)\n",
      "\n",
      "        # index-like\n",
      "        elif (\n",
      "            isinstance(data, Index)\n",
      "            and data._is_backward_compat_public_numeric_index\n",
      "            and dtype is None\n",
      "        ):\n",
      "            return data._constructor(data, name=name, copy=copy)\n",
      "        elif isinstance(data, (np.ndarray, Index, ABCSeries)):\n",
      "\n",
      "            if isinstance(data, ABCMultiIndex):\n",
      "                data = data._values\n",
      "\n",
      "            if dtype is not None:\n",
      "                # we need to avoid having numpy coerce\n",
      "                # things that look like ints/floats to ints unless\n",
      "                # they are actually ints, e.g. '0' and 0.0\n",
      "                # should not be coerced\n",
      "                # GH 11836\n",
      "                data = sanitize_array(data, None, dtype=dtype, copy=copy)\n",
      "\n",
      "                dtype = data.dtype\n",
      "\n",
      "            if data.dtype.kind in [\"i\", \"u\", \"f\"]:\n",
      "                # maybe coerce to a sub-class\n",
      "                arr = data\n",
      "            elif data.dtype.kind in [\"b\", \"c\"]:\n",
      "                # No special subclass, and Index._ensure_array won't do this\n",
      "                #  for us.\n",
      "                arr = np.asarray(data)\n",
      "            else:\n",
      "                arr = com.asarray_tuplesafe(data, dtype=_dtype_obj)\n",
      "\n",
      "                if dtype is None:\n",
      "                    arr = _maybe_cast_data_without_dtype(\n",
      "                        arr, cast_numeric_deprecated=True\n",
      "                    )\n",
      "                    dtype = arr.dtype\n",
      "\n",
      "                    if kwargs:\n",
      "                        return cls(arr, dtype, copy=copy, name=name, **kwargs)\n",
      "\n",
      "            klass = cls._dtype_to_subclass(arr.dtype)\n",
      "            arr = klass._ensure_array(arr, dtype, copy)\n",
      "            disallow_kwargs(kwargs)\n",
      "            return klass._simple_new(arr, name)\n",
      "\n",
      "        elif is_scalar(data):\n",
      "            raise cls._scalar_data_error(data)\n",
      "        elif hasattr(data, \"__array__\"):\n",
      "            return Index(np.asarray(data), dtype=dtype, copy=copy, name=name, **kwargs)\n",
      "        else:\n",
      "\n",
      "            if tupleize_cols and is_list_like(data):\n",
      "                # GH21470: convert iterable to list before determining if empty\n",
      "                if is_iterator(data):\n",
      "                    data = list(data)\n",
      "\n",
      "                if data and all(isinstance(e, tuple) for e in data):\n",
      "                    # we must be all tuples, otherwise don't construct\n",
      "                    # 10697\n",
      "                    from pandas.core.indexes.multi import MultiIndex\n",
      "\n",
      "                    return MultiIndex.from_tuples(\n",
      "                        data, names=name or kwargs.get(\"names\")\n",
      "                    )\n",
      "            # other iterable of some kind\n",
      "\n",
      "            subarr = com.asarray_tuplesafe(data, dtype=_dtype_obj)\n",
      "            if dtype is None:\n",
      "                # with e.g. a list [1, 2, 3] casting to numeric is _not_ deprecated\n",
      "                subarr = _maybe_cast_data_without_dtype(\n",
      "                    subarr, cast_numeric_deprecated=False\n",
      "                )\n",
      "                dtype = subarr.dtype\n",
      "            return Index(subarr, dtype=dtype, copy=copy, name=name, **kwargs)\n",
      "\n",
      "------------\n",
      "Method name: __nonzero__\n",
      "Method definition:     @final\n",
      "    def __nonzero__(self) -> NoReturn:\n",
      "        raise ValueError(\n",
      "            f\"The truth value of a {type(self).__name__} is ambiguous. \"\n",
      "            \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: __or__\n",
      "Method definition:     @final\n",
      "    def __or__(self, other):\n",
      "        warnings.warn(\n",
      "            \"Index.__or__ operating as a set operation is deprecated, \"\n",
      "            \"in the future this will be a logical operation matching \"\n",
      "            \"Series.__or__.  Use index.union(other) instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        return self.union(other)\n",
      "\n",
      "------------\n",
      "Method name: __pos__\n",
      "Method definition:     def __pos__(self) -> Index:\n",
      "        return self._unary_method(operator.pos)\n",
      "\n",
      "------------\n",
      "Method name: __pow__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__pow__\")\n",
      "    def __pow__(self, other):\n",
      "        return self._arith_method(other, operator.pow)\n",
      "\n",
      "------------\n",
      "Method name: __radd__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__radd__\")\n",
      "    def __radd__(self, other):\n",
      "        return self._arith_method(other, roperator.radd)\n",
      "\n",
      "------------\n",
      "Method name: __rand__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rand__\")\n",
      "    def __rand__(self, other):\n",
      "        return self._logical_method(other, roperator.rand_)\n",
      "\n",
      "------------\n",
      "Method name: __rdivmod__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rdivmod__\")\n",
      "    def __rdivmod__(self, other):\n",
      "        return self._arith_method(other, roperator.rdivmod)\n",
      "\n",
      "------------\n",
      "Method name: __reduce__\n",
      "Method definition:     def __reduce__(self):\n",
      "        d = {\"data\": self._data, \"name\": self.name}\n",
      "        return _new_Index, (type(self), d), None\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     @final\n",
      "    def __repr__(self) -> str_t:\n",
      "        \"\"\"\n",
      "        Return a string representation for this object.\n",
      "        \"\"\"\n",
      "        klass_name = type(self).__name__\n",
      "        data = self._format_data()\n",
      "        attrs = self._format_attrs()\n",
      "        space = self._format_space()\n",
      "        attrs_str = [f\"{k}={v}\" for k, v in attrs]\n",
      "        prepr = f\",{space}\".join(attrs_str)\n",
      "\n",
      "        # no data provided, just attributes\n",
      "        if data is None:\n",
      "            data = \"\"\n",
      "\n",
      "        return f\"{klass_name}({data}{prepr})\"\n",
      "\n",
      "------------\n",
      "Method name: __rfloordiv__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rfloordiv\")\n",
      "    def __rfloordiv__(self, other):\n",
      "        return self._arith_method(other, roperator.rfloordiv)\n",
      "\n",
      "------------\n",
      "Method name: __rmod__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rmod__\")\n",
      "    def __rmod__(self, other):\n",
      "        return self._arith_method(other, roperator.rmod)\n",
      "\n",
      "------------\n",
      "Method name: __rmul__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rmul__\")\n",
      "    def __rmul__(self, other):\n",
      "        return self._arith_method(other, roperator.rmul)\n",
      "\n",
      "------------\n",
      "Method name: __ror__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__ror__\")\n",
      "    def __ror__(self, other):\n",
      "        return self._logical_method(other, roperator.ror_)\n",
      "\n",
      "------------\n",
      "Method name: __rpow__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rpow__\")\n",
      "    def __rpow__(self, other):\n",
      "        return self._arith_method(other, roperator.rpow)\n",
      "\n",
      "------------\n",
      "Method name: __rsub__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rsub__\")\n",
      "    def __rsub__(self, other):\n",
      "        return self._arith_method(other, roperator.rsub)\n",
      "\n",
      "------------\n",
      "Method name: __rtruediv__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rtruediv__\")\n",
      "    def __rtruediv__(self, other):\n",
      "        return self._arith_method(other, roperator.rtruediv)\n",
      "\n",
      "------------\n",
      "Method name: __rxor__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rxor__\")\n",
      "    def __rxor__(self, other):\n",
      "        return self._logical_method(other, roperator.rxor)\n",
      "\n",
      "------------\n",
      "Method name: __setitem__\n",
      "Method definition:     @final\n",
      "    def __setitem__(self, key, value):\n",
      "        raise TypeError(\"Index does not support mutable operations\")\n",
      "\n",
      "------------\n",
      "Method name: __sizeof__\n",
      "Method definition:     def __sizeof__(self) -> int:\n",
      "        \"\"\"\n",
      "        Generates the total memory usage for an object that returns\n",
      "        either a value or Series of values\n",
      "        \"\"\"\n",
      "        memory_usage = getattr(self, \"memory_usage\", None)\n",
      "        if memory_usage:\n",
      "            mem = memory_usage(deep=True)\n",
      "            return int(mem if is_scalar(mem) else mem.sum())\n",
      "\n",
      "        # no memory_usage attribute, so fall back to object's 'sizeof'\n",
      "        return super().__sizeof__()\n",
      "\n",
      "------------\n",
      "Method name: __sub__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__sub__\")\n",
      "    def __sub__(self, other):\n",
      "        return self._arith_method(other, operator.sub)\n",
      "\n",
      "------------\n",
      "Method name: __truediv__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__truediv__\")\n",
      "    def __truediv__(self, other):\n",
      "        return self._arith_method(other, operator.truediv)\n",
      "\n",
      "------------\n",
      "Method name: __xor__\n",
      "Method definition:     @final\n",
      "    def __xor__(self, other):\n",
      "        warnings.warn(\n",
      "            \"Index.__xor__ operating as a set operation is deprecated, \"\n",
      "            \"in the future this will be a logical operation matching \"\n",
      "            \"Series.__xor__.  Use index.symmetric_difference(other) instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        return self.symmetric_difference(other)\n",
      "\n",
      "------------\n",
      "Method name: _arith_method\n",
      "Method definition:     def _arith_method(self, other, op):\n",
      "        if (\n",
      "            isinstance(other, Index)\n",
      "            and is_object_dtype(other.dtype)\n",
      "            and type(other) is not Index\n",
      "        ):\n",
      "            # We return NotImplemented for object-dtype index *subclasses* so they have\n",
      "            # a chance to implement ops before we unwrap them.\n",
      "            # See https://github.com/pandas-dev/pandas/issues/31109\n",
      "            return NotImplemented\n",
      "\n",
      "        return super()._arith_method(other, op)\n",
      "\n",
      "------------\n",
      "Method name: _assert_can_do_setop\n",
      "Method definition:     @final\n",
      "    def _assert_can_do_setop(self, other) -> bool:\n",
      "        if not is_list_like(other):\n",
      "            raise TypeError(\"Input must be Index or array-like\")\n",
      "        return True\n",
      "\n",
      "------------\n",
      "Method name: _can_hold_identifiers_and_holds_name\n",
      "Method definition:     @final\n",
      "    def _can_hold_identifiers_and_holds_name(self, name) -> bool:\n",
      "        \"\"\"\n",
      "        Faster check for ``name in self`` when we know `name` is a Python\n",
      "        identifier (e.g. in NDFrame.__getattr__, which hits this to support\n",
      "        . key lookup). For indexes that can't hold identifiers (everything\n",
      "        but object & categorical) we just return False.\n",
      "\n",
      "        https://github.com/pandas-dev/pandas/issues/19764\n",
      "        \"\"\"\n",
      "        if self.is_object() or is_string_dtype(self.dtype) or self.is_categorical():\n",
      "            return name in self\n",
      "        return False\n",
      "\n",
      "------------\n",
      "Method name: _check_indexing_error\n",
      "Method definition:     def _check_indexing_error(self, key):\n",
      "        if not is_scalar(key):\n",
      "            # if key is not a scalar, directly raise an error (the code below\n",
      "            # would convert to numpy arrays and raise later any way) - GH29926\n",
      "            raise InvalidIndexError(key)\n",
      "\n",
      "------------\n",
      "Method name: _check_indexing_method\n",
      "Method definition:     @final\n",
      "    def _check_indexing_method(\n",
      "        self,\n",
      "        method: str_t | None,\n",
      "        limit: int | None = None,\n",
      "        tolerance=None,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        Raise if we have a get_indexer `method` that is not supported or valid.\n",
      "        \"\"\"\n",
      "        if method not in [None, \"bfill\", \"backfill\", \"pad\", \"ffill\", \"nearest\"]:\n",
      "            # in practice the clean_reindex_fill_method call would raise\n",
      "            #  before we get here\n",
      "            raise ValueError(\"Invalid fill method\")  # pragma: no cover\n",
      "\n",
      "        if self._is_multi:\n",
      "            if method == \"nearest\":\n",
      "                raise NotImplementedError(\n",
      "                    \"method='nearest' not implemented yet \"\n",
      "                    \"for MultiIndex; see GitHub issue 9365\"\n",
      "                )\n",
      "            elif method == \"pad\" or method == \"backfill\":\n",
      "                if tolerance is not None:\n",
      "                    raise NotImplementedError(\n",
      "                        \"tolerance not implemented yet for MultiIndex\"\n",
      "                    )\n",
      "\n",
      "        if is_interval_dtype(self.dtype) or is_categorical_dtype(self.dtype):\n",
      "            # GH#37871 for now this is only for IntervalIndex and CategoricalIndex\n",
      "            if method is not None:\n",
      "                raise NotImplementedError(\n",
      "                    f\"method {method} not yet implemented for {type(self).__name__}\"\n",
      "                )\n",
      "\n",
      "        if method is None:\n",
      "            if tolerance is not None:\n",
      "                raise ValueError(\n",
      "                    \"tolerance argument only valid if doing pad, \"\n",
      "                    \"backfill or nearest reindexing\"\n",
      "                )\n",
      "            if limit is not None:\n",
      "                raise ValueError(\n",
      "                    \"limit argument only valid if doing pad, \"\n",
      "                    \"backfill or nearest reindexing\"\n",
      "                )\n",
      "\n",
      "------------\n",
      "Method name: _cleanup\n",
      "Method definition:     @final\n",
      "    def _cleanup(self) -> None:\n",
      "        self._engine.clear_mapping()\n",
      "\n",
      "------------\n",
      "Method name: _cmp_method\n",
      "Method definition:     def _cmp_method(self, other, op):\n",
      "        \"\"\"\n",
      "        Wrapper used to dispatch comparison operations.\n",
      "        \"\"\"\n",
      "        if self.is_(other):\n",
      "            # fastpath\n",
      "            if op in {operator.eq, operator.le, operator.ge}:\n",
      "                arr = np.ones(len(self), dtype=bool)\n",
      "                if self._can_hold_na and not isinstance(self, ABCMultiIndex):\n",
      "                    # TODO: should set MultiIndex._can_hold_na = False?\n",
      "                    arr[self.isna()] = False\n",
      "                return arr\n",
      "            elif op is operator.ne:\n",
      "                arr = np.zeros(len(self), dtype=bool)\n",
      "                if self._can_hold_na and not isinstance(self, ABCMultiIndex):\n",
      "                    arr[self.isna()] = True\n",
      "                return arr\n",
      "\n",
      "        if isinstance(other, (np.ndarray, Index, ABCSeries, ExtensionArray)) and len(\n",
      "            self\n",
      "        ) != len(other):\n",
      "            raise ValueError(\"Lengths must match to compare\")\n",
      "\n",
      "        if not isinstance(other, ABCMultiIndex):\n",
      "            other = extract_array(other, extract_numpy=True)\n",
      "        else:\n",
      "            other = np.asarray(other)\n",
      "\n",
      "        if is_object_dtype(self.dtype) and isinstance(other, ExtensionArray):\n",
      "            # e.g. PeriodArray, Categorical\n",
      "            with np.errstate(all=\"ignore\"):\n",
      "                result = op(self._values, other)\n",
      "\n",
      "        elif isinstance(self._values, ExtensionArray):\n",
      "            result = op(self._values, other)\n",
      "\n",
      "        elif is_object_dtype(self.dtype) and not isinstance(self, ABCMultiIndex):\n",
      "            # don't pass MultiIndex\n",
      "            with np.errstate(all=\"ignore\"):\n",
      "                result = ops.comp_method_OBJECT_ARRAY(op, self._values, other)\n",
      "\n",
      "        else:\n",
      "            with np.errstate(all=\"ignore\"):\n",
      "                result = ops.comparison_op(self._values, other, op)\n",
      "\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _concat\n",
      "Method definition:     def _concat(self, to_concat: list[Index], name: Hashable) -> Index:\n",
      "        \"\"\"\n",
      "        Concatenate multiple Index objects.\n",
      "        \"\"\"\n",
      "        to_concat_vals = [x._values for x in to_concat]\n",
      "\n",
      "        result = concat_compat(to_concat_vals)\n",
      "\n",
      "        is_numeric = result.dtype.kind in [\"i\", \"u\", \"f\"]\n",
      "        if self._is_backward_compat_public_numeric_index and is_numeric:\n",
      "            return type(self)._simple_new(result, name=name)\n",
      "\n",
      "        return Index._with_infer(result, name=name)\n",
      "\n",
      "------------\n",
      "Method name: _construct_result\n",
      "Method definition:     def _construct_result(self, result, name):\n",
      "        if isinstance(result, tuple):\n",
      "            return (\n",
      "                Index._with_infer(result[0], name=name),\n",
      "                Index._with_infer(result[1], name=name),\n",
      "            )\n",
      "        return Index._with_infer(result, name=name)\n",
      "\n",
      "------------\n",
      "Method name: _convert_can_do_setop\n",
      "Method definition:     def _convert_can_do_setop(self, other) -> tuple[Index, Hashable]:\n",
      "        if not isinstance(other, Index):\n",
      "            # TODO(2.0): no need to special-case here once _with_infer\n",
      "            #  deprecation is enforced\n",
      "            if hasattr(other, \"dtype\"):\n",
      "                other = Index(other, name=self.name, dtype=other.dtype)\n",
      "            else:\n",
      "                # e.g. list\n",
      "                other = Index(other, name=self.name)\n",
      "            result_name = self.name\n",
      "        else:\n",
      "            result_name = get_op_result_name(self, other)\n",
      "        return other, result_name\n",
      "\n",
      "------------\n",
      "Method name: _convert_slice_indexer\n",
      "Method definition:     def _convert_slice_indexer(self, key: slice, kind: str_t):\n",
      "        \"\"\"\n",
      "        Convert a slice indexer.\n",
      "\n",
      "        By definition, these are labels unless 'iloc' is passed in.\n",
      "        Floats are not allowed as the start, step, or stop of the slice.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        key : label of the slice bound\n",
      "        kind : {'loc', 'getitem'}\n",
      "        \"\"\"\n",
      "        assert kind in [\"loc\", \"getitem\"], kind\n",
      "\n",
      "        # potentially cast the bounds to integers\n",
      "        start, stop, step = key.start, key.stop, key.step\n",
      "\n",
      "        # figure out if this is a positional indexer\n",
      "        def is_int(v):\n",
      "            return v is None or is_integer(v)\n",
      "\n",
      "        is_index_slice = is_int(start) and is_int(stop) and is_int(step)\n",
      "\n",
      "        # special case for interval_dtype bc we do not do partial-indexing\n",
      "        #  on integer Intervals when slicing\n",
      "        # TODO: write this in terms of e.g. should_partial_index?\n",
      "        ints_are_positional = self._should_fallback_to_positional or is_interval_dtype(\n",
      "            self.dtype\n",
      "        )\n",
      "        is_positional = is_index_slice and ints_are_positional\n",
      "\n",
      "        if kind == \"getitem\":\n",
      "            # called from the getitem slicers, validate that we are in fact integers\n",
      "            if self.is_integer() or is_index_slice:\n",
      "                # Note: these checks are redundant if we know is_index_slice\n",
      "                self._validate_indexer(\"slice\", key.start, \"getitem\")\n",
      "                self._validate_indexer(\"slice\", key.stop, \"getitem\")\n",
      "                self._validate_indexer(\"slice\", key.step, \"getitem\")\n",
      "                return key\n",
      "\n",
      "        # convert the slice to an indexer here\n",
      "\n",
      "        # if we are mixed and have integers\n",
      "        if is_positional:\n",
      "            try:\n",
      "                # Validate start & stop\n",
      "                if start is not None:\n",
      "                    self.get_loc(start)\n",
      "                if stop is not None:\n",
      "                    self.get_loc(stop)\n",
      "                is_positional = False\n",
      "            except KeyError:\n",
      "                pass\n",
      "\n",
      "        if com.is_null_slice(key):\n",
      "            # It doesn't matter if we are positional or label based\n",
      "            indexer = key\n",
      "        elif is_positional:\n",
      "            if kind == \"loc\":\n",
      "                # GH#16121, GH#24612, GH#31810\n",
      "                warnings.warn(\n",
      "                    \"Slicing a positional slice with .loc is not supported, \"\n",
      "                    \"and will raise TypeError in a future version.  \"\n",
      "                    \"Use .loc with labels or .iloc with positions instead.\",\n",
      "                    FutureWarning,\n",
      "                    stacklevel=find_stack_level(),\n",
      "                )\n",
      "            indexer = key\n",
      "        else:\n",
      "            indexer = self.slice_indexer(start, stop, step)\n",
      "\n",
      "        return indexer\n",
      "\n",
      "------------\n",
      "Method name: _convert_tolerance\n",
      "Method definition:     def _convert_tolerance(self, tolerance, target: np.ndarray | Index) -> np.ndarray:\n",
      "        # override this method on subclasses\n",
      "        tolerance = np.asarray(tolerance)\n",
      "        if target.size != tolerance.size and tolerance.size > 1:\n",
      "            raise ValueError(\"list-like tolerance size must match target index size\")\n",
      "        return tolerance\n",
      "\n",
      "------------\n",
      "Method name: _deprecate_dti_setop\n",
      "Method definition:     @final\n",
      "    def _deprecate_dti_setop(self, other: Index, setop: str_t):\n",
      "        \"\"\"\n",
      "        Deprecate setop behavior between timezone-aware DatetimeIndexes with\n",
      "        mismatched timezones.\n",
      "        \"\"\"\n",
      "        # Caller is responsibelf or checking\n",
      "        #  `not is_dtype_equal(self.dtype, other.dtype)`\n",
      "        if (\n",
      "            isinstance(self, ABCDatetimeIndex)\n",
      "            and isinstance(other, ABCDatetimeIndex)\n",
      "            and self.tz is not None\n",
      "            and other.tz is not None\n",
      "        ):\n",
      "            # GH#39328, GH#45357\n",
      "            warnings.warn(\n",
      "                f\"In a future version, the {setop} of DatetimeIndex objects \"\n",
      "                \"with mismatched timezones will cast both to UTC instead of \"\n",
      "                \"object dtype. To retain the old behavior, \"\n",
      "                f\"use `index.astype(object).{setop}(other)`\",\n",
      "                FutureWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "\n",
      "------------\n",
      "Method name: _deprecated_arg\n",
      "Method definition:     @final\n",
      "    def _deprecated_arg(self, value, name: str_t, methodname: str_t) -> None:\n",
      "        \"\"\"\n",
      "        Issue a FutureWarning if the arg/kwarg is not no_default.\n",
      "        \"\"\"\n",
      "        if value is not no_default:\n",
      "            warnings.warn(\n",
      "                f\"'{name}' argument in {methodname} is deprecated \"\n",
      "                \"and will be removed in a future version.  Do not pass it.\",\n",
      "                FutureWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "\n",
      "------------\n",
      "Method name: _difference\n",
      "Method definition:     def _difference(self, other, sort):\n",
      "        # overridden by RangeIndex\n",
      "\n",
      "        this = self.unique()\n",
      "\n",
      "        indexer = this.get_indexer_for(other)\n",
      "        indexer = indexer.take((indexer != -1).nonzero()[0])\n",
      "\n",
      "        label_diff = np.setdiff1d(np.arange(this.size), indexer, assume_unique=True)\n",
      "        the_diff = this._values.take(label_diff)\n",
      "        the_diff = _maybe_try_sort(the_diff, sort)\n",
      "\n",
      "        return the_diff\n",
      "\n",
      "------------\n",
      "Method name: _difference_compat\n",
      "Method definition:     @final\n",
      "    def _difference_compat(\n",
      "        self, target: Index, indexer: npt.NDArray[np.intp]\n",
      "    ) -> ArrayLike:\n",
      "        # Compatibility for PeriodArray, for which __sub__ returns an ndarray[object]\n",
      "        #  of DateOffset objects, which do not support __abs__ (and would be slow\n",
      "        #  if they did)\n",
      "\n",
      "        if isinstance(self.dtype, PeriodDtype):\n",
      "            # Note: we only get here with matching dtypes\n",
      "            own_values = cast(\"PeriodArray\", self._data)._ndarray\n",
      "            target_values = cast(\"PeriodArray\", target._data)._ndarray\n",
      "            diff = own_values[indexer] - target_values\n",
      "        else:\n",
      "            # error: Unsupported left operand type for - (\"ExtensionArray\")\n",
      "            diff = self._values[indexer] - target._values  # type: ignore[operator]\n",
      "        return abs(diff)\n",
      "\n",
      "------------\n",
      "Method name: _dir_additions\n",
      "Method definition:     def _dir_additions(self) -> set[str]:\n",
      "        \"\"\"\n",
      "        Add additional __dir__ for this object.\n",
      "        \"\"\"\n",
      "        return {accessor for accessor in self._accessors if hasattr(self, accessor)}\n",
      "\n",
      "------------\n",
      "Method name: _dir_deletions\n",
      "Method definition:     def _dir_deletions(self) -> set[str]:\n",
      "        \"\"\"\n",
      "        Delete unwanted __dir__ for this object.\n",
      "        \"\"\"\n",
      "        return self._accessors | self._hidden_attrs\n",
      "\n",
      "------------\n",
      "Method name: _drop_level_numbers\n",
      "Method definition:     @final\n",
      "    def _drop_level_numbers(self, levnums: list[int]):\n",
      "        \"\"\"\n",
      "        Drop MultiIndex levels by level _number_, not name.\n",
      "        \"\"\"\n",
      "\n",
      "        if not levnums and not isinstance(self, ABCMultiIndex):\n",
      "            return self\n",
      "        if len(levnums) >= self.nlevels:\n",
      "            raise ValueError(\n",
      "                f\"Cannot remove {len(levnums)} levels from an index with \"\n",
      "                f\"{self.nlevels} levels: at least one level must be left.\"\n",
      "            )\n",
      "        # The two checks above guarantee that here self is a MultiIndex\n",
      "        self = cast(\"MultiIndex\", self)\n",
      "\n",
      "        new_levels = list(self.levels)\n",
      "        new_codes = list(self.codes)\n",
      "        new_names = list(self.names)\n",
      "\n",
      "        for i in levnums:\n",
      "            new_levels.pop(i)\n",
      "            new_codes.pop(i)\n",
      "            new_names.pop(i)\n",
      "\n",
      "        if len(new_levels) == 1:\n",
      "            lev = new_levels[0]\n",
      "\n",
      "            if len(lev) == 0:\n",
      "                # If lev is empty, lev.take will fail GH#42055\n",
      "                if len(new_codes[0]) == 0:\n",
      "                    # GH#45230 preserve RangeIndex here\n",
      "                    #  see test_reset_index_empty_rangeindex\n",
      "                    result = lev[:0]\n",
      "                else:\n",
      "                    res_values = algos.take(lev._values, new_codes[0], allow_fill=True)\n",
      "                    # _constructor instead of type(lev) for RangeIndex compat GH#35230\n",
      "                    result = lev._constructor._simple_new(res_values, name=new_names[0])\n",
      "            else:\n",
      "                # set nan if needed\n",
      "                mask = new_codes[0] == -1\n",
      "                result = new_levels[0].take(new_codes[0])\n",
      "                if mask.any():\n",
      "                    result = result.putmask(mask, np.nan)\n",
      "\n",
      "                result._name = new_names[0]\n",
      "\n",
      "            return result\n",
      "        else:\n",
      "            from pandas.core.indexes.multi import MultiIndex\n",
      "\n",
      "            return MultiIndex(\n",
      "                levels=new_levels,\n",
      "                codes=new_codes,\n",
      "                names=new_names,\n",
      "                verify_integrity=False,\n",
      "            )\n",
      "\n",
      "------------\n",
      "Method name: _duplicated\n",
      "Method definition:     @final\n",
      "    def _duplicated(\n",
      "        self, keep: Literal[\"first\", \"last\", False] = \"first\"\n",
      "    ) -> npt.NDArray[np.bool_]:\n",
      "        return duplicated(self._values, keep=keep)\n",
      "\n",
      "------------\n",
      "Method name: _filter_indexer_tolerance\n",
      "Method definition:     @final\n",
      "    def _filter_indexer_tolerance(\n",
      "        self,\n",
      "        target: Index,\n",
      "        indexer: npt.NDArray[np.intp],\n",
      "        tolerance,\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "\n",
      "        distance = self._difference_compat(target, indexer)\n",
      "\n",
      "        return np.where(distance <= tolerance, indexer, -1)\n",
      "\n",
      "------------\n",
      "Method name: _find_common_type_compat\n",
      "Method definition:     @final\n",
      "    def _find_common_type_compat(self, target) -> DtypeObj:\n",
      "        \"\"\"\n",
      "        Implementation of find_common_type that adjusts for Index-specific\n",
      "        special cases.\n",
      "        \"\"\"\n",
      "        if is_valid_na_for_dtype(target, self.dtype):\n",
      "            # e.g. setting NA value into IntervalArray[int64]\n",
      "            dtype = ensure_dtype_can_hold_na(self.dtype)\n",
      "            if is_dtype_equal(self.dtype, dtype):\n",
      "                raise NotImplementedError(\n",
      "                    \"This should not be reached. Please report a bug at \"\n",
      "                    \"github.com/pandas-dev/pandas\"\n",
      "                )\n",
      "            return dtype\n",
      "\n",
      "        target_dtype, _ = infer_dtype_from(target, pandas_dtype=True)\n",
      "\n",
      "        # special case: if one dtype is uint64 and the other a signed int, return object\n",
      "        # See https://github.com/pandas-dev/pandas/issues/26778 for discussion\n",
      "        # Now it's:\n",
      "        # * float | [u]int -> float\n",
      "        # * uint64 | signed int  -> object\n",
      "        # We may change union(float | [u]int) to go to object.\n",
      "        if self.dtype == \"uint64\" or target_dtype == \"uint64\":\n",
      "            if is_signed_integer_dtype(self.dtype) or is_signed_integer_dtype(\n",
      "                target_dtype\n",
      "            ):\n",
      "                return _dtype_obj\n",
      "\n",
      "        dtype = find_common_type([self.dtype, target_dtype])\n",
      "        dtype = common_dtype_categorical_compat([self, target], dtype)\n",
      "        return dtype\n",
      "\n",
      "------------\n",
      "Method name: _format_attrs\n",
      "Method definition:     def _format_attrs(self) -> list[tuple[str_t, str_t | int | bool | None]]:\n",
      "        \"\"\"\n",
      "        Return a list of tuples of the (attr,formatted_value).\n",
      "        \"\"\"\n",
      "        attrs: list[tuple[str_t, str_t | int | bool | None]] = []\n",
      "\n",
      "        if not self._is_multi:\n",
      "            attrs.append((\"dtype\", f\"'{self.dtype}'\"))\n",
      "\n",
      "        if self.name is not None:\n",
      "            attrs.append((\"name\", default_pprint(self.name)))\n",
      "        elif self._is_multi and any(x is not None for x in self.names):\n",
      "            attrs.append((\"names\", default_pprint(self.names)))\n",
      "\n",
      "        max_seq_items = get_option(\"display.max_seq_items\") or len(self)\n",
      "        if len(self) > max_seq_items:\n",
      "            attrs.append((\"length\", len(self)))\n",
      "        return attrs\n",
      "\n",
      "------------\n",
      "Method name: _format_data\n",
      "Method definition:     def _format_data(self, name=None) -> str_t:\n",
      "        \"\"\"\n",
      "        Return the formatted data as a unicode string.\n",
      "        \"\"\"\n",
      "        # do we want to justify (only do so for non-objects)\n",
      "        is_justify = True\n",
      "\n",
      "        if self.inferred_type == \"string\":\n",
      "            is_justify = False\n",
      "        elif self.inferred_type == \"categorical\":\n",
      "            self = cast(\"CategoricalIndex\", self)\n",
      "            if is_object_dtype(self.categories):\n",
      "                is_justify = False\n",
      "\n",
      "        return format_object_summary(\n",
      "            self,\n",
      "            self._formatter_func,\n",
      "            is_justify=is_justify,\n",
      "            name=name,\n",
      "            line_break_each_value=self._is_multi,\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: _format_duplicate_message\n",
      "Method definition:     @final\n",
      "    def _format_duplicate_message(self) -> DataFrame:\n",
      "        \"\"\"\n",
      "        Construct the DataFrame for a DuplicateLabelError.\n",
      "\n",
      "        This returns a DataFrame indicating the labels and positions\n",
      "        of duplicates in an index. This should only be called when it's\n",
      "        already known that duplicates are present.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['a', 'b', 'a'])\n",
      "        >>> idx._format_duplicate_message()\n",
      "            positions\n",
      "        label\n",
      "        a        [0, 2]\n",
      "        \"\"\"\n",
      "        from pandas import Series\n",
      "\n",
      "        duplicates = self[self.duplicated(keep=\"first\")].unique()\n",
      "        assert len(duplicates)\n",
      "\n",
      "        out = Series(np.arange(len(self))).groupby(self).agg(list)[duplicates]\n",
      "        if self._is_multi:\n",
      "            # test_format_duplicate_labels_message_multi\n",
      "            # error: \"Type[Index]\" has no attribute \"from_tuples\"  [attr-defined]\n",
      "            out.index = type(self).from_tuples(out.index)  # type: ignore[attr-defined]\n",
      "\n",
      "        if self.nlevels == 1:\n",
      "            out = out.rename_axis(\"label\")\n",
      "        return out.to_frame(name=\"positions\")\n",
      "\n",
      "------------\n",
      "Method name: _format_native_types\n",
      "Method definition:     def _format_native_types(\n",
      "        self, *, na_rep=\"\", quoting=None, **kwargs\n",
      "    ) -> npt.NDArray[np.object_]:\n",
      "        \"\"\"\n",
      "        Actually format specific types of the index.\n",
      "        \"\"\"\n",
      "        mask = isna(self)\n",
      "        if not self.is_object() and not quoting:\n",
      "            values = np.asarray(self).astype(str)\n",
      "        else:\n",
      "            values = np.array(self, dtype=object, copy=True)\n",
      "\n",
      "        values[mask] = na_rep\n",
      "        return values\n",
      "\n",
      "------------\n",
      "Method name: _format_space\n",
      "Method definition:     def _format_space(self) -> str_t:\n",
      "\n",
      "        # using space here controls if the attributes\n",
      "        # are line separated or not (the default)\n",
      "\n",
      "        # max_seq_items = get_option('display.max_seq_items')\n",
      "        # if len(self) > max_seq_items:\n",
      "        #    space = \"\\n%s\" % (' ' * (len(klass) + 1))\n",
      "        return \" \"\n",
      "\n",
      "------------\n",
      "Method name: _format_with_header\n",
      "Method definition:     def _format_with_header(self, header: list[str_t], na_rep: str_t) -> list[str_t]:\n",
      "        from pandas.io.formats.format import format_array\n",
      "\n",
      "        values = self._values\n",
      "\n",
      "        if is_object_dtype(values.dtype):\n",
      "            values = cast(np.ndarray, values)\n",
      "            values = lib.maybe_convert_objects(values, safe=True)\n",
      "\n",
      "            result = [pprint_thing(x, escape_chars=(\"\\t\", \"\\r\", \"\\n\")) for x in values]\n",
      "\n",
      "            # could have nans\n",
      "            mask = is_float_nan(values)\n",
      "            if mask.any():\n",
      "                result_arr = np.array(result)\n",
      "                result_arr[mask] = na_rep\n",
      "                result = result_arr.tolist()\n",
      "        else:\n",
      "            result = trim_front(format_array(values, None, justify=\"left\"))\n",
      "        return header + result\n",
      "\n",
      "------------\n",
      "Method name: _from_join_target\n",
      "Method definition:     def _from_join_target(self, result: np.ndarray) -> ArrayLike:\n",
      "        \"\"\"\n",
      "        Cast the ndarray returned from one of the libjoin.foo_indexer functions\n",
      "        back to type(self)._data.\n",
      "        \"\"\"\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _get_attributes_dict\n",
      "Method definition:     @final\n",
      "    def _get_attributes_dict(self) -> dict[str_t, Any]:\n",
      "        \"\"\"\n",
      "        Return an attributes dict for my class.\n",
      "\n",
      "        Temporarily added back for compatibility issue in dask, see\n",
      "        https://github.com/pandas-dev/pandas/pull/43895\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            \"The Index._get_attributes_dict method is deprecated, and will be \"\n",
      "            \"removed in a future version\",\n",
      "            DeprecationWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        return {k: getattr(self, k, None) for k in self._attributes}\n",
      "\n",
      "------------\n",
      "Method name: _get_default_index_names\n",
      "Method definition:     def _get_default_index_names(\n",
      "        self, names: Hashable | Sequence[Hashable] | None = None, default=None\n",
      "    ) -> list[Hashable]:\n",
      "        \"\"\"\n",
      "        Get names of index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        names : int, str or 1-dimensional list, default None\n",
      "            Index names to set.\n",
      "        default : str\n",
      "            Default name of index.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            if names not str or list-like\n",
      "        \"\"\"\n",
      "        from pandas.core.indexes.multi import MultiIndex\n",
      "\n",
      "        if names is not None:\n",
      "            if isinstance(names, str) or isinstance(names, int):\n",
      "                names = [names]\n",
      "\n",
      "        if not isinstance(names, list) and names is not None:\n",
      "            raise ValueError(\"Index names must be str or 1-dimensional list\")\n",
      "\n",
      "        if not names:\n",
      "            if isinstance(self, MultiIndex):\n",
      "                names = com.fill_missing_names(self.names)\n",
      "            else:\n",
      "                names = [default] if self.name is None else [self.name]\n",
      "\n",
      "        return names\n",
      "\n",
      "------------\n",
      "Method name: _get_engine_target\n",
      "Method definition:     def _get_engine_target(self) -> ArrayLike:\n",
      "        \"\"\"\n",
      "        Get the ndarray or ExtensionArray that we can pass to the IndexEngine\n",
      "        constructor.\n",
      "        \"\"\"\n",
      "        vals = self._values\n",
      "        if isinstance(vals, StringArray):\n",
      "            # GH#45652 much more performant than ExtensionEngine\n",
      "            return vals._ndarray\n",
      "        if type(self) is Index and isinstance(self._values, ExtensionArray):\n",
      "            # TODO(ExtensionIndex): remove special-case, just use self._values\n",
      "            return self._values.astype(object)\n",
      "        return vals\n",
      "\n",
      "------------\n",
      "Method name: _get_fill_indexer\n",
      "Method definition:     @final\n",
      "    def _get_fill_indexer(\n",
      "        self, target: Index, method: str_t, limit: int | None = None, tolerance=None\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "\n",
      "        if self._is_multi:\n",
      "            # TODO: get_indexer_with_fill docstring says values must be _sorted_\n",
      "            #  but that doesn't appear to be enforced\n",
      "            # error: \"IndexEngine\" has no attribute \"get_indexer_with_fill\"\n",
      "            engine = self._engine\n",
      "            return engine.get_indexer_with_fill(  # type: ignore[union-attr]\n",
      "                target=target._values, values=self._values, method=method, limit=limit\n",
      "            )\n",
      "\n",
      "        if self.is_monotonic_increasing and target.is_monotonic_increasing:\n",
      "            target_values = target._get_engine_target()\n",
      "            own_values = self._get_engine_target()\n",
      "            if not isinstance(target_values, np.ndarray) or not isinstance(\n",
      "                own_values, np.ndarray\n",
      "            ):\n",
      "                raise NotImplementedError\n",
      "\n",
      "            if method == \"pad\":\n",
      "                indexer = libalgos.pad(own_values, target_values, limit=limit)\n",
      "            else:\n",
      "                # i.e. \"backfill\"\n",
      "                indexer = libalgos.backfill(own_values, target_values, limit=limit)\n",
      "        else:\n",
      "            indexer = self._get_fill_indexer_searchsorted(target, method, limit)\n",
      "        if tolerance is not None and len(self):\n",
      "            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)\n",
      "        return indexer\n",
      "\n",
      "------------\n",
      "Method name: _get_fill_indexer_searchsorted\n",
      "Method definition:     @final\n",
      "    def _get_fill_indexer_searchsorted(\n",
      "        self, target: Index, method: str_t, limit: int | None = None\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "        \"\"\"\n",
      "        Fallback pad/backfill get_indexer that works for monotonic decreasing\n",
      "        indexes and non-monotonic targets.\n",
      "        \"\"\"\n",
      "        if limit is not None:\n",
      "            raise ValueError(\n",
      "                f\"limit argument for {repr(method)} method only well-defined \"\n",
      "                \"if index and target are monotonic\"\n",
      "            )\n",
      "\n",
      "        side: Literal[\"left\", \"right\"] = \"left\" if method == \"pad\" else \"right\"\n",
      "\n",
      "        # find exact matches first (this simplifies the algorithm)\n",
      "        indexer = self.get_indexer(target)\n",
      "        nonexact = indexer == -1\n",
      "        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact], side)\n",
      "        if side == \"left\":\n",
      "            # searchsorted returns \"indices into a sorted array such that,\n",
      "            # if the corresponding elements in v were inserted before the\n",
      "            # indices, the order of a would be preserved\".\n",
      "            # Thus, we need to subtract 1 to find values to the left.\n",
      "            indexer[nonexact] -= 1\n",
      "            # This also mapped not found values (values of 0 from\n",
      "            # np.searchsorted) to -1, which conveniently is also our\n",
      "            # sentinel for missing values\n",
      "        else:\n",
      "            # Mark indices to the right of the largest value as not found\n",
      "            indexer[indexer == len(self)] = -1\n",
      "        return indexer\n",
      "\n",
      "------------\n",
      "Method name: _get_grouper_for_level\n",
      "Method definition:     def _get_grouper_for_level(\n",
      "        self,\n",
      "        mapper,\n",
      "        *,\n",
      "        level=None,\n",
      "        dropna: bool = True,\n",
      "    ) -> tuple[Index, npt.NDArray[np.signedinteger] | None, Index | None]:\n",
      "        \"\"\"\n",
      "        Get index grouper corresponding to an index level\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        mapper: Group mapping function or None\n",
      "            Function mapping index values to groups\n",
      "        level : int or None\n",
      "            Index level, positional\n",
      "        dropna : bool\n",
      "            dropna from groupby\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        grouper : Index\n",
      "            Index of values to group on.\n",
      "        labels : ndarray of int or None\n",
      "            Array of locations in level_index.\n",
      "        uniques : Index or None\n",
      "            Index of unique values for level.\n",
      "        \"\"\"\n",
      "        assert level is None or level == 0\n",
      "        if mapper is None:\n",
      "            grouper = self\n",
      "        else:\n",
      "            grouper = self.map(mapper)\n",
      "\n",
      "        return grouper, None, None\n",
      "\n",
      "------------\n",
      "Method name: _get_indexer\n",
      "Method definition:     def _get_indexer(\n",
      "        self,\n",
      "        target: Index,\n",
      "        method: str_t | None = None,\n",
      "        limit: int | None = None,\n",
      "        tolerance=None,\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "        if tolerance is not None:\n",
      "            tolerance = self._convert_tolerance(tolerance, target)\n",
      "\n",
      "        if method in [\"pad\", \"backfill\"]:\n",
      "            indexer = self._get_fill_indexer(target, method, limit, tolerance)\n",
      "        elif method == \"nearest\":\n",
      "            indexer = self._get_nearest_indexer(target, limit, tolerance)\n",
      "        else:\n",
      "            if target._is_multi and self._is_multi:\n",
      "                engine = self._engine\n",
      "                # error: Item \"IndexEngine\" of \"Union[IndexEngine, ExtensionEngine]\"\n",
      "                # has no attribute \"_extract_level_codes\"\n",
      "                tgt_values = engine._extract_level_codes(  # type: ignore[union-attr]\n",
      "                    target\n",
      "                )\n",
      "            else:\n",
      "                tgt_values = target._get_engine_target()\n",
      "\n",
      "            indexer = self._engine.get_indexer(tgt_values)\n",
      "\n",
      "        return ensure_platform_int(indexer)\n",
      "\n",
      "------------\n",
      "Method name: _get_indexer_non_comparable\n",
      "Method definition:     @final\n",
      "    def _get_indexer_non_comparable(\n",
      "        self, target: Index, method, unique: bool = True\n",
      "    ) -> npt.NDArray[np.intp] | tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
      "        \"\"\"\n",
      "        Called from get_indexer or get_indexer_non_unique when the target\n",
      "        is of a non-comparable dtype.\n",
      "\n",
      "        For get_indexer lookups with method=None, get_indexer is an _equality_\n",
      "        check, so non-comparable dtypes mean we will always have no matches.\n",
      "\n",
      "        For get_indexer lookups with a method, get_indexer is an _inequality_\n",
      "        check, so non-comparable dtypes mean we will always raise TypeError.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        target : Index\n",
      "        method : str or None\n",
      "        unique : bool, default True\n",
      "            * True if called from get_indexer.\n",
      "            * False if called from get_indexer_non_unique.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If doing an inequality check, i.e. method is not None.\n",
      "        \"\"\"\n",
      "        if method is not None:\n",
      "            other = unpack_nested_dtype(target)\n",
      "            raise TypeError(f\"Cannot compare dtypes {self.dtype} and {other.dtype}\")\n",
      "\n",
      "        no_matches = -1 * np.ones(target.shape, dtype=np.intp)\n",
      "        if unique:\n",
      "            # This is for get_indexer\n",
      "            return no_matches\n",
      "        else:\n",
      "            # This is for get_indexer_non_unique\n",
      "            missing = np.arange(len(target), dtype=np.intp)\n",
      "            return no_matches, missing\n",
      "\n",
      "------------\n",
      "Method name: _get_indexer_strict\n",
      "Method definition:     def _get_indexer_strict(self, key, axis_name: str_t) -> tuple[Index, np.ndarray]:\n",
      "        \"\"\"\n",
      "        Analogue to get_indexer that raises if any elements are missing.\n",
      "        \"\"\"\n",
      "        keyarr = key\n",
      "        if not isinstance(keyarr, Index):\n",
      "            keyarr = com.asarray_tuplesafe(keyarr)\n",
      "\n",
      "        if self._index_as_unique:\n",
      "            indexer = self.get_indexer_for(keyarr)\n",
      "            keyarr = self.reindex(keyarr)[0]\n",
      "        else:\n",
      "            keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr)\n",
      "\n",
      "        self._raise_if_missing(keyarr, indexer, axis_name)\n",
      "\n",
      "        keyarr = self.take(indexer)\n",
      "        if isinstance(key, Index):\n",
      "            # GH 42790 - Preserve name from an Index\n",
      "            keyarr.name = key.name\n",
      "        if keyarr.dtype.kind in [\"m\", \"M\"]:\n",
      "            # DTI/TDI.take can infer a freq in some cases when we dont want one\n",
      "            if isinstance(key, list) or (\n",
      "                isinstance(key, type(self))\n",
      "                # \"Index\" has no attribute \"freq\"\n",
      "                and key.freq is None  # type: ignore[attr-defined]\n",
      "            ):\n",
      "                keyarr = keyarr._with_freq(None)\n",
      "\n",
      "        return keyarr, indexer\n",
      "\n",
      "------------\n",
      "Method name: _get_level_names\n",
      "Method definition:     @final\n",
      "    def _get_level_names(self) -> Hashable | Sequence[Hashable]:\n",
      "        \"\"\"\n",
      "        Return a name or list of names with None replaced by the level number.\n",
      "        \"\"\"\n",
      "        if self._is_multi:\n",
      "            return [\n",
      "                level if name is None else name for level, name in enumerate(self.names)\n",
      "            ]\n",
      "        else:\n",
      "            return 0 if self.name is None else self.name\n",
      "\n",
      "------------\n",
      "Method name: _get_level_number\n",
      "Method definition:     def _get_level_number(self, level) -> int:\n",
      "        self._validate_index_level(level)\n",
      "        return 0\n",
      "\n",
      "------------\n",
      "Method name: _get_level_values\n",
      "Method definition:     def _get_level_values(self, level) -> Index:\n",
      "        \"\"\"\n",
      "        Return an Index of values for requested level.\n",
      "\n",
      "        This is primarily useful to get an individual level of values from a\n",
      "        MultiIndex, but is provided on Index as well for compatibility.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        level : int or str\n",
      "            It is either the integer position or the name of the level.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "            Calling object, as there is only one level in the Index.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        For Index, level should be 0, since there are no multiple levels.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(list('abc'))\n",
      "        >>> idx\n",
      "        Index(['a', 'b', 'c'], dtype='object')\n",
      "\n",
      "        Get level values by supplying `level` as integer:\n",
      "\n",
      "        >>> idx.get_level_values(0)\n",
      "        Index(['a', 'b', 'c'], dtype='object')\n",
      "        \"\"\"\n",
      "        self._validate_index_level(level)\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: _get_names\n",
      "Method definition:     def _get_names(self) -> FrozenList:\n",
      "        return FrozenList((self.name,))\n",
      "\n",
      "------------\n",
      "Method name: _get_nearest_indexer\n",
      "Method definition:     @final\n",
      "    def _get_nearest_indexer(\n",
      "        self, target: Index, limit: int | None, tolerance\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "        \"\"\"\n",
      "        Get the indexer for the nearest index labels; requires an index with\n",
      "        values that can be subtracted from each other (e.g., not strings or\n",
      "        tuples).\n",
      "        \"\"\"\n",
      "        if not len(self):\n",
      "            return self._get_fill_indexer(target, \"pad\")\n",
      "\n",
      "        left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n",
      "        right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n",
      "\n",
      "        left_distances = self._difference_compat(target, left_indexer)\n",
      "        right_distances = self._difference_compat(target, right_indexer)\n",
      "\n",
      "        op = operator.lt if self.is_monotonic_increasing else operator.le\n",
      "        indexer = np.where(\n",
      "            # error: Argument 1&2 has incompatible type \"Union[ExtensionArray,\n",
      "            # ndarray[Any, Any]]\"; expected \"Union[SupportsDunderLE,\n",
      "            # SupportsDunderGE, SupportsDunderGT, SupportsDunderLT]\"\n",
      "            op(left_distances, right_distances)  # type: ignore[arg-type]\n",
      "            | (right_indexer == -1),\n",
      "            left_indexer,\n",
      "            right_indexer,\n",
      "        )\n",
      "        if tolerance is not None:\n",
      "            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)\n",
      "        return indexer\n",
      "\n",
      "------------\n",
      "Method name: _get_reconciled_name_object\n",
      "Method definition:     def _get_reconciled_name_object(self, other):\n",
      "        \"\"\"\n",
      "        If the result of a set operation will be self,\n",
      "        return self, unless the name changes, in which\n",
      "        case make a shallow copy of self.\n",
      "        \"\"\"\n",
      "        name = get_op_result_name(self, other)\n",
      "        if self.name is not name:\n",
      "            return self.rename(name)\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: _get_string_slice\n",
      "Method definition:     def _get_string_slice(self, key: str_t):\n",
      "        # this is for partial string indexing,\n",
      "        # overridden in DatetimeIndex, TimedeltaIndex and PeriodIndex\n",
      "        raise NotImplementedError\n",
      "\n",
      "------------\n",
      "Method name: _get_values_for_loc\n",
      "Method definition:     def _get_values_for_loc(self, series: Series, loc, key):\n",
      "        \"\"\"\n",
      "        Do a positional lookup on the given Series, returning either a scalar\n",
      "        or a Series.\n",
      "\n",
      "        Assumes that `series.index is self`\n",
      "\n",
      "        key is included for MultiIndex compat.\n",
      "        \"\"\"\n",
      "        if is_integer(loc):\n",
      "            return series._values[loc]\n",
      "\n",
      "        return series.iloc[loc]\n",
      "\n",
      "------------\n",
      "Method name: _getitem_slice\n",
      "Method definition:     def _getitem_slice(self: _IndexT, slobj: slice) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Fastpath for __getitem__ when we know we have a slice.\n",
      "        \"\"\"\n",
      "        res = self._data[slobj]\n",
      "        return type(self)._simple_new(res, name=self._name)\n",
      "\n",
      "------------\n",
      "Method name: _inner_indexer\n",
      "Method definition:     @final\n",
      "    def _inner_indexer(\n",
      "        self: _IndexT, other: _IndexT\n",
      "    ) -> tuple[ArrayLike, npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
      "        # Caller is responsible for ensuring other.dtype == self.dtype\n",
      "        sv = self._get_engine_target()\n",
      "        ov = other._get_engine_target()\n",
      "        # can_use_libjoin assures sv and ov are ndarrays\n",
      "        sv = cast(np.ndarray, sv)\n",
      "        ov = cast(np.ndarray, ov)\n",
      "        joined_ndarray, lidx, ridx = libjoin.inner_join_indexer(sv, ov)\n",
      "        joined = self._from_join_target(joined_ndarray)\n",
      "        return joined, lidx, ridx\n",
      "\n",
      "------------\n",
      "Method name: _intersection\n",
      "Method definition:     def _intersection(self, other: Index, sort=False):\n",
      "        \"\"\"\n",
      "        intersection specialized to the case with matching dtypes.\n",
      "        \"\"\"\n",
      "        if (\n",
      "            self.is_monotonic_increasing\n",
      "            and other.is_monotonic_increasing\n",
      "            and self._can_use_libjoin\n",
      "        ):\n",
      "            try:\n",
      "                result = self._inner_indexer(other)[0]\n",
      "            except TypeError:\n",
      "                # non-comparable; should only be for object dtype\n",
      "                pass\n",
      "            else:\n",
      "                # TODO: algos.unique1d should preserve DTA/TDA\n",
      "                res = algos.unique1d(result)\n",
      "                return ensure_wrapped_if_datetimelike(res)\n",
      "\n",
      "        res_values = self._intersection_via_get_indexer(other, sort=sort)\n",
      "        res_values = _maybe_try_sort(res_values, sort)\n",
      "        return res_values\n",
      "\n",
      "------------\n",
      "Method name: _intersection_via_get_indexer\n",
      "Method definition:     @final\n",
      "    def _intersection_via_get_indexer(self, other: Index, sort) -> ArrayLike:\n",
      "        \"\"\"\n",
      "        Find the intersection of two Indexes using get_indexer.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        np.ndarray or ExtensionArray\n",
      "            The returned array will be unique.\n",
      "        \"\"\"\n",
      "        left_unique = self.unique()\n",
      "        right_unique = other.unique()\n",
      "\n",
      "        # even though we are unique, we need get_indexer_for for IntervalIndex\n",
      "        indexer = left_unique.get_indexer_for(right_unique)\n",
      "\n",
      "        mask = indexer != -1\n",
      "\n",
      "        taker = indexer.take(mask.nonzero()[0])\n",
      "        if sort is False:\n",
      "            # sort bc we want the elements in the same order they are in self\n",
      "            # unnecessary in the case with sort=None bc we will sort later\n",
      "            taker = np.sort(taker)\n",
      "\n",
      "        result = left_unique.take(taker)._values\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _invalid_indexer\n",
      "Method definition:     @final\n",
      "    def _invalid_indexer(self, form: str_t, key) -> TypeError:\n",
      "        \"\"\"\n",
      "        Consistent invalid indexer message.\n",
      "        \"\"\"\n",
      "        return TypeError(\n",
      "            f\"cannot do {form} indexing on {type(self).__name__} with these \"\n",
      "            f\"indexers [{key}] of type {type(key).__name__}\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: _is_comparable_dtype\n",
      "Method definition:     def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n",
      "        \"\"\"\n",
      "        Can we compare values of the given dtype to our own?\n",
      "        \"\"\"\n",
      "        if self.dtype.kind == \"b\":\n",
      "            return dtype.kind == \"b\"\n",
      "        elif is_numeric_dtype(self.dtype):\n",
      "            return is_numeric_dtype(dtype)\n",
      "        return True\n",
      "\n",
      "------------\n",
      "Method name: _is_memory_usage_qualified\n",
      "Method definition:     def _is_memory_usage_qualified(self) -> bool:\n",
      "        \"\"\"\n",
      "        Return a boolean if we need a qualified .info display.\n",
      "        \"\"\"\n",
      "        return self.is_object()\n",
      "\n",
      "------------\n",
      "Method name: _join_level\n",
      "Method definition:     @final\n",
      "    def _join_level(\n",
      "        self, other: Index, level, how: str_t = \"left\", keep_order: bool = True\n",
      "    ) -> tuple[MultiIndex, npt.NDArray[np.intp] | None, npt.NDArray[np.intp] | None]:\n",
      "        \"\"\"\n",
      "        The join method *only* affects the level of the resulting\n",
      "        MultiIndex. Otherwise it just exactly aligns the Index data to the\n",
      "        labels of the level in the MultiIndex.\n",
      "\n",
      "        If ```keep_order == True```, the order of the data indexed by the\n",
      "        MultiIndex will not be changed; otherwise, it will tie out\n",
      "        with `other`.\n",
      "        \"\"\"\n",
      "        from pandas.core.indexes.multi import MultiIndex\n",
      "\n",
      "        def _get_leaf_sorter(labels: list[np.ndarray]) -> npt.NDArray[np.intp]:\n",
      "            \"\"\"\n",
      "            Returns sorter for the inner most level while preserving the\n",
      "            order of higher levels.\n",
      "\n",
      "            Parameters\n",
      "            ----------\n",
      "            labels : list[np.ndarray]\n",
      "                Each ndarray has signed integer dtype, not necessarily identical.\n",
      "\n",
      "            Returns\n",
      "            -------\n",
      "            np.ndarray[np.intp]\n",
      "            \"\"\"\n",
      "            if labels[0].size == 0:\n",
      "                return np.empty(0, dtype=np.intp)\n",
      "\n",
      "            if len(labels) == 1:\n",
      "                return get_group_index_sorter(ensure_platform_int(labels[0]))\n",
      "\n",
      "            # find indexers of beginning of each set of\n",
      "            # same-key labels w.r.t all but last level\n",
      "            tic = labels[0][:-1] != labels[0][1:]\n",
      "            for lab in labels[1:-1]:\n",
      "                tic |= lab[:-1] != lab[1:]\n",
      "\n",
      "            starts = np.hstack(([True], tic, [True])).nonzero()[0]\n",
      "            lab = ensure_int64(labels[-1])\n",
      "            return lib.get_level_sorter(lab, ensure_platform_int(starts))\n",
      "\n",
      "        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n",
      "            raise TypeError(\"Join on level between two MultiIndex objects is ambiguous\")\n",
      "\n",
      "        left, right = self, other\n",
      "\n",
      "        flip_order = not isinstance(self, MultiIndex)\n",
      "        if flip_order:\n",
      "            left, right = right, left\n",
      "            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n",
      "\n",
      "        assert isinstance(left, MultiIndex)\n",
      "\n",
      "        level = left._get_level_number(level)\n",
      "        old_level = left.levels[level]\n",
      "\n",
      "        if not right.is_unique:\n",
      "            raise NotImplementedError(\n",
      "                \"Index._join_level on non-unique index is not implemented\"\n",
      "            )\n",
      "\n",
      "        new_level, left_lev_indexer, right_lev_indexer = old_level.join(\n",
      "            right, how=how, return_indexers=True\n",
      "        )\n",
      "\n",
      "        if left_lev_indexer is None:\n",
      "            if keep_order or len(left) == 0:\n",
      "                left_indexer = None\n",
      "                join_index = left\n",
      "            else:  # sort the leaves\n",
      "                left_indexer = _get_leaf_sorter(left.codes[: level + 1])\n",
      "                join_index = left[left_indexer]\n",
      "\n",
      "        else:\n",
      "            left_lev_indexer = ensure_platform_int(left_lev_indexer)\n",
      "            rev_indexer = lib.get_reverse_indexer(left_lev_indexer, len(old_level))\n",
      "            old_codes = left.codes[level]\n",
      "\n",
      "            taker = old_codes[old_codes != -1]\n",
      "            new_lev_codes = rev_indexer.take(taker)\n",
      "\n",
      "            new_codes = list(left.codes)\n",
      "            new_codes[level] = new_lev_codes\n",
      "\n",
      "            new_levels = list(left.levels)\n",
      "            new_levels[level] = new_level\n",
      "\n",
      "            if keep_order:  # just drop missing values. o.w. keep order\n",
      "                left_indexer = np.arange(len(left), dtype=np.intp)\n",
      "                left_indexer = cast(np.ndarray, left_indexer)\n",
      "                mask = new_lev_codes != -1\n",
      "                if not mask.all():\n",
      "                    new_codes = [lab[mask] for lab in new_codes]\n",
      "                    left_indexer = left_indexer[mask]\n",
      "\n",
      "            else:  # tie out the order with other\n",
      "                if level == 0:  # outer most level, take the fast route\n",
      "                    max_new_lev = 0 if len(new_lev_codes) == 0 else new_lev_codes.max()\n",
      "                    ngroups = 1 + max_new_lev\n",
      "                    left_indexer, counts = libalgos.groupsort_indexer(\n",
      "                        new_lev_codes, ngroups\n",
      "                    )\n",
      "\n",
      "                    # missing values are placed first; drop them!\n",
      "                    left_indexer = left_indexer[counts[0] :]\n",
      "                    new_codes = [lab[left_indexer] for lab in new_codes]\n",
      "\n",
      "                else:  # sort the leaves\n",
      "                    mask = new_lev_codes != -1\n",
      "                    mask_all = mask.all()\n",
      "                    if not mask_all:\n",
      "                        new_codes = [lab[mask] for lab in new_codes]\n",
      "\n",
      "                    left_indexer = _get_leaf_sorter(new_codes[: level + 1])\n",
      "                    new_codes = [lab[left_indexer] for lab in new_codes]\n",
      "\n",
      "                    # left_indexers are w.r.t masked frame.\n",
      "                    # reverse to original frame!\n",
      "                    if not mask_all:\n",
      "                        left_indexer = mask.nonzero()[0][left_indexer]\n",
      "\n",
      "            join_index = MultiIndex(\n",
      "                levels=new_levels,\n",
      "                codes=new_codes,\n",
      "                names=left.names,\n",
      "                verify_integrity=False,\n",
      "            )\n",
      "\n",
      "        if right_lev_indexer is not None:\n",
      "            right_indexer = right_lev_indexer.take(join_index.codes[level])\n",
      "        else:\n",
      "            right_indexer = join_index.codes[level]\n",
      "\n",
      "        if flip_order:\n",
      "            left_indexer, right_indexer = right_indexer, left_indexer\n",
      "\n",
      "        left_indexer = (\n",
      "            None if left_indexer is None else ensure_platform_int(left_indexer)\n",
      "        )\n",
      "        right_indexer = (\n",
      "            None if right_indexer is None else ensure_platform_int(right_indexer)\n",
      "        )\n",
      "        return join_index, left_indexer, right_indexer\n",
      "\n",
      "------------\n",
      "Method name: _join_monotonic\n",
      "Method definition:     @final\n",
      "    def _join_monotonic(\n",
      "        self, other: Index, how: str_t = \"left\"\n",
      "    ) -> tuple[Index, npt.NDArray[np.intp] | None, npt.NDArray[np.intp] | None]:\n",
      "        # We only get here with matching dtypes and both monotonic increasing\n",
      "        assert other.dtype == self.dtype\n",
      "\n",
      "        if self.equals(other):\n",
      "            ret_index = other if how == \"right\" else self\n",
      "            return ret_index, None, None\n",
      "\n",
      "        ridx: np.ndarray | None\n",
      "        lidx: np.ndarray | None\n",
      "\n",
      "        if self.is_unique and other.is_unique:\n",
      "            # We can perform much better than the general case\n",
      "            if how == \"left\":\n",
      "                join_index = self\n",
      "                lidx = None\n",
      "                ridx = self._left_indexer_unique(other)\n",
      "            elif how == \"right\":\n",
      "                join_index = other\n",
      "                lidx = other._left_indexer_unique(self)\n",
      "                ridx = None\n",
      "            elif how == \"inner\":\n",
      "                join_array, lidx, ridx = self._inner_indexer(other)\n",
      "                join_index = self._wrap_joined_index(join_array, other)\n",
      "            elif how == \"outer\":\n",
      "                join_array, lidx, ridx = self._outer_indexer(other)\n",
      "                join_index = self._wrap_joined_index(join_array, other)\n",
      "        else:\n",
      "            if how == \"left\":\n",
      "                join_array, lidx, ridx = self._left_indexer(other)\n",
      "            elif how == \"right\":\n",
      "                join_array, ridx, lidx = other._left_indexer(self)\n",
      "            elif how == \"inner\":\n",
      "                join_array, lidx, ridx = self._inner_indexer(other)\n",
      "            elif how == \"outer\":\n",
      "                join_array, lidx, ridx = self._outer_indexer(other)\n",
      "\n",
      "            join_index = self._wrap_joined_index(join_array, other)\n",
      "\n",
      "        lidx = None if lidx is None else ensure_platform_int(lidx)\n",
      "        ridx = None if ridx is None else ensure_platform_int(ridx)\n",
      "        return join_index, lidx, ridx\n",
      "\n",
      "------------\n",
      "Method name: _join_multi\n",
      "Method definition:     @final\n",
      "    def _join_multi(self, other: Index, how: str_t):\n",
      "        from pandas.core.indexes.multi import MultiIndex\n",
      "        from pandas.core.reshape.merge import restore_dropped_levels_multijoin\n",
      "\n",
      "        # figure out join names\n",
      "        self_names_list = list(com.not_none(*self.names))\n",
      "        other_names_list = list(com.not_none(*other.names))\n",
      "        self_names_order = self_names_list.index\n",
      "        other_names_order = other_names_list.index\n",
      "        self_names = set(self_names_list)\n",
      "        other_names = set(other_names_list)\n",
      "        overlap = self_names & other_names\n",
      "\n",
      "        # need at least 1 in common\n",
      "        if not overlap:\n",
      "            raise ValueError(\"cannot join with no overlapping index names\")\n",
      "\n",
      "        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n",
      "\n",
      "            # Drop the non-matching levels from left and right respectively\n",
      "            ldrop_names = sorted(self_names - overlap, key=self_names_order)\n",
      "            rdrop_names = sorted(other_names - overlap, key=other_names_order)\n",
      "\n",
      "            # if only the order differs\n",
      "            if not len(ldrop_names + rdrop_names):\n",
      "                self_jnlevels = self\n",
      "                other_jnlevels = other.reorder_levels(self.names)\n",
      "            else:\n",
      "                self_jnlevels = self.droplevel(ldrop_names)\n",
      "                other_jnlevels = other.droplevel(rdrop_names)\n",
      "\n",
      "            # Join left and right\n",
      "            # Join on same leveled multi-index frames is supported\n",
      "            join_idx, lidx, ridx = self_jnlevels.join(\n",
      "                other_jnlevels, how=how, return_indexers=True\n",
      "            )\n",
      "\n",
      "            # Restore the dropped levels\n",
      "            # Returned index level order is\n",
      "            # common levels, ldrop_names, rdrop_names\n",
      "            dropped_names = ldrop_names + rdrop_names\n",
      "\n",
      "            # error: Argument 5/6 to \"restore_dropped_levels_multijoin\" has\n",
      "            # incompatible type \"Optional[ndarray[Any, dtype[signedinteger[Any\n",
      "            # ]]]]\"; expected \"ndarray[Any, dtype[signedinteger[Any]]]\"\n",
      "            levels, codes, names = restore_dropped_levels_multijoin(\n",
      "                self,\n",
      "                other,\n",
      "                dropped_names,\n",
      "                join_idx,\n",
      "                lidx,  # type: ignore[arg-type]\n",
      "                ridx,  # type: ignore[arg-type]\n",
      "            )\n",
      "\n",
      "            # Re-create the multi-index\n",
      "            multi_join_idx = MultiIndex(\n",
      "                levels=levels, codes=codes, names=names, verify_integrity=False\n",
      "            )\n",
      "\n",
      "            multi_join_idx = multi_join_idx.remove_unused_levels()\n",
      "\n",
      "            return multi_join_idx, lidx, ridx\n",
      "\n",
      "        jl = list(overlap)[0]\n",
      "\n",
      "        # Case where only one index is multi\n",
      "        # make the indices into mi's that match\n",
      "        flip_order = False\n",
      "        if isinstance(self, MultiIndex):\n",
      "            self, other = other, self\n",
      "            flip_order = True\n",
      "            # flip if join method is right or left\n",
      "            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n",
      "\n",
      "        level = other.names.index(jl)\n",
      "        result = self._join_level(other, level, how=how)\n",
      "\n",
      "        if flip_order:\n",
      "            return result[0], result[2], result[1]\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _join_non_unique\n",
      "Method definition:     @final\n",
      "    def _join_non_unique(\n",
      "        self, other: Index, how: str_t = \"left\"\n",
      "    ) -> tuple[Index, npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
      "        from pandas.core.reshape.merge import get_join_indexers\n",
      "\n",
      "        # We only get here if dtypes match\n",
      "        assert self.dtype == other.dtype\n",
      "\n",
      "        left_idx, right_idx = get_join_indexers(\n",
      "            [self._values], [other._values], how=how, sort=True\n",
      "        )\n",
      "        mask = left_idx == -1\n",
      "\n",
      "        join_array = self._values.take(left_idx)\n",
      "        right = other._values.take(right_idx)\n",
      "\n",
      "        if isinstance(join_array, np.ndarray):\n",
      "            # error: Argument 3 to \"putmask\" has incompatible type\n",
      "            # \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected\n",
      "            # \"Union[_SupportsArray[dtype[Any]], _NestedSequence[\n",
      "            # _SupportsArray[dtype[Any]]], bool, int, float, complex,\n",
      "            # str, bytes, _NestedSequence[Union[bool, int, float,\n",
      "            # complex, str, bytes]]]\"\n",
      "            np.putmask(join_array, mask, right)  # type: ignore[arg-type]\n",
      "        else:\n",
      "            join_array._putmask(mask, right)\n",
      "\n",
      "        join_index = self._wrap_joined_index(join_array, other)\n",
      "\n",
      "        return join_index, left_idx, right_idx\n",
      "\n",
      "------------\n",
      "Method name: _join_via_get_indexer\n",
      "Method definition:     @final\n",
      "    def _join_via_get_indexer(\n",
      "        self, other: Index, how: str_t, sort: bool\n",
      "    ) -> tuple[Index, npt.NDArray[np.intp] | None, npt.NDArray[np.intp] | None]:\n",
      "        # Fallback if we do not have any fastpaths available based on\n",
      "        #  uniqueness/monotonicity\n",
      "\n",
      "        # Note: at this point we have checked matching dtypes\n",
      "\n",
      "        if how == \"left\":\n",
      "            join_index = self\n",
      "        elif how == \"right\":\n",
      "            join_index = other\n",
      "        elif how == \"inner\":\n",
      "            # TODO: sort=False here for backwards compat. It may\n",
      "            # be better to use the sort parameter passed into join\n",
      "            join_index = self.intersection(other, sort=False)\n",
      "        elif how == \"outer\":\n",
      "            # TODO: sort=True here for backwards compat. It may\n",
      "            # be better to use the sort parameter passed into join\n",
      "            join_index = self.union(other)\n",
      "\n",
      "        if sort:\n",
      "            join_index = join_index.sort_values()\n",
      "\n",
      "        if join_index is self:\n",
      "            lindexer = None\n",
      "        else:\n",
      "            lindexer = self.get_indexer_for(join_index)\n",
      "        if join_index is other:\n",
      "            rindexer = None\n",
      "        else:\n",
      "            rindexer = other.get_indexer_for(join_index)\n",
      "        return join_index, lindexer, rindexer\n",
      "\n",
      "------------\n",
      "Method name: _left_indexer\n",
      "Method definition:     @final\n",
      "    def _left_indexer(\n",
      "        self: _IndexT, other: _IndexT\n",
      "    ) -> tuple[ArrayLike, npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
      "        # Caller is responsible for ensuring other.dtype == self.dtype\n",
      "        sv = self._get_engine_target()\n",
      "        ov = other._get_engine_target()\n",
      "        # can_use_libjoin assures sv and ov are ndarrays\n",
      "        sv = cast(np.ndarray, sv)\n",
      "        ov = cast(np.ndarray, ov)\n",
      "        joined_ndarray, lidx, ridx = libjoin.left_join_indexer(sv, ov)\n",
      "        joined = self._from_join_target(joined_ndarray)\n",
      "        return joined, lidx, ridx\n",
      "\n",
      "------------\n",
      "Method name: _left_indexer_unique\n",
      "Method definition:     @final\n",
      "    def _left_indexer_unique(self: _IndexT, other: _IndexT) -> npt.NDArray[np.intp]:\n",
      "        # Caller is responsible for ensuring other.dtype == self.dtype\n",
      "        sv = self._get_engine_target()\n",
      "        ov = other._get_engine_target()\n",
      "        # can_use_libjoin assures sv and ov are ndarrays\n",
      "        sv = cast(np.ndarray, sv)\n",
      "        ov = cast(np.ndarray, ov)\n",
      "        return libjoin.left_join_indexer_unique(sv, ov)\n",
      "\n",
      "------------\n",
      "Method name: _logical_method\n",
      "Method definition:     def _logical_method(self, other, op):\n",
      "        return NotImplemented\n",
      "\n",
      "------------\n",
      "Method name: _map_values\n",
      "Method definition:     @final\n",
      "    def _map_values(self, mapper, na_action=None):\n",
      "        \"\"\"\n",
      "        An internal function that maps values using the input\n",
      "        correspondence (which can be a dict, Series, or function).\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        mapper : function, dict, or Series\n",
      "            The input correspondence object\n",
      "        na_action : {None, 'ignore'}\n",
      "            If 'ignore', propagate NA values, without passing them to the\n",
      "            mapping function\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Union[Index, MultiIndex], inferred\n",
      "            The output of the mapping function applied to the index.\n",
      "            If the function returns a tuple with more than one element\n",
      "            a MultiIndex will be returned.\n",
      "        \"\"\"\n",
      "        # we can fastpath dict/Series to an efficient map\n",
      "        # as we know that we are not going to have to yield\n",
      "        # python types\n",
      "        if is_dict_like(mapper):\n",
      "            if isinstance(mapper, dict) and hasattr(mapper, \"__missing__\"):\n",
      "                # If a dictionary subclass defines a default value method,\n",
      "                # convert mapper to a lookup function (GH #15999).\n",
      "                dict_with_default = mapper\n",
      "                mapper = lambda x: dict_with_default[x]\n",
      "            else:\n",
      "                # Dictionary does not have a default. Thus it's safe to\n",
      "                # convert to an Series for efficiency.\n",
      "                # we specify the keys here to handle the\n",
      "                # possibility that they are tuples\n",
      "\n",
      "                # The return value of mapping with an empty mapper is\n",
      "                # expected to be pd.Series(np.nan, ...). As np.nan is\n",
      "                # of dtype float64 the return value of this method should\n",
      "                # be float64 as well\n",
      "                mapper = create_series_with_explicit_dtype(\n",
      "                    mapper, dtype_if_empty=np.float64\n",
      "                )\n",
      "\n",
      "        if isinstance(mapper, ABCSeries):\n",
      "            if na_action not in (None, \"ignore\"):\n",
      "                msg = (\n",
      "                    \"na_action must either be 'ignore' or None, \"\n",
      "                    f\"{na_action} was passed\"\n",
      "                )\n",
      "                raise ValueError(msg)\n",
      "\n",
      "            if na_action == \"ignore\":\n",
      "                mapper = mapper[mapper.index.notna()]\n",
      "\n",
      "            # Since values were input this means we came from either\n",
      "            # a dict or a series and mapper should be an index\n",
      "            if is_categorical_dtype(self.dtype):\n",
      "                # use the built in categorical series mapper which saves\n",
      "                # time by mapping the categories instead of all values\n",
      "\n",
      "                cat = cast(\"Categorical\", self._values)\n",
      "                return cat.map(mapper)\n",
      "\n",
      "            values = self._values\n",
      "\n",
      "            indexer = mapper.index.get_indexer(values)\n",
      "            new_values = algorithms.take_nd(mapper._values, indexer)\n",
      "\n",
      "            return new_values\n",
      "\n",
      "        # we must convert to python types\n",
      "        if is_extension_array_dtype(self.dtype) and hasattr(self._values, \"map\"):\n",
      "            # GH#23179 some EAs do not have `map`\n",
      "            values = self._values\n",
      "            if na_action is not None:\n",
      "                raise NotImplementedError\n",
      "            map_f = lambda values, f: values.map(f)\n",
      "        else:\n",
      "            values = self._values.astype(object)\n",
      "            if na_action == \"ignore\":\n",
      "                map_f = lambda values, f: lib.map_infer_mask(\n",
      "                    values, f, isna(values).view(np.uint8)\n",
      "                )\n",
      "            elif na_action is None:\n",
      "                map_f = lib.map_infer\n",
      "            else:\n",
      "                msg = (\n",
      "                    \"na_action must either be 'ignore' or None, \"\n",
      "                    f\"{na_action} was passed\"\n",
      "                )\n",
      "                raise ValueError(msg)\n",
      "\n",
      "        # mapper is a function\n",
      "        new_values = map_f(values, mapper)\n",
      "\n",
      "        return new_values\n",
      "\n",
      "------------\n",
      "Method name: _maybe_cast_indexer\n",
      "Method definition:     def _maybe_cast_indexer(self, key):\n",
      "        \"\"\"\n",
      "        If we have a float key and are not a floating index, then try to cast\n",
      "        to an int if equivalent.\n",
      "        \"\"\"\n",
      "        return key\n",
      "\n",
      "------------\n",
      "Method name: _maybe_cast_listlike_indexer\n",
      "Method definition:     def _maybe_cast_listlike_indexer(self, target) -> Index:\n",
      "        \"\"\"\n",
      "        Analogue to maybe_cast_indexer for get_indexer instead of get_loc.\n",
      "        \"\"\"\n",
      "        return ensure_index(target)\n",
      "\n",
      "------------\n",
      "Method name: _maybe_cast_slice_bound\n",
      "Method definition:     def _maybe_cast_slice_bound(self, label, side: str_t, kind=no_default):\n",
      "        \"\"\"\n",
      "        This function should be overloaded in subclasses that allow non-trivial\n",
      "        casting on label-slice bounds, e.g. datetime-like indices allowing\n",
      "        strings containing formatted datetimes.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        label : object\n",
      "        side : {'left', 'right'}\n",
      "        kind : {'loc', 'getitem'} or None\n",
      "\n",
      "            .. deprecated:: 1.3.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        label : object\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Value of `side` parameter should be validated in caller.\n",
      "        \"\"\"\n",
      "        assert kind in [\"loc\", \"getitem\", None, no_default]\n",
      "        self._deprecated_arg(kind, \"kind\", \"_maybe_cast_slice_bound\")\n",
      "\n",
      "        # We are a plain index here (sub-class override this method if they\n",
      "        # wish to have special treatment for floats/ints, e.g. Float64Index and\n",
      "        # datetimelike Indexes\n",
      "        # reject them, if index does not contain label\n",
      "        if (is_float(label) or is_integer(label)) and label not in self:\n",
      "            raise self._invalid_indexer(\"slice\", label)\n",
      "\n",
      "        return label\n",
      "\n",
      "------------\n",
      "Method name: _maybe_check_unique\n",
      "Method definition:     @final\n",
      "    def _maybe_check_unique(self) -> None:\n",
      "        \"\"\"\n",
      "        Check that an Index has no duplicates.\n",
      "\n",
      "        This is typically only called via\n",
      "        `NDFrame.flags.allows_duplicate_labels.setter` when it's set to\n",
      "        True (duplicates aren't allowed).\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        DuplicateLabelError\n",
      "            When the index is not unique.\n",
      "        \"\"\"\n",
      "        if not self.is_unique:\n",
      "            msg = \"\"\"Index has duplicates.\"\"\"\n",
      "            duplicates = self._format_duplicate_message()\n",
      "            msg += f\"\\n{duplicates}\"\n",
      "\n",
      "            raise DuplicateLabelError(msg)\n",
      "\n",
      "------------\n",
      "Method name: _maybe_disable_logical_methods\n",
      "Method definition:     @final\n",
      "    def _maybe_disable_logical_methods(self, opname: str_t) -> None:\n",
      "        \"\"\"\n",
      "        raise if this Index subclass does not support any or all.\n",
      "        \"\"\"\n",
      "        if (\n",
      "            isinstance(self, ABCMultiIndex)\n",
      "            or needs_i8_conversion(self.dtype)\n",
      "            or is_interval_dtype(self.dtype)\n",
      "            or is_categorical_dtype(self.dtype)\n",
      "            or is_float_dtype(self.dtype)\n",
      "        ):\n",
      "            # This call will raise\n",
      "            make_invalid_op(opname)(self)\n",
      "\n",
      "------------\n",
      "Method name: _maybe_disallow_fill\n",
      "Method definition:     @final\n",
      "    def _maybe_disallow_fill(self, allow_fill: bool, fill_value, indices) -> bool:\n",
      "        \"\"\"\n",
      "        We only use pandas-style take when allow_fill is True _and_\n",
      "        fill_value is not None.\n",
      "        \"\"\"\n",
      "        if allow_fill and fill_value is not None:\n",
      "            # only fill if we are passing a non-None fill_value\n",
      "            if self._can_hold_na:\n",
      "                if (indices < -1).any():\n",
      "                    raise ValueError(\n",
      "                        \"When allow_fill=True and fill_value is not None, \"\n",
      "                        \"all indices must be >= -1\"\n",
      "                    )\n",
      "            else:\n",
      "                cls_name = type(self).__name__\n",
      "                raise ValueError(\n",
      "                    f\"Unable to fill values because {cls_name} cannot contain NA\"\n",
      "                )\n",
      "        else:\n",
      "            allow_fill = False\n",
      "        return allow_fill\n",
      "\n",
      "------------\n",
      "Method name: _maybe_preserve_names\n",
      "Method definition:     def _maybe_preserve_names(self, target: Index, preserve_names: bool):\n",
      "        if preserve_names and target.nlevels == 1 and target.name != self.name:\n",
      "            target = target.copy(deep=False)\n",
      "            target.name = self.name\n",
      "        return target\n",
      "\n",
      "------------\n",
      "Method name: _maybe_promote\n",
      "Method definition:     @final\n",
      "    def _maybe_promote(self, other: Index) -> tuple[Index, Index]:\n",
      "        \"\"\"\n",
      "        When dealing with an object-dtype Index and a non-object Index, see\n",
      "        if we can upcast the object-dtype one to improve performance.\n",
      "        \"\"\"\n",
      "\n",
      "        if isinstance(self, ABCDatetimeIndex) and isinstance(other, ABCDatetimeIndex):\n",
      "            if (\n",
      "                self.tz is not None\n",
      "                and other.tz is not None\n",
      "                and not tz_compare(self.tz, other.tz)\n",
      "            ):\n",
      "                # standardize on UTC\n",
      "                return self.tz_convert(\"UTC\"), other.tz_convert(\"UTC\")\n",
      "\n",
      "        elif self.inferred_type == \"date\" and isinstance(other, ABCDatetimeIndex):\n",
      "            try:\n",
      "                return type(other)(self), other\n",
      "            except OutOfBoundsDatetime:\n",
      "                return self, other\n",
      "        elif self.inferred_type == \"timedelta\" and isinstance(other, ABCTimedeltaIndex):\n",
      "            # TODO: we dont have tests that get here\n",
      "            return type(other)(self), other\n",
      "\n",
      "        elif self.dtype.kind == \"u\" and other.dtype.kind == \"i\":\n",
      "            # GH#41873\n",
      "            if other.min() >= 0:\n",
      "                # lookup min as it may be cached\n",
      "                # TODO: may need itemsize check if we have non-64-bit Indexes\n",
      "                return self, other.astype(self.dtype)\n",
      "\n",
      "        elif self._is_multi and not other._is_multi:\n",
      "            try:\n",
      "                # \"Type[Index]\" has no attribute \"from_tuples\"\n",
      "                other = type(self).from_tuples(other)  # type: ignore[attr-defined]\n",
      "            except (TypeError, ValueError):\n",
      "                # let's instead try with a straight Index\n",
      "                self = Index(self._values)\n",
      "\n",
      "        if not is_object_dtype(self.dtype) and is_object_dtype(other.dtype):\n",
      "            # Reverse op so we dont need to re-implement on the subclasses\n",
      "            other, self = other._maybe_promote(self)\n",
      "\n",
      "        return self, other\n",
      "\n",
      "------------\n",
      "Method name: _memory_usage\n",
      "Method definition:     def _memory_usage(self, deep: bool = False) -> int:\n",
      "        \"\"\"\n",
      "        Memory usage of the values.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        deep : bool, default False\n",
      "            Introspect the data deeply, interrogate\n",
      "            `object` dtypes for system-level memory consumption.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bytes used\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n",
      "            array.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Memory usage does not include memory consumed by elements that\n",
      "        are not components of the array if deep=False or if used on PyPy\n",
      "        \"\"\"\n",
      "        if hasattr(self.array, \"memory_usage\"):\n",
      "            # https://github.com/python/mypy/issues/1424\n",
      "            # error: \"ExtensionArray\" has no attribute \"memory_usage\"\n",
      "            return self.array.memory_usage(deep=deep)  # type: ignore[attr-defined]\n",
      "\n",
      "        v = self.array.nbytes\n",
      "        if deep and is_object_dtype(self) and not PYPY:\n",
      "            values = cast(np.ndarray, self._values)\n",
      "            v += lib.memory_usage_of_objects(values)\n",
      "        return v\n",
      "\n",
      "------------\n",
      "Method name: _mpl_repr\n",
      "Method definition:     @final\n",
      "    def _mpl_repr(self) -> np.ndarray:\n",
      "        # how to represent ourselves to matplotlib\n",
      "        if isinstance(self.dtype, np.dtype) and self.dtype.kind != \"M\":\n",
      "            return cast(np.ndarray, self.values)\n",
      "        return self.astype(object, copy=False)._values\n",
      "\n",
      "------------\n",
      "Method name: _outer_indexer\n",
      "Method definition:     @final\n",
      "    def _outer_indexer(\n",
      "        self: _IndexT, other: _IndexT\n",
      "    ) -> tuple[ArrayLike, npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
      "        # Caller is responsible for ensuring other.dtype == self.dtype\n",
      "        sv = self._get_engine_target()\n",
      "        ov = other._get_engine_target()\n",
      "        # can_use_libjoin assures sv and ov are ndarrays\n",
      "        sv = cast(np.ndarray, sv)\n",
      "        ov = cast(np.ndarray, ov)\n",
      "        joined_ndarray, lidx, ridx = libjoin.outer_join_indexer(sv, ov)\n",
      "        joined = self._from_join_target(joined_ndarray)\n",
      "        return joined, lidx, ridx\n",
      "\n",
      "------------\n",
      "Method name: _raise_if_missing\n",
      "Method definition:     def _raise_if_missing(self, key, indexer, axis_name: str_t) -> None:\n",
      "        \"\"\"\n",
      "        Check that indexer can be used to return a result.\n",
      "\n",
      "        e.g. at least one element was found,\n",
      "        unless the list of keys was actually empty.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        key : list-like\n",
      "            Targeted labels (only used to show correct error message).\n",
      "        indexer: array-like of booleans\n",
      "            Indices corresponding to the key,\n",
      "            (with -1 indicating not found).\n",
      "        axis_name : str\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        KeyError\n",
      "            If at least one key was requested but none was found.\n",
      "        \"\"\"\n",
      "        if len(key) == 0:\n",
      "            return\n",
      "\n",
      "        # Count missing values\n",
      "        missing_mask = indexer < 0\n",
      "        nmissing = missing_mask.sum()\n",
      "\n",
      "        if nmissing:\n",
      "\n",
      "            # TODO: remove special-case; this is just to keep exception\n",
      "            #  message tests from raising while debugging\n",
      "            use_interval_msg = is_interval_dtype(self.dtype) or (\n",
      "                is_categorical_dtype(self.dtype)\n",
      "                # \"Index\" has no attribute \"categories\"  [attr-defined]\n",
      "                and is_interval_dtype(\n",
      "                    self.categories.dtype  # type: ignore[attr-defined]\n",
      "                )\n",
      "            )\n",
      "\n",
      "            if nmissing == len(indexer):\n",
      "                if use_interval_msg:\n",
      "                    key = list(key)\n",
      "                raise KeyError(f\"None of [{key}] are in the [{axis_name}]\")\n",
      "\n",
      "            not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())\n",
      "            raise KeyError(f\"{not_found} not in index\")\n",
      "\n",
      "------------\n",
      "Method name: _reduce\n",
      "Method definition:     def _reduce(\n",
      "        self,\n",
      "        op,\n",
      "        name: str,\n",
      "        *,\n",
      "        axis=0,\n",
      "        skipna=True,\n",
      "        numeric_only=None,\n",
      "        filter_type=None,\n",
      "        **kwds,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Perform the reduction type operation if we can.\n",
      "        \"\"\"\n",
      "        func = getattr(self, name, None)\n",
      "        if func is None:\n",
      "            raise TypeError(\n",
      "                f\"{type(self).__name__} cannot perform the operation {name}\"\n",
      "            )\n",
      "        return func(skipna=skipna, **kwds)\n",
      "\n",
      "------------\n",
      "Method name: _reindex_non_unique\n",
      "Method definition:     @final\n",
      "    def _reindex_non_unique(\n",
      "        self, target: Index\n",
      "    ) -> tuple[Index, npt.NDArray[np.intp], npt.NDArray[np.intp] | None]:\n",
      "        \"\"\"\n",
      "        Create a new index with target's values (move/add/delete values as\n",
      "        necessary) use with non-unique Index and a possibly non-unique target.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        target : an iterable\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        new_index : pd.Index\n",
      "            Resulting index.\n",
      "        indexer : np.ndarray[np.intp]\n",
      "            Indices of output values in original index.\n",
      "        new_indexer : np.ndarray[np.intp] or None\n",
      "\n",
      "        \"\"\"\n",
      "        target = ensure_index(target)\n",
      "        if len(target) == 0:\n",
      "            # GH#13691\n",
      "            return self[:0], np.array([], dtype=np.intp), None\n",
      "\n",
      "        indexer, missing = self.get_indexer_non_unique(target)\n",
      "        check = indexer != -1\n",
      "        new_labels = self.take(indexer[check])\n",
      "        new_indexer = None\n",
      "\n",
      "        if len(missing):\n",
      "            length = np.arange(len(indexer), dtype=np.intp)\n",
      "\n",
      "            missing = ensure_platform_int(missing)\n",
      "            missing_labels = target.take(missing)\n",
      "            missing_indexer = length[~check]\n",
      "            cur_labels = self.take(indexer[check]).values\n",
      "            cur_indexer = length[check]\n",
      "\n",
      "            # Index constructor below will do inference\n",
      "            new_labels = np.empty((len(indexer),), dtype=object)\n",
      "            new_labels[cur_indexer] = cur_labels\n",
      "            new_labels[missing_indexer] = missing_labels\n",
      "\n",
      "            # GH#38906\n",
      "            if not len(self):\n",
      "\n",
      "                new_indexer = np.arange(0, dtype=np.intp)\n",
      "\n",
      "            # a unique indexer\n",
      "            elif target.is_unique:\n",
      "\n",
      "                # see GH5553, make sure we use the right indexer\n",
      "                new_indexer = np.arange(len(indexer), dtype=np.intp)\n",
      "                new_indexer[cur_indexer] = np.arange(len(cur_labels))\n",
      "                new_indexer[missing_indexer] = -1\n",
      "\n",
      "            # we have a non_unique selector, need to use the original\n",
      "            # indexer here\n",
      "            else:\n",
      "\n",
      "                # need to retake to have the same size as the indexer\n",
      "                indexer[~check] = -1\n",
      "\n",
      "                # reset the new indexer to account for the new size\n",
      "                new_indexer = np.arange(len(self.take(indexer)), dtype=np.intp)\n",
      "                new_indexer[~check] = -1\n",
      "\n",
      "        if isinstance(self, ABCMultiIndex):\n",
      "            new_index = type(self).from_tuples(new_labels, names=self.names)\n",
      "        else:\n",
      "            new_index = Index._with_infer(new_labels, name=self.name)\n",
      "        return new_index, indexer, new_indexer\n",
      "\n",
      "------------\n",
      "Method name: _rename\n",
      "Method definition:     @final\n",
      "    def _rename(self: _IndexT, name: Hashable) -> _IndexT:\n",
      "        \"\"\"\n",
      "        fastpath for rename if new name is already validated.\n",
      "        \"\"\"\n",
      "        result = self._view()\n",
      "        result._name = name\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _require_scalar\n",
      "Method definition:     @final\n",
      "    def _require_scalar(self, value):\n",
      "        \"\"\"\n",
      "        Check that this is a scalar value that we can use for setitem-like\n",
      "        operations without changing dtype.\n",
      "        \"\"\"\n",
      "        if not is_scalar(value):\n",
      "            raise TypeError(f\"'value' must be a scalar, passed: {type(value).__name__}\")\n",
      "        return value\n",
      "\n",
      "------------\n",
      "Method name: _reset_cache\n",
      "Method definition:     def _reset_cache(self, key: str | None = None) -> None:\n",
      "        \"\"\"\n",
      "        Reset cached properties. If ``key`` is passed, only clears that key.\n",
      "        \"\"\"\n",
      "        if not hasattr(self, \"_cache\"):\n",
      "            return\n",
      "        if key is None:\n",
      "            self._cache.clear()\n",
      "        else:\n",
      "            self._cache.pop(key, None)\n",
      "\n",
      "------------\n",
      "Method name: _reset_identity\n",
      "Method definition:     @final\n",
      "    def _reset_identity(self) -> None:\n",
      "        \"\"\"\n",
      "        Initializes or resets ``_id`` attribute with new object.\n",
      "        \"\"\"\n",
      "        self._id = object()\n",
      "\n",
      "------------\n",
      "Method name: _searchsorted_monotonic\n",
      "Method definition:     def _searchsorted_monotonic(self, label, side: Literal[\"left\", \"right\"] = \"left\"):\n",
      "        if self.is_monotonic_increasing:\n",
      "            return self.searchsorted(label, side=side)\n",
      "        elif self.is_monotonic_decreasing:\n",
      "            # np.searchsorted expects ascending sort order, have to reverse\n",
      "            # everything for it to work (element ordering, search side and\n",
      "            # resulting value).\n",
      "            pos = self[::-1].searchsorted(\n",
      "                label, side=\"right\" if side == \"left\" else \"left\"\n",
      "            )\n",
      "            return len(self) - pos\n",
      "\n",
      "        raise ValueError(\"index must be monotonic increasing or decreasing\")\n",
      "\n",
      "------------\n",
      "Method name: _set_names\n",
      "Method definition:     def _set_names(self, values, *, level=None) -> None:\n",
      "        \"\"\"\n",
      "        Set new names on index. Each name has to be a hashable type.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        values : str or sequence\n",
      "            name(s) to set\n",
      "        level : int, level name, or sequence of int/level names (default None)\n",
      "            If the index is a MultiIndex (hierarchical), level(s) to set (None\n",
      "            for all levels).  Otherwise level must be None\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError if each name is not hashable.\n",
      "        \"\"\"\n",
      "        if not is_list_like(values):\n",
      "            raise ValueError(\"Names must be a list-like\")\n",
      "        if len(values) != 1:\n",
      "            raise ValueError(f\"Length of new names must be 1, got {len(values)}\")\n",
      "\n",
      "        # GH 20527\n",
      "        # All items in 'name' need to be hashable:\n",
      "        validate_all_hashable(*values, error_name=f\"{type(self).__name__}.name\")\n",
      "\n",
      "        self._name = values[0]\n",
      "\n",
      "------------\n",
      "Method name: _shallow_copy\n",
      "Method definition:     def _shallow_copy(self: _IndexT, values, name: Hashable = no_default) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Create a new Index with the same class as the caller, don't copy the\n",
      "        data, use the same object attributes with passed in attributes taking\n",
      "        precedence.\n",
      "\n",
      "        *this is an internal non-public method*\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        values : the values to create the new Index, optional\n",
      "        name : Label, defaults to self.name\n",
      "        \"\"\"\n",
      "        name = self._name if name is no_default else name\n",
      "\n",
      "        return self._simple_new(values, name=name)\n",
      "\n",
      "------------\n",
      "Method name: _should_compare\n",
      "Method definition:     @final\n",
      "    def _should_compare(self, other: Index) -> bool:\n",
      "        \"\"\"\n",
      "        Check if `self == other` can ever have non-False entries.\n",
      "        \"\"\"\n",
      "\n",
      "        if (other.is_boolean() and self.is_numeric()) or (\n",
      "            self.is_boolean() and other.is_numeric()\n",
      "        ):\n",
      "            # GH#16877 Treat boolean labels passed to a numeric index as not\n",
      "            #  found. Without this fix False and True would be treated as 0 and 1\n",
      "            #  respectively.\n",
      "            return False\n",
      "\n",
      "        other = unpack_nested_dtype(other)\n",
      "        dtype = other.dtype\n",
      "        return self._is_comparable_dtype(dtype) or is_object_dtype(dtype)\n",
      "\n",
      "------------\n",
      "Method name: _should_partial_index\n",
      "Method definition:     @final\n",
      "    def _should_partial_index(self, target: Index) -> bool:\n",
      "        \"\"\"\n",
      "        Should we attempt partial-matching indexing?\n",
      "        \"\"\"\n",
      "        if is_interval_dtype(self.dtype):\n",
      "            if is_interval_dtype(target.dtype):\n",
      "                return False\n",
      "            # See https://github.com/pandas-dev/pandas/issues/47772 the commented\n",
      "            # out code can be restored (instead of hardcoding `return True`)\n",
      "            # once that issue if fixed\n",
      "            # \"Index\" has no attribute \"left\"\n",
      "            # return self.left._should_compare(target)  # type: ignore[attr-defined]\n",
      "            return True\n",
      "        return False\n",
      "\n",
      "------------\n",
      "Method name: _sort_levels_monotonic\n",
      "Method definition:     def _sort_levels_monotonic(self: _IndexT) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Compat with MultiIndex.\n",
      "        \"\"\"\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: _summary\n",
      "Method definition:     def _summary(self, name=None) -> str_t:\n",
      "        \"\"\"\n",
      "        Return a summarized representation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        name : str\n",
      "            name to use in the summary representation\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        String with a summarized representation of the index\n",
      "        \"\"\"\n",
      "        if len(self) > 0:\n",
      "            head = self[0]\n",
      "            if hasattr(head, \"format\") and not isinstance(head, str):\n",
      "                head = head.format()\n",
      "            elif needs_i8_conversion(self.dtype):\n",
      "                # e.g. Timedelta, display as values, not quoted\n",
      "                head = self._formatter_func(head).replace(\"'\", \"\")\n",
      "            tail = self[-1]\n",
      "            if hasattr(tail, \"format\") and not isinstance(tail, str):\n",
      "                tail = tail.format()\n",
      "            elif needs_i8_conversion(self.dtype):\n",
      "                # e.g. Timedelta, display as values, not quoted\n",
      "                tail = self._formatter_func(tail).replace(\"'\", \"\")\n",
      "\n",
      "            index_summary = f\", {head} to {tail}\"\n",
      "        else:\n",
      "            index_summary = \"\"\n",
      "\n",
      "        if name is None:\n",
      "            name = type(self).__name__\n",
      "        return f\"{name}: {len(self)} entries{index_summary}\"\n",
      "\n",
      "------------\n",
      "Method name: _transform_index\n",
      "Method definition:     @final\n",
      "    def _transform_index(self, func, *, level=None) -> Index:\n",
      "        \"\"\"\n",
      "        Apply function to all values found in index.\n",
      "\n",
      "        This includes transforming multiindex entries separately.\n",
      "        Only apply function to one level of the MultiIndex if level is specified.\n",
      "        \"\"\"\n",
      "        if isinstance(self, ABCMultiIndex):\n",
      "            if level is not None:\n",
      "                # Caller is responsible for ensuring level is positional.\n",
      "                items = [\n",
      "                    tuple(func(y) if i == level else y for i, y in enumerate(x))\n",
      "                    for x in self\n",
      "                ]\n",
      "            else:\n",
      "                items = [tuple(func(y) for y in x) for x in self]\n",
      "            return type(self).from_tuples(items, names=self.names)\n",
      "        else:\n",
      "            items = [func(x) for x in self]\n",
      "            return Index(items, name=self.name, tupleize_cols=False)\n",
      "\n",
      "------------\n",
      "Method name: _unary_method\n",
      "Method definition:     @final\n",
      "    def _unary_method(self, op):\n",
      "        result = op(self._values)\n",
      "        return Index(result, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: _union\n",
      "Method definition:     def _union(self, other: Index, sort):\n",
      "        \"\"\"\n",
      "        Specific union logic should go here. In subclasses, union behavior\n",
      "        should be overwritten here rather than in `self.union`.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index or array-like\n",
      "        sort : False or None, default False\n",
      "            Whether to sort the resulting index.\n",
      "\n",
      "            * False : do not sort the result.\n",
      "            * None : sort the result, except when `self` and `other` are equal\n",
      "              or when the values cannot be compared.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "        \"\"\"\n",
      "        lvals = self._values\n",
      "        rvals = other._values\n",
      "\n",
      "        if (\n",
      "            sort is None\n",
      "            and self.is_monotonic_increasing\n",
      "            and other.is_monotonic_increasing\n",
      "            and not (self.has_duplicates and other.has_duplicates)\n",
      "            and self._can_use_libjoin\n",
      "        ):\n",
      "            # Both are monotonic and at least one is unique, so can use outer join\n",
      "            #  (actually don't need either unique, but without this restriction\n",
      "            #  test_union_same_value_duplicated_in_both fails)\n",
      "            try:\n",
      "                return self._outer_indexer(other)[0]\n",
      "            except (TypeError, IncompatibleFrequency):\n",
      "                # incomparable objects; should only be for object dtype\n",
      "                value_list = list(lvals)\n",
      "\n",
      "                # worth making this faster? a very unusual case\n",
      "                value_set = set(lvals)\n",
      "                value_list.extend([x for x in rvals if x not in value_set])\n",
      "                # If objects are unorderable, we must have object dtype.\n",
      "                return np.array(value_list, dtype=object)\n",
      "\n",
      "        elif not other.is_unique:\n",
      "            # other has duplicates\n",
      "            result = algos.union_with_duplicates(lvals, rvals)\n",
      "            return _maybe_try_sort(result, sort)\n",
      "\n",
      "        # Self may have duplicates; other already checked as unique\n",
      "        # find indexes of things in \"other\" that are not in \"self\"\n",
      "        if self._index_as_unique:\n",
      "            indexer = self.get_indexer(other)\n",
      "            missing = (indexer == -1).nonzero()[0]\n",
      "        else:\n",
      "            missing = algos.unique1d(self.get_indexer_non_unique(other)[1])\n",
      "\n",
      "        if len(missing) > 0:\n",
      "            other_diff = rvals.take(missing)\n",
      "            result = concat_compat((lvals, other_diff))\n",
      "        else:\n",
      "            result = lvals\n",
      "\n",
      "        if not self.is_monotonic_increasing or not other.is_monotonic_increasing:\n",
      "            # if both are monotonic then result should already be sorted\n",
      "            result = _maybe_try_sort(result, sort)\n",
      "\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _validate_can_reindex\n",
      "Method definition:     @final\n",
      "    def _validate_can_reindex(self, indexer: np.ndarray) -> None:\n",
      "        \"\"\"\n",
      "        Check if we are allowing reindexing with this particular indexer.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        indexer : an integer ndarray\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError if its a duplicate axis\n",
      "        \"\"\"\n",
      "        # trying to reindex on an axis with duplicates\n",
      "        if not self._index_as_unique and len(indexer):\n",
      "            raise ValueError(\"cannot reindex on an axis with duplicate labels\")\n",
      "\n",
      "------------\n",
      "Method name: _validate_fill_value\n",
      "Method definition:     def _validate_fill_value(self, value):\n",
      "        \"\"\"\n",
      "        Check if the value can be inserted into our array without casting,\n",
      "        and convert it to an appropriate native type if necessary.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If the value cannot be inserted into an array of this dtype.\n",
      "        \"\"\"\n",
      "        dtype = self.dtype\n",
      "        if isinstance(dtype, np.dtype) and dtype.kind not in [\"m\", \"M\"]:\n",
      "            # return np_can_hold_element(dtype, value)\n",
      "            try:\n",
      "                return np_can_hold_element(dtype, value)\n",
      "            except LossySetitemError as err:\n",
      "                # re-raise as TypeError for consistency\n",
      "                raise TypeError from err\n",
      "        elif not can_hold_element(self._values, value):\n",
      "            raise TypeError\n",
      "        return value\n",
      "\n",
      "------------\n",
      "Method name: _validate_index_level\n",
      "Method definition:     @final\n",
      "    def _validate_index_level(self, level) -> None:\n",
      "        \"\"\"\n",
      "        Validate index level.\n",
      "\n",
      "        For single-level Index getting level number is a no-op, but some\n",
      "        verification must be done like in MultiIndex.\n",
      "\n",
      "        \"\"\"\n",
      "        if isinstance(level, int):\n",
      "            if level < 0 and level != -1:\n",
      "                raise IndexError(\n",
      "                    \"Too many levels: Index has only 1 level, \"\n",
      "                    f\"{level} is not a valid level number\"\n",
      "                )\n",
      "            elif level > 0:\n",
      "                raise IndexError(\n",
      "                    f\"Too many levels: Index has only 1 level, not {level + 1}\"\n",
      "                )\n",
      "        elif level != self.name:\n",
      "            raise KeyError(\n",
      "                f\"Requested level ({level}) does not match index name ({self.name})\"\n",
      "            )\n",
      "\n",
      "------------\n",
      "Method name: _validate_indexer\n",
      "Method definition:     @final\n",
      "    def _validate_indexer(self, form: str_t, key, kind: str_t):\n",
      "        \"\"\"\n",
      "        If we are positional indexer, validate that we have appropriate\n",
      "        typed bounds must be an integer.\n",
      "        \"\"\"\n",
      "        assert kind in [\"getitem\", \"iloc\"]\n",
      "\n",
      "        if key is not None and not is_integer(key):\n",
      "            raise self._invalid_indexer(form, key)\n",
      "\n",
      "------------\n",
      "Method name: _validate_names\n",
      "Method definition:     @final\n",
      "    def _validate_names(\n",
      "        self, name=None, names=None, deep: bool = False\n",
      "    ) -> list[Hashable]:\n",
      "        \"\"\"\n",
      "        Handles the quirks of having a singular 'name' parameter for general\n",
      "        Index and plural 'names' parameter for MultiIndex.\n",
      "        \"\"\"\n",
      "        from copy import deepcopy\n",
      "\n",
      "        if names is not None and name is not None:\n",
      "            raise TypeError(\"Can only provide one of `names` and `name`\")\n",
      "        elif names is None and name is None:\n",
      "            new_names = deepcopy(self.names) if deep else self.names\n",
      "        elif names is not None:\n",
      "            if not is_list_like(names):\n",
      "                raise TypeError(\"Must pass list-like as `names`.\")\n",
      "            new_names = names\n",
      "        elif not is_list_like(name):\n",
      "            new_names = [name]\n",
      "        else:\n",
      "            new_names = name\n",
      "\n",
      "        if len(new_names) != len(self.names):\n",
      "            raise ValueError(\n",
      "                f\"Length of new names must be {len(self.names)}, got {len(new_names)}\"\n",
      "            )\n",
      "\n",
      "        # All items in 'new_names' need to be hashable\n",
      "        validate_all_hashable(*new_names, error_name=f\"{type(self).__name__}.name\")\n",
      "\n",
      "        return new_names\n",
      "\n",
      "------------\n",
      "Method name: _validate_positional_slice\n",
      "Method definition:     @final\n",
      "    def _validate_positional_slice(self, key: slice) -> None:\n",
      "        \"\"\"\n",
      "        For positional indexing, a slice must have either int or None\n",
      "        for each of start, stop, and step.\n",
      "        \"\"\"\n",
      "        self._validate_indexer(\"positional\", key.start, \"iloc\")\n",
      "        self._validate_indexer(\"positional\", key.stop, \"iloc\")\n",
      "        self._validate_indexer(\"positional\", key.step, \"iloc\")\n",
      "\n",
      "------------\n",
      "Method name: _validate_sort_keyword\n",
      "Method definition:     @final\n",
      "    def _validate_sort_keyword(self, sort):\n",
      "        if sort not in [None, False]:\n",
      "            raise ValueError(\n",
      "                \"The 'sort' keyword only takes the values of \"\n",
      "                f\"None or False; {sort} was passed.\"\n",
      "            )\n",
      "\n",
      "------------\n",
      "Method name: _view\n",
      "Method definition:     def _view(self: _IndexT) -> _IndexT:\n",
      "        \"\"\"\n",
      "        fastpath to make a shallow copy, i.e. new object with same data.\n",
      "        \"\"\"\n",
      "        result = self._simple_new(self._values, name=self._name)\n",
      "\n",
      "        result._cache = self._cache\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _wrap_difference_result\n",
      "Method definition:     def _wrap_difference_result(self, other, result):\n",
      "        # We will override for MultiIndex to handle empty results\n",
      "        return self._wrap_setop_result(other, result)\n",
      "\n",
      "------------\n",
      "Method name: _wrap_intersection_result\n",
      "Method definition:     def _wrap_intersection_result(self, other, result):\n",
      "        # We will override for MultiIndex to handle empty results\n",
      "        return self._wrap_setop_result(other, result)\n",
      "\n",
      "------------\n",
      "Method name: _wrap_joined_index\n",
      "Method definition:     def _wrap_joined_index(self: _IndexT, joined: ArrayLike, other: _IndexT) -> _IndexT:\n",
      "        assert other.dtype == self.dtype\n",
      "\n",
      "        if isinstance(self, ABCMultiIndex):\n",
      "            name = self.names if self.names == other.names else None\n",
      "            # error: Incompatible return value type (got \"MultiIndex\",\n",
      "            # expected \"_IndexT\")\n",
      "            return self._constructor(joined, name=name)  # type: ignore[return-value]\n",
      "        else:\n",
      "            name = get_op_result_name(self, other)\n",
      "            return self._constructor._with_infer(joined, name=name, dtype=self.dtype)\n",
      "\n",
      "------------\n",
      "Method name: _wrap_reindex_result\n",
      "Method definition:     def _wrap_reindex_result(self, target, indexer, preserve_names: bool):\n",
      "        target = self._maybe_preserve_names(target, preserve_names)\n",
      "        return target\n",
      "\n",
      "------------\n",
      "Method name: _wrap_setop_result\n",
      "Method definition:     @final\n",
      "    def _wrap_setop_result(self, other: Index, result) -> Index:\n",
      "        name = get_op_result_name(self, other)\n",
      "        if isinstance(result, Index):\n",
      "            if result.name != name:\n",
      "                result = result.rename(name)\n",
      "        else:\n",
      "            result = self._shallow_copy(result, name=name)\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: all\n",
      "Method definition:     def all(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Return whether all elements are Truthy.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        *args\n",
      "            Required for compatibility with numpy.\n",
      "        **kwargs\n",
      "            Required for compatibility with numpy.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        all : bool or array-like (if axis is specified)\n",
      "            A single element array-like may be converted to bool.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.any : Return whether any element in an Index is True.\n",
      "        Series.any : Return whether any element in a Series is True.\n",
      "        Series.all : Return whether all elements in a Series are True.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Not a Number (NaN), positive infinity and negative infinity\n",
      "        evaluate to True because these are not equal to zero.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        True, because nonzero integers are considered True.\n",
      "\n",
      "        >>> pd.Index([1, 2, 3]).all()\n",
      "        True\n",
      "\n",
      "        False, because ``0`` is considered False.\n",
      "\n",
      "        >>> pd.Index([0, 1, 2]).all()\n",
      "        False\n",
      "        \"\"\"\n",
      "        nv.validate_all(args, kwargs)\n",
      "        self._maybe_disable_logical_methods(\"all\")\n",
      "        # error: Argument 1 to \"all\" has incompatible type \"ArrayLike\"; expected\n",
      "        # \"Union[Union[int, float, complex, str, bytes, generic], Sequence[Union[int,\n",
      "        # float, complex, str, bytes, generic]], Sequence[Sequence[Any]],\n",
      "        # _SupportsArray]\"\n",
      "        return np.all(self.values)  # type: ignore[arg-type]\n",
      "\n",
      "------------\n",
      "Method name: any\n",
      "Method definition:     def any(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Return whether any element is Truthy.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        *args\n",
      "            Required for compatibility with numpy.\n",
      "        **kwargs\n",
      "            Required for compatibility with numpy.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        any : bool or array-like (if axis is specified)\n",
      "            A single element array-like may be converted to bool.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.all : Return whether all elements are True.\n",
      "        Series.all : Return whether all elements are True.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Not a Number (NaN), positive infinity and negative infinity\n",
      "        evaluate to True because these are not equal to zero.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> index = pd.Index([0, 1, 2])\n",
      "        >>> index.any()\n",
      "        True\n",
      "\n",
      "        >>> index = pd.Index([0, 0, 0])\n",
      "        >>> index.any()\n",
      "        False\n",
      "        \"\"\"\n",
      "        nv.validate_any(args, kwargs)\n",
      "        self._maybe_disable_logical_methods(\"any\")\n",
      "        # error: Argument 1 to \"any\" has incompatible type \"ArrayLike\"; expected\n",
      "        # \"Union[Union[int, float, complex, str, bytes, generic], Sequence[Union[int,\n",
      "        # float, complex, str, bytes, generic]], Sequence[Sequence[Any]],\n",
      "        # _SupportsArray]\"\n",
      "        return np.any(self.values)  # type: ignore[arg-type]\n",
      "\n",
      "------------\n",
      "Method name: append\n",
      "Method definition:     def append(self, other: Index | Sequence[Index]) -> Index:\n",
      "        \"\"\"\n",
      "        Append a collection of Index options together.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index or list/tuple of indices\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "        \"\"\"\n",
      "        to_concat = [self]\n",
      "\n",
      "        if isinstance(other, (list, tuple)):\n",
      "            to_concat += list(other)\n",
      "        else:\n",
      "            # error: Argument 1 to \"append\" of \"list\" has incompatible type\n",
      "            # \"Union[Index, Sequence[Index]]\"; expected \"Index\"\n",
      "            to_concat.append(other)  # type: ignore[arg-type]\n",
      "\n",
      "        for obj in to_concat:\n",
      "            if not isinstance(obj, Index):\n",
      "                raise TypeError(\"all inputs must be Index\")\n",
      "\n",
      "        names = {obj.name for obj in to_concat}\n",
      "        name = None if len(names) > 1 else self.name\n",
      "\n",
      "        return self._concat(to_concat, name)\n",
      "\n",
      "------------\n",
      "Method name: argmax\n",
      "Method definition:     @Appender(IndexOpsMixin.argmax.__doc__)\n",
      "    def argmax(self, axis=None, skipna=True, *args, **kwargs) -> int:\n",
      "        nv.validate_argmax(args, kwargs)\n",
      "        nv.validate_minmax_axis(axis)\n",
      "\n",
      "        if not self._is_multi and self.hasnans:\n",
      "            # Take advantage of cache\n",
      "            mask = self._isnan\n",
      "            if not skipna or mask.all():\n",
      "                return -1\n",
      "        return super().argmax(skipna=skipna)\n",
      "\n",
      "------------\n",
      "Method name: argmin\n",
      "Method definition:     @Appender(IndexOpsMixin.argmin.__doc__)\n",
      "    def argmin(self, axis=None, skipna=True, *args, **kwargs) -> int:\n",
      "        nv.validate_argmin(args, kwargs)\n",
      "        nv.validate_minmax_axis(axis)\n",
      "\n",
      "        if not self._is_multi and self.hasnans:\n",
      "            # Take advantage of cache\n",
      "            mask = self._isnan\n",
      "            if not skipna or mask.all():\n",
      "                return -1\n",
      "        return super().argmin(skipna=skipna)\n",
      "\n",
      "------------\n",
      "Method name: argsort\n",
      "Method definition:     def argsort(self, *args, **kwargs) -> npt.NDArray[np.intp]:\n",
      "        \"\"\"\n",
      "        Return the integer indices that would sort the index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        *args\n",
      "            Passed to `numpy.ndarray.argsort`.\n",
      "        **kwargs\n",
      "            Passed to `numpy.ndarray.argsort`.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        np.ndarray[np.intp]\n",
      "            Integer indices that would sort the index if used as\n",
      "            an indexer.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.argsort : Similar method for NumPy arrays.\n",
      "        Index.sort_values : Return sorted copy of Index.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\n",
      "        >>> idx\n",
      "        Index(['b', 'a', 'd', 'c'], dtype='object')\n",
      "\n",
      "        >>> order = idx.argsort()\n",
      "        >>> order\n",
      "        array([1, 0, 3, 2])\n",
      "\n",
      "        >>> idx[order]\n",
      "        Index(['a', 'b', 'c', 'd'], dtype='object')\n",
      "        \"\"\"\n",
      "        # This works for either ndarray or EA, is overridden\n",
      "        #  by RangeIndex, MultIIndex\n",
      "        return self._data.argsort(*args, **kwargs)\n",
      "\n",
      "------------\n",
      "Method name: asof\n",
      "Method definition:     @final\n",
      "    def asof(self, label):\n",
      "        \"\"\"\n",
      "        Return the label from the index, or, if not present, the previous one.\n",
      "\n",
      "        Assuming that the index is sorted, return the passed index label if it\n",
      "        is in the index, or return the previous index label if the passed one\n",
      "        is not in the index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        label : object\n",
      "            The label up to which the method returns the latest index label.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        object\n",
      "            The passed label if it is in the index. The previous label if the\n",
      "            passed label is not in the sorted index or `NaN` if there is no\n",
      "            such label.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.asof : Return the latest value in a Series up to the\n",
      "            passed index.\n",
      "        merge_asof : Perform an asof merge (similar to left join but it\n",
      "            matches on nearest key rather than equal key).\n",
      "        Index.get_loc : An `asof` is a thin wrapper around `get_loc`\n",
      "            with method='pad'.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        `Index.asof` returns the latest index label up to the passed label.\n",
      "\n",
      "        >>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n",
      "        >>> idx.asof('2014-01-01')\n",
      "        '2013-12-31'\n",
      "\n",
      "        If the label is in the index, the method returns the passed label.\n",
      "\n",
      "        >>> idx.asof('2014-01-02')\n",
      "        '2014-01-02'\n",
      "\n",
      "        If all of the labels in the index are later than the passed label,\n",
      "        NaN is returned.\n",
      "\n",
      "        >>> idx.asof('1999-01-02')\n",
      "        nan\n",
      "\n",
      "        If the index is not sorted, an error is raised.\n",
      "\n",
      "        >>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n",
      "        ...                            '2014-01-03'])\n",
      "        >>> idx_not_sorted.asof('2013-12-31')\n",
      "        Traceback (most recent call last):\n",
      "        ValueError: index must be monotonic increasing or decreasing\n",
      "        \"\"\"\n",
      "        self._searchsorted_monotonic(label)  # validate sortedness\n",
      "        try:\n",
      "            loc = self.get_loc(label)\n",
      "        except (KeyError, TypeError):\n",
      "            # KeyError -> No exact match, try for padded\n",
      "            # TypeError -> passed e.g. non-hashable, fall through to get\n",
      "            #  the tested exception message\n",
      "            indexer = self.get_indexer([label], method=\"pad\")\n",
      "            if indexer.ndim > 1 or indexer.size > 1:\n",
      "                raise TypeError(\"asof requires scalar valued input\")\n",
      "            loc = indexer.item()\n",
      "            if loc == -1:\n",
      "                return self._na_value\n",
      "        else:\n",
      "            if isinstance(loc, slice):\n",
      "                loc = loc.indices(len(self))[-1]\n",
      "\n",
      "        return self[loc]\n",
      "\n",
      "------------\n",
      "Method name: asof_locs\n",
      "Method definition:     def asof_locs(\n",
      "        self, where: Index, mask: npt.NDArray[np.bool_]\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "        \"\"\"\n",
      "        Return the locations (indices) of labels in the index.\n",
      "\n",
      "        As in the `asof` function, if the label (a particular entry in\n",
      "        `where`) is not in the index, the latest index label up to the\n",
      "        passed label is chosen and its index returned.\n",
      "\n",
      "        If all of the labels in the index are later than a label in `where`,\n",
      "        -1 is returned.\n",
      "\n",
      "        `mask` is used to ignore NA values in the index during calculation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        where : Index\n",
      "            An Index consisting of an array of timestamps.\n",
      "        mask : np.ndarray[bool]\n",
      "            Array of booleans denoting where values in the original\n",
      "            data are not NA.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        np.ndarray[np.intp]\n",
      "            An array of locations (indices) of the labels from the Index\n",
      "            which correspond to the return values of the `asof` function\n",
      "            for every element in `where`.\n",
      "        \"\"\"\n",
      "        # error: No overload variant of \"searchsorted\" of \"ndarray\" matches argument\n",
      "        # types \"Union[ExtensionArray, ndarray[Any, Any]]\", \"str\"\n",
      "        # TODO: will be fixed when ExtensionArray.searchsorted() is fixed\n",
      "        locs = self._values[mask].searchsorted(\n",
      "            where._values, side=\"right\"  # type: ignore[call-overload]\n",
      "        )\n",
      "        locs = np.where(locs > 0, locs - 1, 0)\n",
      "\n",
      "        result = np.arange(len(self), dtype=np.intp)[mask].take(locs)\n",
      "\n",
      "        first_value = self._values[mask.argmax()]\n",
      "        result[(locs == 0) & (where._values < first_value)] = -1\n",
      "\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: astype\n",
      "Method definition:     def astype(self, dtype, copy: bool = True):\n",
      "        \"\"\"\n",
      "        Create an Index with values cast to dtypes.\n",
      "\n",
      "        The class of a new Index is determined by dtype. When conversion is\n",
      "        impossible, a TypeError exception is raised.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        dtype : numpy dtype or pandas type\n",
      "            Note that any signed integer `dtype` is treated as ``'int64'``,\n",
      "            and any unsigned integer `dtype` is treated as ``'uint64'``,\n",
      "            regardless of the size.\n",
      "        copy : bool, default True\n",
      "            By default, astype always returns a newly allocated object.\n",
      "            If copy is set to False and internal requirements on dtype are\n",
      "            satisfied, the original data is used to create a new Index\n",
      "            or the original Index is returned.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "            Index with values cast to specified dtype.\n",
      "        \"\"\"\n",
      "        if dtype is not None:\n",
      "            dtype = pandas_dtype(dtype)\n",
      "\n",
      "        if is_dtype_equal(self.dtype, dtype):\n",
      "            # Ensure that self.astype(self.dtype) is self\n",
      "            return self.copy() if copy else self\n",
      "\n",
      "        values = self._data\n",
      "        if isinstance(values, ExtensionArray):\n",
      "            if isinstance(dtype, np.dtype) and dtype.kind == \"M\" and is_unitless(dtype):\n",
      "                # TODO(2.0): remove this special-casing once this is enforced\n",
      "                #  in DTA.astype\n",
      "                raise TypeError(f\"Cannot cast {type(self).__name__} to dtype\")\n",
      "\n",
      "            with rewrite_exception(type(values).__name__, type(self).__name__):\n",
      "                new_values = values.astype(dtype, copy=copy)\n",
      "\n",
      "        elif is_float_dtype(self.dtype) and needs_i8_conversion(dtype):\n",
      "            # NB: this must come before the ExtensionDtype check below\n",
      "            # TODO: this differs from Series behavior; can/should we align them?\n",
      "            raise TypeError(\n",
      "                f\"Cannot convert Float64Index to dtype {dtype}; integer \"\n",
      "                \"values are required for conversion\"\n",
      "            )\n",
      "\n",
      "        elif isinstance(dtype, ExtensionDtype):\n",
      "            cls = dtype.construct_array_type()\n",
      "            # Note: for RangeIndex and CategoricalDtype self vs self._values\n",
      "            #  behaves differently here.\n",
      "            new_values = cls._from_sequence(self, dtype=dtype, copy=copy)\n",
      "\n",
      "        else:\n",
      "            try:\n",
      "                if dtype == str:\n",
      "                    # GH#38607\n",
      "                    new_values = values.astype(dtype, copy=copy)\n",
      "                else:\n",
      "                    # GH#13149 specifically use astype_nansafe instead of astype\n",
      "                    new_values = astype_nansafe(values, dtype=dtype, copy=copy)\n",
      "            except IntCastingNaNError:\n",
      "                raise\n",
      "            except (TypeError, ValueError) as err:\n",
      "                if dtype.kind == \"u\" and \"losslessly\" in str(err):\n",
      "                    # keep the message from _astype_float_to_int_nansafe\n",
      "                    raise\n",
      "                raise TypeError(\n",
      "                    f\"Cannot cast {type(self).__name__} to dtype {dtype}\"\n",
      "                ) from err\n",
      "\n",
      "        # pass copy=False because any copying will be done in the astype above\n",
      "        if self._is_backward_compat_public_numeric_index:\n",
      "            # this block is needed so e.g. NumericIndex[int8].astype(\"int32\") returns\n",
      "            # NumericIndex[int32] and not Int64Index with dtype int64.\n",
      "            # When Int64Index etc. are removed from the code base, removed this also.\n",
      "            if isinstance(dtype, np.dtype) and is_numeric_dtype(dtype):\n",
      "                return self._constructor(\n",
      "                    new_values, name=self.name, dtype=dtype, copy=False\n",
      "                )\n",
      "        return Index(new_values, name=self.name, dtype=new_values.dtype, copy=False)\n",
      "\n",
      "------------\n",
      "Method name: copy\n",
      "Method definition:     def copy(\n",
      "        self: _IndexT,\n",
      "        name: Hashable | None = None,\n",
      "        deep: bool = False,\n",
      "        dtype: Dtype | None = None,\n",
      "        names: Sequence[Hashable] | None = None,\n",
      "    ) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Make a copy of this object.\n",
      "\n",
      "        Name and dtype sets those attributes on the new object.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        name : Label, optional\n",
      "            Set name for new object.\n",
      "        deep : bool, default False\n",
      "        dtype : numpy dtype or pandas type, optional\n",
      "            Set dtype for new object.\n",
      "\n",
      "            .. deprecated:: 1.2.0\n",
      "                use ``astype`` method instead.\n",
      "        names : list-like, optional\n",
      "            Kept for compatibility with MultiIndex. Should not be used.\n",
      "\n",
      "            .. deprecated:: 1.4.0\n",
      "                use ``name`` instead.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "            Index refer to new object which is a copy of this object.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        In most cases, there should be no functional difference from using\n",
      "        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n",
      "        \"\"\"\n",
      "        if names is not None:\n",
      "            warnings.warn(\n",
      "                \"parameter names is deprecated and will be removed in a future \"\n",
      "                \"version. Use the name parameter instead.\",\n",
      "                FutureWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "\n",
      "        name = self._validate_names(name=name, names=names, deep=deep)[0]\n",
      "        if deep:\n",
      "            new_data = self._data.copy()\n",
      "            new_index = type(self)._simple_new(new_data, name=name)\n",
      "        else:\n",
      "            new_index = self._rename(name=name)\n",
      "\n",
      "        if dtype:\n",
      "            warnings.warn(\n",
      "                \"parameter dtype is deprecated and will be removed in a future \"\n",
      "                \"version. Use the astype method instead.\",\n",
      "                FutureWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "            new_index = new_index.astype(dtype)\n",
      "        return new_index\n",
      "\n",
      "------------\n",
      "Method name: delete\n",
      "Method definition:     def delete(self: _IndexT, loc) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Make new Index with passed location(-s) deleted.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        loc : int or list of int\n",
      "            Location of item(-s) which will be deleted.\n",
      "            Use a list of locations to delete more than one value at the same time.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "            Will be same type as self, except for RangeIndex.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.delete : Delete any rows and column from NumPy array (ndarray).\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['a', 'b', 'c'])\n",
      "        >>> idx.delete(1)\n",
      "        Index(['a', 'c'], dtype='object')\n",
      "\n",
      "        >>> idx = pd.Index(['a', 'b', 'c'])\n",
      "        >>> idx.delete([0, 2])\n",
      "        Index(['b'], dtype='object')\n",
      "        \"\"\"\n",
      "        values = self._values\n",
      "        res_values: ArrayLike\n",
      "        if isinstance(values, np.ndarray):\n",
      "            # TODO(__array_function__): special casing will be unnecessary\n",
      "            res_values = np.delete(values, loc)\n",
      "        else:\n",
      "            res_values = values.delete(loc)\n",
      "\n",
      "        # _constructor so RangeIndex->Int64Index\n",
      "        return self._constructor._simple_new(res_values, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: difference\n",
      "Method definition:     @final\n",
      "    def difference(self, other, sort=None):\n",
      "        \"\"\"\n",
      "        Return a new Index with elements of index not in `other`.\n",
      "\n",
      "        This is the set difference of two Index objects.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index or array-like\n",
      "        sort : False or None, default None\n",
      "            Whether to sort the resulting index. By default, the\n",
      "            values are attempted to be sorted, but any TypeError from\n",
      "            incomparable elements is caught by pandas.\n",
      "\n",
      "            * None : Attempt to sort the result, but catch any TypeErrors\n",
      "              from comparing incomparable elements.\n",
      "            * False : Do not sort the result.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        difference : Index\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx1 = pd.Index([2, 1, 3, 4])\n",
      "        >>> idx2 = pd.Index([3, 4, 5, 6])\n",
      "        >>> idx1.difference(idx2)\n",
      "        Int64Index([1, 2], dtype='int64')\n",
      "        >>> idx1.difference(idx2, sort=False)\n",
      "        Int64Index([2, 1], dtype='int64')\n",
      "        \"\"\"\n",
      "        self._validate_sort_keyword(sort)\n",
      "        self._assert_can_do_setop(other)\n",
      "        other, result_name = self._convert_can_do_setop(other)\n",
      "\n",
      "        # Note: we do NOT call _deprecate_dti_setop here, as there\n",
      "        #  is no requirement that .difference be commutative, so it does\n",
      "        #  not cast to object.\n",
      "\n",
      "        if self.equals(other):\n",
      "            # Note: we do not (yet) sort even if sort=None GH#24959\n",
      "            return self[:0].rename(result_name)\n",
      "\n",
      "        if len(other) == 0:\n",
      "            # Note: we do not (yet) sort even if sort=None GH#24959\n",
      "            return self.rename(result_name)\n",
      "\n",
      "        if not self._should_compare(other):\n",
      "            # Nothing matches -> difference is everything\n",
      "            return self.rename(result_name)\n",
      "\n",
      "        result = self._difference(other, sort=sort)\n",
      "        return self._wrap_difference_result(other, result)\n",
      "\n",
      "------------\n",
      "Method name: drop\n",
      "Method definition:     def drop(\n",
      "        self,\n",
      "        labels: Index | np.ndarray | Iterable[Hashable],\n",
      "        errors: IgnoreRaise = \"raise\",\n",
      "    ) -> Index:\n",
      "        \"\"\"\n",
      "        Make new Index with passed list of labels deleted.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        labels : array-like or scalar\n",
      "        errors : {'ignore', 'raise'}, default 'raise'\n",
      "            If 'ignore', suppress error and existing labels are dropped.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        dropped : Index\n",
      "            Will be same type as self, except for RangeIndex.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        KeyError\n",
      "            If not all of the labels are found in the selected axis\n",
      "        \"\"\"\n",
      "        if not isinstance(labels, Index):\n",
      "            # avoid materializing e.g. RangeIndex\n",
      "            arr_dtype = \"object\" if self.dtype == \"object\" else None\n",
      "            labels = com.index_labels_to_array(labels, dtype=arr_dtype)\n",
      "\n",
      "        indexer = self.get_indexer_for(labels)\n",
      "        mask = indexer == -1\n",
      "        if mask.any():\n",
      "            if errors != \"ignore\":\n",
      "                raise KeyError(f\"{list(labels[mask])} not found in axis\")\n",
      "            indexer = indexer[~mask]\n",
      "        return self.delete(indexer)\n",
      "\n",
      "------------\n",
      "Method name: drop_duplicates\n",
      "Method definition:     @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n",
      "    def drop_duplicates(self: _IndexT, keep: str_t | bool = \"first\") -> _IndexT:\n",
      "        \"\"\"\n",
      "        Return Index with duplicate values removed.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        keep : {'first', 'last', ``False``}, default 'first'\n",
      "            - 'first' : Drop duplicates except for the first occurrence.\n",
      "            - 'last' : Drop duplicates except for the last occurrence.\n",
      "            - ``False`` : Drop all duplicates.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        deduplicated : Index\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.drop_duplicates : Equivalent method on Series.\n",
      "        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n",
      "        Index.duplicated : Related method on Index, indicating duplicate\n",
      "            Index values.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Generate an pandas.Index with duplicate values.\n",
      "\n",
      "        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])\n",
      "\n",
      "        The `keep` parameter controls  which duplicate values are removed.\n",
      "        The value 'first' keeps the first occurrence for each\n",
      "        set of duplicated entries. The default value of keep is 'first'.\n",
      "\n",
      "        >>> idx.drop_duplicates(keep='first')\n",
      "        Index(['lama', 'cow', 'beetle', 'hippo'], dtype='object')\n",
      "\n",
      "        The value 'last' keeps the last occurrence for each set of duplicated\n",
      "        entries.\n",
      "\n",
      "        >>> idx.drop_duplicates(keep='last')\n",
      "        Index(['cow', 'beetle', 'lama', 'hippo'], dtype='object')\n",
      "\n",
      "        The value ``False`` discards all sets of duplicated entries.\n",
      "\n",
      "        >>> idx.drop_duplicates(keep=False)\n",
      "        Index(['cow', 'beetle', 'hippo'], dtype='object')\n",
      "        \"\"\"\n",
      "        if self.is_unique:\n",
      "            return self._view()\n",
      "\n",
      "        return super().drop_duplicates(keep=keep)\n",
      "\n",
      "------------\n",
      "Method name: droplevel\n",
      "Method definition:     @final\n",
      "    def droplevel(self, level=0):\n",
      "        \"\"\"\n",
      "        Return index with requested level(s) removed.\n",
      "\n",
      "        If resulting index has only 1 level left, the result will be\n",
      "        of Index type, not MultiIndex.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        level : int, str, or list-like, default 0\n",
      "            If a string is given, must be the name of a level\n",
      "            If list-like, elements must be names or indexes of levels.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index or MultiIndex\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> mi = pd.MultiIndex.from_arrays(\n",
      "        ... [[1, 2], [3, 4], [5, 6]], names=['x', 'y', 'z'])\n",
      "        >>> mi\n",
      "        MultiIndex([(1, 3, 5),\n",
      "                    (2, 4, 6)],\n",
      "                   names=['x', 'y', 'z'])\n",
      "\n",
      "        >>> mi.droplevel()\n",
      "        MultiIndex([(3, 5),\n",
      "                    (4, 6)],\n",
      "                   names=['y', 'z'])\n",
      "\n",
      "        >>> mi.droplevel(2)\n",
      "        MultiIndex([(1, 3),\n",
      "                    (2, 4)],\n",
      "                   names=['x', 'y'])\n",
      "\n",
      "        >>> mi.droplevel('z')\n",
      "        MultiIndex([(1, 3),\n",
      "                    (2, 4)],\n",
      "                   names=['x', 'y'])\n",
      "\n",
      "        >>> mi.droplevel(['x', 'y'])\n",
      "        Int64Index([5, 6], dtype='int64', name='z')\n",
      "        \"\"\"\n",
      "        if not isinstance(level, (tuple, list)):\n",
      "            level = [level]\n",
      "\n",
      "        levnums = sorted(self._get_level_number(lev) for lev in level)[::-1]\n",
      "\n",
      "        return self._drop_level_numbers(levnums)\n",
      "\n",
      "------------\n",
      "Method name: dropna\n",
      "Method definition:     def dropna(self: _IndexT, how: str_t = \"any\") -> _IndexT:\n",
      "        \"\"\"\n",
      "        Return Index without NA/NaN values.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        how : {'any', 'all'}, default 'any'\n",
      "            If the Index is a MultiIndex, drop the value when any or all levels\n",
      "            are NaN.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "        \"\"\"\n",
      "        if how not in (\"any\", \"all\"):\n",
      "            raise ValueError(f\"invalid how option: {how}\")\n",
      "\n",
      "        if self.hasnans:\n",
      "            res_values = self._values[~self._isnan]\n",
      "            return type(self)._simple_new(res_values, name=self.name)\n",
      "        return self._view()\n",
      "\n",
      "------------\n",
      "Method name: duplicated\n",
      "Method definition:     def duplicated(\n",
      "        self, keep: Literal[\"first\", \"last\", False] = \"first\"\n",
      "    ) -> npt.NDArray[np.bool_]:\n",
      "        \"\"\"\n",
      "        Indicate duplicate index values.\n",
      "\n",
      "        Duplicated values are indicated as ``True`` values in the resulting\n",
      "        array. Either all duplicates, all except the first, or all except the\n",
      "        last occurrence of duplicates can be indicated.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        keep : {'first', 'last', False}, default 'first'\n",
      "            The value or values in a set of duplicates to mark as missing.\n",
      "\n",
      "            - 'first' : Mark duplicates as ``True`` except for the first\n",
      "              occurrence.\n",
      "            - 'last' : Mark duplicates as ``True`` except for the last\n",
      "              occurrence.\n",
      "            - ``False`` : Mark all duplicates as ``True``.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        np.ndarray[bool]\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.duplicated : Equivalent method on pandas.Series.\n",
      "        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n",
      "        Index.drop_duplicates : Remove duplicate values from Index.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        By default, for each set of duplicated values, the first occurrence is\n",
      "        set to False and all others to True:\n",
      "\n",
      "        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n",
      "        >>> idx.duplicated()\n",
      "        array([False, False,  True, False,  True])\n",
      "\n",
      "        which is equivalent to\n",
      "\n",
      "        >>> idx.duplicated(keep='first')\n",
      "        array([False, False,  True, False,  True])\n",
      "\n",
      "        By using 'last', the last occurrence of each set of duplicated values\n",
      "        is set on False and all others on True:\n",
      "\n",
      "        >>> idx.duplicated(keep='last')\n",
      "        array([ True, False,  True, False, False])\n",
      "\n",
      "        By setting keep on ``False``, all duplicates are True:\n",
      "\n",
      "        >>> idx.duplicated(keep=False)\n",
      "        array([ True, False,  True, False,  True])\n",
      "        \"\"\"\n",
      "        if self.is_unique:\n",
      "            # fastpath available bc we are immutable\n",
      "            return np.zeros(len(self), dtype=bool)\n",
      "        return self._duplicated(keep=keep)\n",
      "\n",
      "------------\n",
      "Method name: equals\n",
      "Method definition:     def equals(self, other: Any) -> bool:\n",
      "        \"\"\"\n",
      "        Determine if two Index object are equal.\n",
      "\n",
      "        The things that are being compared are:\n",
      "\n",
      "        * The elements inside the Index object.\n",
      "        * The order of the elements inside the Index object.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Any\n",
      "            The other object to compare against.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            True if \"other\" is an Index and it has the same elements and order\n",
      "            as the calling index; False otherwise.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx1 = pd.Index([1, 2, 3])\n",
      "        >>> idx1\n",
      "        Int64Index([1, 2, 3], dtype='int64')\n",
      "        >>> idx1.equals(pd.Index([1, 2, 3]))\n",
      "        True\n",
      "\n",
      "        The elements inside are compared\n",
      "\n",
      "        >>> idx2 = pd.Index([\"1\", \"2\", \"3\"])\n",
      "        >>> idx2\n",
      "        Index(['1', '2', '3'], dtype='object')\n",
      "\n",
      "        >>> idx1.equals(idx2)\n",
      "        False\n",
      "\n",
      "        The order is compared\n",
      "\n",
      "        >>> ascending_idx = pd.Index([1, 2, 3])\n",
      "        >>> ascending_idx\n",
      "        Int64Index([1, 2, 3], dtype='int64')\n",
      "        >>> descending_idx = pd.Index([3, 2, 1])\n",
      "        >>> descending_idx\n",
      "        Int64Index([3, 2, 1], dtype='int64')\n",
      "        >>> ascending_idx.equals(descending_idx)\n",
      "        False\n",
      "\n",
      "        The dtype is *not* compared\n",
      "\n",
      "        >>> int64_idx = pd.Index([1, 2, 3], dtype='int64')\n",
      "        >>> int64_idx\n",
      "        Int64Index([1, 2, 3], dtype='int64')\n",
      "        >>> uint64_idx = pd.Index([1, 2, 3], dtype='uint64')\n",
      "        >>> uint64_idx\n",
      "        UInt64Index([1, 2, 3], dtype='uint64')\n",
      "        >>> int64_idx.equals(uint64_idx)\n",
      "        True\n",
      "        \"\"\"\n",
      "        if self.is_(other):\n",
      "            return True\n",
      "\n",
      "        if not isinstance(other, Index):\n",
      "            return False\n",
      "\n",
      "        if is_object_dtype(self.dtype) and not is_object_dtype(other.dtype):\n",
      "            # if other is not object, use other's logic for coercion\n",
      "            return other.equals(self)\n",
      "\n",
      "        if isinstance(other, ABCMultiIndex):\n",
      "            # d-level MultiIndex can equal d-tuple Index\n",
      "            return other.equals(self)\n",
      "\n",
      "        if isinstance(self._values, ExtensionArray):\n",
      "            # Dispatch to the ExtensionArray's .equals method.\n",
      "            if not isinstance(other, type(self)):\n",
      "                return False\n",
      "\n",
      "            earr = cast(ExtensionArray, self._data)\n",
      "            return earr.equals(other._data)\n",
      "\n",
      "        if is_extension_array_dtype(other.dtype):\n",
      "            # All EA-backed Index subclasses override equals\n",
      "            return other.equals(self)\n",
      "\n",
      "        return array_equivalent(self._values, other._values)\n",
      "\n",
      "------------\n",
      "Method name: factorize\n",
      "Method definition:     @doc(\n",
      "        algorithms.factorize,\n",
      "        values=\"\",\n",
      "        order=\"\",\n",
      "        size_hint=\"\",\n",
      "        sort=textwrap.dedent(\n",
      "            \"\"\"\\\n",
      "            sort : bool, default False\n",
      "                Sort `uniques` and shuffle `codes` to maintain the\n",
      "                relationship.\n",
      "            \"\"\"\n",
      "        ),\n",
      "    )\n",
      "    def factorize(\n",
      "        self,\n",
      "        sort: bool = False,\n",
      "        na_sentinel: int | lib.NoDefault = lib.no_default,\n",
      "        use_na_sentinel: bool | lib.NoDefault = lib.no_default,\n",
      "    ):\n",
      "        return algorithms.factorize(\n",
      "            self, sort=sort, na_sentinel=na_sentinel, use_na_sentinel=use_na_sentinel\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: fillna\n",
      "Method definition:     def fillna(self, value=None, downcast=None):\n",
      "        \"\"\"\n",
      "        Fill NA/NaN values with the specified value.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        value : scalar\n",
      "            Scalar value to use to fill holes (e.g. 0).\n",
      "            This value cannot be a list-likes.\n",
      "        downcast : dict, default is None\n",
      "            A dict of item->dtype of what to downcast if possible,\n",
      "            or the string 'infer' which will try to downcast to an appropriate\n",
      "            equal type (e.g. float64 to int64 if possible).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        DataFrame.fillna : Fill NaN values of a DataFrame.\n",
      "        Series.fillna : Fill NaN Values of a Series.\n",
      "        \"\"\"\n",
      "\n",
      "        value = self._require_scalar(value)\n",
      "        if self.hasnans:\n",
      "            result = self.putmask(self._isnan, value)\n",
      "            if downcast is None:\n",
      "                # no need to care metadata other than name\n",
      "                # because it can't have freq if it has NaTs\n",
      "                return Index._with_infer(result, name=self.name)\n",
      "            raise NotImplementedError(\n",
      "                f\"{type(self).__name__}.fillna does not support 'downcast' \"\n",
      "                \"argument values other than 'None'.\"\n",
      "            )\n",
      "        return self._view()\n",
      "\n",
      "------------\n",
      "Method name: format\n",
      "Method definition:     def format(\n",
      "        self,\n",
      "        name: bool = False,\n",
      "        formatter: Callable | None = None,\n",
      "        na_rep: str_t = \"NaN\",\n",
      "    ) -> list[str_t]:\n",
      "        \"\"\"\n",
      "        Render a string representation of the Index.\n",
      "        \"\"\"\n",
      "        header = []\n",
      "        if name:\n",
      "            header.append(\n",
      "                pprint_thing(self.name, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n",
      "                if self.name is not None\n",
      "                else \"\"\n",
      "            )\n",
      "\n",
      "        if formatter is not None:\n",
      "            return header + list(self.map(formatter))\n",
      "\n",
      "        return self._format_with_header(header, na_rep=na_rep)\n",
      "\n",
      "------------\n",
      "Method name: get_indexer\n",
      "Method definition:     @Appender(_index_shared_docs[\"get_indexer\"] % _index_doc_kwargs)\n",
      "    @final\n",
      "    def get_indexer(\n",
      "        self,\n",
      "        target,\n",
      "        method: str_t | None = None,\n",
      "        limit: int | None = None,\n",
      "        tolerance=None,\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "        method = missing.clean_reindex_fill_method(method)\n",
      "        orig_target = target\n",
      "        target = self._maybe_cast_listlike_indexer(target)\n",
      "\n",
      "        self._check_indexing_method(method, limit, tolerance)\n",
      "\n",
      "        if not self._index_as_unique:\n",
      "            raise InvalidIndexError(self._requires_unique_msg)\n",
      "\n",
      "        if len(target) == 0:\n",
      "            return np.array([], dtype=np.intp)\n",
      "\n",
      "        if not self._should_compare(target) and not self._should_partial_index(target):\n",
      "            # IntervalIndex get special treatment bc numeric scalars can be\n",
      "            #  matched to Interval scalars\n",
      "            return self._get_indexer_non_comparable(target, method=method, unique=True)\n",
      "\n",
      "        if is_categorical_dtype(self.dtype):\n",
      "            # _maybe_cast_listlike_indexer ensures target has our dtype\n",
      "            #  (could improve perf by doing _should_compare check earlier?)\n",
      "            assert is_dtype_equal(self.dtype, target.dtype)\n",
      "\n",
      "            indexer = self._engine.get_indexer(target.codes)\n",
      "            if self.hasnans and target.hasnans:\n",
      "                # After _maybe_cast_listlike_indexer, target elements which do not\n",
      "                # belong to some category are changed to NaNs\n",
      "                # Mask to track actual NaN values compared to inserted NaN values\n",
      "                # GH#45361\n",
      "                target_nans = isna(orig_target)\n",
      "                loc = self.get_loc(np.nan)\n",
      "                mask = target.isna()\n",
      "                indexer[target_nans] = loc\n",
      "                indexer[mask & ~target_nans] = -1\n",
      "            return indexer\n",
      "\n",
      "        if is_categorical_dtype(target.dtype):\n",
      "            # potential fastpath\n",
      "            # get an indexer for unique categories then propagate to codes via take_nd\n",
      "            # get_indexer instead of _get_indexer needed for MultiIndex cases\n",
      "            #  e.g. test_append_different_columns_types\n",
      "            categories_indexer = self.get_indexer(target.categories)\n",
      "\n",
      "            indexer = algos.take_nd(categories_indexer, target.codes, fill_value=-1)\n",
      "\n",
      "            if (not self._is_multi and self.hasnans) and target.hasnans:\n",
      "                # Exclude MultiIndex because hasnans raises NotImplementedError\n",
      "                # we should only get here if we are unique, so loc is an integer\n",
      "                # GH#41934\n",
      "                loc = self.get_loc(np.nan)\n",
      "                mask = target.isna()\n",
      "                indexer[mask] = loc\n",
      "\n",
      "            return ensure_platform_int(indexer)\n",
      "\n",
      "        pself, ptarget = self._maybe_promote(target)\n",
      "        if pself is not self or ptarget is not target:\n",
      "            return pself.get_indexer(\n",
      "                ptarget, method=method, limit=limit, tolerance=tolerance\n",
      "            )\n",
      "\n",
      "        if is_dtype_equal(self.dtype, target.dtype) and self.equals(target):\n",
      "            # Only call equals if we have same dtype to avoid inference/casting\n",
      "            return np.arange(len(target), dtype=np.intp)\n",
      "\n",
      "        if not is_dtype_equal(self.dtype, target.dtype) and not is_interval_dtype(\n",
      "            self.dtype\n",
      "        ):\n",
      "            # IntervalIndex gets special treatment for partial-indexing\n",
      "            dtype = self._find_common_type_compat(target)\n",
      "\n",
      "            this = self.astype(dtype, copy=False)\n",
      "            target = target.astype(dtype, copy=False)\n",
      "            return this._get_indexer(\n",
      "                target, method=method, limit=limit, tolerance=tolerance\n",
      "            )\n",
      "\n",
      "        return self._get_indexer(target, method, limit, tolerance)\n",
      "\n",
      "------------\n",
      "Method name: get_indexer_for\n",
      "Method definition:     @final\n",
      "    def get_indexer_for(self, target) -> npt.NDArray[np.intp]:\n",
      "        \"\"\"\n",
      "        Guaranteed return of an indexer even when non-unique.\n",
      "\n",
      "        This dispatches to get_indexer or get_indexer_non_unique\n",
      "        as appropriate.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        np.ndarray[np.intp]\n",
      "            List of indices.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([np.nan, 'var1', np.nan])\n",
      "        >>> idx.get_indexer_for([np.nan])\n",
      "        array([0, 2])\n",
      "        \"\"\"\n",
      "        if self._index_as_unique:\n",
      "            return self.get_indexer(target)\n",
      "        indexer, _ = self.get_indexer_non_unique(target)\n",
      "        return indexer\n",
      "\n",
      "------------\n",
      "Method name: get_indexer_non_unique\n",
      "Method definition:     @Appender(_index_shared_docs[\"get_indexer_non_unique\"] % _index_doc_kwargs)\n",
      "    def get_indexer_non_unique(\n",
      "        self, target\n",
      "    ) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
      "        target = ensure_index(target)\n",
      "        target = self._maybe_cast_listlike_indexer(target)\n",
      "\n",
      "        if not self._should_compare(target) and not is_interval_dtype(self.dtype):\n",
      "            # IntervalIndex get special treatment bc numeric scalars can be\n",
      "            #  matched to Interval scalars\n",
      "            return self._get_indexer_non_comparable(target, method=None, unique=False)\n",
      "\n",
      "        pself, ptarget = self._maybe_promote(target)\n",
      "        if pself is not self or ptarget is not target:\n",
      "            return pself.get_indexer_non_unique(ptarget)\n",
      "\n",
      "        if not is_dtype_equal(self.dtype, target.dtype):\n",
      "            # TODO: if object, could use infer_dtype to preempt costly\n",
      "            #  conversion if still non-comparable?\n",
      "            dtype = self._find_common_type_compat(target)\n",
      "\n",
      "            this = self.astype(dtype, copy=False)\n",
      "            that = target.astype(dtype, copy=False)\n",
      "            return this.get_indexer_non_unique(that)\n",
      "\n",
      "        # Note: _maybe_promote ensures we never get here with MultiIndex\n",
      "        #  self and non-Multi target\n",
      "        tgt_values = target._get_engine_target()\n",
      "        if self._is_multi and target._is_multi:\n",
      "            engine = self._engine\n",
      "            # Item \"IndexEngine\" of \"Union[IndexEngine, ExtensionEngine]\" has\n",
      "            # no attribute \"_extract_level_codes\"\n",
      "            tgt_values = engine._extract_level_codes(target)  # type: ignore[union-attr]\n",
      "\n",
      "        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n",
      "        return ensure_platform_int(indexer), ensure_platform_int(missing)\n",
      "\n",
      "------------\n",
      "Method name: get_level_values\n",
      "Method definition:     def _get_level_values(self, level) -> Index:\n",
      "        \"\"\"\n",
      "        Return an Index of values for requested level.\n",
      "\n",
      "        This is primarily useful to get an individual level of values from a\n",
      "        MultiIndex, but is provided on Index as well for compatibility.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        level : int or str\n",
      "            It is either the integer position or the name of the level.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "            Calling object, as there is only one level in the Index.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        For Index, level should be 0, since there are no multiple levels.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(list('abc'))\n",
      "        >>> idx\n",
      "        Index(['a', 'b', 'c'], dtype='object')\n",
      "\n",
      "        Get level values by supplying `level` as integer:\n",
      "\n",
      "        >>> idx.get_level_values(0)\n",
      "        Index(['a', 'b', 'c'], dtype='object')\n",
      "        \"\"\"\n",
      "        self._validate_index_level(level)\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: get_loc\n",
      "Method definition:     def get_loc(self, key, method=None, tolerance=None):\n",
      "        \"\"\"\n",
      "        Get integer location, slice or boolean mask for requested label.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        key : label\n",
      "        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n",
      "            * default: exact matches only.\n",
      "            * pad / ffill: find the PREVIOUS index value if no exact match.\n",
      "            * backfill / bfill: use NEXT index value if no exact match\n",
      "            * nearest: use the NEAREST index value if no exact match. Tied\n",
      "              distances are broken by preferring the larger index value.\n",
      "\n",
      "            .. deprecated:: 1.4\n",
      "                Use index.get_indexer([item], method=...) instead.\n",
      "\n",
      "        tolerance : int or float, optional\n",
      "            Maximum distance from index value for inexact matches. The value of\n",
      "            the index at the matching location must satisfy the equation\n",
      "            ``abs(index[loc] - key) <= tolerance``.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        loc : int if unique index, slice if monotonic index, else mask\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> unique_index = pd.Index(list('abc'))\n",
      "        >>> unique_index.get_loc('b')\n",
      "        1\n",
      "\n",
      "        >>> monotonic_index = pd.Index(list('abbc'))\n",
      "        >>> monotonic_index.get_loc('b')\n",
      "        slice(1, 3, None)\n",
      "\n",
      "        >>> non_monotonic_index = pd.Index(list('abcb'))\n",
      "        >>> non_monotonic_index.get_loc('b')\n",
      "        array([False,  True, False,  True])\n",
      "        \"\"\"\n",
      "        if method is None:\n",
      "            if tolerance is not None:\n",
      "                raise ValueError(\n",
      "                    \"tolerance argument only valid if using pad, \"\n",
      "                    \"backfill or nearest lookups\"\n",
      "                )\n",
      "            casted_key = self._maybe_cast_indexer(key)\n",
      "            try:\n",
      "                return self._engine.get_loc(casted_key)\n",
      "            except KeyError as err:\n",
      "                raise KeyError(key) from err\n",
      "            except TypeError:\n",
      "                # If we have a listlike key, _check_indexing_error will raise\n",
      "                #  InvalidIndexError. Otherwise we fall through and re-raise\n",
      "                #  the TypeError.\n",
      "                self._check_indexing_error(key)\n",
      "                raise\n",
      "\n",
      "        # GH#42269\n",
      "        warnings.warn(\n",
      "            f\"Passing method to {type(self).__name__}.get_loc is deprecated \"\n",
      "            \"and will raise in a future version. Use \"\n",
      "            \"index.get_indexer([item], method=...) instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "\n",
      "        if is_scalar(key) and isna(key) and not self.hasnans:\n",
      "            raise KeyError(key)\n",
      "\n",
      "        if tolerance is not None:\n",
      "            tolerance = self._convert_tolerance(tolerance, np.asarray(key))\n",
      "\n",
      "        indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n",
      "        if indexer.ndim > 1 or indexer.size > 1:\n",
      "            raise TypeError(\"get_loc requires scalar valued input\")\n",
      "        loc = indexer.item()\n",
      "        if loc == -1:\n",
      "            raise KeyError(key)\n",
      "        return loc\n",
      "\n",
      "------------\n",
      "Method name: get_slice_bound\n",
      "Method definition:     def get_slice_bound(\n",
      "        self, label, side: Literal[\"left\", \"right\"], kind=no_default\n",
      "    ) -> int:\n",
      "        \"\"\"\n",
      "        Calculate slice bound that corresponds to given label.\n",
      "\n",
      "        Returns leftmost (one-past-the-rightmost if ``side=='right'``) position\n",
      "        of given label.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        label : object\n",
      "        side : {'left', 'right'}\n",
      "        kind : {'loc', 'getitem'} or None\n",
      "\n",
      "            .. deprecated:: 1.4.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        int\n",
      "            Index of label.\n",
      "        \"\"\"\n",
      "        assert kind in [\"loc\", \"getitem\", None, no_default]\n",
      "        self._deprecated_arg(kind, \"kind\", \"get_slice_bound\")\n",
      "\n",
      "        if side not in (\"left\", \"right\"):\n",
      "            raise ValueError(\n",
      "                \"Invalid value for side kwarg, must be either \"\n",
      "                f\"'left' or 'right': {side}\"\n",
      "            )\n",
      "\n",
      "        original_label = label\n",
      "\n",
      "        # For datetime indices label may be a string that has to be converted\n",
      "        # to datetime boundary according to its resolution.\n",
      "        label = self._maybe_cast_slice_bound(label, side)\n",
      "\n",
      "        # we need to look up the label\n",
      "        try:\n",
      "            slc = self.get_loc(label)\n",
      "        except KeyError as err:\n",
      "            try:\n",
      "                return self._searchsorted_monotonic(label, side)\n",
      "            except ValueError:\n",
      "                # raise the original KeyError\n",
      "                raise err\n",
      "\n",
      "        if isinstance(slc, np.ndarray):\n",
      "            # get_loc may return a boolean array, which\n",
      "            # is OK as long as they are representable by a slice.\n",
      "            assert is_bool_dtype(slc.dtype)\n",
      "            slc = lib.maybe_booleans_to_slice(slc.view(\"u1\"))\n",
      "            if isinstance(slc, np.ndarray):\n",
      "                raise KeyError(\n",
      "                    f\"Cannot get {side} slice bound for non-unique \"\n",
      "                    f\"label: {repr(original_label)}\"\n",
      "                )\n",
      "\n",
      "        if isinstance(slc, slice):\n",
      "            if side == \"left\":\n",
      "                return slc.start\n",
      "            else:\n",
      "                return slc.stop\n",
      "        else:\n",
      "            if side == \"right\":\n",
      "                return slc + 1\n",
      "            else:\n",
      "                return slc\n",
      "\n",
      "------------\n",
      "Method name: get_value\n",
      "Method definition:     @final\n",
      "    def get_value(self, series: Series, key):\n",
      "        \"\"\"\n",
      "        Fast lookup of value from 1-dimensional ndarray.\n",
      "\n",
      "        Only use this if you know what you're doing.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        scalar or Series\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            \"get_value is deprecated and will be removed in a future version. \"\n",
      "            \"Use Series[key] instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "\n",
      "        self._check_indexing_error(key)\n",
      "\n",
      "        try:\n",
      "            # GH 20882, 21257\n",
      "            # First try to convert the key to a location\n",
      "            # If that fails, raise a KeyError if an integer\n",
      "            # index, otherwise, see if key is an integer, and\n",
      "            # try that\n",
      "            loc = self.get_loc(key)\n",
      "        except KeyError:\n",
      "            if not self._should_fallback_to_positional:\n",
      "                raise\n",
      "            elif is_integer(key):\n",
      "                # If the Index cannot hold integer, then this is unambiguously\n",
      "                #  a locational lookup.\n",
      "                loc = key\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        return self._get_values_for_loc(series, loc, key)\n",
      "\n",
      "------------\n",
      "Method name: groupby\n",
      "Method definition:     @final\n",
      "    def groupby(self, values) -> PrettyDict[Hashable, np.ndarray]:\n",
      "        \"\"\"\n",
      "        Group the index labels by a given array of values.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        values : array\n",
      "            Values used to determine the groups.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "            {group name -> group labels}\n",
      "        \"\"\"\n",
      "        # TODO: if we are a MultiIndex, we can do better\n",
      "        # that converting to tuples\n",
      "        if isinstance(values, ABCMultiIndex):\n",
      "            values = values._values\n",
      "        values = Categorical(values)\n",
      "        result = values._reverse_indexer()\n",
      "\n",
      "        # map to the label\n",
      "        result = {k: self.take(v) for k, v in result.items()}\n",
      "\n",
      "        return PrettyDict(result)\n",
      "\n",
      "------------\n",
      "Method name: holds_integer\n",
      "Method definition:     @final\n",
      "    def holds_integer(self) -> bool:\n",
      "        \"\"\"\n",
      "        Whether the type is an integer type.\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"integer\", \"mixed-integer\"]\n",
      "\n",
      "------------\n",
      "Method name: identical\n",
      "Method definition:     @final\n",
      "    def identical(self, other) -> bool:\n",
      "        \"\"\"\n",
      "        Similar to equals, but checks that object attributes and types are also equal.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            If two Index objects have equal elements and same type True,\n",
      "            otherwise False.\n",
      "        \"\"\"\n",
      "        return (\n",
      "            self.equals(other)\n",
      "            and all(\n",
      "                getattr(self, c, None) == getattr(other, c, None)\n",
      "                for c in self._comparables\n",
      "            )\n",
      "            and type(self) == type(other)\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: insert\n",
      "Method definition:     def insert(self, loc: int, item) -> Index:\n",
      "        \"\"\"\n",
      "        Make new Index inserting new item at location.\n",
      "\n",
      "        Follows Python numpy.insert semantics for negative values.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        loc : int\n",
      "        item : object\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        new_index : Index\n",
      "        \"\"\"\n",
      "        item = lib.item_from_zerodim(item)\n",
      "        if is_valid_na_for_dtype(item, self.dtype) and self.dtype != object:\n",
      "            item = self._na_value\n",
      "\n",
      "        arr = self._values\n",
      "\n",
      "        try:\n",
      "            if isinstance(arr, ExtensionArray):\n",
      "                res_values = arr.insert(loc, item)\n",
      "                return type(self)._simple_new(res_values, name=self.name)\n",
      "            else:\n",
      "                item = self._validate_fill_value(item)\n",
      "        except (TypeError, ValueError, LossySetitemError):\n",
      "            # e.g. trying to insert an integer into a DatetimeIndex\n",
      "            #  We cannot keep the same dtype, so cast to the (often object)\n",
      "            #  minimal shared dtype before doing the insert.\n",
      "            dtype = self._find_common_type_compat(item)\n",
      "            return self.astype(dtype).insert(loc, item)\n",
      "\n",
      "        if arr.dtype != object or not isinstance(\n",
      "            item, (tuple, np.datetime64, np.timedelta64)\n",
      "        ):\n",
      "            # with object-dtype we need to worry about numpy incorrectly casting\n",
      "            # dt64/td64 to integer, also about treating tuples as sequences\n",
      "            # special-casing dt64/td64 https://github.com/numpy/numpy/issues/12550\n",
      "            casted = arr.dtype.type(item)\n",
      "            new_values = np.insert(arr, loc, casted)\n",
      "\n",
      "        else:\n",
      "            # error: No overload variant of \"insert\" matches argument types\n",
      "            # \"ndarray[Any, Any]\", \"int\", \"None\"\n",
      "            new_values = np.insert(arr, loc, None)  # type: ignore[call-overload]\n",
      "            loc = loc if loc >= 0 else loc - 1\n",
      "            new_values[loc] = item\n",
      "\n",
      "        if self._typ == \"numericindex\":\n",
      "            # Use self._constructor instead of Index to retain NumericIndex GH#43921\n",
      "            # TODO(2.0) can use Index instead of self._constructor\n",
      "            return self._constructor._with_infer(new_values, name=self.name)\n",
      "        else:\n",
      "            return Index._with_infer(new_values, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: intersection\n",
      "Method definition:     @final\n",
      "    def intersection(self, other, sort=False):\n",
      "        \"\"\"\n",
      "        Form the intersection of two Index objects.\n",
      "\n",
      "        This returns a new Index with elements common to the index and `other`.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index or array-like\n",
      "        sort : False or None, default False\n",
      "            Whether to sort the resulting index.\n",
      "\n",
      "            * False : do not sort the result.\n",
      "            * None : sort the result, except when `self` and `other` are equal\n",
      "              or when the values cannot be compared.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        intersection : Index\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx1 = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx2 = pd.Index([3, 4, 5, 6])\n",
      "        >>> idx1.intersection(idx2)\n",
      "        Int64Index([3, 4], dtype='int64')\n",
      "        \"\"\"\n",
      "        self._validate_sort_keyword(sort)\n",
      "        self._assert_can_do_setop(other)\n",
      "        other, result_name = self._convert_can_do_setop(other)\n",
      "\n",
      "        if not is_dtype_equal(self.dtype, other.dtype):\n",
      "            self._deprecate_dti_setop(other, \"intersection\")\n",
      "\n",
      "        if self.equals(other):\n",
      "            if self.has_duplicates:\n",
      "                return self.unique()._get_reconciled_name_object(other)\n",
      "            return self._get_reconciled_name_object(other)\n",
      "\n",
      "        if len(self) == 0 or len(other) == 0:\n",
      "            # fastpath; we need to be careful about having commutativity\n",
      "\n",
      "            if self._is_multi or other._is_multi:\n",
      "                # _convert_can_do_setop ensures that we have both or neither\n",
      "                # We retain self.levels\n",
      "                return self[:0].rename(result_name)\n",
      "\n",
      "            dtype = self._find_common_type_compat(other)\n",
      "            if is_dtype_equal(self.dtype, dtype):\n",
      "                # Slicing allows us to retain DTI/TDI.freq, RangeIndex\n",
      "\n",
      "                # Note: self[:0] vs other[:0] affects\n",
      "                #  1) which index's `freq` we get in DTI/TDI cases\n",
      "                #     This may be a historical artifact, i.e. no documented\n",
      "                #     reason for this choice.\n",
      "                #  2) The `step` we get in RangeIndex cases\n",
      "                if len(self) == 0:\n",
      "                    return self[:0].rename(result_name)\n",
      "                else:\n",
      "                    return other[:0].rename(result_name)\n",
      "\n",
      "            return Index([], dtype=dtype, name=result_name)\n",
      "\n",
      "        elif not self._should_compare(other):\n",
      "            # We can infer that the intersection is empty.\n",
      "            if isinstance(self, ABCMultiIndex):\n",
      "                return self[:0].rename(result_name)\n",
      "            return Index([], name=result_name)\n",
      "\n",
      "        elif not is_dtype_equal(self.dtype, other.dtype):\n",
      "            dtype = self._find_common_type_compat(other)\n",
      "            this = self.astype(dtype, copy=False)\n",
      "            other = other.astype(dtype, copy=False)\n",
      "            return this.intersection(other, sort=sort)\n",
      "\n",
      "        result = self._intersection(other, sort=sort)\n",
      "        return self._wrap_intersection_result(other, result)\n",
      "\n",
      "------------\n",
      "Method name: is_\n",
      "Method definition:     @final\n",
      "    def is_(self, other) -> bool:\n",
      "        \"\"\"\n",
      "        More flexible, faster check like ``is`` but that works through views.\n",
      "\n",
      "        Note: this is *not* the same as ``Index.identical()``, which checks\n",
      "        that metadata is also the same.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : object\n",
      "            Other object to compare against.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            True if both have same underlying data, False otherwise.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.identical : Works like ``Index.is_`` but also checks metadata.\n",
      "        \"\"\"\n",
      "        if self is other:\n",
      "            return True\n",
      "        elif not hasattr(other, \"_id\"):\n",
      "            return False\n",
      "        elif self._id is None or other._id is None:\n",
      "            return False\n",
      "        else:\n",
      "            return self._id is other._id\n",
      "\n",
      "------------\n",
      "Method name: is_boolean\n",
      "Method definition:     @final\n",
      "    def is_boolean(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index only consists of booleans.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index only consists of booleans.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([True, False, True])\n",
      "        >>> idx.is_boolean()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([\"True\", \"False\", \"True\"])\n",
      "        >>> idx.is_boolean()\n",
      "        False\n",
      "\n",
      "        >>> idx = pd.Index([True, False, \"True\"])\n",
      "        >>> idx.is_boolean()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"boolean\"]\n",
      "\n",
      "------------\n",
      "Method name: is_categorical\n",
      "Method definition:     @final\n",
      "    def is_categorical(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index holds categorical data.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            True if the Index is categorical.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        CategoricalIndex : Index for categorical data.\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n",
      "        ...                 \"Watermelon\"]).astype(\"category\")\n",
      "        >>> idx.is_categorical()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 3, 5, 7])\n",
      "        >>> idx.is_categorical()\n",
      "        False\n",
      "\n",
      "        >>> s = pd.Series([\"Peter\", \"Victor\", \"Elisabeth\", \"Mar\"])\n",
      "        >>> s\n",
      "        0        Peter\n",
      "        1       Victor\n",
      "        2    Elisabeth\n",
      "        3          Mar\n",
      "        dtype: object\n",
      "        >>> s.index.is_categorical()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"categorical\"]\n",
      "\n",
      "------------\n",
      "Method name: is_floating\n",
      "Method definition:     @final\n",
      "    def is_floating(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index is a floating type.\n",
      "\n",
      "        The Index may consist of only floats, NaNs, or a mix of floats,\n",
      "        integers, or NaNs.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index only consists of only consists of floats, NaNs, or\n",
      "            a mix of floats, integers, or NaNs.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n",
      "        >>> idx.is_floating()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1.0, 2.0, np.nan, 4.0])\n",
      "        >>> idx.is_floating()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 2, 3, 4, np.nan])\n",
      "        >>> idx.is_floating()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx.is_floating()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"floating\", \"mixed-integer-float\", \"integer-na\"]\n",
      "\n",
      "------------\n",
      "Method name: is_integer\n",
      "Method definition:     @final\n",
      "    def is_integer(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index only consists of integers.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index only consists of integers.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx.is_integer()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n",
      "        >>> idx.is_integer()\n",
      "        False\n",
      "\n",
      "        >>> idx = pd.Index([\"Apple\", \"Mango\", \"Watermelon\"])\n",
      "        >>> idx.is_integer()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"integer\"]\n",
      "\n",
      "------------\n",
      "Method name: is_interval\n",
      "Method definition:     @final\n",
      "    def is_interval(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index holds Interval objects.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index holds Interval objects.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        IntervalIndex : Index for Interval objects.\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([pd.Interval(left=0, right=5),\n",
      "        ...                 pd.Interval(left=5, right=10)])\n",
      "        >>> idx.is_interval()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 3, 5, 7])\n",
      "        >>> idx.is_interval()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"interval\"]\n",
      "\n",
      "------------\n",
      "Method name: is_mixed\n",
      "Method definition:     @final\n",
      "    def is_mixed(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index holds data with mixed data types.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['a', np.nan, 'b'])\n",
      "        >>> idx.is_mixed()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1.0, 2.0, 3.0, 5.0])\n",
      "        >>> idx.is_mixed()\n",
      "        False\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            \"Index.is_mixed is deprecated and will be removed in a future version. \"\n",
      "            \"Check index.inferred_type directly instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        return self.inferred_type in [\"mixed\"]\n",
      "\n",
      "------------\n",
      "Method name: is_numeric\n",
      "Method definition:     @final\n",
      "    def is_numeric(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index only consists of numeric data.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index only consists of numeric data.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n",
      "        >>> idx.is_numeric()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 2, 3, 4.0])\n",
      "        >>> idx.is_numeric()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx.is_numeric()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 2, 3, 4.0, np.nan])\n",
      "        >>> idx.is_numeric()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 2, 3, 4.0, np.nan, \"Apple\"])\n",
      "        >>> idx.is_numeric()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"integer\", \"floating\"]\n",
      "\n",
      "------------\n",
      "Method name: is_object\n",
      "Method definition:     @final\n",
      "    def is_object(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index is of the object dtype.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index is of the object dtype.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([\"Apple\", \"Mango\", \"Watermelon\"])\n",
      "        >>> idx.is_object()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([\"Apple\", \"Mango\", 2.0])\n",
      "        >>> idx.is_object()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n",
      "        ...                 \"Watermelon\"]).astype(\"category\")\n",
      "        >>> idx.is_object()\n",
      "        False\n",
      "\n",
      "        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n",
      "        >>> idx.is_object()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return is_object_dtype(self.dtype)\n",
      "\n",
      "------------\n",
      "Method name: is_type_compatible\n",
      "Method definition:     def is_type_compatible(self, kind: str_t) -> bool:\n",
      "        \"\"\"\n",
      "        Whether the index type is compatible with the provided type.\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            \"Index.is_type_compatible is deprecated and will be removed in a \"\n",
      "            \"future version.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        return kind == self.inferred_type\n",
      "\n",
      "------------\n",
      "Method name: isin\n",
      "Method definition:     def isin(self, values, level=None) -> npt.NDArray[np.bool_]:\n",
      "        \"\"\"\n",
      "        Return a boolean array where the index values are in `values`.\n",
      "\n",
      "        Compute boolean array of whether each index value is found in the\n",
      "        passed set of values. The length of the returned boolean array matches\n",
      "        the length of the index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        values : set or list-like\n",
      "            Sought values.\n",
      "        level : str or int, optional\n",
      "            Name or position of the index level to use (if the index is a\n",
      "            `MultiIndex`).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        np.ndarray[bool]\n",
      "            NumPy array of boolean values.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.isin : Same for Series.\n",
      "        DataFrame.isin : Same method for DataFrames.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        In the case of `MultiIndex` you must either specify `values` as a\n",
      "        list-like object containing tuples that are the same length as the\n",
      "        number of levels, or specify `level`. Otherwise it will raise a\n",
      "        ``ValueError``.\n",
      "\n",
      "        If `level` is specified:\n",
      "\n",
      "        - if it is the name of one *and only one* index level, use that level;\n",
      "        - otherwise it should be a number indicating level position.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([1,2,3])\n",
      "        >>> idx\n",
      "        Int64Index([1, 2, 3], dtype='int64')\n",
      "\n",
      "        Check whether each index value in a list of values.\n",
      "\n",
      "        >>> idx.isin([1, 4])\n",
      "        array([ True, False, False])\n",
      "\n",
      "        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n",
      "        ...                                  ['red', 'blue', 'green']],\n",
      "        ...                                  names=('number', 'color'))\n",
      "        >>> midx\n",
      "        MultiIndex([(1,   'red'),\n",
      "                    (2,  'blue'),\n",
      "                    (3, 'green')],\n",
      "                   names=['number', 'color'])\n",
      "\n",
      "        Check whether the strings in the 'color' level of the MultiIndex\n",
      "        are in a list of colors.\n",
      "\n",
      "        >>> midx.isin(['red', 'orange', 'yellow'], level='color')\n",
      "        array([ True, False, False])\n",
      "\n",
      "        To check across the levels of a MultiIndex, pass a list of tuples:\n",
      "\n",
      "        >>> midx.isin([(1, 'red'), (3, 'red')])\n",
      "        array([ True, False, False])\n",
      "\n",
      "        For a DatetimeIndex, string values in `values` are converted to\n",
      "        Timestamps.\n",
      "\n",
      "        >>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n",
      "        >>> dti = pd.to_datetime(dates)\n",
      "        >>> dti\n",
      "        DatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\n",
      "        dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "        >>> dti.isin(['2000-03-11'])\n",
      "        array([ True, False, False])\n",
      "        \"\"\"\n",
      "        if level is not None:\n",
      "            self._validate_index_level(level)\n",
      "        return algos.isin(self._values, values)\n",
      "\n",
      "------------\n",
      "Method name: isna\n",
      "Method definition:     @final\n",
      "    def isna(self) -> npt.NDArray[np.bool_]:\n",
      "        \"\"\"\n",
      "        Detect missing values.\n",
      "\n",
      "        Return a boolean same-sized object indicating if the values are NA.\n",
      "        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n",
      "        mapped to ``True`` values.\n",
      "        Everything else get mapped to ``False`` values. Characters such as\n",
      "        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n",
      "        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray[bool]\n",
      "            A boolean array of whether my values are NA.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.notna : Boolean inverse of isna.\n",
      "        Index.dropna : Omit entries with missing values.\n",
      "        isna : Top-level isna.\n",
      "        Series.isna : Detect missing values in Series object.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Show which entries in a pandas.Index are NA. The result is an\n",
      "        array.\n",
      "\n",
      "        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n",
      "        >>> idx\n",
      "        Float64Index([5.2, 6.0, nan], dtype='float64')\n",
      "        >>> idx.isna()\n",
      "        array([False, False,  True])\n",
      "\n",
      "        Empty strings are not considered NA values. None is considered an NA\n",
      "        value.\n",
      "\n",
      "        >>> idx = pd.Index(['black', '', 'red', None])\n",
      "        >>> idx\n",
      "        Index(['black', '', 'red', None], dtype='object')\n",
      "        >>> idx.isna()\n",
      "        array([False, False, False,  True])\n",
      "\n",
      "        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n",
      "\n",
      "        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n",
      "        ...                         pd.Timestamp(''), None, pd.NaT])\n",
      "        >>> idx\n",
      "        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n",
      "                      dtype='datetime64[ns]', freq=None)\n",
      "        >>> idx.isna()\n",
      "        array([False,  True,  True,  True])\n",
      "        \"\"\"\n",
      "        return self._isnan\n",
      "\n",
      "------------\n",
      "Method name: isnull\n",
      "Method definition:     @final\n",
      "    def isna(self) -> npt.NDArray[np.bool_]:\n",
      "        \"\"\"\n",
      "        Detect missing values.\n",
      "\n",
      "        Return a boolean same-sized object indicating if the values are NA.\n",
      "        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n",
      "        mapped to ``True`` values.\n",
      "        Everything else get mapped to ``False`` values. Characters such as\n",
      "        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n",
      "        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray[bool]\n",
      "            A boolean array of whether my values are NA.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.notna : Boolean inverse of isna.\n",
      "        Index.dropna : Omit entries with missing values.\n",
      "        isna : Top-level isna.\n",
      "        Series.isna : Detect missing values in Series object.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Show which entries in a pandas.Index are NA. The result is an\n",
      "        array.\n",
      "\n",
      "        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n",
      "        >>> idx\n",
      "        Float64Index([5.2, 6.0, nan], dtype='float64')\n",
      "        >>> idx.isna()\n",
      "        array([False, False,  True])\n",
      "\n",
      "        Empty strings are not considered NA values. None is considered an NA\n",
      "        value.\n",
      "\n",
      "        >>> idx = pd.Index(['black', '', 'red', None])\n",
      "        >>> idx\n",
      "        Index(['black', '', 'red', None], dtype='object')\n",
      "        >>> idx.isna()\n",
      "        array([False, False, False,  True])\n",
      "\n",
      "        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n",
      "\n",
      "        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n",
      "        ...                         pd.Timestamp(''), None, pd.NaT])\n",
      "        >>> idx\n",
      "        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n",
      "                      dtype='datetime64[ns]', freq=None)\n",
      "        >>> idx.isna()\n",
      "        array([False,  True,  True,  True])\n",
      "        \"\"\"\n",
      "        return self._isnan\n",
      "\n",
      "------------\n",
      "Method name: item\n",
      "Method definition:     def item(self):\n",
      "        \"\"\"\n",
      "        Return the first element of the underlying data as a Python scalar.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        scalar\n",
      "            The first element of %(klass)s.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If the data is not length-1.\n",
      "        \"\"\"\n",
      "        if len(self) == 1:\n",
      "            return next(iter(self))\n",
      "        raise ValueError(\"can only convert an array of size 1 to a Python scalar\")\n",
      "\n",
      "------------\n",
      "Method name: join\n",
      "Method definition:     @final\n",
      "    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"other\"])\n",
      "    @_maybe_return_indexers\n",
      "    def join(\n",
      "        self,\n",
      "        other: Index,\n",
      "        how: str_t = \"left\",\n",
      "        level: Level = None,\n",
      "        return_indexers: bool = False,\n",
      "        sort: bool = False,\n",
      "    ) -> Index | tuple[Index, npt.NDArray[np.intp] | None, npt.NDArray[np.intp] | None]:\n",
      "        \"\"\"\n",
      "        Compute join_index and indexers to conform data structures to the new index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index\n",
      "        how : {'left', 'right', 'inner', 'outer'}\n",
      "        level : int or level name, default None\n",
      "        return_indexers : bool, default False\n",
      "        sort : bool, default False\n",
      "            Sort the join keys lexicographically in the result Index. If False,\n",
      "            the order of the join keys depends on the join type (how keyword).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        join_index, (left_indexer, right_indexer)\n",
      "        \"\"\"\n",
      "        other = ensure_index(other)\n",
      "\n",
      "        if isinstance(self, ABCDatetimeIndex) and isinstance(other, ABCDatetimeIndex):\n",
      "            if (self.tz is None) ^ (other.tz is None):\n",
      "                # Raise instead of casting to object below.\n",
      "                raise TypeError(\"Cannot join tz-naive with tz-aware DatetimeIndex\")\n",
      "\n",
      "        if not self._is_multi and not other._is_multi:\n",
      "            # We have specific handling for MultiIndex below\n",
      "            pself, pother = self._maybe_promote(other)\n",
      "            if pself is not self or pother is not other:\n",
      "                return pself.join(\n",
      "                    pother, how=how, level=level, return_indexers=True, sort=sort\n",
      "                )\n",
      "\n",
      "        lindexer: np.ndarray | None\n",
      "        rindexer: np.ndarray | None\n",
      "\n",
      "        # try to figure out the join level\n",
      "        # GH3662\n",
      "        if level is None and (self._is_multi or other._is_multi):\n",
      "\n",
      "            # have the same levels/names so a simple join\n",
      "            if self.names == other.names:\n",
      "                pass\n",
      "            else:\n",
      "                return self._join_multi(other, how=how)\n",
      "\n",
      "        # join on the level\n",
      "        if level is not None and (self._is_multi or other._is_multi):\n",
      "            return self._join_level(other, level, how=how)\n",
      "\n",
      "        if len(other) == 0:\n",
      "            if how in (\"left\", \"outer\"):\n",
      "                join_index = self._view()\n",
      "                rindexer = np.broadcast_to(np.intp(-1), len(join_index))\n",
      "                return join_index, None, rindexer\n",
      "            elif how in (\"right\", \"inner\", \"cross\"):\n",
      "                join_index = other._view()\n",
      "                lindexer = np.array([])\n",
      "                return join_index, lindexer, None\n",
      "\n",
      "        if len(self) == 0:\n",
      "            if how in (\"right\", \"outer\"):\n",
      "                join_index = other._view()\n",
      "                lindexer = np.broadcast_to(np.intp(-1), len(join_index))\n",
      "                return join_index, lindexer, None\n",
      "            elif how in (\"left\", \"inner\", \"cross\"):\n",
      "                join_index = self._view()\n",
      "                rindexer = np.array([])\n",
      "                return join_index, None, rindexer\n",
      "\n",
      "        if self._join_precedence < other._join_precedence:\n",
      "            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n",
      "            join_index, lidx, ridx = other.join(\n",
      "                self, how=how, level=level, return_indexers=True\n",
      "            )\n",
      "            lidx, ridx = ridx, lidx\n",
      "            return join_index, lidx, ridx\n",
      "\n",
      "        if not is_dtype_equal(self.dtype, other.dtype):\n",
      "            dtype = self._find_common_type_compat(other)\n",
      "            this = self.astype(dtype, copy=False)\n",
      "            other = other.astype(dtype, copy=False)\n",
      "            return this.join(other, how=how, return_indexers=True)\n",
      "\n",
      "        _validate_join_method(how)\n",
      "\n",
      "        if not self.is_unique and not other.is_unique:\n",
      "            return self._join_non_unique(other, how=how)\n",
      "        elif not self.is_unique or not other.is_unique:\n",
      "            if self.is_monotonic_increasing and other.is_monotonic_increasing:\n",
      "                if not is_interval_dtype(self.dtype):\n",
      "                    # otherwise we will fall through to _join_via_get_indexer\n",
      "                    # GH#39133\n",
      "                    # go through object dtype for ea till engine is supported properly\n",
      "                    return self._join_monotonic(other, how=how)\n",
      "            else:\n",
      "                return self._join_non_unique(other, how=how)\n",
      "        elif (\n",
      "            self.is_monotonic_increasing\n",
      "            and other.is_monotonic_increasing\n",
      "            and self._can_use_libjoin\n",
      "            and (\n",
      "                not isinstance(self, ABCMultiIndex)\n",
      "                or not any(is_categorical_dtype(dtype) for dtype in self.dtypes)\n",
      "            )\n",
      "            and not is_categorical_dtype(self.dtype)\n",
      "        ):\n",
      "            # Categorical is monotonic if data are ordered as categories, but join can\n",
      "            #  not handle this in case of not lexicographically monotonic GH#38502\n",
      "            try:\n",
      "                return self._join_monotonic(other, how=how)\n",
      "            except TypeError:\n",
      "                # object dtype; non-comparable objects\n",
      "                pass\n",
      "\n",
      "        return self._join_via_get_indexer(other, how, sort)\n",
      "\n",
      "------------\n",
      "Method name: map\n",
      "Method definition:     def map(self, mapper, na_action=None):\n",
      "        \"\"\"\n",
      "        Map values using an input mapping or function.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        mapper : function, dict, or Series\n",
      "            Mapping correspondence.\n",
      "        na_action : {None, 'ignore'}\n",
      "            If 'ignore', propagate NA values, without passing them to the\n",
      "            mapping correspondence.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        applied : Union[Index, MultiIndex], inferred\n",
      "            The output of the mapping function applied to the index.\n",
      "            If the function returns a tuple with more than one element\n",
      "            a MultiIndex will be returned.\n",
      "        \"\"\"\n",
      "        from pandas.core.indexes.multi import MultiIndex\n",
      "\n",
      "        new_values = self._map_values(mapper, na_action=na_action)\n",
      "\n",
      "        # we can return a MultiIndex\n",
      "        if new_values.size and isinstance(new_values[0], tuple):\n",
      "            if isinstance(self, MultiIndex):\n",
      "                names = self.names\n",
      "            elif self.name:\n",
      "                names = [self.name] * len(new_values[0])\n",
      "            else:\n",
      "                names = None\n",
      "            return MultiIndex.from_tuples(new_values, names=names)\n",
      "\n",
      "        dtype = None\n",
      "        if not new_values.size:\n",
      "            # empty\n",
      "            dtype = self.dtype\n",
      "\n",
      "        # e.g. if we are floating and new_values is all ints, then we\n",
      "        #  don't want to cast back to floating.  But if we are UInt64\n",
      "        #  and new_values is all ints, we want to try.\n",
      "        same_dtype = lib.infer_dtype(new_values, skipna=False) == self.inferred_type\n",
      "        if same_dtype:\n",
      "            new_values = maybe_cast_pointwise_result(\n",
      "                new_values, self.dtype, same_dtype=same_dtype\n",
      "            )\n",
      "\n",
      "        if self._is_backward_compat_public_numeric_index and is_numeric_dtype(\n",
      "            new_values.dtype\n",
      "        ):\n",
      "            return self._constructor(\n",
      "                new_values, dtype=dtype, copy=False, name=self.name\n",
      "            )\n",
      "\n",
      "        return Index._with_infer(new_values, dtype=dtype, copy=False, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: max\n",
      "Method definition:     @doc(IndexOpsMixin.max)\n",
      "    def max(self, axis=None, skipna=True, *args, **kwargs):\n",
      "        nv.validate_max(args, kwargs)\n",
      "        nv.validate_minmax_axis(axis)\n",
      "\n",
      "        if not len(self):\n",
      "            return self._na_value\n",
      "\n",
      "        if len(self) and self.is_monotonic_increasing:\n",
      "            # quick check\n",
      "            last = self[-1]\n",
      "            if not isna(last):\n",
      "                return last\n",
      "\n",
      "        if not self._is_multi and self.hasnans:\n",
      "            # Take advantage of cache\n",
      "            mask = self._isnan\n",
      "            if not skipna or mask.all():\n",
      "                return self._na_value\n",
      "\n",
      "        if not self._is_multi and not isinstance(self._values, np.ndarray):\n",
      "            # \"ExtensionArray\" has no attribute \"max\"\n",
      "            return self._values.max(skipna=skipna)  # type: ignore[attr-defined]\n",
      "\n",
      "        return super().max(skipna=skipna)\n",
      "\n",
      "------------\n",
      "Method name: memory_usage\n",
      "Method definition:     @doc(IndexOpsMixin._memory_usage)\n",
      "    def memory_usage(self, deep: bool = False) -> int:\n",
      "        result = self._memory_usage(deep=deep)\n",
      "\n",
      "        # include our engine hashtable\n",
      "        result += self._engine.sizeof(deep=deep)\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: min\n",
      "Method definition:     @doc(IndexOpsMixin.min)\n",
      "    def min(self, axis=None, skipna=True, *args, **kwargs):\n",
      "        nv.validate_min(args, kwargs)\n",
      "        nv.validate_minmax_axis(axis)\n",
      "\n",
      "        if not len(self):\n",
      "            return self._na_value\n",
      "\n",
      "        if len(self) and self.is_monotonic_increasing:\n",
      "            # quick check\n",
      "            first = self[0]\n",
      "            if not isna(first):\n",
      "                return first\n",
      "\n",
      "        if not self._is_multi and self.hasnans:\n",
      "            # Take advantage of cache\n",
      "            mask = self._isnan\n",
      "            if not skipna or mask.all():\n",
      "                return self._na_value\n",
      "\n",
      "        if not self._is_multi and not isinstance(self._values, np.ndarray):\n",
      "            # \"ExtensionArray\" has no attribute \"min\"\n",
      "            return self._values.min(skipna=skipna)  # type: ignore[attr-defined]\n",
      "\n",
      "        return super().min(skipna=skipna)\n",
      "\n",
      "------------\n",
      "Method name: notna\n",
      "Method definition:     @final\n",
      "    def notna(self) -> npt.NDArray[np.bool_]:\n",
      "        \"\"\"\n",
      "        Detect existing (non-missing) values.\n",
      "\n",
      "        Return a boolean same-sized object indicating if the values are not NA.\n",
      "        Non-missing values get mapped to ``True``. Characters such as empty\n",
      "        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n",
      "        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      "        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n",
      "        values.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray[bool]\n",
      "            Boolean array to indicate which entries are not NA.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.notnull : Alias of notna.\n",
      "        Index.isna: Inverse of notna.\n",
      "        notna : Top-level notna.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Show which entries in an Index are not NA. The result is an\n",
      "        array.\n",
      "\n",
      "        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n",
      "        >>> idx\n",
      "        Float64Index([5.2, 6.0, nan], dtype='float64')\n",
      "        >>> idx.notna()\n",
      "        array([ True,  True, False])\n",
      "\n",
      "        Empty strings are not considered NA values. None is considered a NA\n",
      "        value.\n",
      "\n",
      "        >>> idx = pd.Index(['black', '', 'red', None])\n",
      "        >>> idx\n",
      "        Index(['black', '', 'red', None], dtype='object')\n",
      "        >>> idx.notna()\n",
      "        array([ True,  True,  True, False])\n",
      "        \"\"\"\n",
      "        return ~self.isna()\n",
      "\n",
      "------------\n",
      "Method name: notnull\n",
      "Method definition:     @final\n",
      "    def notna(self) -> npt.NDArray[np.bool_]:\n",
      "        \"\"\"\n",
      "        Detect existing (non-missing) values.\n",
      "\n",
      "        Return a boolean same-sized object indicating if the values are not NA.\n",
      "        Non-missing values get mapped to ``True``. Characters such as empty\n",
      "        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n",
      "        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      "        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n",
      "        values.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray[bool]\n",
      "            Boolean array to indicate which entries are not NA.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.notnull : Alias of notna.\n",
      "        Index.isna: Inverse of notna.\n",
      "        notna : Top-level notna.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Show which entries in an Index are not NA. The result is an\n",
      "        array.\n",
      "\n",
      "        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n",
      "        >>> idx\n",
      "        Float64Index([5.2, 6.0, nan], dtype='float64')\n",
      "        >>> idx.notna()\n",
      "        array([ True,  True, False])\n",
      "\n",
      "        Empty strings are not considered NA values. None is considered a NA\n",
      "        value.\n",
      "\n",
      "        >>> idx = pd.Index(['black', '', 'red', None])\n",
      "        >>> idx\n",
      "        Index(['black', '', 'red', None], dtype='object')\n",
      "        >>> idx.notna()\n",
      "        array([ True,  True,  True, False])\n",
      "        \"\"\"\n",
      "        return ~self.isna()\n",
      "\n",
      "------------\n",
      "Method name: nunique\n",
      "Method definition:     def nunique(self, dropna: bool = True) -> int:\n",
      "        \"\"\"\n",
      "        Return number of unique elements in the object.\n",
      "\n",
      "        Excludes NA values by default.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        dropna : bool, default True\n",
      "            Don't include NaN in the count.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        int\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        DataFrame.nunique: Method nunique for DataFrame.\n",
      "        Series.count: Count non-NA/null observations in the Series.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> s = pd.Series([1, 3, 5, 7, 7])\n",
      "        >>> s\n",
      "        0    1\n",
      "        1    3\n",
      "        2    5\n",
      "        3    7\n",
      "        4    7\n",
      "        dtype: int64\n",
      "\n",
      "        >>> s.nunique()\n",
      "        4\n",
      "        \"\"\"\n",
      "        uniqs = self.unique()\n",
      "        if dropna:\n",
      "            uniqs = remove_na_arraylike(uniqs)\n",
      "        return len(uniqs)\n",
      "\n",
      "------------\n",
      "Method name: putmask\n",
      "Method definition:     @final\n",
      "    def putmask(self, mask, value) -> Index:\n",
      "        \"\"\"\n",
      "        Return a new Index of the values set with the mask.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.ndarray.putmask : Changes elements of an array\n",
      "            based on conditional and input values.\n",
      "        \"\"\"\n",
      "        mask, noop = validate_putmask(self._values, mask)\n",
      "        if noop:\n",
      "            return self.copy()\n",
      "\n",
      "        if self.dtype != object and is_valid_na_for_dtype(value, self.dtype):\n",
      "            # e.g. None -> np.nan, see also Block._standardize_fill_value\n",
      "            value = self._na_value\n",
      "        try:\n",
      "            converted = self._validate_fill_value(value)\n",
      "        except (LossySetitemError, ValueError, TypeError) as err:\n",
      "            if is_object_dtype(self):  # pragma: no cover\n",
      "                raise err\n",
      "\n",
      "            dtype = self._find_common_type_compat(value)\n",
      "            return self.astype(dtype).putmask(mask, value)\n",
      "\n",
      "        values = self._values.copy()\n",
      "\n",
      "        if isinstance(values, np.ndarray):\n",
      "            converted = setitem_datetimelike_compat(values, mask.sum(), converted)\n",
      "            np.putmask(values, mask, converted)\n",
      "\n",
      "        else:\n",
      "            # Note: we use the original value here, not converted, as\n",
      "            #  _validate_fill_value is not idempotent\n",
      "            values._putmask(mask, value)\n",
      "\n",
      "        return self._shallow_copy(values)\n",
      "\n",
      "------------\n",
      "Method name: ravel\n",
      "Method definition:     @final\n",
      "    def ravel(self, order=\"C\"):\n",
      "        \"\"\"\n",
      "        Return an ndarray of the flattened values of the underlying data.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "            Flattened array.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.ndarray.ravel : Return a flattened array.\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            \"Index.ravel returning ndarray is deprecated; in a future version \"\n",
      "            \"this will return a view on self.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        if needs_i8_conversion(self.dtype):\n",
      "            # Item \"ndarray[Any, Any]\" of \"Union[ExtensionArray, ndarray[Any, Any]]\"\n",
      "            # has no attribute \"_ndarray\"\n",
      "            values = self._data._ndarray  # type: ignore[union-attr]\n",
      "        elif is_interval_dtype(self.dtype):\n",
      "            values = np.asarray(self._data)\n",
      "        else:\n",
      "            values = self._get_engine_target()\n",
      "        return values.ravel(order=order)\n",
      "\n",
      "------------\n",
      "Method name: reindex\n",
      "Method definition:     def reindex(\n",
      "        self, target, method=None, level=None, limit=None, tolerance=None\n",
      "    ) -> tuple[Index, npt.NDArray[np.intp] | None]:\n",
      "        \"\"\"\n",
      "        Create index with target's values.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        target : an iterable\n",
      "        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n",
      "            * default: exact matches only.\n",
      "            * pad / ffill: find the PREVIOUS index value if no exact match.\n",
      "            * backfill / bfill: use NEXT index value if no exact match\n",
      "            * nearest: use the NEAREST index value if no exact match. Tied\n",
      "              distances are broken by preferring the larger index value.\n",
      "        level : int, optional\n",
      "            Level of multiindex.\n",
      "        limit : int, optional\n",
      "            Maximum number of consecutive labels in ``target`` to match for\n",
      "            inexact matches.\n",
      "        tolerance : int or float, optional\n",
      "            Maximum distance between original and new labels for inexact\n",
      "            matches. The values of the index at the matching locations must\n",
      "            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      "\n",
      "            Tolerance may be a scalar value, which applies the same tolerance\n",
      "            to all values, or list-like, which applies variable tolerance per\n",
      "            element. List-like includes list, tuple, array, Series, and must be\n",
      "            the same size as the index and its dtype must exactly match the\n",
      "            index's type.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        new_index : pd.Index\n",
      "            Resulting index.\n",
      "        indexer : np.ndarray[np.intp] or None\n",
      "            Indices of output values in original index.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If ``method`` passed along with ``level``.\n",
      "        ValueError\n",
      "            If non-unique multi-index\n",
      "        ValueError\n",
      "            If non-unique index and ``method`` or ``limit`` passed.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.reindex : Conform Series to new index with optional filling logic.\n",
      "        DataFrame.reindex : Conform DataFrame to new index with optional filling logic.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['car', 'bike', 'train', 'tractor'])\n",
      "        >>> idx\n",
      "        Index(['car', 'bike', 'train', 'tractor'], dtype='object')\n",
      "        >>> idx.reindex(['car', 'bike'])\n",
      "        (Index(['car', 'bike'], dtype='object'), array([0, 1]))\n",
      "        \"\"\"\n",
      "        # GH6552: preserve names when reindexing to non-named target\n",
      "        # (i.e. neither Index nor Series).\n",
      "        preserve_names = not hasattr(target, \"name\")\n",
      "\n",
      "        # GH7774: preserve dtype/tz if target is empty and not an Index.\n",
      "        target = ensure_has_len(target)  # target may be an iterator\n",
      "\n",
      "        if not isinstance(target, Index) and len(target) == 0:\n",
      "            if level is not None and self._is_multi:\n",
      "                # \"Index\" has no attribute \"levels\"; maybe \"nlevels\"?\n",
      "                idx = self.levels[level]  # type: ignore[attr-defined]\n",
      "            else:\n",
      "                idx = self\n",
      "            target = idx[:0]\n",
      "        else:\n",
      "            target = ensure_index(target)\n",
      "\n",
      "        if level is not None and (\n",
      "            isinstance(self, ABCMultiIndex) or isinstance(target, ABCMultiIndex)\n",
      "        ):\n",
      "            if method is not None:\n",
      "                raise TypeError(\"Fill method not supported if level passed\")\n",
      "\n",
      "            # TODO: tests where passing `keep_order=not self._is_multi`\n",
      "            #  makes a difference for non-MultiIndex case\n",
      "            target, indexer, _ = self._join_level(\n",
      "                target, level, how=\"right\", keep_order=not self._is_multi\n",
      "            )\n",
      "\n",
      "        else:\n",
      "            if self.equals(target):\n",
      "                indexer = None\n",
      "            else:\n",
      "                if self._index_as_unique:\n",
      "                    indexer = self.get_indexer(\n",
      "                        target, method=method, limit=limit, tolerance=tolerance\n",
      "                    )\n",
      "                elif self._is_multi:\n",
      "                    raise ValueError(\"cannot handle a non-unique multi-index!\")\n",
      "                else:\n",
      "                    if method is not None or limit is not None:\n",
      "                        raise ValueError(\n",
      "                            \"cannot reindex a non-unique index \"\n",
      "                            \"with a method or limit\"\n",
      "                        )\n",
      "                    indexer, _ = self.get_indexer_non_unique(target)\n",
      "\n",
      "                if not self.is_unique:\n",
      "                    # GH#42568\n",
      "                    warnings.warn(\n",
      "                        \"reindexing with a non-unique Index is deprecated and \"\n",
      "                        \"will raise in a future version.\",\n",
      "                        FutureWarning,\n",
      "                        stacklevel=find_stack_level(),\n",
      "                    )\n",
      "\n",
      "        target = self._wrap_reindex_result(target, indexer, preserve_names)\n",
      "        return target, indexer\n",
      "\n",
      "------------\n",
      "Method name: rename\n",
      "Method definition:     def rename(self, name, inplace=False):\n",
      "        \"\"\"\n",
      "        Alter Index or MultiIndex name.\n",
      "\n",
      "        Able to set new names without level. Defaults to returning new index.\n",
      "        Length of names must match number of levels in MultiIndex.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        name : label or list of labels\n",
      "            Name(s) to set.\n",
      "        inplace : bool, default False\n",
      "            Modifies the object directly, instead of creating a new Index or\n",
      "            MultiIndex.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index or None\n",
      "            The same type as the caller or None if ``inplace=True``.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.set_names : Able to set new names partially and by level.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['A', 'C', 'A', 'B'], name='score')\n",
      "        >>> idx.rename('grade')\n",
      "        Index(['A', 'C', 'A', 'B'], dtype='object', name='grade')\n",
      "\n",
      "        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n",
      "        ...                                   [2018, 2019]],\n",
      "        ...                                   names=['kind', 'year'])\n",
      "        >>> idx\n",
      "        MultiIndex([('python', 2018),\n",
      "                    ('python', 2019),\n",
      "                    ( 'cobra', 2018),\n",
      "                    ( 'cobra', 2019)],\n",
      "                   names=['kind', 'year'])\n",
      "        >>> idx.rename(['species', 'year'])\n",
      "        MultiIndex([('python', 2018),\n",
      "                    ('python', 2019),\n",
      "                    ( 'cobra', 2018),\n",
      "                    ( 'cobra', 2019)],\n",
      "                   names=['species', 'year'])\n",
      "        >>> idx.rename('species')\n",
      "        Traceback (most recent call last):\n",
      "        TypeError: Must pass list-like as `names`.\n",
      "        \"\"\"\n",
      "        return self.set_names([name], inplace=inplace)\n",
      "\n",
      "------------\n",
      "Method name: repeat\n",
      "Method definition:     @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n",
      "    def repeat(self, repeats, axis=None):\n",
      "        repeats = ensure_platform_int(repeats)\n",
      "        nv.validate_repeat((), {\"axis\": axis})\n",
      "        res_values = self._values.repeat(repeats)\n",
      "\n",
      "        # _constructor so RangeIndex->Int64Index\n",
      "        return self._constructor._simple_new(res_values, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: searchsorted\n",
      "Method definition:     @doc(_shared_docs[\"searchsorted\"], klass=\"Index\")\n",
      "    def searchsorted(\n",
      "        self,\n",
      "        value: NumpyValueArrayLike | ExtensionArray,\n",
      "        side: Literal[\"left\", \"right\"] = \"left\",\n",
      "        sorter: NumpySorter = None,\n",
      "    ) -> npt.NDArray[np.intp] | np.intp:\n",
      "\n",
      "        values = self._values\n",
      "        if not isinstance(values, np.ndarray):\n",
      "            # Going through EA.searchsorted directly improves performance GH#38083\n",
      "            return values.searchsorted(value, side=side, sorter=sorter)\n",
      "\n",
      "        return algorithms.searchsorted(\n",
      "            values,\n",
      "            value,\n",
      "            side=side,\n",
      "            sorter=sorter,\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: set_names\n",
      "Method definition:     @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"names\"])\n",
      "    def set_names(self, names, level=None, inplace: bool = False):\n",
      "        \"\"\"\n",
      "        Set Index or MultiIndex name.\n",
      "\n",
      "        Able to set new names partially and by level.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "\n",
      "        names : label or list of label or dict-like for MultiIndex\n",
      "            Name(s) to set.\n",
      "\n",
      "            .. versionchanged:: 1.3.0\n",
      "\n",
      "        level : int, label or list of int or label, optional\n",
      "            If the index is a MultiIndex and names is not dict-like, level(s) to set\n",
      "            (None for all levels). Otherwise level must be None.\n",
      "\n",
      "            .. versionchanged:: 1.3.0\n",
      "\n",
      "        inplace : bool, default False\n",
      "            Modifies the object directly, instead of creating a new Index or\n",
      "            MultiIndex.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index or None\n",
      "            The same type as the caller or None if ``inplace=True``.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.rename : Able to set new names without level.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx\n",
      "        Int64Index([1, 2, 3, 4], dtype='int64')\n",
      "        >>> idx.set_names('quarter')\n",
      "        Int64Index([1, 2, 3, 4], dtype='int64', name='quarter')\n",
      "\n",
      "        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n",
      "        ...                                   [2018, 2019]])\n",
      "        >>> idx\n",
      "        MultiIndex([('python', 2018),\n",
      "                    ('python', 2019),\n",
      "                    ( 'cobra', 2018),\n",
      "                    ( 'cobra', 2019)],\n",
      "                   )\n",
      "        >>> idx.set_names(['kind', 'year'], inplace=True)\n",
      "        >>> idx\n",
      "        MultiIndex([('python', 2018),\n",
      "                    ('python', 2019),\n",
      "                    ( 'cobra', 2018),\n",
      "                    ( 'cobra', 2019)],\n",
      "                   names=['kind', 'year'])\n",
      "        >>> idx.set_names('species', level=0)\n",
      "        MultiIndex([('python', 2018),\n",
      "                    ('python', 2019),\n",
      "                    ( 'cobra', 2018),\n",
      "                    ( 'cobra', 2019)],\n",
      "                   names=['species', 'year'])\n",
      "\n",
      "        When renaming levels with a dict, levels can not be passed.\n",
      "\n",
      "        >>> idx.set_names({'kind': 'snake'})\n",
      "        MultiIndex([('python', 2018),\n",
      "                    ('python', 2019),\n",
      "                    ( 'cobra', 2018),\n",
      "                    ( 'cobra', 2019)],\n",
      "                   names=['snake', 'year'])\n",
      "        \"\"\"\n",
      "        if level is not None and not isinstance(self, ABCMultiIndex):\n",
      "            raise ValueError(\"Level must be None for non-MultiIndex\")\n",
      "\n",
      "        elif level is not None and not is_list_like(level) and is_list_like(names):\n",
      "            raise TypeError(\"Names must be a string when a single level is provided.\")\n",
      "\n",
      "        elif not is_list_like(names) and level is None and self.nlevels > 1:\n",
      "            raise TypeError(\"Must pass list-like as `names`.\")\n",
      "\n",
      "        elif is_dict_like(names) and not isinstance(self, ABCMultiIndex):\n",
      "            raise TypeError(\"Can only pass dict-like as `names` for MultiIndex.\")\n",
      "\n",
      "        elif is_dict_like(names) and level is not None:\n",
      "            raise TypeError(\"Can not pass level for dictlike `names`.\")\n",
      "\n",
      "        if isinstance(self, ABCMultiIndex) and is_dict_like(names) and level is None:\n",
      "            # Transform dict to list of new names and corresponding levels\n",
      "            level, names_adjusted = [], []\n",
      "            for i, name in enumerate(self.names):\n",
      "                if name in names.keys():\n",
      "                    level.append(i)\n",
      "                    names_adjusted.append(names[name])\n",
      "            names = names_adjusted\n",
      "\n",
      "        if not is_list_like(names):\n",
      "            names = [names]\n",
      "        if level is not None and not is_list_like(level):\n",
      "            level = [level]\n",
      "\n",
      "        if inplace:\n",
      "            idx = self\n",
      "        else:\n",
      "            idx = self._view()\n",
      "\n",
      "        idx._set_names(names, level=level)\n",
      "        if not inplace:\n",
      "            return idx\n",
      "\n",
      "------------\n",
      "Method name: set_value\n",
      "Method definition:     @final\n",
      "    def set_value(self, arr, key, value) -> None:\n",
      "        \"\"\"\n",
      "        Fast lookup of value from 1-dimensional ndarray.\n",
      "\n",
      "        .. deprecated:: 1.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Only use this if you know what you're doing.\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            (\n",
      "                \"The 'set_value' method is deprecated, and \"\n",
      "                \"will be removed in a future version.\"\n",
      "            ),\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        loc = self._engine.get_loc(key)\n",
      "        if not can_hold_element(arr, value):\n",
      "            raise ValueError\n",
      "        arr[loc] = value\n",
      "\n",
      "------------\n",
      "Method name: shift\n",
      "Method definition:     def shift(self, periods=1, freq=None):\n",
      "        \"\"\"\n",
      "        Shift index by desired number of time frequency increments.\n",
      "\n",
      "        This method is for shifting the values of datetime-like indexes\n",
      "        by a specified time increment a given number of times.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        periods : int, default 1\n",
      "            Number of periods (or increments) to shift by,\n",
      "            can be positive or negative.\n",
      "        freq : pandas.DateOffset, pandas.Timedelta or str, optional\n",
      "            Frequency increment to shift by.\n",
      "            If None, the index is shifted by its own `freq` attribute.\n",
      "            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        pandas.Index\n",
      "            Shifted index.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.shift : Shift values of Series.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This method is only implemented for datetime-like index classes,\n",
      "        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Put the first 5 month starts of 2011 into an index.\n",
      "\n",
      "        >>> month_starts = pd.date_range('1/1/2011', periods=5, freq='MS')\n",
      "        >>> month_starts\n",
      "        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n",
      "                       '2011-05-01'],\n",
      "                      dtype='datetime64[ns]', freq='MS')\n",
      "\n",
      "        Shift the index by 10 days.\n",
      "\n",
      "        >>> month_starts.shift(10, freq='D')\n",
      "        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n",
      "                       '2011-05-11'],\n",
      "                      dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "        The default value of `freq` is the `freq` attribute of the index,\n",
      "        which is 'MS' (month start) in this example.\n",
      "\n",
      "        >>> month_starts.shift(10)\n",
      "        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n",
      "                       '2012-03-01'],\n",
      "                      dtype='datetime64[ns]', freq='MS')\n",
      "        \"\"\"\n",
      "        raise NotImplementedError(\n",
      "            f\"This method is only implemented for DatetimeIndex, PeriodIndex and \"\n",
      "            f\"TimedeltaIndex; Got type {type(self).__name__}\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: slice_indexer\n",
      "Method definition:     def slice_indexer(\n",
      "        self,\n",
      "        start: Hashable | None = None,\n",
      "        end: Hashable | None = None,\n",
      "        step: int | None = None,\n",
      "        kind=no_default,\n",
      "    ) -> slice:\n",
      "        \"\"\"\n",
      "        Compute the slice indexer for input labels and step.\n",
      "\n",
      "        Index needs to be ordered and unique.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        start : label, default None\n",
      "            If None, defaults to the beginning.\n",
      "        end : label, default None\n",
      "            If None, defaults to the end.\n",
      "        step : int, default None\n",
      "        kind : str, default None\n",
      "\n",
      "            .. deprecated:: 1.4.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        indexer : slice\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        KeyError : If key does not exist, or key is not unique and index is\n",
      "            not ordered.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This function assumes that the data is sorted, so use at your own peril\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        This is a method on all index types. For example you can do:\n",
      "\n",
      "        >>> idx = pd.Index(list('abcd'))\n",
      "        >>> idx.slice_indexer(start='b', end='c')\n",
      "        slice(1, 3, None)\n",
      "\n",
      "        >>> idx = pd.MultiIndex.from_arrays([list('abcd'), list('efgh')])\n",
      "        >>> idx.slice_indexer(start='b', end=('c', 'g'))\n",
      "        slice(1, 3, None)\n",
      "        \"\"\"\n",
      "        self._deprecated_arg(kind, \"kind\", \"slice_indexer\")\n",
      "\n",
      "        start_slice, end_slice = self.slice_locs(start, end, step=step)\n",
      "\n",
      "        # return a slice\n",
      "        if not is_scalar(start_slice):\n",
      "            raise AssertionError(\"Start slice bound is non-scalar\")\n",
      "        if not is_scalar(end_slice):\n",
      "            raise AssertionError(\"End slice bound is non-scalar\")\n",
      "\n",
      "        return slice(start_slice, end_slice, step)\n",
      "\n",
      "------------\n",
      "Method name: slice_locs\n",
      "Method definition:     def slice_locs(\n",
      "        self, start=None, end=None, step=None, kind=no_default\n",
      "    ) -> tuple[int, int]:\n",
      "        \"\"\"\n",
      "        Compute slice locations for input labels.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        start : label, default None\n",
      "            If None, defaults to the beginning.\n",
      "        end : label, default None\n",
      "            If None, defaults to the end.\n",
      "        step : int, defaults None\n",
      "            If None, defaults to 1.\n",
      "        kind : {'loc', 'getitem'} or None\n",
      "\n",
      "            .. deprecated:: 1.4.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        start, end : int\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.get_loc : Get location for a single label.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This method only works if the index is monotonic or unique.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(list('abcd'))\n",
      "        >>> idx.slice_locs(start='b', end='c')\n",
      "        (1, 3)\n",
      "        \"\"\"\n",
      "        self._deprecated_arg(kind, \"kind\", \"slice_locs\")\n",
      "        inc = step is None or step >= 0\n",
      "\n",
      "        if not inc:\n",
      "            # If it's a reverse slice, temporarily swap bounds.\n",
      "            start, end = end, start\n",
      "\n",
      "        # GH 16785: If start and end happen to be date strings with UTC offsets\n",
      "        # attempt to parse and check that the offsets are the same\n",
      "        if isinstance(start, (str, datetime)) and isinstance(end, (str, datetime)):\n",
      "            try:\n",
      "                ts_start = Timestamp(start)\n",
      "                ts_end = Timestamp(end)\n",
      "            except (ValueError, TypeError):\n",
      "                pass\n",
      "            else:\n",
      "                if not tz_compare(ts_start.tzinfo, ts_end.tzinfo):\n",
      "                    raise ValueError(\"Both dates must have the same UTC offset\")\n",
      "\n",
      "        start_slice = None\n",
      "        if start is not None:\n",
      "            start_slice = self.get_slice_bound(start, \"left\")\n",
      "        if start_slice is None:\n",
      "            start_slice = 0\n",
      "\n",
      "        end_slice = None\n",
      "        if end is not None:\n",
      "            end_slice = self.get_slice_bound(end, \"right\")\n",
      "        if end_slice is None:\n",
      "            end_slice = len(self)\n",
      "\n",
      "        if not inc:\n",
      "            # Bounds at this moment are swapped, swap them back and shift by 1.\n",
      "            #\n",
      "            # slice_locs('B', 'A', step=-1): s='B', e='A'\n",
      "            #\n",
      "            #              s='A'                 e='B'\n",
      "            # AFTER SWAP:    |                     |\n",
      "            #                v ------------------> V\n",
      "            #           -----------------------------------\n",
      "            #           | | |A|A|A|A| | | | | |B|B| | | | |\n",
      "            #           -----------------------------------\n",
      "            #              ^ <------------------ ^\n",
      "            # SHOULD BE:   |                     |\n",
      "            #           end=s-1              start=e-1\n",
      "            #\n",
      "            end_slice, start_slice = start_slice - 1, end_slice - 1\n",
      "\n",
      "            # i == -1 triggers ``len(self) + i`` selection that points to the\n",
      "            # last element, not before-the-first one, subtracting len(self)\n",
      "            # compensates that.\n",
      "            if end_slice == -1:\n",
      "                end_slice -= len(self)\n",
      "            if start_slice == -1:\n",
      "                start_slice -= len(self)\n",
      "\n",
      "        return start_slice, end_slice\n",
      "\n",
      "------------\n",
      "Method name: sort\n",
      "Method definition:     @final\n",
      "    def sort(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Use sort_values instead.\n",
      "        \"\"\"\n",
      "        raise TypeError(\"cannot sort an Index object in-place, use sort_values instead\")\n",
      "\n",
      "------------\n",
      "Method name: sort_values\n",
      "Method definition:     def sort_values(\n",
      "        self,\n",
      "        return_indexer: bool = False,\n",
      "        ascending: bool = True,\n",
      "        na_position: str_t = \"last\",\n",
      "        key: Callable | None = None,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Return a sorted copy of the index.\n",
      "\n",
      "        Return a sorted copy of the index, and optionally return the indices\n",
      "        that sorted the index itself.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        return_indexer : bool, default False\n",
      "            Should the indices that would sort the index be returned.\n",
      "        ascending : bool, default True\n",
      "            Should the index values be sorted in an ascending order.\n",
      "        na_position : {'first' or 'last'}, default 'last'\n",
      "            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n",
      "            the end.\n",
      "\n",
      "            .. versionadded:: 1.2.0\n",
      "\n",
      "        key : callable, optional\n",
      "            If not None, apply the key function to the index values\n",
      "            before sorting. This is similar to the `key` argument in the\n",
      "            builtin :meth:`sorted` function, with the notable difference that\n",
      "            this `key` function should be *vectorized*. It should expect an\n",
      "            ``Index`` and return an ``Index`` of the same shape.\n",
      "\n",
      "            .. versionadded:: 1.1.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        sorted_index : pandas.Index\n",
      "            Sorted copy of the index.\n",
      "        indexer : numpy.ndarray, optional\n",
      "            The indices that the index itself was sorted by.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.sort_values : Sort values of a Series.\n",
      "        DataFrame.sort_values : Sort values in a DataFrame.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([10, 100, 1, 1000])\n",
      "        >>> idx\n",
      "        Int64Index([10, 100, 1, 1000], dtype='int64')\n",
      "\n",
      "        Sort values in ascending order (default behavior).\n",
      "\n",
      "        >>> idx.sort_values()\n",
      "        Int64Index([1, 10, 100, 1000], dtype='int64')\n",
      "\n",
      "        Sort values in descending order, and also get the indices `idx` was\n",
      "        sorted by.\n",
      "\n",
      "        >>> idx.sort_values(ascending=False, return_indexer=True)\n",
      "        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\n",
      "        \"\"\"\n",
      "        idx = ensure_key_mapped(self, key)\n",
      "\n",
      "        # GH 35584. Sort missing values according to na_position kwarg\n",
      "        # ignore na_position for MultiIndex\n",
      "        if not isinstance(self, ABCMultiIndex):\n",
      "            _as = nargsort(\n",
      "                items=idx, ascending=ascending, na_position=na_position, key=key\n",
      "            )\n",
      "        else:\n",
      "            _as = idx.argsort()\n",
      "            if not ascending:\n",
      "                _as = _as[::-1]\n",
      "\n",
      "        sorted_index = self.take(_as)\n",
      "\n",
      "        if return_indexer:\n",
      "            return sorted_index, _as\n",
      "        else:\n",
      "            return sorted_index\n",
      "\n",
      "------------\n",
      "Method name: sortlevel\n",
      "Method definition:     def sortlevel(self, level=None, ascending=True, sort_remaining=None):\n",
      "        \"\"\"\n",
      "        For internal compatibility with the Index API.\n",
      "\n",
      "        Sort the Index. This is for compat with MultiIndex\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        ascending : bool, default True\n",
      "            False to sort in descending order\n",
      "\n",
      "        level, sort_remaining are compat parameters\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "        \"\"\"\n",
      "        if not isinstance(ascending, (list, bool)):\n",
      "            raise TypeError(\n",
      "                \"ascending must be a single bool value or\"\n",
      "                \"a list of bool values of length 1\"\n",
      "            )\n",
      "\n",
      "        if isinstance(ascending, list):\n",
      "            if len(ascending) != 1:\n",
      "                raise TypeError(\"ascending must be a list of bool values of length 1\")\n",
      "            ascending = ascending[0]\n",
      "\n",
      "        if not isinstance(ascending, bool):\n",
      "            raise TypeError(\"ascending must be a bool value\")\n",
      "\n",
      "        return self.sort_values(return_indexer=True, ascending=ascending)\n",
      "\n",
      "------------\n",
      "Method name: symmetric_difference\n",
      "Method definition:     def symmetric_difference(self, other, result_name=None, sort=None):\n",
      "        \"\"\"\n",
      "        Compute the symmetric difference of two Index objects.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index or array-like\n",
      "        result_name : str\n",
      "        sort : False or None, default None\n",
      "            Whether to sort the resulting index. By default, the\n",
      "            values are attempted to be sorted, but any TypeError from\n",
      "            incomparable elements is caught by pandas.\n",
      "\n",
      "            * None : Attempt to sort the result, but catch any TypeErrors\n",
      "              from comparing incomparable elements.\n",
      "            * False : Do not sort the result.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        symmetric_difference : Index\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        ``symmetric_difference`` contains elements that appear in either\n",
      "        ``idx1`` or ``idx2`` but not both. Equivalent to the Index created by\n",
      "        ``idx1.difference(idx2) | idx2.difference(idx1)`` with duplicates\n",
      "        dropped.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx1 = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx2 = pd.Index([2, 3, 4, 5])\n",
      "        >>> idx1.symmetric_difference(idx2)\n",
      "        Int64Index([1, 5], dtype='int64')\n",
      "        \"\"\"\n",
      "        self._validate_sort_keyword(sort)\n",
      "        self._assert_can_do_setop(other)\n",
      "        other, result_name_update = self._convert_can_do_setop(other)\n",
      "        if result_name is None:\n",
      "            result_name = result_name_update\n",
      "\n",
      "        if not is_dtype_equal(self.dtype, other.dtype):\n",
      "            self._deprecate_dti_setop(other, \"symmetric_difference\")\n",
      "\n",
      "        if not self._should_compare(other):\n",
      "            return self.union(other, sort=sort).rename(result_name)\n",
      "\n",
      "        elif not is_dtype_equal(self.dtype, other.dtype):\n",
      "            dtype = self._find_common_type_compat(other)\n",
      "            this = self.astype(dtype, copy=False)\n",
      "            that = other.astype(dtype, copy=False)\n",
      "            return this.symmetric_difference(that, sort=sort).rename(result_name)\n",
      "\n",
      "        this = self.unique()\n",
      "        other = other.unique()\n",
      "        indexer = this.get_indexer_for(other)\n",
      "\n",
      "        # {this} minus {other}\n",
      "        common_indexer = indexer.take((indexer != -1).nonzero()[0])\n",
      "        left_indexer = np.setdiff1d(\n",
      "            np.arange(this.size), common_indexer, assume_unique=True\n",
      "        )\n",
      "        left_diff = this._values.take(left_indexer)\n",
      "\n",
      "        # {other} minus {this}\n",
      "        right_indexer = (indexer == -1).nonzero()[0]\n",
      "        right_diff = other._values.take(right_indexer)\n",
      "\n",
      "        res_values = concat_compat([left_diff, right_diff])\n",
      "        res_values = _maybe_try_sort(res_values, sort)\n",
      "\n",
      "        # pass dtype so we retain object dtype\n",
      "        result = Index(res_values, name=result_name, dtype=res_values.dtype)\n",
      "\n",
      "        if self._is_multi:\n",
      "            self = cast(\"MultiIndex\", self)\n",
      "            if len(result) == 0:\n",
      "                # On equal symmetric_difference MultiIndexes the difference is empty.\n",
      "                # Therefore, an empty MultiIndex is returned GH#13490\n",
      "                return type(self)(\n",
      "                    levels=[[] for _ in range(self.nlevels)],\n",
      "                    codes=[[] for _ in range(self.nlevels)],\n",
      "                    names=result.name,\n",
      "                )\n",
      "            return type(self).from_tuples(result, names=result.name)\n",
      "\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: take\n",
      "Method definition:     @Appender(_index_shared_docs[\"take\"] % _index_doc_kwargs)\n",
      "    def take(\n",
      "        self, indices, axis: int = 0, allow_fill: bool = True, fill_value=None, **kwargs\n",
      "    ):\n",
      "        if kwargs:\n",
      "            nv.validate_take((), kwargs)\n",
      "        if is_scalar(indices):\n",
      "            raise TypeError(\"Expected indices to be array-like\")\n",
      "        indices = ensure_platform_int(indices)\n",
      "        allow_fill = self._maybe_disallow_fill(allow_fill, fill_value, indices)\n",
      "\n",
      "        # Note: we discard fill_value and use self._na_value, only relevant\n",
      "        #  in the case where allow_fill is True and fill_value is not None\n",
      "        values = self._values\n",
      "        if isinstance(values, np.ndarray):\n",
      "            taken = algos.take(\n",
      "                values, indices, allow_fill=allow_fill, fill_value=self._na_value\n",
      "            )\n",
      "        else:\n",
      "            # algos.take passes 'axis' keyword which not all EAs accept\n",
      "            taken = values.take(\n",
      "                indices, allow_fill=allow_fill, fill_value=self._na_value\n",
      "            )\n",
      "        # _constructor so RangeIndex->Int64Index\n",
      "        return self._constructor._simple_new(taken, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: to_flat_index\n",
      "Method definition:     def to_flat_index(self: _IndexT) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Identity method.\n",
      "\n",
      "        This is implemented for compatibility with subclass implementations\n",
      "        when chaining.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        pd.Index\n",
      "            Caller.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        MultiIndex.to_flat_index : Subclass implementation.\n",
      "        \"\"\"\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: to_frame\n",
      "Method definition:     def to_frame(\n",
      "        self, index: bool = True, name: Hashable = lib.no_default\n",
      "    ) -> DataFrame:\n",
      "        \"\"\"\n",
      "        Create a DataFrame with a column containing the Index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        index : bool, default True\n",
      "            Set the index of the returned DataFrame as the original Index.\n",
      "\n",
      "        name : object, default None\n",
      "            The passed name should substitute for the index name (if it has\n",
      "            one).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        DataFrame\n",
      "            DataFrame containing the original Index data.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.to_series : Convert an Index to a Series.\n",
      "        Series.to_frame : Convert Series to DataFrame.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n",
      "        >>> idx.to_frame()\n",
      "               animal\n",
      "        animal\n",
      "        Ant       Ant\n",
      "        Bear     Bear\n",
      "        Cow       Cow\n",
      "\n",
      "        By default, the original Index is reused. To enforce a new Index:\n",
      "\n",
      "        >>> idx.to_frame(index=False)\n",
      "            animal\n",
      "        0   Ant\n",
      "        1  Bear\n",
      "        2   Cow\n",
      "\n",
      "        To override the name of the resulting column, specify `name`:\n",
      "\n",
      "        >>> idx.to_frame(index=False, name='zoo')\n",
      "            zoo\n",
      "        0   Ant\n",
      "        1  Bear\n",
      "        2   Cow\n",
      "        \"\"\"\n",
      "        from pandas import DataFrame\n",
      "\n",
      "        if name is None:\n",
      "            warnings.warn(\n",
      "                \"Explicitly passing `name=None` currently preserves the Index's name \"\n",
      "                \"or uses a default name of 0. This behaviour is deprecated, and in \"\n",
      "                \"the future `None` will be used as the name of the resulting \"\n",
      "                \"DataFrame column.\",\n",
      "                FutureWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "            name = lib.no_default\n",
      "\n",
      "        if name is lib.no_default:\n",
      "            name = self._get_level_names()\n",
      "        result = DataFrame({name: self._values.copy()})\n",
      "\n",
      "        if index:\n",
      "            result.index = self\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: to_list\n",
      "Method definition:     def tolist(self):\n",
      "        \"\"\"\n",
      "        Return a list of the values.\n",
      "\n",
      "        These are each a scalar type, which is a Python scalar\n",
      "        (for str, int, float) or a pandas scalar\n",
      "        (for Timestamp/Timedelta/Interval/Period)\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        list\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.ndarray.tolist : Return the array as an a.ndim-levels deep\n",
      "            nested list of Python scalars.\n",
      "        \"\"\"\n",
      "        return self._values.tolist()\n",
      "\n",
      "------------\n",
      "Method name: to_native_types\n",
      "Method definition:     @final\n",
      "    def to_native_types(self, slicer=None, **kwargs) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Format specified values of `self` and return them.\n",
      "\n",
      "        .. deprecated:: 1.2.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        slicer : int, array-like\n",
      "            An indexer into `self` that specifies which values\n",
      "            are used in the formatting process.\n",
      "        kwargs : dict\n",
      "            Options for specifying how the values should be formatted.\n",
      "            These options include the following:\n",
      "\n",
      "            1) na_rep : str\n",
      "                The value that serves as a placeholder for NULL values\n",
      "            2) quoting : bool or None\n",
      "                Whether or not there are quoted values in `self`\n",
      "            3) date_format : str\n",
      "                The format used to represent date-like values.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "            Formatted values.\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            \"The 'to_native_types' method is deprecated and will be removed in \"\n",
      "            \"a future version. Use 'astype(str)' instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        values = self\n",
      "        if slicer is not None:\n",
      "            values = values[slicer]\n",
      "        return values._format_native_types(**kwargs)\n",
      "\n",
      "------------\n",
      "Method name: to_numpy\n",
      "Method definition:     def to_numpy(\n",
      "        self,\n",
      "        dtype: npt.DTypeLike | None = None,\n",
      "        copy: bool = False,\n",
      "        na_value: object = lib.no_default,\n",
      "        **kwargs,\n",
      "    ) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        A NumPy ndarray representing the values in this Series or Index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        dtype : str or numpy.dtype, optional\n",
      "            The dtype to pass to :meth:`numpy.asarray`.\n",
      "        copy : bool, default False\n",
      "            Whether to ensure that the returned value is not a view on\n",
      "            another array. Note that ``copy=False`` does not *ensure* that\n",
      "            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n",
      "            a copy is made, even if not strictly necessary.\n",
      "        na_value : Any, optional\n",
      "            The value to use for missing values. The default value depends\n",
      "            on `dtype` and the type of the array.\n",
      "\n",
      "            .. versionadded:: 1.0.0\n",
      "\n",
      "        **kwargs\n",
      "            Additional keywords passed through to the ``to_numpy`` method\n",
      "            of the underlying array (for extension arrays).\n",
      "\n",
      "            .. versionadded:: 1.0.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.array : Get the actual data stored within.\n",
      "        Index.array : Get the actual data stored within.\n",
      "        DataFrame.to_numpy : Similar method for DataFrame.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The returned array will be the same up to equality (values equal\n",
      "        in `self` will be equal in the returned array; likewise for values\n",
      "        that are not equal). When `self` contains an ExtensionArray, the\n",
      "        dtype may be different. For example, for a category-dtype Series,\n",
      "        ``to_numpy()`` will return a NumPy array and the categorical dtype\n",
      "        will be lost.\n",
      "\n",
      "        For NumPy dtypes, this will be a reference to the actual data stored\n",
      "        in this Series or Index (assuming ``copy=False``). Modifying the result\n",
      "        in place will modify the data stored in the Series or Index (not that\n",
      "        we recommend doing that).\n",
      "\n",
      "        For extension types, ``to_numpy()`` *may* require copying data and\n",
      "        coercing the result to a NumPy type (possibly object), which may be\n",
      "        expensive. When you need a no-copy reference to the underlying data,\n",
      "        :attr:`Series.array` should be used instead.\n",
      "\n",
      "        This table lays out the different dtypes and default return types of\n",
      "        ``to_numpy()`` for various dtypes within pandas.\n",
      "\n",
      "        ================== ================================\n",
      "        dtype              array type\n",
      "        ================== ================================\n",
      "        category[T]        ndarray[T] (same dtype as input)\n",
      "        period             ndarray[object] (Periods)\n",
      "        interval           ndarray[object] (Intervals)\n",
      "        IntegerNA          ndarray[object]\n",
      "        datetime64[ns]     datetime64[ns]\n",
      "        datetime64[ns, tz] ndarray[object] (Timestamps)\n",
      "        ================== ================================\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\n",
      "        >>> ser.to_numpy()\n",
      "        array(['a', 'b', 'a'], dtype=object)\n",
      "\n",
      "        Specify the `dtype` to control how datetime-aware data is represented.\n",
      "        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\n",
      "        objects, each with the correct ``tz``.\n",
      "\n",
      "        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n",
      "        >>> ser.to_numpy(dtype=object)\n",
      "        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\n",
      "               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\n",
      "              dtype=object)\n",
      "\n",
      "        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\n",
      "        datetime64 values. The values are converted to UTC and the timezone\n",
      "        info is dropped.\n",
      "\n",
      "        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],\n",
      "              dtype='datetime64[ns]')\n",
      "        \"\"\"\n",
      "        if is_extension_array_dtype(self.dtype):\n",
      "            return self.array.to_numpy(dtype, copy=copy, na_value=na_value, **kwargs)\n",
      "        elif kwargs:\n",
      "            bad_keys = list(kwargs.keys())[0]\n",
      "            raise TypeError(\n",
      "                f\"to_numpy() got an unexpected keyword argument '{bad_keys}'\"\n",
      "            )\n",
      "\n",
      "        result = np.asarray(self._values, dtype=dtype)\n",
      "        # TODO(GH-24345): Avoid potential double copy\n",
      "        if copy or na_value is not lib.no_default:\n",
      "            result = result.copy()\n",
      "            if na_value is not lib.no_default:\n",
      "                result[np.asanyarray(self.isna())] = na_value\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: to_series\n",
      "Method definition:     def to_series(self, index=None, name: Hashable = None) -> Series:\n",
      "        \"\"\"\n",
      "        Create a Series with both index and values equal to the index keys.\n",
      "\n",
      "        Useful with map for returning an indexer based on an index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        index : Index, optional\n",
      "            Index of resulting Series. If None, defaults to original index.\n",
      "        name : str, optional\n",
      "            Name of resulting Series. If None, defaults to name of original\n",
      "            index.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Series\n",
      "            The dtype will be based on the type of the Index values.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.to_frame : Convert an Index to a DataFrame.\n",
      "        Series.to_frame : Convert Series to DataFrame.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n",
      "\n",
      "        By default, the original Index and original name is reused.\n",
      "\n",
      "        >>> idx.to_series()\n",
      "        animal\n",
      "        Ant      Ant\n",
      "        Bear    Bear\n",
      "        Cow      Cow\n",
      "        Name: animal, dtype: object\n",
      "\n",
      "        To enforce a new Index, specify new labels to ``index``:\n",
      "\n",
      "        >>> idx.to_series(index=[0, 1, 2])\n",
      "        0     Ant\n",
      "        1    Bear\n",
      "        2     Cow\n",
      "        Name: animal, dtype: object\n",
      "\n",
      "        To override the name of the resulting column, specify `name`:\n",
      "\n",
      "        >>> idx.to_series(name='zoo')\n",
      "        animal\n",
      "        Ant      Ant\n",
      "        Bear    Bear\n",
      "        Cow      Cow\n",
      "        Name: zoo, dtype: object\n",
      "        \"\"\"\n",
      "        from pandas import Series\n",
      "\n",
      "        if index is None:\n",
      "            index = self._view()\n",
      "        if name is None:\n",
      "            name = self.name\n",
      "\n",
      "        return Series(self._values.copy(), index=index, name=name)\n",
      "\n",
      "------------\n",
      "Method name: tolist\n",
      "Method definition:     def tolist(self):\n",
      "        \"\"\"\n",
      "        Return a list of the values.\n",
      "\n",
      "        These are each a scalar type, which is a Python scalar\n",
      "        (for str, int, float) or a pandas scalar\n",
      "        (for Timestamp/Timedelta/Interval/Period)\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        list\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.ndarray.tolist : Return the array as an a.ndim-levels deep\n",
      "            nested list of Python scalars.\n",
      "        \"\"\"\n",
      "        return self._values.tolist()\n",
      "\n",
      "------------\n",
      "Method name: transpose\n",
      "Method definition:     def transpose(self: _T, *args, **kwargs) -> _T:\n",
      "        \"\"\"\n",
      "        Return the transpose, which is by definition self.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        %(klass)s\n",
      "        \"\"\"\n",
      "        nv.validate_transpose(args, kwargs)\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: union\n",
      "Method definition:     @final\n",
      "    def union(self, other, sort=None):\n",
      "        \"\"\"\n",
      "        Form the union of two Index objects.\n",
      "\n",
      "        If the Index objects are incompatible, both Index objects will be\n",
      "        cast to dtype('object') first.\n",
      "\n",
      "            .. versionchanged:: 0.25.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index or array-like\n",
      "        sort : bool or None, default None\n",
      "            Whether to sort the resulting Index.\n",
      "\n",
      "            * None : Sort the result, except when\n",
      "\n",
      "              1. `self` and `other` are equal.\n",
      "              2. `self` or `other` has length 0.\n",
      "              3. Some values in `self` or `other` cannot be compared.\n",
      "                 A RuntimeWarning is issued in this case.\n",
      "\n",
      "            * False : do not sort the result.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        union : Index\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Union matching dtypes\n",
      "\n",
      "        >>> idx1 = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx2 = pd.Index([3, 4, 5, 6])\n",
      "        >>> idx1.union(idx2)\n",
      "        Int64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n",
      "\n",
      "        Union mismatched dtypes\n",
      "\n",
      "        >>> idx1 = pd.Index(['a', 'b', 'c', 'd'])\n",
      "        >>> idx2 = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx1.union(idx2)\n",
      "        Index(['a', 'b', 'c', 'd', 1, 2, 3, 4], dtype='object')\n",
      "\n",
      "        MultiIndex case\n",
      "\n",
      "        >>> idx1 = pd.MultiIndex.from_arrays(\n",
      "        ...     [[1, 1, 2, 2], [\"Red\", \"Blue\", \"Red\", \"Blue\"]]\n",
      "        ... )\n",
      "        >>> idx1\n",
      "        MultiIndex([(1,  'Red'),\n",
      "            (1, 'Blue'),\n",
      "            (2,  'Red'),\n",
      "            (2, 'Blue')],\n",
      "           )\n",
      "        >>> idx2 = pd.MultiIndex.from_arrays(\n",
      "        ...     [[3, 3, 2, 2], [\"Red\", \"Green\", \"Red\", \"Green\"]]\n",
      "        ... )\n",
      "        >>> idx2\n",
      "        MultiIndex([(3,   'Red'),\n",
      "            (3, 'Green'),\n",
      "            (2,   'Red'),\n",
      "            (2, 'Green')],\n",
      "           )\n",
      "        >>> idx1.union(idx2)\n",
      "        MultiIndex([(1,  'Blue'),\n",
      "            (1,   'Red'),\n",
      "            (2,  'Blue'),\n",
      "            (2, 'Green'),\n",
      "            (2,   'Red'),\n",
      "            (3, 'Green'),\n",
      "            (3,   'Red')],\n",
      "           )\n",
      "        >>> idx1.union(idx2, sort=False)\n",
      "        MultiIndex([(1,   'Red'),\n",
      "            (1,  'Blue'),\n",
      "            (2,   'Red'),\n",
      "            (2,  'Blue'),\n",
      "            (3,   'Red'),\n",
      "            (3, 'Green'),\n",
      "            (2, 'Green')],\n",
      "           )\n",
      "        \"\"\"\n",
      "        self._validate_sort_keyword(sort)\n",
      "        self._assert_can_do_setop(other)\n",
      "        other, result_name = self._convert_can_do_setop(other)\n",
      "\n",
      "        if not is_dtype_equal(self.dtype, other.dtype):\n",
      "            if (\n",
      "                isinstance(self, ABCMultiIndex)\n",
      "                and not is_object_dtype(unpack_nested_dtype(other))\n",
      "                and len(other) > 0\n",
      "            ):\n",
      "                raise NotImplementedError(\n",
      "                    \"Can only union MultiIndex with MultiIndex or Index of tuples, \"\n",
      "                    \"try mi.to_flat_index().union(other) instead.\"\n",
      "                )\n",
      "            self._deprecate_dti_setop(other, \"union\")\n",
      "\n",
      "            dtype = self._find_common_type_compat(other)\n",
      "            left = self.astype(dtype, copy=False)\n",
      "            right = other.astype(dtype, copy=False)\n",
      "            return left.union(right, sort=sort)\n",
      "\n",
      "        elif not len(other) or self.equals(other):\n",
      "            # NB: whether this (and the `if not len(self)` check below) come before\n",
      "            #  or after the is_dtype_equal check above affects the returned dtype\n",
      "            return self._get_reconciled_name_object(other)\n",
      "\n",
      "        elif not len(self):\n",
      "            return other._get_reconciled_name_object(self)\n",
      "\n",
      "        result = self._union(other, sort=sort)\n",
      "\n",
      "        return self._wrap_setop_result(other, result)\n",
      "\n",
      "------------\n",
      "Method name: unique\n",
      "Method definition:     def unique(self: _IndexT, level: Hashable | None = None) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Return unique values in the index.\n",
      "\n",
      "        Unique values are returned in order of appearance, this does NOT sort.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        level : int or hashable, optional\n",
      "            Only return values from specified level (for MultiIndex).\n",
      "            If int, gets the level by integer position, else by level name.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        unique : Numpy array of unique values in that column.\n",
      "        Series.unique : Return unique values of Series object.\n",
      "        \"\"\"\n",
      "        if level is not None:\n",
      "            self._validate_index_level(level)\n",
      "\n",
      "        if self.is_unique:\n",
      "            return self._view()\n",
      "\n",
      "        result = super().unique()\n",
      "        return self._shallow_copy(result)\n",
      "\n",
      "------------\n",
      "Method name: value_counts\n",
      "Method definition:     def value_counts(\n",
      "        self,\n",
      "        normalize: bool = False,\n",
      "        sort: bool = True,\n",
      "        ascending: bool = False,\n",
      "        bins=None,\n",
      "        dropna: bool = True,\n",
      "    ) -> Series:\n",
      "        \"\"\"\n",
      "        Return a Series containing counts of unique values.\n",
      "\n",
      "        The resulting object will be in descending order so that the\n",
      "        first element is the most frequently-occurring element.\n",
      "        Excludes NA values by default.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        normalize : bool, default False\n",
      "            If True then the object returned will contain the relative\n",
      "            frequencies of the unique values.\n",
      "        sort : bool, default True\n",
      "            Sort by frequencies.\n",
      "        ascending : bool, default False\n",
      "            Sort in ascending order.\n",
      "        bins : int, optional\n",
      "            Rather than count values, group them into half-open bins,\n",
      "            a convenience for ``pd.cut``, only works with numeric data.\n",
      "        dropna : bool, default True\n",
      "            Don't include counts of NaN.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Series\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.count: Number of non-NA elements in a Series.\n",
      "        DataFrame.count: Number of non-NA elements in a DataFrame.\n",
      "        DataFrame.value_counts: Equivalent method on DataFrames.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> index = pd.Index([3, 1, 2, 3, 4, np.nan])\n",
      "        >>> index.value_counts()\n",
      "        3.0    2\n",
      "        1.0    1\n",
      "        2.0    1\n",
      "        4.0    1\n",
      "        dtype: int64\n",
      "\n",
      "        With `normalize` set to `True`, returns the relative frequency by\n",
      "        dividing all values by the sum of values.\n",
      "\n",
      "        >>> s = pd.Series([3, 1, 2, 3, 4, np.nan])\n",
      "        >>> s.value_counts(normalize=True)\n",
      "        3.0    0.4\n",
      "        1.0    0.2\n",
      "        2.0    0.2\n",
      "        4.0    0.2\n",
      "        dtype: float64\n",
      "\n",
      "        **bins**\n",
      "\n",
      "        Bins can be useful for going from a continuous variable to a\n",
      "        categorical variable; instead of counting unique\n",
      "        apparitions of values, divide the index in the specified\n",
      "        number of half-open bins.\n",
      "\n",
      "        >>> s.value_counts(bins=3)\n",
      "        (0.996, 2.0]    2\n",
      "        (2.0, 3.0]      2\n",
      "        (3.0, 4.0]      1\n",
      "        dtype: int64\n",
      "\n",
      "        **dropna**\n",
      "\n",
      "        With `dropna` set to `False` we can also see NaN index values.\n",
      "\n",
      "        >>> s.value_counts(dropna=False)\n",
      "        3.0    2\n",
      "        1.0    1\n",
      "        2.0    1\n",
      "        4.0    1\n",
      "        NaN    1\n",
      "        dtype: int64\n",
      "        \"\"\"\n",
      "        return value_counts(\n",
      "            self,\n",
      "            sort=sort,\n",
      "            ascending=ascending,\n",
      "            normalize=normalize,\n",
      "            bins=bins,\n",
      "            dropna=dropna,\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: view\n",
      "Method definition:     def view(self, cls=None):\n",
      "\n",
      "        # we need to see if we are subclassing an\n",
      "        # index type here\n",
      "        if cls is not None and not hasattr(cls, \"_typ\"):\n",
      "            dtype = cls\n",
      "            if isinstance(cls, str):\n",
      "                dtype = pandas_dtype(cls)\n",
      "\n",
      "            if isinstance(dtype, (np.dtype, ExtensionDtype)) and needs_i8_conversion(\n",
      "                dtype\n",
      "            ):\n",
      "                if dtype.kind == \"m\" and dtype != \"m8[ns]\":\n",
      "                    # e.g. m8[s]\n",
      "                    return self._data.view(cls)\n",
      "\n",
      "                idx_cls = self._dtype_to_subclass(dtype)\n",
      "                # NB: we only get here for subclasses that override\n",
      "                #  _data_cls such that it is a type and not a tuple\n",
      "                #  of types.\n",
      "                arr_cls = idx_cls._data_cls\n",
      "                arr = arr_cls(self._data.view(\"i8\"), dtype=dtype)\n",
      "                return idx_cls._simple_new(arr, name=self.name)\n",
      "\n",
      "            result = self._data.view(cls)\n",
      "        else:\n",
      "            result = self._view()\n",
      "        if isinstance(result, Index):\n",
      "            result._id = self._id\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: where\n",
      "Method definition:     @final\n",
      "    def where(self, cond, other=None) -> Index:\n",
      "        \"\"\"\n",
      "        Replace values where the condition is False.\n",
      "\n",
      "        The replacement is taken from other.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cond : bool array-like with the same length as self\n",
      "            Condition to select the values on.\n",
      "        other : scalar, or array-like, default None\n",
      "            Replacement if the condition is False.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        pandas.Index\n",
      "            A copy of self with values replaced from other\n",
      "            where the condition is False.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.where : Same method for Series.\n",
      "        DataFrame.where : Same method for DataFrame.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['car', 'bike', 'train', 'tractor'])\n",
      "        >>> idx\n",
      "        Index(['car', 'bike', 'train', 'tractor'], dtype='object')\n",
      "        >>> idx.where(idx.isin(['car', 'train']), 'other')\n",
      "        Index(['car', 'other', 'train', 'other'], dtype='object')\n",
      "        \"\"\"\n",
      "        if isinstance(self, ABCMultiIndex):\n",
      "            raise NotImplementedError(\n",
      "                \".where is not supported for MultiIndex operations\"\n",
      "            )\n",
      "        cond = np.asarray(cond, dtype=bool)\n",
      "        return self.putmask(~cond, other)\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     def __eq__(self, other: Any) -> bool:\n",
      "        return isinstance(other, self.__class__) and self.__dict__ == other.__dict__\n",
      "\n",
      "------------\n",
      "Method name: __hash__\n",
      "Method definition:     def __hash__(self) -> int:\n",
      "        return hash(str(self))\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     def __ne__(self, other: Any) -> bool:\n",
      "        return not self.__eq__(other)\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        return self.__class__.__name__ + \"()\"\n",
      "\n",
      "------------\n",
      "Method name: fromInternal\n",
      "Method definition:     def fromInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts an internal SQL object into a native Python object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: json\n",
      "Method definition:     def json(self) -> str:\n",
      "        return json.dumps(self.jsonValue(), separators=(\",\", \":\"), sort_keys=True)\n",
      "\n",
      "------------\n",
      "Method name: jsonValue\n",
      "Method definition:     def jsonValue(self) -> Union[str, Dict[str, Any]]:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: needConversion\n",
      "Method definition:     def needConversion(self) -> bool:\n",
      "        \"\"\"\n",
      "        Does this type needs conversion between Python object and internal SQL object.\n",
      "\n",
      "        This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "        \"\"\"\n",
      "        return False\n",
      "\n",
      "------------\n",
      "Method name: simpleString\n",
      "Method definition:     def simpleString(self) -> str:\n",
      "        return \"int\"\n",
      "\n",
      "------------\n",
      "Method name: toInternal\n",
      "Method definition:     def toInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts a Python object into an internal SQL object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: __enter__\n",
      "Method definition:     @since(2.0)\n",
      "    def __enter__(self) -> \"SparkSession\":\n",
      "        \"\"\"\n",
      "        Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      "        \"\"\"\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: __exit__\n",
      "Method definition:     @since(2.0)\n",
      "    def __exit__(\n",
      "        self,\n",
      "        exc_type: Optional[Type[BaseException]],\n",
      "        exc_val: Optional[BaseException],\n",
      "        exc_tb: Optional[TracebackType],\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      "\n",
      "        Specifically stop the SparkSession on exit of the with block.\n",
      "        \"\"\"\n",
      "        self.stop()\n",
      "\n",
      "------------\n",
      "Method name: __init__\n",
      "Method definition:     def __init__(\n",
      "        self,\n",
      "        sparkContext: SparkContext,\n",
      "        jsparkSession: Optional[JavaObject] = None,\n",
      "        options: Dict[str, Any] = {},\n",
      "    ):\n",
      "        self._sc = sparkContext\n",
      "        self._jsc = self._sc._jsc\n",
      "        self._jvm = self._sc._jvm\n",
      "\n",
      "        assert self._jvm is not None\n",
      "\n",
      "        if jsparkSession is None:\n",
      "            if (\n",
      "                self._jvm.SparkSession.getDefaultSession().isDefined()\n",
      "                and not self._jvm.SparkSession.getDefaultSession().get().sparkContext().isStopped()\n",
      "            ):\n",
      "                jsparkSession = self._jvm.SparkSession.getDefaultSession().get()\n",
      "                getattr(getattr(self._jvm, \"SparkSession$\"), \"MODULE$\").applyModifiableSettings(\n",
      "                    jsparkSession, options\n",
      "                )\n",
      "            else:\n",
      "                jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)\n",
      "        else:\n",
      "            getattr(getattr(self._jvm, \"SparkSession$\"), \"MODULE$\").applyModifiableSettings(\n",
      "                jsparkSession, options\n",
      "            )\n",
      "        self._jsparkSession = jsparkSession\n",
      "        _monkey_patch_RDD(self)\n",
      "        install_exception_handler()\n",
      "        # If we had an instantiated SparkSession attached with a SparkContext\n",
      "        # which is stopped now, we need to renew the instantiated SparkSession.\n",
      "        # Otherwise, we will use invalid SparkSession when we call Builder.getOrCreate.\n",
      "        if (\n",
      "            SparkSession._instantiatedSession is None\n",
      "            or SparkSession._instantiatedSession._sc._jsc is None\n",
      "        ):\n",
      "            SparkSession._instantiatedSession = self\n",
      "            SparkSession._activeSession = self\n",
      "            assert self._jvm is not None\n",
      "            self._jvm.SparkSession.setDefaultSession(self._jsparkSession)\n",
      "            self._jvm.SparkSession.setActiveSession(self._jsparkSession)\n",
      "\n",
      "------------\n",
      "Method name: _convert_from_pandas\n",
      "Method definition:     def _convert_from_pandas(\n",
      "        self, pdf: \"PandasDataFrameLike\", schema: Union[StructType, str, List[str]], timezone: str\n",
      "    ) -> List:\n",
      "        \"\"\"\n",
      "        Convert a pandas.DataFrame to list of records that can be used to make a DataFrame\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        list\n",
      "            list of records\n",
      "        \"\"\"\n",
      "        import pandas as pd\n",
      "        from pyspark.sql import SparkSession\n",
      "\n",
      "        assert isinstance(self, SparkSession)\n",
      "\n",
      "        if timezone is not None:\n",
      "            from pyspark.sql.pandas.types import _check_series_convert_timestamps_tz_local\n",
      "            from pandas.core.dtypes.common import is_datetime64tz_dtype, is_timedelta64_dtype\n",
      "\n",
      "            copied = False\n",
      "            if isinstance(schema, StructType):\n",
      "                for field in schema:\n",
      "                    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n",
      "                    if isinstance(field.dataType, TimestampType):\n",
      "                        s = _check_series_convert_timestamps_tz_local(pdf[field.name], timezone)\n",
      "                        if s is not pdf[field.name]:\n",
      "                            if not copied:\n",
      "                                # Copy once if the series is modified to prevent the original\n",
      "                                # Pandas DataFrame from being updated\n",
      "                                pdf = pdf.copy()\n",
      "                                copied = True\n",
      "                            pdf[field.name] = s\n",
      "            else:\n",
      "                should_localize = not is_timestamp_ntz_preferred()\n",
      "                for column, series in pdf.iteritems():\n",
      "                    s = series\n",
      "                    if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n",
      "                        s = _check_series_convert_timestamps_tz_local(series, timezone)\n",
      "                    if s is not series:\n",
      "                        if not copied:\n",
      "                            # Copy once if the series is modified to prevent the original\n",
      "                            # Pandas DataFrame from being updated\n",
      "                            pdf = pdf.copy()\n",
      "                            copied = True\n",
      "                        pdf[column] = s\n",
      "\n",
      "            for column, series in pdf.iteritems():\n",
      "                if is_timedelta64_dtype(series):\n",
      "                    if not copied:\n",
      "                        pdf = pdf.copy()\n",
      "                        copied = True\n",
      "                    # Explicitly set the timedelta as object so the output of numpy records can\n",
      "                    # hold the timedelta instances as are. Otherwise, it converts to the internal\n",
      "                    # numeric values.\n",
      "                    ser = pdf[column]\n",
      "                    pdf[column] = pd.Series(\n",
      "                        ser.dt.to_pytimedelta(), index=ser.index, dtype=\"object\", name=ser.name\n",
      "                    )\n",
      "\n",
      "        # Convert pandas.DataFrame to list of numpy records\n",
      "        np_records = pdf.to_records(index=False)\n",
      "\n",
      "        # Check if any columns need to be fixed for Spark to infer properly\n",
      "        if len(np_records) > 0:\n",
      "            record_dtype = self._get_numpy_record_dtype(np_records[0])\n",
      "            if record_dtype is not None:\n",
      "                return [r.astype(record_dtype).tolist() for r in np_records]\n",
      "\n",
      "        # Convert list of numpy records to python lists\n",
      "        return [r.tolist() for r in np_records]\n",
      "\n",
      "------------\n",
      "Method name: _createFromLocal\n",
      "Method definition:     def _createFromLocal(\n",
      "        self, data: Iterable[Any], schema: Optional[Union[DataType, List[str]]]\n",
      "    ) -> Tuple[RDD[Tuple], StructType]:\n",
      "        \"\"\"\n",
      "        Create an RDD for DataFrame from a list or pandas.DataFrame, returns\n",
      "        the RDD and schema.\n",
      "        \"\"\"\n",
      "        # make sure data could consumed multiple times\n",
      "        if not isinstance(data, list):\n",
      "            data = list(data)\n",
      "\n",
      "        if schema is None or isinstance(schema, (list, tuple)):\n",
      "            struct = self._inferSchemaFromList(data, names=schema)\n",
      "            converter = _create_converter(struct)\n",
      "            tupled_data: Iterable[Tuple] = map(converter, data)\n",
      "            if isinstance(schema, (list, tuple)):\n",
      "                for i, name in enumerate(schema):\n",
      "                    struct.fields[i].name = name\n",
      "                    struct.names[i] = name\n",
      "\n",
      "        elif isinstance(schema, StructType):\n",
      "            struct = schema\n",
      "            tupled_data = data\n",
      "\n",
      "        else:\n",
      "            raise TypeError(\"schema should be StructType or list or None, but got: %s\" % schema)\n",
      "\n",
      "        # convert python objects to sql data\n",
      "        internal_data = [struct.toInternal(row) for row in tupled_data]\n",
      "        return self._sc.parallelize(internal_data), struct\n",
      "\n",
      "------------\n",
      "Method name: _createFromRDD\n",
      "Method definition:     def _createFromRDD(\n",
      "        self,\n",
      "        rdd: RDD[Any],\n",
      "        schema: Optional[Union[DataType, List[str]]],\n",
      "        samplingRatio: Optional[float],\n",
      "    ) -> Tuple[RDD[Tuple], StructType]:\n",
      "        \"\"\"\n",
      "        Create an RDD for DataFrame from an existing RDD, returns the RDD and schema.\n",
      "        \"\"\"\n",
      "        if schema is None or isinstance(schema, (list, tuple)):\n",
      "            struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "            converter = _create_converter(struct)\n",
      "            tupled_rdd = rdd.map(converter)\n",
      "            if isinstance(schema, (list, tuple)):\n",
      "                for i, name in enumerate(schema):\n",
      "                    struct.fields[i].name = name\n",
      "                    struct.names[i] = name\n",
      "\n",
      "        elif isinstance(schema, StructType):\n",
      "            struct = schema\n",
      "            tupled_rdd = rdd\n",
      "\n",
      "        else:\n",
      "            raise TypeError(\"schema should be StructType or list or None, but got: %s\" % schema)\n",
      "\n",
      "        # convert python objects to sql data\n",
      "        internal_rdd = tupled_rdd.map(struct.toInternal)\n",
      "        return internal_rdd, struct\n",
      "\n",
      "------------\n",
      "Method name: _create_dataframe\n",
      "Method definition:     def _create_dataframe(\n",
      "        self,\n",
      "        data: Union[RDD[Any], Iterable[Any]],\n",
      "        schema: Optional[Union[DataType, List[str]]],\n",
      "        samplingRatio: Optional[float],\n",
      "        verifySchema: bool,\n",
      "    ) -> DataFrame:\n",
      "        if isinstance(schema, StructType):\n",
      "            verify_func = _make_type_verifier(schema) if verifySchema else lambda _: True\n",
      "\n",
      "            @no_type_check\n",
      "            def prepare(obj):\n",
      "                verify_func(obj)\n",
      "                return obj\n",
      "\n",
      "        elif isinstance(schema, DataType):\n",
      "            dataType = schema\n",
      "            schema = StructType().add(\"value\", schema)\n",
      "\n",
      "            verify_func = (\n",
      "                _make_type_verifier(dataType, name=\"field value\")\n",
      "                if verifySchema\n",
      "                else lambda _: True\n",
      "            )\n",
      "\n",
      "            @no_type_check\n",
      "            def prepare(obj):\n",
      "                verify_func(obj)\n",
      "                return (obj,)\n",
      "\n",
      "        else:\n",
      "\n",
      "            def prepare(obj: Any) -> Any:\n",
      "                return obj\n",
      "\n",
      "        if isinstance(data, RDD):\n",
      "            rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "        else:\n",
      "            rdd, struct = self._createFromLocal(map(prepare, data), schema)\n",
      "        assert self._jvm is not None\n",
      "        jrdd = self._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())\n",
      "        jdf = self._jsparkSession.applySchemaToPythonRDD(jrdd.rdd(), struct.json())\n",
      "        df = DataFrame(jdf, self)\n",
      "        df._schema = struct\n",
      "        return df\n",
      "\n",
      "------------\n",
      "Method name: _create_from_pandas_with_arrow\n",
      "Method definition:     def _create_from_pandas_with_arrow(\n",
      "        self, pdf: \"PandasDataFrameLike\", schema: Union[StructType, List[str]], timezone: str\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Create a DataFrame from a given pandas.DataFrame by slicing it into partitions, converting\n",
      "        to Arrow data, then sending to the JVM to parallelize. If a schema is passed in, the\n",
      "        data types will be used to coerce the data in Pandas to Arrow conversion.\n",
      "        \"\"\"\n",
      "        from pyspark.sql import SparkSession\n",
      "        from pyspark.sql.dataframe import DataFrame\n",
      "\n",
      "        assert isinstance(self, SparkSession)\n",
      "\n",
      "        from pyspark.sql.pandas.serializers import ArrowStreamPandasSerializer\n",
      "        from pyspark.sql.types import TimestampType\n",
      "        from pyspark.sql.pandas.types import from_arrow_type, to_arrow_type\n",
      "        from pyspark.sql.pandas.utils import (\n",
      "            require_minimum_pandas_version,\n",
      "            require_minimum_pyarrow_version,\n",
      "        )\n",
      "\n",
      "        require_minimum_pandas_version()\n",
      "        require_minimum_pyarrow_version()\n",
      "\n",
      "        from pandas.api.types import (  # type: ignore[attr-defined]\n",
      "            is_datetime64_dtype,\n",
      "            is_datetime64tz_dtype,\n",
      "        )\n",
      "        import pyarrow as pa\n",
      "\n",
      "        # Create the Spark schema from list of names passed in with Arrow types\n",
      "        if isinstance(schema, (list, tuple)):\n",
      "            arrow_schema = pa.Schema.from_pandas(pdf, preserve_index=False)\n",
      "            struct = StructType()\n",
      "            prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n",
      "            for name, field in zip(schema, arrow_schema):\n",
      "                struct.add(\n",
      "                    name, from_arrow_type(field.type, prefer_timestamp_ntz), nullable=field.nullable\n",
      "                )\n",
      "            schema = struct\n",
      "\n",
      "        # Determine arrow types to coerce data when creating batches\n",
      "        if isinstance(schema, StructType):\n",
      "            arrow_types = [to_arrow_type(f.dataType) for f in schema.fields]\n",
      "        elif isinstance(schema, DataType):\n",
      "            raise ValueError(\"Single data type %s is not supported with Arrow\" % str(schema))\n",
      "        else:\n",
      "            # Any timestamps must be coerced to be compatible with Spark\n",
      "            arrow_types = [\n",
      "                to_arrow_type(TimestampType())\n",
      "                if is_datetime64_dtype(t) or is_datetime64tz_dtype(t)\n",
      "                else None\n",
      "                for t in pdf.dtypes\n",
      "            ]\n",
      "\n",
      "        # Slice the DataFrame to be batched\n",
      "        step = -(-len(pdf) // self.sparkContext.defaultParallelism)  # round int up\n",
      "        pdf_slices = (pdf.iloc[start : start + step] for start in range(0, len(pdf), step))\n",
      "\n",
      "        # Create list of Arrow (columns, type) for serializer dump_stream\n",
      "        arrow_data = [\n",
      "            [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n",
      "            for pdf_slice in pdf_slices\n",
      "        ]\n",
      "\n",
      "        jsparkSession = self._jsparkSession\n",
      "\n",
      "        safecheck = self._jconf.arrowSafeTypeConversion()\n",
      "        col_by_name = True  # col by name only applies to StructType columns, can't happen here\n",
      "        ser = ArrowStreamPandasSerializer(timezone, safecheck, col_by_name)\n",
      "\n",
      "        @no_type_check\n",
      "        def reader_func(temp_filename):\n",
      "            return self._jvm.PythonSQLUtils.readArrowStreamFromFile(jsparkSession, temp_filename)\n",
      "\n",
      "        @no_type_check\n",
      "        def create_RDD_server():\n",
      "            return self._jvm.ArrowRDDServer(jsparkSession)\n",
      "\n",
      "        # Create Spark DataFrame from Arrow stream file, using one batch per partition\n",
      "        jrdd = self._sc._serialize_to_jvm(arrow_data, ser, reader_func, create_RDD_server)\n",
      "        assert self._jvm is not None\n",
      "        jdf = self._jvm.PythonSQLUtils.toDataFrame(jrdd, schema.json(), jsparkSession)\n",
      "        df = DataFrame(jdf, self)\n",
      "        df._schema = schema\n",
      "        return df\n",
      "\n",
      "------------\n",
      "Method name: _create_shell_session\n",
      "Method definition:     @staticmethod\n",
      "    def _create_shell_session() -> \"SparkSession\":\n",
      "        \"\"\"\n",
      "        Initialize a :class:`SparkSession` for a pyspark shell session. This is called from\n",
      "        shell.py to make error handling simpler without needing to declare local variables in\n",
      "        that script, which would expose those to users.\n",
      "        \"\"\"\n",
      "        import py4j\n",
      "        from pyspark.conf import SparkConf\n",
      "        from pyspark.context import SparkContext\n",
      "\n",
      "        try:\n",
      "            # Try to access HiveConf, it will raise exception if Hive is not added\n",
      "            conf = SparkConf()\n",
      "            assert SparkContext._jvm is not None\n",
      "            if conf.get(\"spark.sql.catalogImplementation\", \"hive\").lower() == \"hive\":\n",
      "                SparkContext._jvm.org.apache.hadoop.hive.conf.HiveConf()\n",
      "                return SparkSession.builder.enableHiveSupport().getOrCreate()\n",
      "            else:\n",
      "                return SparkSession._getActiveSessionOrCreate()\n",
      "        except (py4j.protocol.Py4JError, TypeError):\n",
      "            if conf.get(\"spark.sql.catalogImplementation\", \"\").lower() == \"hive\":\n",
      "                warnings.warn(\n",
      "                    \"Fall back to non-hive support because failing to access HiveConf, \"\n",
      "                    \"please make sure you build spark with hive\"\n",
      "                )\n",
      "\n",
      "        return SparkSession._getActiveSessionOrCreate()\n",
      "\n",
      "------------\n",
      "Method name: _getActiveSessionOrCreate\n",
      "Method definition:     @staticmethod\n",
      "    def _getActiveSessionOrCreate(**static_conf: Any) -> \"SparkSession\":\n",
      "        \"\"\"\n",
      "        Returns the active :class:`SparkSession` for the current thread, returned by the builder,\n",
      "        or if there is no existing one, creates a new one based on the options set in the builder.\n",
      "\n",
      "        NOTE that 'static_conf' might not be set if there's an active or default Spark session\n",
      "        running.\n",
      "        \"\"\"\n",
      "        spark = SparkSession.getActiveSession()\n",
      "        if spark is None:\n",
      "            builder = SparkSession.builder\n",
      "            for k, v in static_conf.items():\n",
      "                builder = builder.config(k, v)\n",
      "            spark = builder.getOrCreate()\n",
      "        return spark\n",
      "\n",
      "------------\n",
      "Method name: _get_numpy_record_dtype\n",
      "Method definition:     def _get_numpy_record_dtype(self, rec: \"np.recarray\") -> Optional[\"np.dtype\"]:\n",
      "        \"\"\"\n",
      "        Used when converting a pandas.DataFrame to Spark using to_records(), this will correct\n",
      "        the dtypes of fields in a record so they can be properly loaded into Spark.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        rec : numpy.record\n",
      "            a numpy record to check field dtypes\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.dtype\n",
      "            corrected dtype for a numpy.record or None if no correction needed\n",
      "        \"\"\"\n",
      "        import numpy as np\n",
      "\n",
      "        cur_dtypes = rec.dtype\n",
      "        col_names = cur_dtypes.names\n",
      "        record_type_list = []\n",
      "        has_rec_fix = False\n",
      "        for i in range(len(cur_dtypes)):\n",
      "            curr_type = cur_dtypes[i]\n",
      "            # If type is a datetime64 timestamp, convert to microseconds\n",
      "            # NOTE: if dtype is datetime[ns] then np.record.tolist() will output values as longs,\n",
      "            # conversion from [us] or lower will lead to py datetime objects, see SPARK-22417\n",
      "            if curr_type == np.dtype(\"datetime64[ns]\"):\n",
      "                curr_type = \"datetime64[us]\"\n",
      "                has_rec_fix = True\n",
      "            record_type_list.append((str(col_names[i]), curr_type))\n",
      "        return np.dtype(record_type_list) if has_rec_fix else None\n",
      "\n",
      "------------\n",
      "Method name: _inferSchema\n",
      "Method definition:     def _inferSchema(\n",
      "        self,\n",
      "        rdd: RDD[Any],\n",
      "        samplingRatio: Optional[float] = None,\n",
      "        names: Optional[List[str]] = None,\n",
      "    ) -> StructType:\n",
      "        \"\"\"\n",
      "        Infer schema from an RDD of Row, dict, or tuple.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        rdd : :class:`RDD`\n",
      "            an RDD of Row, dict, or tuple\n",
      "        samplingRatio : float, optional\n",
      "            sampling ratio, or no sampling (default)\n",
      "        names : list, optional\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`pyspark.sql.types.StructType`\n",
      "        \"\"\"\n",
      "        first = rdd.first()\n",
      "        if not first:\n",
      "            raise ValueError(\"The first row in RDD is empty, \" \"can not infer schema\")\n",
      "\n",
      "        infer_dict_as_struct = self._jconf.inferDictAsStruct()\n",
      "        prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n",
      "        if samplingRatio is None:\n",
      "            schema = _infer_schema(\n",
      "                first,\n",
      "                names=names,\n",
      "                infer_dict_as_struct=infer_dict_as_struct,\n",
      "                prefer_timestamp_ntz=prefer_timestamp_ntz,\n",
      "            )\n",
      "            if _has_nulltype(schema):\n",
      "                for row in rdd.take(100)[1:]:\n",
      "                    schema = _merge_type(\n",
      "                        schema,\n",
      "                        _infer_schema(\n",
      "                            row,\n",
      "                            names=names,\n",
      "                            infer_dict_as_struct=infer_dict_as_struct,\n",
      "                            prefer_timestamp_ntz=prefer_timestamp_ntz,\n",
      "                        ),\n",
      "                    )\n",
      "                    if not _has_nulltype(schema):\n",
      "                        break\n",
      "                else:\n",
      "                    raise ValueError(\n",
      "                        \"Some of types cannot be determined by the \"\n",
      "                        \"first 100 rows, please try again with sampling\"\n",
      "                    )\n",
      "        else:\n",
      "            if samplingRatio < 0.99:\n",
      "                rdd = rdd.sample(False, float(samplingRatio))\n",
      "            schema = rdd.map(\n",
      "                lambda row: _infer_schema(\n",
      "                    row,\n",
      "                    names,\n",
      "                    infer_dict_as_struct=infer_dict_as_struct,\n",
      "                    prefer_timestamp_ntz=prefer_timestamp_ntz,\n",
      "                )\n",
      "            ).reduce(_merge_type)\n",
      "        return schema\n",
      "\n",
      "------------\n",
      "Method name: _inferSchemaFromList\n",
      "Method definition:     def _inferSchemaFromList(\n",
      "        self, data: Iterable[Any], names: Optional[List[str]] = None\n",
      "    ) -> StructType:\n",
      "        \"\"\"\n",
      "        Infer schema from list of Row, dict, or tuple.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        data : iterable\n",
      "            list of Row, dict, or tuple\n",
      "        names : list, optional\n",
      "            list of column names\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`pyspark.sql.types.StructType`\n",
      "        \"\"\"\n",
      "        if not data:\n",
      "            raise ValueError(\"can not infer schema from empty dataset\")\n",
      "        infer_dict_as_struct = self._jconf.inferDictAsStruct()\n",
      "        prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n",
      "        schema = reduce(\n",
      "            _merge_type,\n",
      "            (_infer_schema(row, names, infer_dict_as_struct, prefer_timestamp_ntz) for row in data),\n",
      "        )\n",
      "        if _has_nulltype(schema):\n",
      "            raise ValueError(\"Some of types cannot be determined after inferring\")\n",
      "        return schema\n",
      "\n",
      "------------\n",
      "Method name: _repr_html_\n",
      "Method definition:     def _repr_html_(self) -> str:\n",
      "        return \"\"\"\n",
      "            <div>\n",
      "                <p><b>SparkSession - {catalogImplementation}</b></p>\n",
      "                {sc_HTML}\n",
      "            </div>\n",
      "        \"\"\".format(\n",
      "            catalogImplementation=self.conf.get(\"spark.sql.catalogImplementation\"),\n",
      "            sc_HTML=self.sparkContext._repr_html_(),\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: createDataFrame\n",
      "Method definition:     def createDataFrame(  # type: ignore[misc]\n",
      "        self,\n",
      "        data: Union[RDD[Any], Iterable[Any], \"PandasDataFrameLike\"],\n",
      "        schema: Optional[Union[AtomicType, StructType, str]] = None,\n",
      "        samplingRatio: Optional[float] = None,\n",
      "        verifySchema: bool = True,\n",
      "    ) -> DataFrame:\n",
      "        \"\"\"\n",
      "        Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "\n",
      "        When ``schema`` is a list of column names, the type of each column\n",
      "        will be inferred from ``data``.\n",
      "\n",
      "        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "        from ``data``, which should be an RDD of either :class:`Row`,\n",
      "        :class:`namedtuple`, or :class:`dict`.\n",
      "\n",
      "        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      "        the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "        :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      "        Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "\n",
      "        If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      "        rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        .. versionchanged:: 2.1.0\n",
      "           Added verifySchema.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        data : :class:`RDD` or iterable\n",
      "            an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "            :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      "            :class:`pandas.DataFrame`.\n",
      "        schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "            a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "            column names, default is None.  The data type string format equals to\n",
      "            :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "            omit the ``struct<>``.\n",
      "        samplingRatio : float, optional\n",
      "            the sample ratio of rows used for inferring\n",
      "        verifySchema : bool, optional\n",
      "            verify data types of every row against schema. Enabled by default.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`DataFrame`\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> l = [('Alice', 1)]\n",
      "        >>> spark.createDataFrame(l).collect()\n",
      "        [Row(_1='Alice', _2=1)]\n",
      "        >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      "        [Row(name='Alice', age=1)]\n",
      "\n",
      "        >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "        >>> spark.createDataFrame(d).collect()\n",
      "        [Row(age=1, name='Alice')]\n",
      "\n",
      "        >>> rdd = sc.parallelize(l)\n",
      "        >>> spark.createDataFrame(rdd).collect()\n",
      "        [Row(_1='Alice', _2=1)]\n",
      "        >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      "        >>> df.collect()\n",
      "        [Row(name='Alice', age=1)]\n",
      "\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> Person = Row('name', 'age')\n",
      "        >>> person = rdd.map(lambda r: Person(*r))\n",
      "        >>> df2 = spark.createDataFrame(person)\n",
      "        >>> df2.collect()\n",
      "        [Row(name='Alice', age=1)]\n",
      "\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> schema = StructType([\n",
      "        ...    StructField(\"name\", StringType(), True),\n",
      "        ...    StructField(\"age\", IntegerType(), True)])\n",
      "        >>> df3 = spark.createDataFrame(rdd, schema)\n",
      "        >>> df3.collect()\n",
      "        [Row(name='Alice', age=1)]\n",
      "\n",
      "        >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "        [Row(name='Alice', age=1)]\n",
      "        >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "        [Row(0=1, 1=2)]\n",
      "\n",
      "        >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "        [Row(a='Alice', b=1)]\n",
      "        >>> rdd = rdd.map(lambda row: row[1])\n",
      "        >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      "        [Row(value=1)]\n",
      "        >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "        Traceback (most recent call last):\n",
      "            ...\n",
      "        Py4JJavaError: ...\n",
      "        \"\"\"\n",
      "        SparkSession._activeSession = self\n",
      "        assert self._jvm is not None\n",
      "        self._jvm.SparkSession.setActiveSession(self._jsparkSession)\n",
      "        if isinstance(data, DataFrame):\n",
      "            raise TypeError(\"data is already a DataFrame\")\n",
      "\n",
      "        if isinstance(schema, str):\n",
      "            schema = cast(Union[AtomicType, StructType, str], _parse_datatype_string(schema))\n",
      "        elif isinstance(schema, (list, tuple)):\n",
      "            # Must re-encode any unicode strings to be consistent with StructField names\n",
      "            schema = [x.encode(\"utf-8\") if not isinstance(x, str) else x for x in schema]\n",
      "\n",
      "        try:\n",
      "            import pandas\n",
      "\n",
      "            has_pandas = True\n",
      "        except Exception:\n",
      "            has_pandas = False\n",
      "        if has_pandas and isinstance(data, pandas.DataFrame):\n",
      "            # Create a DataFrame from pandas DataFrame.\n",
      "            return super(SparkSession, self).createDataFrame(  # type: ignore[call-overload]\n",
      "                data, schema, samplingRatio, verifySchema\n",
      "            )\n",
      "        return self._create_dataframe(\n",
      "            data, schema, samplingRatio, verifySchema  # type: ignore[arg-type]\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: newSession\n",
      "Method definition:     @since(2.0)\n",
      "    def newSession(self) -> \"SparkSession\":\n",
      "        \"\"\"\n",
      "        Returns a new :class:`SparkSession` as new session, that has separate SQLConf,\n",
      "        registered temporary views and UDFs, but shared :class:`SparkContext` and\n",
      "        table cache.\n",
      "        \"\"\"\n",
      "        return self.__class__(self._sc, self._jsparkSession.newSession())\n",
      "\n",
      "------------\n",
      "Method name: range\n",
      "Method definition:     def range(\n",
      "        self,\n",
      "        start: int,\n",
      "        end: Optional[int] = None,\n",
      "        step: int = 1,\n",
      "        numPartitions: Optional[int] = None,\n",
      "    ) -> DataFrame:\n",
      "        \"\"\"\n",
      "        Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      "        ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      "        step value ``step``.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        start : int\n",
      "            the start value\n",
      "        end : int, optional\n",
      "            the end value (exclusive)\n",
      "        step : int, optional\n",
      "            the incremental step (default: 1)\n",
      "        numPartitions : int, optional\n",
      "            the number of partitions of the DataFrame\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`DataFrame`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(1, 7, 2).collect()\n",
      "        [Row(id=1), Row(id=3), Row(id=5)]\n",
      "\n",
      "        If only one argument is specified, it will be used as the end value.\n",
      "\n",
      "        >>> spark.range(3).collect()\n",
      "        [Row(id=0), Row(id=1), Row(id=2)]\n",
      "        \"\"\"\n",
      "        if numPartitions is None:\n",
      "            numPartitions = self._sc.defaultParallelism\n",
      "\n",
      "        if end is None:\n",
      "            jdf = self._jsparkSession.range(0, int(start), int(step), int(numPartitions))\n",
      "        else:\n",
      "            jdf = self._jsparkSession.range(int(start), int(end), int(step), int(numPartitions))\n",
      "\n",
      "        return DataFrame(jdf, self)\n",
      "\n",
      "------------\n",
      "Method name: sql\n",
      "Method definition:     def sql(self, sqlQuery: str, **kwargs: Any) -> DataFrame:\n",
      "        \"\"\"Returns a :class:`DataFrame` representing the result of the given query.\n",
      "        When ``kwargs`` is specified, this method formats the given string by using the Python\n",
      "        standard formatter.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        sqlQuery : str\n",
      "            SQL query string.\n",
      "        kwargs : dict\n",
      "            Other variables that the user wants to set that can be referenced in the query\n",
      "\n",
      "            .. versionchanged:: 3.3.0\n",
      "               Added optional argument ``kwargs`` to specify the mapping of variables in the query.\n",
      "               This feature is experimental and unstable.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`DataFrame`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Executing a SQL query.\n",
      "\n",
      "        >>> spark.sql(\"SELECT * FROM range(10) where id > 7\").show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  8|\n",
      "        |  9|\n",
      "        +---+\n",
      "\n",
      "        Executing a SQL query with variables as Python formatter standard.\n",
      "\n",
      "        >>> spark.sql(\n",
      "        ...     \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\", bound1=7, bound2=9\n",
      "        ... ).show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  8|\n",
      "        +---+\n",
      "\n",
      "        >>> mydf = spark.range(10)\n",
      "        >>> spark.sql(\n",
      "        ...     \"SELECT {col} FROM {mydf} WHERE id IN {x}\",\n",
      "        ...     col=mydf.id, mydf=mydf, x=tuple(range(4))).show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  0|\n",
      "        |  1|\n",
      "        |  2|\n",
      "        |  3|\n",
      "        +---+\n",
      "\n",
      "        >>> spark.sql('''\n",
      "        ...   SELECT m1.a, m2.b\n",
      "        ...   FROM {table1} m1 INNER JOIN {table2} m2\n",
      "        ...   ON m1.key = m2.key\n",
      "        ...   ORDER BY m1.a, m2.b''',\n",
      "        ...   table1=spark.createDataFrame([(1, \"a\"), (2, \"b\")], [\"a\", \"key\"]),\n",
      "        ...   table2=spark.createDataFrame([(3, \"a\"), (4, \"b\"), (5, \"b\")], [\"b\", \"key\"])).show()\n",
      "        +---+---+\n",
      "        |  a|  b|\n",
      "        +---+---+\n",
      "        |  1|  3|\n",
      "        |  2|  4|\n",
      "        |  2|  5|\n",
      "        +---+---+\n",
      "\n",
      "        Also, it is possible to query using class:`Column` from :class:`DataFrame`.\n",
      "\n",
      "        >>> mydf = spark.createDataFrame([(1, 4), (2, 4), (3, 6)], [\"A\", \"B\"])\n",
      "        >>> spark.sql(\"SELECT {df.A}, {df[B]} FROM {df}\", df=mydf).show()\n",
      "        +---+---+\n",
      "        |  A|  B|\n",
      "        +---+---+\n",
      "        |  1|  4|\n",
      "        |  2|  4|\n",
      "        |  3|  6|\n",
      "        +---+---+\n",
      "        \"\"\"\n",
      "\n",
      "        formatter = SQLStringFormatter(self)\n",
      "        if len(kwargs) > 0:\n",
      "            sqlQuery = formatter.format(sqlQuery, **kwargs)\n",
      "        try:\n",
      "            return DataFrame(self._jsparkSession.sql(sqlQuery), self)\n",
      "        finally:\n",
      "            if len(kwargs) > 0:\n",
      "                formatter.clear()\n",
      "\n",
      "------------\n",
      "Method name: stop\n",
      "Method definition:     @since(2.0)\n",
      "    def stop(self) -> None:\n",
      "        \"\"\"Stop the underlying :class:`SparkContext`.\"\"\"\n",
      "        from pyspark.sql.context import SQLContext\n",
      "\n",
      "        self._sc.stop()\n",
      "        # We should clean the default session up. See SPARK-23228.\n",
      "        assert self._jvm is not None\n",
      "        self._jvm.SparkSession.clearDefaultSession()\n",
      "        self._jvm.SparkSession.clearActiveSession()\n",
      "        SparkSession._instantiatedSession = None\n",
      "        SparkSession._activeSession = None\n",
      "        SQLContext._instantiatedContext = None\n",
      "\n",
      "------------\n",
      "Method name: table\n",
      "Method definition:     def table(self, tableName: str) -> DataFrame:\n",
      "        \"\"\"Returns the specified table as a :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`DataFrame`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.createOrReplaceTempView(\"table1\")\n",
      "        >>> df2 = spark.table(\"table1\")\n",
      "        >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "        True\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     def __eq__(self, other: Any) -> bool:\n",
      "        return isinstance(other, self.__class__) and self.__dict__ == other.__dict__\n",
      "\n",
      "------------\n",
      "Method name: __hash__\n",
      "Method definition:     def __hash__(self) -> int:\n",
      "        return hash(str(self))\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     def __ne__(self, other: Any) -> bool:\n",
      "        return not self.__eq__(other)\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        return self.__class__.__name__ + \"()\"\n",
      "\n",
      "------------\n",
      "Method name: fromInternal\n",
      "Method definition:     def fromInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts an internal SQL object into a native Python object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: json\n",
      "Method definition:     def json(self) -> str:\n",
      "        return json.dumps(self.jsonValue(), separators=(\",\", \":\"), sort_keys=True)\n",
      "\n",
      "------------\n",
      "Method name: jsonValue\n",
      "Method definition:     def jsonValue(self) -> Union[str, Dict[str, Any]]:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: needConversion\n",
      "Method definition:     def needConversion(self) -> bool:\n",
      "        \"\"\"\n",
      "        Does this type needs conversion between Python object and internal SQL object.\n",
      "\n",
      "        This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "        \"\"\"\n",
      "        return False\n",
      "\n",
      "------------\n",
      "Method name: simpleString\n",
      "Method definition:     def simpleString(self) -> str:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: toInternal\n",
      "Method definition:     def toInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts a Python object into an internal SQL object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Function name: broadcast\n",
      "Function definition: @since(1.6)\n",
      "def broadcast(df: DataFrame) -> DataFrame:\n",
      "    \"\"\"Marks a DataFrame as small enough for use in broadcast joins.\"\"\"\n",
      "\n",
      "    sc = SparkContext._active_spark_context\n",
      "    assert sc is not None and sc._jvm is not None\n",
      "    return DataFrame(sc._jvm.functions.broadcast(df._jdf), df.sparkSession)\n",
      "\n",
      "------------\n",
      "Function name: urlparse\n",
      "Function definition: def urlparse(url, scheme='', allow_fragments=True):\n",
      "    \"\"\"Parse a URL into 6 components:\n",
      "    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n",
      "\n",
      "    The result is a named 6-tuple with fields corresponding to the\n",
      "    above. It is either a ParseResult or ParseResultBytes object,\n",
      "    depending on the type of the url parameter.\n",
      "\n",
      "    The username, password, hostname, and port sub-components of netloc\n",
      "    can also be accessed as attributes of the returned object.\n",
      "\n",
      "    The scheme argument provides the default value of the scheme\n",
      "    component when no scheme is found in url.\n",
      "\n",
      "    If allow_fragments is False, no attempt is made to separate the\n",
      "    fragment component from the previous component, which can be either\n",
      "    path or query.\n",
      "\n",
      "    Note that % escapes are not expanded.\n",
      "    \"\"\"\n",
      "    url, scheme, _coerce_result = _coerce_args(url, scheme)\n",
      "    splitresult = urlsplit(url, scheme, allow_fragments)\n",
      "    scheme, netloc, url, query, fragment = splitresult\n",
      "    if scheme in uses_params and ';' in url:\n",
      "        url, params = _splitparams(url)\n",
      "    else:\n",
      "        params = ''\n",
      "    result = ParseResult(scheme, netloc, url, params, query, fragment)\n",
      "    return _coerce_result(result)\n",
      "\n",
      "------------\n",
      "Method name: __init__\n",
      "Method definition:     def __init__(\n",
      "        self,\n",
      "        desc: Optional[str] = None,\n",
      "        stackTrace: Optional[str] = None,\n",
      "        cause: Optional[Py4JJavaError] = None,\n",
      "        origin: Optional[Py4JJavaError] = None,\n",
      "    ):\n",
      "        # desc & stackTrace vs origin are mutually exclusive.\n",
      "        # cause is optional.\n",
      "        assert (origin is not None and desc is None and stackTrace is None) or (\n",
      "            origin is None and desc is not None and stackTrace is not None\n",
      "        )\n",
      "\n",
      "        self.desc = desc if desc is not None else cast(Py4JJavaError, origin).getMessage()\n",
      "        assert SparkContext._jvm is not None\n",
      "        self.stackTrace = (\n",
      "            stackTrace\n",
      "            if stackTrace is not None\n",
      "            else (SparkContext._jvm.org.apache.spark.util.Utils.exceptionString(origin))\n",
      "        )\n",
      "        self.cause = convert_exception(cause) if cause is not None else None\n",
      "        if self.cause is None and origin is not None and origin.getCause() is not None:\n",
      "            self.cause = convert_exception(origin.getCause())\n",
      "        self._origin = origin\n",
      "\n",
      "------------\n",
      "Method name: __str__\n",
      "Method definition:     def __str__(self) -> str:\n",
      "        assert SparkContext._jvm is not None\n",
      "\n",
      "        jvm = SparkContext._jvm\n",
      "        sql_conf = jvm.org.apache.spark.sql.internal.SQLConf.get()\n",
      "        debug_enabled = sql_conf.pysparkJVMStacktraceEnabled()\n",
      "        desc = self.desc\n",
      "        if debug_enabled:\n",
      "            desc = desc + \"\\n\\nJVM stacktrace:\\n%s\" % self.stackTrace\n",
      "        return str(desc)\n",
      "\n",
      "------------\n",
      "Method name: getErrorClass\n",
      "Method definition:     def getErrorClass(self) -> Optional[str]:\n",
      "        assert SparkContext._gateway is not None\n",
      "\n",
      "        gw = SparkContext._gateway\n",
      "        if self._origin is not None and is_instance_of(\n",
      "            gw, self._origin, \"org.apache.spark.SparkThrowable\"\n",
      "        ):\n",
      "            return self._origin.getErrorClass()\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "------------\n",
      "Method name: getSqlState\n",
      "Method definition:     def getSqlState(self) -> Optional[str]:\n",
      "        assert SparkContext._gateway is not None\n",
      "\n",
      "        gw = SparkContext._gateway\n",
      "        if self._origin is not None and is_instance_of(\n",
      "            gw, self._origin, \"org.apache.spark.SparkThrowable\"\n",
      "        ):\n",
      "            return self._origin.getSqlState()\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "------------\n",
      "Method name: __init__\n",
      "Method definition:     def __init__(self, error_response, operation_name):\n",
      "        retry_info = self._get_retry_info(error_response)\n",
      "        error = error_response.get('Error', {})\n",
      "        msg = self.MSG_TEMPLATE.format(\n",
      "            error_code=error.get('Code', 'Unknown'),\n",
      "            error_message=error.get('Message', 'Unknown'),\n",
      "            operation_name=operation_name,\n",
      "            retry_info=retry_info,\n",
      "        )\n",
      "        super().__init__(msg)\n",
      "        self.response = error_response\n",
      "        self.operation_name = operation_name\n",
      "\n",
      "------------\n",
      "Method name: __reduce__\n",
      "Method definition:     def __reduce__(self):\n",
      "        # Subclasses of ClientError's are dynamically generated and\n",
      "        # cannot be pickled unless they are attributes of a\n",
      "        # module. So at the very least return a ClientError back.\n",
      "        return ClientError, (self.response, self.operation_name)\n",
      "\n",
      "------------\n",
      "Method name: _get_retry_info\n",
      "Method definition:     def _get_retry_info(self, response):\n",
      "        retry_info = ''\n",
      "        if 'ResponseMetadata' in response:\n",
      "            metadata = response['ResponseMetadata']\n",
      "            if metadata.get('MaxAttemptsReached', False):\n",
      "                if 'RetryAttempts' in metadata:\n",
      "                    retry_info = (\n",
      "                        f\" (reached max retries: {metadata['RetryAttempts']})\"\n",
      "                    )\n",
      "        return retry_info\n",
      "\n",
      "------------\n",
      "Method name: __add__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __and__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __bool__\n",
      "Method definition:     def __nonzero__(self) -> None:\n",
      "        raise ValueError(\n",
      "            \"Cannot convert column into bool: please use '&' for 'and', '|' for 'or', \"\n",
      "            \"'~' for 'not' when building DataFrame boolean expressions.\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: __contains__\n",
      "Method definition:     def __contains__(self, item: Any) -> None:\n",
      "        raise ValueError(\n",
      "            \"Cannot apply 'in' operator against a column: please use 'contains' \"\n",
      "            \"in a string column or 'array_contains' function for an array column.\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: __div__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     def __eq__(  # type: ignore[override]\n",
      "        self,\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        \"\"\"binary function\"\"\"\n",
      "        return _bin_op(\"equalTo\")(self, other)\n",
      "\n",
      "------------\n",
      "Method name: __ge__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __getattr__\n",
      "Method definition:     def __getattr__(self, item: Any) -> \"Column\":\n",
      "        if item.startswith(\"__\"):\n",
      "            raise AttributeError(item)\n",
      "        return self[item]\n",
      "\n",
      "------------\n",
      "Method name: __getitem__\n",
      "Method definition:     def __getitem__(self, k: Any) -> \"Column\":\n",
      "        if isinstance(k, slice):\n",
      "            if k.step is not None:\n",
      "                raise ValueError(\"slice with step is not supported.\")\n",
      "            return self.substr(k.start, k.stop)\n",
      "        else:\n",
      "            return _bin_op(\"apply\")(self, k)\n",
      "\n",
      "------------\n",
      "Method name: __gt__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __init__\n",
      "Method definition:     def __init__(self, jc: JavaObject) -> None:\n",
      "        self._jc = jc\n",
      "\n",
      "------------\n",
      "Method name: __invert__\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None and sc._jvm is not None\n",
      "        jc = getattr(sc._jvm.functions, name)(self._jc)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __iter__\n",
      "Method definition:     def __iter__(self) -> None:\n",
      "        raise TypeError(\"Column is not iterable\")\n",
      "\n",
      "------------\n",
      "Method name: __le__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __lt__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __mod__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __mul__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     def __ne__(  # type: ignore[override]\n",
      "        self,\n",
      "        other: Any,\n",
      "    ) -> \"Column\":\n",
      "        \"\"\"binary function\"\"\"\n",
      "        return _bin_op(\"notEqual\")(self, other)\n",
      "\n",
      "------------\n",
      "Method name: __neg__\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None and sc._jvm is not None\n",
      "        jc = getattr(sc._jvm.functions, name)(self._jc)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __nonzero__\n",
      "Method definition:     def __nonzero__(self) -> None:\n",
      "        raise ValueError(\n",
      "            \"Cannot convert column into bool: please use '&' for 'and', '|' for 'or', \"\n",
      "            \"'~' for 'not' when building DataFrame boolean expressions.\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: __or__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __pow__\n",
      "Method definition:     def _(self: \"Column\", other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\"]) -> \"Column\":\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None and sc._jvm is not None\n",
      "        fn = getattr(sc._jvm.functions, name)\n",
      "        jc = other._jc if isinstance(other, Column) else _create_column_from_literal(other)\n",
      "        njc = fn(self._jc, jc) if not reverse else fn(jc, self._jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __radd__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __rand__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __rdiv__\n",
      "Method definition:     def _(self: \"Column\", other: Union[\"LiteralType\", \"DecimalLiteral\"]) -> \"Column\":\n",
      "        jother = _create_column_from_literal(other)\n",
      "        jc = getattr(jother, name)(self._jc)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        return \"Column<'%s'>\" % self._jc.toString()\n",
      "\n",
      "------------\n",
      "Method name: __rmod__\n",
      "Method definition:     def _(self: \"Column\", other: Union[\"LiteralType\", \"DecimalLiteral\"]) -> \"Column\":\n",
      "        jother = _create_column_from_literal(other)\n",
      "        jc = getattr(jother, name)(self._jc)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __rmul__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __ror__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __rpow__\n",
      "Method definition:     def _(self: \"Column\", other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\"]) -> \"Column\":\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None and sc._jvm is not None\n",
      "        fn = getattr(sc._jvm.functions, name)\n",
      "        jc = other._jc if isinstance(other, Column) else _create_column_from_literal(other)\n",
      "        njc = fn(self._jc, jc) if not reverse else fn(jc, self._jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __rsub__\n",
      "Method definition:     def _(self: \"Column\", other: Union[\"LiteralType\", \"DecimalLiteral\"]) -> \"Column\":\n",
      "        jother = _create_column_from_literal(other)\n",
      "        jc = getattr(jother, name)(self._jc)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __rtruediv__\n",
      "Method definition:     def _(self: \"Column\", other: Union[\"LiteralType\", \"DecimalLiteral\"]) -> \"Column\":\n",
      "        jother = _create_column_from_literal(other)\n",
      "        jc = getattr(jother, name)(self._jc)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __sub__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: __truediv__\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: alias\n",
      "Method definition:     def alias(self, *alias: str, **kwargs: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Returns this column aliased with a new name or names (in the case of expressions that\n",
      "        return more than one column, such as explode).\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        alias : str\n",
      "            desired column names (collects all positional arguments passed)\n",
      "\n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        metadata: dict\n",
      "            a dict of information to be stored in ``metadata`` attribute of the\n",
      "            corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      "            only argument)\n",
      "\n",
      "            .. versionchanged:: 2.2.0\n",
      "               Added optional ``metadata`` argument.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.age.alias(\"age2\")).collect()\n",
      "        [Row(age2=2), Row(age2=5)]\n",
      "        >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      "        99\n",
      "        \"\"\"\n",
      "\n",
      "        metadata = kwargs.pop(\"metadata\", None)\n",
      "        assert not kwargs, \"Unexpected kwargs where passed: %s\" % kwargs\n",
      "\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None\n",
      "        if len(alias) == 1:\n",
      "            if metadata:\n",
      "                assert sc._jvm is not None\n",
      "                jmeta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(json.dumps(metadata))\n",
      "                return Column(getattr(self._jc, \"as\")(alias[0], jmeta))\n",
      "            else:\n",
      "                return Column(getattr(self._jc, \"as\")(alias[0]))\n",
      "        else:\n",
      "            if metadata:\n",
      "                raise ValueError(\"metadata can only be provided for a single column\")\n",
      "            return Column(getattr(self._jc, \"as\")(_to_seq(sc, list(alias))))\n",
      "\n",
      "------------\n",
      "Method name: asc\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: asc_nulls_first\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: asc_nulls_last\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: astype\n",
      "Method definition:     def cast(self, dataType: Union[DataType, str]) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Casts the column into type ``dataType``.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      "        [Row(ages='2'), Row(ages='5')]\n",
      "        >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      "        [Row(ages='2'), Row(ages='5')]\n",
      "        \"\"\"\n",
      "        if isinstance(dataType, str):\n",
      "            jc = self._jc.cast(dataType)\n",
      "        elif isinstance(dataType, DataType):\n",
      "            from pyspark.sql import SparkSession\n",
      "\n",
      "            spark = SparkSession._getActiveSessionOrCreate()\n",
      "            jdt = spark._jsparkSession.parseDataType(dataType.json())\n",
      "            jc = self._jc.cast(jdt)\n",
      "        else:\n",
      "            raise TypeError(\"unexpected type: %s\" % type(dataType))\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: between\n",
      "Method definition:     def between(\n",
      "        self,\n",
      "        lowerBound: Union[\"Column\", \"LiteralType\", \"DateTimeLiteral\", \"DecimalLiteral\"],\n",
      "        upperBound: Union[\"Column\", \"LiteralType\", \"DateTimeLiteral\", \"DecimalLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        \"\"\"\n",
      "        True if the current column is between the lower bound and upper bound, inclusive.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.name, df.age.between(2, 4)).show()\n",
      "        +-----+---------------------------+\n",
      "        | name|((age >= 2) AND (age <= 4))|\n",
      "        +-----+---------------------------+\n",
      "        |Alice|                       true|\n",
      "        |  Bob|                      false|\n",
      "        +-----+---------------------------+\n",
      "        \"\"\"\n",
      "        return (self >= lowerBound) & (self <= upperBound)\n",
      "\n",
      "------------\n",
      "Method name: bitwiseAND\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: bitwiseOR\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: bitwiseXOR\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: cast\n",
      "Method definition:     def cast(self, dataType: Union[DataType, str]) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Casts the column into type ``dataType``.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      "        [Row(ages='2'), Row(ages='5')]\n",
      "        >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      "        [Row(ages='2'), Row(ages='5')]\n",
      "        \"\"\"\n",
      "        if isinstance(dataType, str):\n",
      "            jc = self._jc.cast(dataType)\n",
      "        elif isinstance(dataType, DataType):\n",
      "            from pyspark.sql import SparkSession\n",
      "\n",
      "            spark = SparkSession._getActiveSessionOrCreate()\n",
      "            jdt = spark._jsparkSession.parseDataType(dataType.json())\n",
      "            jc = self._jc.cast(jdt)\n",
      "        else:\n",
      "            raise TypeError(\"unexpected type: %s\" % type(dataType))\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: contains\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: desc\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: desc_nulls_first\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: desc_nulls_last\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: dropFields\n",
      "Method definition:     def dropFields(self, *fieldNames: str) -> \"Column\":\n",
      "        \"\"\"\n",
      "        An expression that drops fields in :class:`StructType` by name.\n",
      "        This is a no-op if schema doesn't contain field name(s).\n",
      "\n",
      "        .. versionadded:: 3.1.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> from pyspark.sql.functions import col, lit\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
      "        >>> df.withColumn('a', df['a'].dropFields('b')).show()\n",
      "        +-----------------+\n",
      "        |                a|\n",
      "        +-----------------+\n",
      "        |{2, 3, {4, 5, 6}}|\n",
      "        +-----------------+\n",
      "\n",
      "        >>> df.withColumn('a', df['a'].dropFields('b', 'c')).show()\n",
      "        +--------------+\n",
      "        |             a|\n",
      "        +--------------+\n",
      "        |{3, {4, 5, 6}}|\n",
      "        +--------------+\n",
      "\n",
      "        This method supports dropping multiple nested fields directly e.g.\n",
      "\n",
      "        >>> df.withColumn(\"a\", col(\"a\").dropFields(\"e.g\", \"e.h\")).show()\n",
      "        +--------------+\n",
      "        |             a|\n",
      "        +--------------+\n",
      "        |{1, 2, 3, {4}}|\n",
      "        +--------------+\n",
      "\n",
      "        However, if you are going to add/replace multiple nested fields,\n",
      "        it is preferred to extract out the nested struct before\n",
      "        adding/replacing multiple fields e.g.\n",
      "\n",
      "        >>> df.select(col(\"a\").withField(\n",
      "        ...     \"e\", col(\"a.e\").dropFields(\"g\", \"h\")).alias(\"a\")\n",
      "        ... ).show()\n",
      "        +--------------+\n",
      "        |             a|\n",
      "        +--------------+\n",
      "        |{1, 2, 3, {4}}|\n",
      "        +--------------+\n",
      "\n",
      "        \"\"\"\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None\n",
      "        jc = self._jc.dropFields(_to_seq(sc, fieldNames))\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: endswith\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: eqNullSafe\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: getField\n",
      "Method definition:     def getField(self, name: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        An expression that gets a field by name in a :class:`StructType`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
      "        >>> df.select(df.r.getField(\"b\")).show()\n",
      "        +---+\n",
      "        |r.b|\n",
      "        +---+\n",
      "        |  b|\n",
      "        +---+\n",
      "        >>> df.select(df.r.a).show()\n",
      "        +---+\n",
      "        |r.a|\n",
      "        +---+\n",
      "        |  1|\n",
      "        +---+\n",
      "        \"\"\"\n",
      "        if isinstance(name, Column):\n",
      "            warnings.warn(\n",
      "                \"A column as 'name' in getField is deprecated as of Spark 3.0, and will not \"\n",
      "                \"be supported in the future release. Use `column[name]` or `column.name` syntax \"\n",
      "                \"instead.\",\n",
      "                FutureWarning,\n",
      "            )\n",
      "        return self[name]\n",
      "\n",
      "------------\n",
      "Method name: getItem\n",
      "Method definition:     def getItem(self, key: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        An expression that gets an item at position ``ordinal`` out of a list,\n",
      "        or gets an item by key out of a dict.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      "        >>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n",
      "        +----+------+\n",
      "        |l[0]|d[key]|\n",
      "        +----+------+\n",
      "        |   1| value|\n",
      "        +----+------+\n",
      "        \"\"\"\n",
      "        if isinstance(key, Column):\n",
      "            warnings.warn(\n",
      "                \"A column as 'key' in getItem is deprecated as of Spark 3.0, and will not \"\n",
      "                \"be supported in the future release. Use `column[key]` or `column.key` syntax \"\n",
      "                \"instead.\",\n",
      "                FutureWarning,\n",
      "            )\n",
      "        return self[key]\n",
      "\n",
      "------------\n",
      "Method name: ilike\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: isNotNull\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: isNull\n",
      "Method definition:     def _(self: \"Column\") -> \"Column\":\n",
      "        jc = getattr(self._jc, name)()\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: isin\n",
      "Method definition:     def isin(self, *cols: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        A boolean expression that is evaluated to true if the value of this\n",
      "        expression is contained by the evaluated values of the arguments.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        >>> df[df.age.isin([1, 2, 3])].collect()\n",
      "        [Row(age=2, name='Alice')]\n",
      "        \"\"\"\n",
      "        if len(cols) == 1 and isinstance(cols[0], (list, set)):\n",
      "            cols = cast(Tuple, cols[0])\n",
      "        cols = cast(\n",
      "            Tuple,\n",
      "            [c._jc if isinstance(c, Column) else _create_column_from_literal(c) for c in cols],\n",
      "        )\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None\n",
      "        jc = getattr(self._jc, \"isin\")(_to_seq(sc, cols))\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: like\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: name\n",
      "Method definition:     def alias(self, *alias: str, **kwargs: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Returns this column aliased with a new name or names (in the case of expressions that\n",
      "        return more than one column, such as explode).\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        alias : str\n",
      "            desired column names (collects all positional arguments passed)\n",
      "\n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        metadata: dict\n",
      "            a dict of information to be stored in ``metadata`` attribute of the\n",
      "            corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      "            only argument)\n",
      "\n",
      "            .. versionchanged:: 2.2.0\n",
      "               Added optional ``metadata`` argument.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.age.alias(\"age2\")).collect()\n",
      "        [Row(age2=2), Row(age2=5)]\n",
      "        >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      "        99\n",
      "        \"\"\"\n",
      "\n",
      "        metadata = kwargs.pop(\"metadata\", None)\n",
      "        assert not kwargs, \"Unexpected kwargs where passed: %s\" % kwargs\n",
      "\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None\n",
      "        if len(alias) == 1:\n",
      "            if metadata:\n",
      "                assert sc._jvm is not None\n",
      "                jmeta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(json.dumps(metadata))\n",
      "                return Column(getattr(self._jc, \"as\")(alias[0], jmeta))\n",
      "            else:\n",
      "                return Column(getattr(self._jc, \"as\")(alias[0]))\n",
      "        else:\n",
      "            if metadata:\n",
      "                raise ValueError(\"metadata can only be provided for a single column\")\n",
      "            return Column(getattr(self._jc, \"as\")(_to_seq(sc, list(alias))))\n",
      "\n",
      "------------\n",
      "Method name: otherwise\n",
      "Method definition:     def otherwise(self, value: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        value\n",
      "            a literal value, or a :class:`Column` expression.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import functions as F\n",
      "        >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n",
      "        +-----+-------------------------------------+\n",
      "        | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      "        +-----+-------------------------------------+\n",
      "        |Alice|                                    0|\n",
      "        |  Bob|                                    1|\n",
      "        +-----+-------------------------------------+\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        pyspark.sql.functions.when\n",
      "        \"\"\"\n",
      "        v = value._jc if isinstance(value, Column) else value\n",
      "        jc = self._jc.otherwise(v)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: over\n",
      "Method definition:     def over(self, window: \"WindowSpec\") -> \"Column\":\n",
      "        \"\"\"\n",
      "        Define a windowing column.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        window : :class:`WindowSpec`\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`Column`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> window = Window.partitionBy(\"name\").orderBy(\"age\") \\\n",
      "                .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      "        >>> from pyspark.sql.functions import rank, min\n",
      "        >>> from pyspark.sql.functions import desc\n",
      "        >>> df.withColumn(\"rank\", rank().over(window)) \\\n",
      "                .withColumn(\"min\", min('age').over(window)).sort(desc(\"age\")).show()\n",
      "        +---+-----+----+---+\n",
      "        |age| name|rank|min|\n",
      "        +---+-----+----+---+\n",
      "        |  5|  Bob|   1|  5|\n",
      "        |  2|Alice|   1|  2|\n",
      "        +---+-----+----+---+\n",
      "        \"\"\"\n",
      "        from pyspark.sql.window import WindowSpec\n",
      "\n",
      "        if not isinstance(window, WindowSpec):\n",
      "            raise TypeError(\"window should be WindowSpec\")\n",
      "        jc = self._jc.over(window._jspec)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: rlike\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: startswith\n",
      "Method definition:     def _(\n",
      "        self: \"Column\",\n",
      "        other: Union[\"Column\", \"LiteralType\", \"DecimalLiteral\", \"DateTimeLiteral\"],\n",
      "    ) -> \"Column\":\n",
      "        jc = other._jc if isinstance(other, Column) else other\n",
      "        njc = getattr(self._jc, name)(jc)\n",
      "        return Column(njc)\n",
      "\n",
      "------------\n",
      "Method name: substr\n",
      "Method definition:     def substr(self, startPos: Union[int, \"Column\"], length: Union[int, \"Column\"]) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Return a :class:`Column` which is a substring of the column.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        startPos : :class:`Column` or int\n",
      "            start position\n",
      "        length : :class:`Column` or int\n",
      "            length of the substring\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
      "        [Row(col='Ali'), Row(col='Bob')]\n",
      "        \"\"\"\n",
      "        if type(startPos) != type(length):\n",
      "            raise TypeError(\n",
      "                \"startPos and length must be the same type. \"\n",
      "                \"Got {startPos_t} and {length_t}, respectively.\".format(\n",
      "                    startPos_t=type(startPos),\n",
      "                    length_t=type(length),\n",
      "                )\n",
      "            )\n",
      "        if isinstance(startPos, int):\n",
      "            jc = self._jc.substr(startPos, length)\n",
      "        elif isinstance(startPos, Column):\n",
      "            jc = self._jc.substr(startPos._jc, cast(\"Column\", length)._jc)\n",
      "        else:\n",
      "            raise TypeError(\"Unexpected type: %s\" % type(startPos))\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: when\n",
      "Method definition:     def when(self, condition: \"Column\", value: Any) -> \"Column\":\n",
      "        \"\"\"\n",
      "        Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        condition : :class:`Column`\n",
      "            a boolean :class:`Column` expression.\n",
      "        value\n",
      "            a literal value, or a :class:`Column` expression.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import functions as F\n",
      "        >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n",
      "        +-----+------------------------------------------------------------+\n",
      "        | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      "        +-----+------------------------------------------------------------+\n",
      "        |Alice|                                                          -1|\n",
      "        |  Bob|                                                           1|\n",
      "        +-----+------------------------------------------------------------+\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        pyspark.sql.functions.when\n",
      "        \"\"\"\n",
      "        if not isinstance(condition, Column):\n",
      "            raise TypeError(\"condition should be a Column\")\n",
      "        v = value._jc if isinstance(value, Column) else value\n",
      "        jc = self._jc.when(condition._jc, v)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: withField\n",
      "Method definition:     def withField(self, fieldName: str, col: \"Column\") -> \"Column\":\n",
      "        \"\"\"\n",
      "        An expression that adds/replaces a field in :class:`StructType` by name.\n",
      "\n",
      "        .. versionadded:: 3.1.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> from pyspark.sql.functions import lit\n",
      "        >>> df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n",
      "        >>> df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n",
      "        +---+\n",
      "        |  b|\n",
      "        +---+\n",
      "        |  3|\n",
      "        +---+\n",
      "        >>> df.withColumn('a', df['a'].withField('d', lit(4))).select('a.d').show()\n",
      "        +---+\n",
      "        |  d|\n",
      "        +---+\n",
      "        |  4|\n",
      "        +---+\n",
      "        \"\"\"\n",
      "        if not isinstance(fieldName, str):\n",
      "            raise TypeError(\"fieldName should be a string\")\n",
      "\n",
      "        if not isinstance(col, Column):\n",
      "            raise TypeError(\"col should be a Column\")\n",
      "\n",
      "        return Column(self._jc.withField(fieldName, col._jc))\n",
      "\n",
      "------------\n",
      "Method name: __init__\n",
      "Method definition:     def __init__(\n",
      "        self,\n",
      "        source_df: DataFrame,\n",
      "        spark_context: SparkSession.builder.getOrCreate,\n",
      "        config_path: str,\n",
      "        file_name: str,\n",
      "        src_system: str,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        A class checking the quality of a source data frame based on the given criteria.\n",
      "        :param source_df (DataFrame): the source data frame to apply the data quality checks on.\n",
      "        :param spark_context (SparkSession.builder.getOrCreate): the spark session to run the data quality check.\n",
      "        :param config_path (str): the path to config csv file.\n",
      "        :param file_name (str): the name of the file to check the data quality check on\n",
      "        :param src_system (str): the source of the file where it comes from (vendor)\n",
      "        :raise KeyError: raise a key error if the columns in the source data frame is not in the config file.\n",
      "        \"\"\"\n",
      "        # Set variables\n",
      "        self.spark = spark_context\n",
      "        self.source_df = source_df\n",
      "\n",
      "        self.error_df = None\n",
      "        self.error_columns = []\n",
      "        self.error_counter = 0\n",
      "        self.schema_dict = {\n",
      "            \"StringType\": StringType,\n",
      "            \"DateType\": DateType,\n",
      "            \"IntegerType\": IntegerType,\n",
      "            \"FloatType\": FloatType,\n",
      "            \"DoubleType\": DoubleType,\n",
      "        }\n",
      "\n",
      "        # Initial configuration\n",
      "        config_content = self.read_s3_file(config_path).decode()\n",
      "        self.config = self.resolve_config(config_path.replace(\"config.json\", \"env.json\"), json.loads(config_content))\n",
      "\n",
      "        dq_rule_path = self.config[src_system][\"dq_rule_path\"]\n",
      "        # dq_rule_content = self.read_s3_file(dq_rule_path)\n",
      "        self.rule_df = pd.read_csv(dq_rule_path, index_col=\"column_name\")\n",
      "        self.file_name = file_name\n",
      "        self.rule_df = self.rule_df[(self.rule_df[\"file_name\"] == self.file_name)]\n",
      "        self.rule_df = self.rule_df.applymap(lambda x: x if x else np.nan)\n",
      "        self.rule_df.sort_index(inplace=True)\n",
      "        self.sns_message = []\n",
      "\n",
      "        self.input_columns = self.source_df.columns\n",
      "        self.output_columns = self.config[src_system][\"sources\"][self.file_name][\"dq_output_columns\"]\n",
      "        for index in range(len(self.input_columns)):\n",
      "            if \".\" in self.input_columns[index]:\n",
      "                self.input_columns[index] = \"`\" + self.input_columns[index] + \"`\"\n",
      "\n",
      "        missed_columns = set(self.input_columns) - set(self.rule_df.index)\n",
      "        if len(missed_columns) > 0:\n",
      "            logger.warning(f\"[{missed_columns}] are not found in the rule file.\")\n",
      "\n",
      "        self.spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
      "        # self.spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\n",
      "        self.spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
      "        self.spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
      "\n",
      "------------\n",
      "Method name: add_error_col\n",
      "Method definition:     def add_error_col(self, error_msg: str, condition: Column, error_col_name: str) -> None:\n",
      "        \"\"\"\n",
      "        Add an error column based on the condition to filter and the error message\n",
      "        :param error_msg: the error message to be added if the condition is met\n",
      "        :param condition: the condition of the error to be met in order to return the error column\n",
      "        :param error_col_name: the name of the error column\n",
      "        \"\"\"\n",
      "        if condition is not None and error_col_name and error_msg:\n",
      "            col_condition = f.when(condition, f.lit(error_msg)).otherwise(f.lit(None))\n",
      "            error_col_name = error_col_name + str(self.error_counter)\n",
      "            self.source_df = self.source_df.withColumn(error_col_name, col_condition)\n",
      "            self.error_columns.append(f.col(error_col_name))\n",
      "            self.error_counter += 1\n",
      "\n",
      "------------\n",
      "Method name: columns_to_check\n",
      "Method definition:     def columns_to_check(self, criteria: str) -> Index:\n",
      "        \"\"\"\n",
      "        Returns the indexes to be used while working on the check rules.\n",
      "        :param criteria: whether it is data type, nullable, etc.\n",
      "        :return Index: the index of the columns that this condition should be met.\n",
      "        \"\"\"\n",
      "        return self.rule_df[(self.rule_df[criteria]).notna()].index\n",
      "\n",
      "------------\n",
      "Method name: data_type_check\n",
      "Method definition:     def data_type_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        Checks the data type of all columns and give an error column for each column\n",
      "        :param input_col: the column to apply this check.\n",
      "        \"\"\"\n",
      "        print(\"start data type check\")\n",
      "        dtype_key = self.rule_df.loc[input_col, \"type\"]\n",
      "        if dtype_key == \"DateType\":\n",
      "            date_format = self.rule_df.loc[input_col, \"date_format\"]\n",
      "            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.to_date(f.col(input_col), date_format))\n",
      "            # self.source_df.select(input_col + ' schema').cache()\n",
      "        else:\n",
      "            dtype = self.schema_dict[dtype_key]()\n",
      "            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.col(input_col).cast(dtype))\n",
      "            # self.source_df.select(input_col + ' schema').cache()\n",
      "        # dtype_cond should check if the given input_col is not null and the one with schema is null.\n",
      "        dtype_cond = ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")) & (\n",
      "            f.col(input_col + \" schema\").isNull()\n",
      "        )\n",
      "        type_error_msg = f\"data_type_FAIL: Column [{input_col}] should be {dtype_key}\"\n",
      "        self.add_error_col(error_msg=type_error_msg, condition=dtype_cond, error_col_name=input_col + \" type_check\")\n",
      "        logger.info(f\"[{input_col}] dtype check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: is_float\n",
      "Method definition:     @staticmethod\n",
      "    def is_float(element) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the given input can be returned as float or not.\n",
      "        :param element: The input which can be anything (_type_).\n",
      "        :return bool: whether it is float (True) or not (False).\n",
      "        \"\"\"\n",
      "        try:\n",
      "            float(element)\n",
      "            return True\n",
      "        except ValueError:\n",
      "            return False\n",
      "\n",
      "------------\n",
      "Method name: limit_finder\n",
      "Method definition:     def limit_finder(self, input_col: str, rule_value: Union[str, int, float]) -> Union[float, Column, None]:\n",
      "        \"\"\"\n",
      "        Finds the limit based on the given column. If it is a number it returns the number or if it is a column it returns the f.col.\n",
      "        :param input_col: the column to check this condition on\n",
      "        :param rule_value: value of the limit no matter the datatype\n",
      "        :return Union[str, Column, None]: whether a float or a column in order to check for range check.\n",
      "        :raise KeyError: if the input_col needs other columns that is not in the dataset it will raise an error.\n",
      "        \"\"\"\n",
      "        if self.is_float(rule_value):\n",
      "            rule_value = float(rule_value)\n",
      "            if math.isnan(rule_value):\n",
      "                return None\n",
      "            else:\n",
      "                return rule_value\n",
      "        elif type(rule_value) == str:\n",
      "            if rule_value not in self.input_columns:\n",
      "                print(rule_value)\n",
      "                self.sns_message.append(\n",
      "                    f\"column {rule_value} is not in report {self.file_name} while it is {input_col} needed for range check\"\n",
      "                )\n",
      "                return None\n",
      "            return f.col(rule_value)\n",
      "\n",
      "------------\n",
      "Method name: null_check\n",
      "Method definition:     def null_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        Checks not nullable columns and give an error column for input_col\n",
      "        :param input_col: The column to apply the null check.\n",
      "        \"\"\"\n",
      "        print(\"start null_check\")\n",
      "        if not math.isnan(self.rule_df.loc[input_col, \"nullable\"]):\n",
      "            return\n",
      "        null_condition = self.null_cond_syntax(input_col)\n",
      "        null_error_msg = f\"null_FAIL: Column [{input_col}] cannot be null\"\n",
      "        self.add_error_col(error_msg=null_error_msg, condition=null_condition, error_col_name=input_col + \" null_check\")\n",
      "        logger.info(f\"[{input_col}] null check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: null_cond_syntax\n",
      "Method definition:     def null_cond_syntax(self, input_col: str) -> Column:\n",
      "        \"\"\"\n",
      "        The condition for a null check.\n",
      "        :param input_col: the column to apply the check on.\n",
      "        :raise Column: The not null condition column.\n",
      "        \"\"\"\n",
      "        return (f.col(input_col) == \"\") | (f.col(input_col).isNull())\n",
      "\n",
      "------------\n",
      "Method name: read_s3_file\n",
      "Method definition:     def read_s3_file(self, file_path) -> bytes:\n",
      "        \"\"\"\n",
      "        Read s3 file content and return it in byte format\n",
      "        :param file_path: full s3 object path\n",
      "        :return byte content of the file\n",
      "        \"\"\"\n",
      "        file_res = urlparse(file_path)\n",
      "        try:\n",
      "            file_obj = s3_resource.Object(file_res.netloc, file_res.path.lstrip(\"/\"))\n",
      "            return file_obj.get()[\"Body\"].read()\n",
      "        except ClientError:\n",
      "            raise FileNotFoundError(f\"File cannot be found in S3 given path '{file_path}'\")\n",
      "\n",
      "------------\n",
      "Method name: resolve_config\n",
      "Method definition:     def resolve_config(self, env_path, config_content):\n",
      "        \"\"\"\n",
      "        Read config content and resolve env variables\n",
      "        :param env_path: environment file path\n",
      "        :param config_content: environment agnostic config file\n",
      "        :return environmentally resolved config\n",
      "        \"\"\"\n",
      "        env_content = self.read_s3_file(env_path).decode()\n",
      "        env_sub = json.loads(env_content)[\"subs\"]\n",
      "\n",
      "        config_content_str = (\n",
      "            str(config_content)\n",
      "            .replace(\"<env>\", env_sub[\"<env>\"])\n",
      "            .replace(\"<_env>\", env_sub[\"<_env>\"])\n",
      "            .replace(\"<bucket_name>\", env_sub[\"<bucket_name>\"])\n",
      "            .replace(\"<account>\", env_sub[\"<account>\"])\n",
      "        )\n",
      "\n",
      "        return ast.literal_eval(config_content_str)\n",
      "\n",
      "------------\n",
      "Method name: sum_check_syntax\n",
      "Method definition:     def sum_check_syntax(self, input_col1: str, input_col2: str, syntax_value: float) -> Column:\n",
      "        \"\"\"\n",
      "        The syntax to check for sum of 2 columns to equal a syntax value, to be used in conditional checks.\n",
      "        :param input_col1: column 1 to check\n",
      "        :param input_col2: column 2 to check\n",
      "        :param syntax_value: column value that column 1 and 2 should equal to.\n",
      "        :return Column: The sum_check Column condition.\n",
      "        \"\"\"\n",
      "        return ~(f.col(input_col1) + f.col(input_col2) != syntax_value)\n",
      "\n",
      "------------\n",
      "Method name: __getattr__\n",
      "Method definition:     def __getattr__(self, name: str) -> Column:\n",
      "        \"\"\"Returns the :class:`Column` denoted by ``name``.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df.age).collect()\n",
      "        [Row(age=2), Row(age=5)]\n",
      "        \"\"\"\n",
      "        if name not in self.columns:\n",
      "            raise AttributeError(\n",
      "                \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name)\n",
      "            )\n",
      "        jc = self._jdf.apply(name)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: __getitem__\n",
      "Method definition:     def __getitem__(self, item: Union[int, str, Column, List, Tuple]) -> Union[Column, \"DataFrame\"]:\n",
      "        \"\"\"Returns the column as a :class:`Column`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(df['age']).collect()\n",
      "        [Row(age=2), Row(age=5)]\n",
      "        >>> df[ [\"name\", \"age\"]].collect()\n",
      "        [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "        >>> df[ df.age > 3 ].collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        >>> df[df[0] > 3].collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        \"\"\"\n",
      "        if isinstance(item, str):\n",
      "            jc = self._jdf.apply(item)\n",
      "            return Column(jc)\n",
      "        elif isinstance(item, Column):\n",
      "            return self.filter(item)\n",
      "        elif isinstance(item, (list, tuple)):\n",
      "            return self.select(*item)\n",
      "        elif isinstance(item, int):\n",
      "            jc = self._jdf.apply(self.columns[item])\n",
      "            return Column(jc)\n",
      "        else:\n",
      "            raise TypeError(\"unexpected item type: %s\" % type(item))\n",
      "\n",
      "------------\n",
      "Method name: __init__\n",
      "Method definition:     def __init__(\n",
      "        self,\n",
      "        jdf: JavaObject,\n",
      "        sql_ctx: Union[\"SQLContext\", \"SparkSession\"],\n",
      "    ):\n",
      "        from pyspark.sql.context import SQLContext\n",
      "\n",
      "        self._sql_ctx: Optional[\"SQLContext\"] = None\n",
      "\n",
      "        if isinstance(sql_ctx, SQLContext):\n",
      "            assert not os.environ.get(\"SPARK_TESTING\")  # Sanity check for our internal usage.\n",
      "            assert isinstance(sql_ctx, SQLContext)\n",
      "            # We should remove this if-else branch in the future release, and rename\n",
      "            # sql_ctx to session in the constructor. This is an internal code path but\n",
      "            # was kept with an warning because it's used intensively by third-party libraries.\n",
      "            warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "            self._sql_ctx = sql_ctx\n",
      "            session = sql_ctx.sparkSession\n",
      "        else:\n",
      "            session = sql_ctx\n",
      "        self._session: \"SparkSession\" = session\n",
      "\n",
      "        self._sc: SparkContext = sql_ctx._sc\n",
      "        self._jdf: JavaObject = jdf\n",
      "        self.is_cached = False\n",
      "        # initialized lazily\n",
      "        self._schema: Optional[StructType] = None\n",
      "        self._lazy_rdd: Optional[RDD[Row]] = None\n",
      "        # Check whether _repr_html is supported or not, we use it to avoid calling _jdf twice\n",
      "        # by __repr__ and _repr_html_ while eager evaluation opened.\n",
      "        self._support_repr_html = False\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        if not self._support_repr_html and self.sparkSession._jconf.isReplEagerEvalEnabled():\n",
      "            vertical = False\n",
      "            return self._jdf.showString(\n",
      "                self.sparkSession._jconf.replEagerEvalMaxNumRows(),\n",
      "                self.sparkSession._jconf.replEagerEvalTruncate(),\n",
      "                vertical,\n",
      "            )\n",
      "        else:\n",
      "            return \"DataFrame[%s]\" % (\", \".join(\"%s: %s\" % c for c in self.dtypes))\n",
      "\n",
      "------------\n",
      "Method name: _collect_as_arrow\n",
      "Method definition:     def _collect_as_arrow(self, split_batches: bool = False) -> List[\"pa.RecordBatch\"]:\n",
      "        \"\"\"\n",
      "        Returns all records as a list of ArrowRecordBatches, pyarrow must be installed\n",
      "        and available on driver and worker Python environments.\n",
      "        This is an experimental feature.\n",
      "\n",
      "        :param split_batches: split batches such that each column is in its own allocation, so\n",
      "            that the selfDestruct optimization is effective; default False.\n",
      "\n",
      "        .. note:: Experimental.\n",
      "        \"\"\"\n",
      "        from pyspark.sql.dataframe import DataFrame\n",
      "\n",
      "        assert isinstance(self, DataFrame)\n",
      "\n",
      "        with SCCallSiteSync(self._sc):\n",
      "            (\n",
      "                port,\n",
      "                auth_secret,\n",
      "                jsocket_auth_server,\n",
      "            ) = self._jdf.collectAsArrowToPython()\n",
      "\n",
      "        # Collect list of un-ordered batches where last element is a list of correct order indices\n",
      "        try:\n",
      "            batch_stream = _load_from_socket((port, auth_secret), ArrowCollectSerializer())\n",
      "            if split_batches:\n",
      "                # When spark.sql.execution.arrow.pyspark.selfDestruct.enabled, ensure\n",
      "                # each column in each record batch is contained in its own allocation.\n",
      "                # Otherwise, selfDestruct does nothing; it frees each column as its\n",
      "                # converted, but each column will actually be a list of slices of record\n",
      "                # batches, and so no memory is actually freed until all columns are\n",
      "                # converted.\n",
      "                import pyarrow as pa\n",
      "\n",
      "                results = []\n",
      "                for batch_or_indices in batch_stream:\n",
      "                    if isinstance(batch_or_indices, pa.RecordBatch):\n",
      "                        batch_or_indices = pa.RecordBatch.from_arrays(\n",
      "                            [\n",
      "                                # This call actually reallocates the array\n",
      "                                pa.concat_arrays([array])\n",
      "                                for array in batch_or_indices\n",
      "                            ],\n",
      "                            schema=batch_or_indices.schema,\n",
      "                        )\n",
      "                    results.append(batch_or_indices)\n",
      "            else:\n",
      "                results = list(batch_stream)\n",
      "        finally:\n",
      "            # Join serving thread and raise any exceptions from collectAsArrowToPython\n",
      "            jsocket_auth_server.getResult()\n",
      "\n",
      "        # Separate RecordBatches from batch order indices in results\n",
      "        batches = results[:-1]\n",
      "        batch_order = results[-1]\n",
      "\n",
      "        # Re-order the batch list using the correct order\n",
      "        return [batches[i] for i in batch_order]\n",
      "\n",
      "------------\n",
      "Method name: _jcols\n",
      "Method definition:     def _jcols(self, *cols: \"ColumnOrName\") -> JavaObject:\n",
      "        \"\"\"Return a JVM Seq of Columns from a list of Column or column names\n",
      "\n",
      "        If `cols` has only one list in it, cols[0] will be used as the list.\n",
      "        \"\"\"\n",
      "        if len(cols) == 1 and isinstance(cols[0], list):\n",
      "            cols = cols[0]\n",
      "        return self._jseq(cols, _to_java_column)\n",
      "\n",
      "------------\n",
      "Method name: _jmap\n",
      "Method definition:     def _jmap(self, jm: Dict) -> JavaObject:\n",
      "        \"\"\"Return a JVM Scala Map from a dict\"\"\"\n",
      "        return _to_scala_map(self.sparkSession._sc, jm)\n",
      "\n",
      "------------\n",
      "Method name: _joinAsOf\n",
      "Method definition:     def _joinAsOf(\n",
      "        self,\n",
      "        other: \"DataFrame\",\n",
      "        leftAsOfColumn: Union[str, Column],\n",
      "        rightAsOfColumn: Union[str, Column],\n",
      "        on: Optional[Union[str, List[str], Column, List[Column]]] = None,\n",
      "        how: Optional[str] = None,\n",
      "        *,\n",
      "        tolerance: Optional[Column] = None,\n",
      "        allowExactMatches: bool = True,\n",
      "        direction: str = \"backward\",\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Perform an as-of join.\n",
      "\n",
      "        This is similar to a left-join except that we match on nearest\n",
      "        key rather than equal keys.\n",
      "\n",
      "        .. versionadded:: 3.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : :class:`DataFrame`\n",
      "            Right side of the join\n",
      "        leftAsOfColumn : str or :class:`Column`\n",
      "            a string for the as-of join column name, or a Column\n",
      "        rightAsOfColumn : str or :class:`Column`\n",
      "            a string for the as-of join column name, or a Column\n",
      "        on : str, list or :class:`Column`, optional\n",
      "            a string for the join column name, a list of column names,\n",
      "            a join expression (Column), or a list of Columns.\n",
      "            If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "            the column(s) must exist on both sides, and this performs an equi-join.\n",
      "        how : str, optional\n",
      "            default ``inner``. Must be one of: ``inner`` and ``left``.\n",
      "        tolerance : :class:`Column`, optional\n",
      "            an asof tolerance within this range; must be compatible\n",
      "            with the merge index.\n",
      "        allowExactMatches : bool, optional\n",
      "            default ``True``.\n",
      "        direction : str, optional\n",
      "            default ``backward``. Must be one of: ``backward``, ``forward``, and ``nearest``.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The following performs an as-of join between ``left`` and ``right``.\n",
      "\n",
      "        >>> left = spark.createDataFrame([(1, \"a\"), (5, \"b\"), (10,  \"c\")], [\"a\", \"left_val\"])\n",
      "        >>> right = spark.createDataFrame([(1, 1), (2, 2), (3, 3), (6, 6), (7, 7)],\n",
      "        ...                               [\"a\", \"right_val\"])\n",
      "        >>> left._joinAsOf(\n",
      "        ...     right, leftAsOfColumn=\"a\", rightAsOfColumn=\"a\"\n",
      "        ... ).select(left.a, 'left_val', 'right_val').sort(\"a\").collect()\n",
      "        [Row(a=1, left_val='a', right_val=1),\n",
      "         Row(a=5, left_val='b', right_val=3),\n",
      "         Row(a=10, left_val='c', right_val=7)]\n",
      "\n",
      "        >>> from pyspark.sql import functions as F\n",
      "        >>> left._joinAsOf(\n",
      "        ...     right, leftAsOfColumn=\"a\", rightAsOfColumn=\"a\", tolerance=F.lit(1)\n",
      "        ... ).select(left.a, 'left_val', 'right_val').sort(\"a\").collect()\n",
      "        [Row(a=1, left_val='a', right_val=1)]\n",
      "\n",
      "        >>> left._joinAsOf(\n",
      "        ...     right, leftAsOfColumn=\"a\", rightAsOfColumn=\"a\", how=\"left\", tolerance=F.lit(1)\n",
      "        ... ).select(left.a, 'left_val', 'right_val').sort(\"a\").collect()\n",
      "        [Row(a=1, left_val='a', right_val=1),\n",
      "         Row(a=5, left_val='b', right_val=None),\n",
      "         Row(a=10, left_val='c', right_val=None)]\n",
      "\n",
      "        >>> left._joinAsOf(\n",
      "        ...     right, leftAsOfColumn=\"a\", rightAsOfColumn=\"a\", allowExactMatches=False\n",
      "        ... ).select(left.a, 'left_val', 'right_val').sort(\"a\").collect()\n",
      "        [Row(a=5, left_val='b', right_val=3),\n",
      "         Row(a=10, left_val='c', right_val=7)]\n",
      "\n",
      "        >>> left._joinAsOf(\n",
      "        ...     right, leftAsOfColumn=\"a\", rightAsOfColumn=\"a\", direction=\"forward\"\n",
      "        ... ).select(left.a, 'left_val', 'right_val').sort(\"a\").collect()\n",
      "        [Row(a=1, left_val='a', right_val=1),\n",
      "         Row(a=5, left_val='b', right_val=6)]\n",
      "        \"\"\"\n",
      "        if isinstance(leftAsOfColumn, str):\n",
      "            leftAsOfColumn = self[leftAsOfColumn]\n",
      "        left_as_of_jcol = leftAsOfColumn._jc\n",
      "        if isinstance(rightAsOfColumn, str):\n",
      "            rightAsOfColumn = other[rightAsOfColumn]\n",
      "        right_as_of_jcol = rightAsOfColumn._jc\n",
      "\n",
      "        if on is not None and not isinstance(on, list):\n",
      "            on = [on]  # type: ignore[assignment]\n",
      "\n",
      "        if on is not None:\n",
      "            if isinstance(on[0], str):\n",
      "                on = self._jseq(cast(List[str], on))\n",
      "            else:\n",
      "                assert isinstance(on[0], Column), \"on should be Column or list of Column\"\n",
      "                on = reduce(lambda x, y: x.__and__(y), cast(List[Column], on))\n",
      "                on = on._jc\n",
      "\n",
      "        if how is None:\n",
      "            how = \"inner\"\n",
      "        assert isinstance(how, str), \"how should be a string\"\n",
      "\n",
      "        if tolerance is not None:\n",
      "            assert isinstance(tolerance, Column), \"tolerance should be Column\"\n",
      "            tolerance = tolerance._jc\n",
      "\n",
      "        jdf = self._jdf.joinAsOf(\n",
      "            other._jdf,\n",
      "            left_as_of_jcol,\n",
      "            right_as_of_jcol,\n",
      "            on,\n",
      "            how,\n",
      "            tolerance,\n",
      "            allowExactMatches,\n",
      "            direction,\n",
      "        )\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: _jseq\n",
      "Method definition:     def _jseq(\n",
      "        self,\n",
      "        cols: Sequence,\n",
      "        converter: Optional[Callable[..., Union[\"PrimitiveType\", JavaObject]]] = None,\n",
      "    ) -> JavaObject:\n",
      "        \"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\n",
      "        return _to_seq(self.sparkSession._sc, cols, converter)\n",
      "\n",
      "------------\n",
      "Method name: _repr_html_\n",
      "Method definition:     def _repr_html_(self) -> Optional[str]:\n",
      "        \"\"\"Returns a :class:`DataFrame` with html code when you enabled eager evaluation\n",
      "        by 'spark.sql.repl.eagerEval.enabled', this only called by REPL you are\n",
      "        using support eager evaluation with HTML.\n",
      "        \"\"\"\n",
      "        if not self._support_repr_html:\n",
      "            self._support_repr_html = True\n",
      "        if self.sparkSession._jconf.isReplEagerEvalEnabled():\n",
      "            max_num_rows = max(self.sparkSession._jconf.replEagerEvalMaxNumRows(), 0)\n",
      "            sock_info = self._jdf.getRowsToPython(\n",
      "                max_num_rows,\n",
      "                self.sparkSession._jconf.replEagerEvalTruncate(),\n",
      "            )\n",
      "            rows = list(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "            head = rows[0]\n",
      "            row_data = rows[1:]\n",
      "            has_more_data = len(row_data) > max_num_rows\n",
      "            row_data = row_data[:max_num_rows]\n",
      "\n",
      "            html = \"<table border='1'>\\n\"\n",
      "            # generate table head\n",
      "            html += \"<tr><th>%s</th></tr>\\n\" % \"</th><th>\".join(map(lambda x: html_escape(x), head))\n",
      "            # generate table rows\n",
      "            for row in row_data:\n",
      "                html += \"<tr><td>%s</td></tr>\\n\" % \"</td><td>\".join(\n",
      "                    map(lambda x: html_escape(x), row)\n",
      "                )\n",
      "            html += \"</table>\\n\"\n",
      "            if has_more_data:\n",
      "                html += \"only showing top %d %s\\n\" % (\n",
      "                    max_num_rows,\n",
      "                    \"row\" if max_num_rows == 1 else \"rows\",\n",
      "                )\n",
      "            return html\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "------------\n",
      "Method name: _sort_cols\n",
      "Method definition:     def _sort_cols(\n",
      "        self, cols: Sequence[Union[str, Column, List[Union[str, Column]]]], kwargs: Dict[str, Any]\n",
      "    ) -> JavaObject:\n",
      "        \"\"\"Return a JVM Seq of Columns that describes the sort order\"\"\"\n",
      "        if not cols:\n",
      "            raise ValueError(\"should sort by at least one column\")\n",
      "        if len(cols) == 1 and isinstance(cols[0], list):\n",
      "            cols = cols[0]\n",
      "        jcols = [_to_java_column(cast(\"ColumnOrName\", c)) for c in cols]\n",
      "        ascending = kwargs.get(\"ascending\", True)\n",
      "        if isinstance(ascending, (bool, int)):\n",
      "            if not ascending:\n",
      "                jcols = [jc.desc() for jc in jcols]\n",
      "        elif isinstance(ascending, list):\n",
      "            jcols = [jc if asc else jc.desc() for asc, jc in zip(ascending, jcols)]\n",
      "        else:\n",
      "            raise TypeError(\"ascending can only be boolean or list, but got %s\" % type(ascending))\n",
      "        return self._jseq(jcols)\n",
      "\n",
      "------------\n",
      "Method name: _to_corrected_pandas_type\n",
      "Method definition:     @staticmethod\n",
      "    def _to_corrected_pandas_type(dt: DataType) -> Optional[Type]:\n",
      "        \"\"\"\n",
      "        When converting Spark SQL records to Pandas `pandas.DataFrame`, the inferred data type\n",
      "        may be wrong. This method gets the corrected data type for Pandas if that type may be\n",
      "        inferred incorrectly.\n",
      "        \"\"\"\n",
      "        import numpy as np\n",
      "\n",
      "        if type(dt) == ByteType:\n",
      "            return np.int8\n",
      "        elif type(dt) == ShortType:\n",
      "            return np.int16\n",
      "        elif type(dt) == IntegerType:\n",
      "            return np.int32\n",
      "        elif type(dt) == LongType:\n",
      "            return np.int64\n",
      "        elif type(dt) == FloatType:\n",
      "            return np.float32\n",
      "        elif type(dt) == DoubleType:\n",
      "            return np.float64\n",
      "        elif type(dt) == BooleanType:\n",
      "            return np.bool  # type: ignore[attr-defined]\n",
      "        elif type(dt) == TimestampType:\n",
      "            return np.datetime64\n",
      "        elif type(dt) == TimestampNTZType:\n",
      "            return np.datetime64\n",
      "        elif type(dt) == DayTimeIntervalType:\n",
      "            return np.timedelta64\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "------------\n",
      "Method name: agg\n",
      "Method definition:     def agg(self, *exprs: Union[Column, Dict[str, str]]) -> \"DataFrame\":\n",
      "        \"\"\"Aggregate on the entire :class:`DataFrame` without groups\n",
      "        (shorthand for ``df.groupBy().agg()``).\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.agg({\"age\": \"max\"}).collect()\n",
      "        [Row(max(age)=5)]\n",
      "        >>> from pyspark.sql import functions as F\n",
      "        >>> df.agg(F.min(df.age)).collect()\n",
      "        [Row(min(age)=2)]\n",
      "        \"\"\"\n",
      "        return self.groupBy().agg(*exprs)  # type: ignore[arg-type]\n",
      "\n",
      "------------\n",
      "Method name: alias\n",
      "Method definition:     def alias(self, alias: str) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` with an alias set.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        alias : str\n",
      "            an alias name to be set for the :class:`DataFrame`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import *\n",
      "        >>> df_as1 = df.alias(\"df_as1\")\n",
      "        >>> df_as2 = df.alias(\"df_as2\")\n",
      "        >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      "        >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\") \\\n",
      "                .sort(desc(\"df_as1.name\")).collect()\n",
      "        [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n",
      "        \"\"\"\n",
      "        assert isinstance(alias, str), \"alias should be a string\"\n",
      "        return DataFrame(getattr(self._jdf, \"as\")(alias), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: approxQuantile\n",
      "Method definition:     def approxQuantile(\n",
      "        self,\n",
      "        col: Union[str, List[str], Tuple[str]],\n",
      "        probabilities: Union[List[float], Tuple[float]],\n",
      "        relativeError: float,\n",
      "    ) -> Union[List[float], List[List[float]]]:\n",
      "        \"\"\"\n",
      "        Calculates the approximate quantiles of numerical columns of a\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        The result of this algorithm has the following deterministic bound:\n",
      "        If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      "        probability `p` up to error `err`, then the algorithm will return\n",
      "        a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      "        close to (p * N). More precisely,\n",
      "\n",
      "          floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      "\n",
      "        This method implements a variation of the Greenwald-Khanna\n",
      "        algorithm (with some speed optimizations). The algorithm was first\n",
      "        present in [[https://doi.org/10.1145/375663.375670\n",
      "        Space-efficient Online Computation of Quantile Summaries]]\n",
      "        by Greenwald and Khanna.\n",
      "\n",
      "        Note that null values will be ignored in numerical columns before calculation.\n",
      "        For columns only containing null values, an empty list is returned.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        col: str, tuple or list\n",
      "            Can be a single column name, or a list of names for multiple columns.\n",
      "\n",
      "            .. versionchanged:: 2.2\n",
      "               Added support for multiple columns.\n",
      "        probabilities : list or tuple\n",
      "            a list of quantile probabilities\n",
      "            Each number must belong to [0, 1].\n",
      "            For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      "        relativeError : float\n",
      "            The relative target precision to achieve\n",
      "            (>= 0). If set to zero, the exact quantiles are computed, which\n",
      "            could be very expensive. Note that values greater than 1 are\n",
      "            accepted but give the same result as 1.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        list\n",
      "            the approximate quantiles at the given probabilities. If\n",
      "            the input `col` is a string, the output is a list of floats. If the\n",
      "            input `col` is a list or tuple of strings, the output is also a\n",
      "            list, but each element in it is a list of floats, i.e., the output\n",
      "            is a list of list of floats.\n",
      "        \"\"\"\n",
      "\n",
      "        if not isinstance(col, (str, list, tuple)):\n",
      "            raise TypeError(\"col should be a string, list or tuple, but got %r\" % type(col))\n",
      "\n",
      "        isStr = isinstance(col, str)\n",
      "\n",
      "        if isinstance(col, tuple):\n",
      "            col = list(col)\n",
      "        elif isStr:\n",
      "            col = [cast(str, col)]\n",
      "\n",
      "        for c in col:\n",
      "            if not isinstance(c, str):\n",
      "                raise TypeError(\"columns should be strings, but got %r\" % type(c))\n",
      "        col = _to_list(self._sc, cast(List[\"ColumnOrName\"], col))\n",
      "\n",
      "        if not isinstance(probabilities, (list, tuple)):\n",
      "            raise TypeError(\"probabilities should be a list or tuple\")\n",
      "        if isinstance(probabilities, tuple):\n",
      "            probabilities = list(probabilities)\n",
      "        for p in probabilities:\n",
      "            if not isinstance(p, (float, int)) or p < 0 or p > 1:\n",
      "                raise ValueError(\"probabilities should be numerical (float, int) in [0,1].\")\n",
      "        probabilities = _to_list(self._sc, cast(List[\"ColumnOrName\"], probabilities))\n",
      "\n",
      "        if not isinstance(relativeError, (float, int)):\n",
      "            raise TypeError(\"relativeError should be numerical (float, int)\")\n",
      "        if relativeError < 0:\n",
      "            raise ValueError(\"relativeError should be >= 0.\")\n",
      "        relativeError = float(relativeError)\n",
      "\n",
      "        jaq = self._jdf.stat().approxQuantile(col, probabilities, relativeError)\n",
      "        jaq_list = [list(j) for j in jaq]\n",
      "        return jaq_list[0] if isStr else jaq_list\n",
      "\n",
      "------------\n",
      "Method name: cache\n",
      "Method definition:     def cache(self) -> \"DataFrame\":\n",
      "        \"\"\"Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      "        \"\"\"\n",
      "        self.is_cached = True\n",
      "        self._jdf.cache()\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: checkpoint\n",
      "Method definition:     def checkpoint(self, eager: bool = True) -> \"DataFrame\":\n",
      "        \"\"\"Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\n",
      "        truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      "        iterative algorithms where the plan may grow exponentially. It will be saved to files\n",
      "        inside the checkpoint directory set with :meth:`SparkContext.setCheckpointDir`.\n",
      "\n",
      "        .. versionadded:: 2.1.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        eager : bool, optional\n",
      "            Whether to checkpoint this :class:`DataFrame` immediately\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This API is experimental.\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.checkpoint(eager)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: coalesce\n",
      "Method definition:     def coalesce(self, numPartitions: int) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      "\n",
      "        Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      "        narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      "        there will not be a shuffle, instead each of the 100 new partitions will\n",
      "        claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      "        it will stay at the current number of partitions.\n",
      "\n",
      "        However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      "        this may result in your computation taking place on fewer nodes than\n",
      "        you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      "        you can call repartition(). This will add a shuffle step, but means the\n",
      "        current upstream partitions will be executed in parallel (per whatever\n",
      "        the current partitioning is).\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        numPartitions : int\n",
      "            specify the target number of partitions\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.coalesce(1).rdd.getNumPartitions()\n",
      "        1\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.coalesce(numPartitions), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: colRegex\n",
      "Method definition:     def colRegex(self, colName: str) -> Column:\n",
      "        \"\"\"\n",
      "        Selects column based on the column name specified as a regex and returns it\n",
      "        as :class:`Column`.\n",
      "\n",
      "        .. versionadded:: 2.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        colName : str\n",
      "            string, column name specified as a regex.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      "        >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      "        +----+\n",
      "        |Col2|\n",
      "        +----+\n",
      "        |   1|\n",
      "        |   2|\n",
      "        |   3|\n",
      "        +----+\n",
      "        \"\"\"\n",
      "        if not isinstance(colName, str):\n",
      "            raise TypeError(\"colName should be provided as string\")\n",
      "        jc = self._jdf.colRegex(colName)\n",
      "        return Column(jc)\n",
      "\n",
      "------------\n",
      "Method name: collect\n",
      "Method definition:     def collect(self) -> List[Row]:\n",
      "        \"\"\"Returns all the records as a list of :class:`Row`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.collect()\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        \"\"\"\n",
      "        with SCCallSiteSync(self._sc):\n",
      "            sock_info = self._jdf.collectToPython()\n",
      "        return list(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\n",
      "------------\n",
      "Method name: corr\n",
      "Method definition:     def corr(self, col1: str, col2: str, method: Optional[str] = None) -> float:\n",
      "        \"\"\"\n",
      "        Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      "        Currently only supports the Pearson Correlation Coefficient.\n",
      "        :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str\n",
      "            The name of the first column\n",
      "        col2 : str\n",
      "            The name of the second column\n",
      "        method : str, optional\n",
      "            The correlation method. Currently only supports \"pearson\"\n",
      "        \"\"\"\n",
      "        if not isinstance(col1, str):\n",
      "            raise TypeError(\"col1 should be a string.\")\n",
      "        if not isinstance(col2, str):\n",
      "            raise TypeError(\"col2 should be a string.\")\n",
      "        if not method:\n",
      "            method = \"pearson\"\n",
      "        if not method == \"pearson\":\n",
      "            raise ValueError(\n",
      "                \"Currently only the calculation of the Pearson Correlation \"\n",
      "                + \"coefficient is supported.\"\n",
      "            )\n",
      "        return self._jdf.stat().corr(col1, col2, method)\n",
      "\n",
      "------------\n",
      "Method name: count\n",
      "Method definition:     def count(self) -> int:\n",
      "        \"\"\"Returns the number of rows in this :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.count()\n",
      "        2\n",
      "        \"\"\"\n",
      "        return int(self._jdf.count())\n",
      "\n",
      "------------\n",
      "Method name: cov\n",
      "Method definition:     def cov(self, col1: str, col2: str) -> float:\n",
      "        \"\"\"\n",
      "        Calculate the sample covariance for the given columns, specified by their names, as a\n",
      "        double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str\n",
      "            The name of the first column\n",
      "        col2 : str\n",
      "            The name of the second column\n",
      "        \"\"\"\n",
      "        if not isinstance(col1, str):\n",
      "            raise TypeError(\"col1 should be a string.\")\n",
      "        if not isinstance(col2, str):\n",
      "            raise TypeError(\"col2 should be a string.\")\n",
      "        return self._jdf.stat().cov(col1, col2)\n",
      "\n",
      "------------\n",
      "Method name: createGlobalTempView\n",
      "Method definition:     def createGlobalTempView(self, name: str) -> None:\n",
      "        \"\"\"Creates a global temporary view with this :class:`DataFrame`.\n",
      "\n",
      "        The lifetime of this temporary view is tied to this Spark application.\n",
      "        throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      "        catalog.\n",
      "\n",
      "        .. versionadded:: 2.1.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.createGlobalTempView(\"people\")\n",
      "        >>> df2 = spark.sql(\"select * from global_temp.people\")\n",
      "        >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "        True\n",
      "        >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      "        >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      "        True\n",
      "\n",
      "        \"\"\"\n",
      "        self._jdf.createGlobalTempView(name)\n",
      "\n",
      "------------\n",
      "Method name: createOrReplaceGlobalTempView\n",
      "Method definition:     def createOrReplaceGlobalTempView(self, name: str) -> None:\n",
      "        \"\"\"Creates or replaces a global temporary view using the given name.\n",
      "\n",
      "        The lifetime of this temporary view is tied to this Spark application.\n",
      "\n",
      "        .. versionadded:: 2.2.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      "        >>> df2 = df.filter(df.age > 3)\n",
      "        >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      "        >>> df3 = spark.sql(\"select * from global_temp.people\")\n",
      "        >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      "        True\n",
      "        >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      "        True\n",
      "\n",
      "        \"\"\"\n",
      "        self._jdf.createOrReplaceGlobalTempView(name)\n",
      "\n",
      "------------\n",
      "Method name: createOrReplaceTempView\n",
      "Method definition:     def createOrReplaceTempView(self, name: str) -> None:\n",
      "        \"\"\"Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      "\n",
      "        The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "        that was used to create this :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.createOrReplaceTempView(\"people\")\n",
      "        >>> df2 = df.filter(df.age > 3)\n",
      "        >>> df2.createOrReplaceTempView(\"people\")\n",
      "        >>> df3 = spark.sql(\"select * from people\")\n",
      "        >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      "        True\n",
      "        >>> spark.catalog.dropTempView(\"people\")\n",
      "        True\n",
      "\n",
      "        \"\"\"\n",
      "        self._jdf.createOrReplaceTempView(name)\n",
      "\n",
      "------------\n",
      "Method name: createTempView\n",
      "Method definition:     def createTempView(self, name: str) -> None:\n",
      "        \"\"\"Creates a local temporary view with this :class:`DataFrame`.\n",
      "\n",
      "        The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "        that was used to create this :class:`DataFrame`.\n",
      "        throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      "        catalog.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.createTempView(\"people\")\n",
      "        >>> df2 = spark.sql(\"select * from people\")\n",
      "        >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "        True\n",
      "        >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      "        >>> spark.catalog.dropTempView(\"people\")\n",
      "        True\n",
      "\n",
      "        \"\"\"\n",
      "        self._jdf.createTempView(name)\n",
      "\n",
      "------------\n",
      "Method name: crossJoin\n",
      "Method definition:     def crossJoin(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Returns the cartesian product with another :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 2.1.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : :class:`DataFrame`\n",
      "            Right side of the cartesian product.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select(\"age\", \"name\").collect()\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        >>> df2.select(\"name\", \"height\").collect()\n",
      "        [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n",
      "        >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n",
      "        [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n",
      "         Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n",
      "        \"\"\"\n",
      "\n",
      "        jdf = self._jdf.crossJoin(other._jdf)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: crosstab\n",
      "Method definition:     def crosstab(self, col1: str, col2: str) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      "        table. The number of distinct values for each column should be less than 1e4. At most 1e6\n",
      "        non-zero pair frequencies will be returned.\n",
      "        The first column of each row will be the distinct values of `col1` and the column names\n",
      "        will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      "        Pairs that have no occurrences will have zero as their counts.\n",
      "        :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str\n",
      "            The name of the first column. Distinct items will make the first item of\n",
      "            each row.\n",
      "        col2 : str\n",
      "            The name of the second column. Distinct items will make the column names\n",
      "            of the :class:`DataFrame`.\n",
      "        \"\"\"\n",
      "        if not isinstance(col1, str):\n",
      "            raise TypeError(\"col1 should be a string.\")\n",
      "        if not isinstance(col2, str):\n",
      "            raise TypeError(\"col2 should be a string.\")\n",
      "        return DataFrame(self._jdf.stat().crosstab(col1, col2), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: cube\n",
      "Method definition:     def cube(self, *cols: \"ColumnOrName\") -> \"GroupedData\":  # type: ignore[misc]\n",
      "        \"\"\"\n",
      "        Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      "        the specified columns, so we can run aggregations on them.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      "        +-----+----+-----+\n",
      "        | name| age|count|\n",
      "        +-----+----+-----+\n",
      "        | null|null|    2|\n",
      "        | null|   2|    1|\n",
      "        | null|   5|    1|\n",
      "        |Alice|null|    1|\n",
      "        |Alice|   2|    1|\n",
      "        |  Bob|null|    1|\n",
      "        |  Bob|   5|    1|\n",
      "        +-----+----+-----+\n",
      "        \"\"\"\n",
      "        jgd = self._jdf.cube(self._jcols(*cols))\n",
      "        from pyspark.sql.group import GroupedData\n",
      "\n",
      "        return GroupedData(jgd, self)\n",
      "\n",
      "------------\n",
      "Method name: describe\n",
      "Method definition:     def describe(self, *cols: Union[str, List[str]]) -> \"DataFrame\":\n",
      "        \"\"\"Computes basic statistics for numeric and string columns.\n",
      "\n",
      "        .. versionadded:: 1.3.1\n",
      "\n",
      "        This include count, mean, stddev, min, and max. If no columns are\n",
      "        given, this function computes statistics for all numerical or string columns.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This function is meant for exploratory data analysis, as we make no\n",
      "        guarantee about the backward compatibility of the schema of the resulting\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        Use summary for expanded statistics and control over which statistics to compute.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      "        ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      "        ... )\n",
      "        >>> df.describe(['age']).show()\n",
      "        +-------+----+\n",
      "        |summary| age|\n",
      "        +-------+----+\n",
      "        |  count|   3|\n",
      "        |   mean|12.0|\n",
      "        | stddev| 1.0|\n",
      "        |    min|  11|\n",
      "        |    max|  13|\n",
      "        +-------+----+\n",
      "\n",
      "        >>> df.describe(['age', 'weight', 'height']).show()\n",
      "        +-------+----+------------------+-----------------+\n",
      "        |summary| age|            weight|           height|\n",
      "        +-------+----+------------------+-----------------+\n",
      "        |  count|   3|                 3|                3|\n",
      "        |   mean|12.0| 40.73333333333333|            145.0|\n",
      "        | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      "        |    min|  11|              37.8|            142.2|\n",
      "        |    max|  13|              44.1|            150.5|\n",
      "        +-------+----+------------------+-----------------+\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        DataFrame.summary\n",
      "        \"\"\"\n",
      "        if len(cols) == 1 and isinstance(cols[0], list):\n",
      "            cols = cols[0]  # type: ignore[assignment]\n",
      "        jdf = self._jdf.describe(self._jseq(cols))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: distinct\n",
      "Method definition:     def distinct(self) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.distinct().count()\n",
      "        2\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.distinct(), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: drop\n",
      "Method definition:     def drop(self, *cols: \"ColumnOrName\") -> \"DataFrame\":  # type: ignore[misc]\n",
      "        \"\"\"Returns a new :class:`DataFrame` that drops the specified column.\n",
      "        This is a no-op if schema doesn't contain the given column name(s).\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols: str or :class:`Column`\n",
      "            a name of the column, or the :class:`Column` to drop\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.drop('age').collect()\n",
      "        [Row(name='Alice'), Row(name='Bob')]\n",
      "\n",
      "        >>> df.drop(df.age).collect()\n",
      "        [Row(name='Alice'), Row(name='Bob')]\n",
      "\n",
      "        >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      "        [Row(age=5, height=85, name='Bob')]\n",
      "\n",
      "        >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      "        [Row(age=5, name='Bob', height=85)]\n",
      "\n",
      "        >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      "        [Row(name='Bob')]\n",
      "        \"\"\"\n",
      "        if len(cols) == 1:\n",
      "            col = cols[0]\n",
      "            if isinstance(col, str):\n",
      "                jdf = self._jdf.drop(col)\n",
      "            elif isinstance(col, Column):\n",
      "                jdf = self._jdf.drop(col._jc)\n",
      "            else:\n",
      "                raise TypeError(\"col should be a string or a Column\")\n",
      "        else:\n",
      "            for col in cols:\n",
      "                if not isinstance(col, str):\n",
      "                    raise TypeError(\"each col in the param list should be a string\")\n",
      "            jdf = self._jdf.drop(self._jseq(cols))\n",
      "\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: dropDuplicates\n",
      "Method definition:     def dropDuplicates(self, subset: Optional[List[str]] = None) -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "        optionally only considering certain columns.\n",
      "\n",
      "        For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      "        :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      "        duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      "        be and system will accordingly limit the state. In addition, too late data older than\n",
      "        watermark will be dropped to avoid any possibility of duplicates.\n",
      "\n",
      "        :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = sc.parallelize([ \\\\\n",
      "        ...     Row(name='Alice', age=5, height=80), \\\\\n",
      "        ...     Row(name='Alice', age=5, height=80), \\\\\n",
      "        ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      "        >>> df.dropDuplicates().show()\n",
      "        +-----+---+------+\n",
      "        | name|age|height|\n",
      "        +-----+---+------+\n",
      "        |Alice|  5|    80|\n",
      "        |Alice| 10|    80|\n",
      "        +-----+---+------+\n",
      "\n",
      "        >>> df.dropDuplicates(['name', 'height']).show()\n",
      "        +-----+---+------+\n",
      "        | name|age|height|\n",
      "        +-----+---+------+\n",
      "        |Alice|  5|    80|\n",
      "        +-----+---+------+\n",
      "        \"\"\"\n",
      "        if subset is not None and (not isinstance(subset, Iterable) or isinstance(subset, str)):\n",
      "            raise TypeError(\"Parameter 'subset' must be a list of columns\")\n",
      "\n",
      "        if subset is None:\n",
      "            jdf = self._jdf.dropDuplicates()\n",
      "        else:\n",
      "            jdf = self._jdf.dropDuplicates(self._jseq(subset))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: drop_duplicates\n",
      "Method definition:     def dropDuplicates(self, subset: Optional[List[str]] = None) -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "        optionally only considering certain columns.\n",
      "\n",
      "        For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      "        :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      "        duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      "        be and system will accordingly limit the state. In addition, too late data older than\n",
      "        watermark will be dropped to avoid any possibility of duplicates.\n",
      "\n",
      "        :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = sc.parallelize([ \\\\\n",
      "        ...     Row(name='Alice', age=5, height=80), \\\\\n",
      "        ...     Row(name='Alice', age=5, height=80), \\\\\n",
      "        ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      "        >>> df.dropDuplicates().show()\n",
      "        +-----+---+------+\n",
      "        | name|age|height|\n",
      "        +-----+---+------+\n",
      "        |Alice|  5|    80|\n",
      "        |Alice| 10|    80|\n",
      "        +-----+---+------+\n",
      "\n",
      "        >>> df.dropDuplicates(['name', 'height']).show()\n",
      "        +-----+---+------+\n",
      "        | name|age|height|\n",
      "        +-----+---+------+\n",
      "        |Alice|  5|    80|\n",
      "        +-----+---+------+\n",
      "        \"\"\"\n",
      "        if subset is not None and (not isinstance(subset, Iterable) or isinstance(subset, str)):\n",
      "            raise TypeError(\"Parameter 'subset' must be a list of columns\")\n",
      "\n",
      "        if subset is None:\n",
      "            jdf = self._jdf.dropDuplicates()\n",
      "        else:\n",
      "            jdf = self._jdf.dropDuplicates(self._jseq(subset))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: dropna\n",
      "Method definition:     def dropna(\n",
      "        self,\n",
      "        how: str = \"any\",\n",
      "        thresh: Optional[int] = None,\n",
      "        subset: Optional[Union[str, Tuple[str, ...], List[str]]] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` omitting rows with null values.\n",
      "        :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      "\n",
      "        .. versionadded:: 1.3.1\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        how : str, optional\n",
      "            'any' or 'all'.\n",
      "            If 'any', drop a row if it contains any nulls.\n",
      "            If 'all', drop a row only if all its values are null.\n",
      "        thresh: int, optional\n",
      "            default None\n",
      "            If specified, drop rows that have less than `thresh` non-null values.\n",
      "            This overwrites the `how` parameter.\n",
      "        subset : str, tuple or list, optional\n",
      "            optional list of column names to consider.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df4.na.drop().show()\n",
      "        +---+------+-----+\n",
      "        |age|height| name|\n",
      "        +---+------+-----+\n",
      "        | 10|    80|Alice|\n",
      "        +---+------+-----+\n",
      "        \"\"\"\n",
      "        if how is not None and how not in [\"any\", \"all\"]:\n",
      "            raise ValueError(\"how ('\" + how + \"') should be 'any' or 'all'\")\n",
      "\n",
      "        if subset is None:\n",
      "            subset = self.columns\n",
      "        elif isinstance(subset, str):\n",
      "            subset = [subset]\n",
      "        elif not isinstance(subset, (list, tuple)):\n",
      "            raise TypeError(\"subset should be a list or tuple of column names\")\n",
      "\n",
      "        if thresh is None:\n",
      "            thresh = len(subset) if how == \"any\" else 1\n",
      "\n",
      "        return DataFrame(self._jdf.na().drop(thresh, self._jseq(subset)), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: exceptAll\n",
      "Method definition:     def exceptAll(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      "        not in another :class:`DataFrame` while preserving duplicates.\n",
      "\n",
      "        This is equivalent to `EXCEPT ALL` in SQL.\n",
      "        As standard in SQL, this function resolves columns by position (not by name).\n",
      "\n",
      "        .. versionadded:: 2.4.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame(\n",
      "        ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "        >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "\n",
      "        >>> df1.exceptAll(df2).show()\n",
      "        +---+---+\n",
      "        | C1| C2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  c|  4|\n",
      "        +---+---+\n",
      "\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.exceptAll(other._jdf), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: explain\n",
      "Method definition:     def explain(\n",
      "        self, extended: Optional[Union[bool, str]] = None, mode: Optional[str] = None\n",
      "    ) -> None:\n",
      "        \"\"\"Prints the (logical and physical) plans to the console for debugging purpose.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        extended : bool, optional\n",
      "            default ``False``. If ``False``, prints only the physical plan.\n",
      "            When this is a string without specifying the ``mode``, it works as the mode is\n",
      "            specified.\n",
      "        mode : str, optional\n",
      "            specifies the expected output format of plans.\n",
      "\n",
      "            * ``simple``: Print only a physical plan.\n",
      "            * ``extended``: Print both logical and physical plans.\n",
      "            * ``codegen``: Print a physical plan and generated codes if they are available.\n",
      "            * ``cost``: Print a logical plan and statistics if they are available.\n",
      "            * ``formatted``: Split explain output into two sections: a physical plan outline \\\n",
      "                and node details.\n",
      "\n",
      "            .. versionchanged:: 3.0.0\n",
      "               Added optional argument `mode` to specify the expected output format of plans.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.explain()\n",
      "        == Physical Plan ==\n",
      "        *(1) Scan ExistingRDD[age#0,name#1]\n",
      "\n",
      "        >>> df.explain(True)\n",
      "        == Parsed Logical Plan ==\n",
      "        ...\n",
      "        == Analyzed Logical Plan ==\n",
      "        ...\n",
      "        == Optimized Logical Plan ==\n",
      "        ...\n",
      "        == Physical Plan ==\n",
      "        ...\n",
      "\n",
      "        >>> df.explain(mode=\"formatted\")\n",
      "        == Physical Plan ==\n",
      "        * Scan ExistingRDD (1)\n",
      "        (1) Scan ExistingRDD [codegen id : 1]\n",
      "        Output [2]: [age#0, name#1]\n",
      "        ...\n",
      "\n",
      "        >>> df.explain(\"cost\")\n",
      "        == Optimized Logical Plan ==\n",
      "        ...Statistics...\n",
      "        ...\n",
      "        \"\"\"\n",
      "\n",
      "        if extended is not None and mode is not None:\n",
      "            raise ValueError(\"extended and mode should not be set together.\")\n",
      "\n",
      "        # For the no argument case: df.explain()\n",
      "        is_no_argument = extended is None and mode is None\n",
      "\n",
      "        # For the cases below:\n",
      "        #   explain(True)\n",
      "        #   explain(extended=False)\n",
      "        is_extended_case = isinstance(extended, bool) and mode is None\n",
      "\n",
      "        # For the case when extended is mode:\n",
      "        #   df.explain(\"formatted\")\n",
      "        is_extended_as_mode = isinstance(extended, str) and mode is None\n",
      "\n",
      "        # For the mode specified:\n",
      "        #   df.explain(mode=\"formatted\")\n",
      "        is_mode_case = extended is None and isinstance(mode, str)\n",
      "\n",
      "        if not (is_no_argument or is_extended_case or is_extended_as_mode or is_mode_case):\n",
      "            argtypes = [str(type(arg)) for arg in [extended, mode] if arg is not None]\n",
      "            raise TypeError(\n",
      "                \"extended (optional) and mode (optional) should be a string \"\n",
      "                \"and bool; however, got [%s].\" % \", \".join(argtypes)\n",
      "            )\n",
      "\n",
      "        # Sets an explain mode depending on a given argument\n",
      "        if is_no_argument:\n",
      "            explain_mode = \"simple\"\n",
      "        elif is_extended_case:\n",
      "            explain_mode = \"extended\" if extended else \"simple\"\n",
      "        elif is_mode_case:\n",
      "            explain_mode = cast(str, mode)\n",
      "        elif is_extended_as_mode:\n",
      "            explain_mode = cast(str, extended)\n",
      "        assert self._sc._jvm is not None\n",
      "        print(self._sc._jvm.PythonSQLUtils.explainString(self._jdf.queryExecution(), explain_mode))\n",
      "\n",
      "------------\n",
      "Method name: fillna\n",
      "Method definition:     def fillna(\n",
      "        self,\n",
      "        value: Union[\"LiteralType\", Dict[str, \"LiteralType\"]],\n",
      "        subset: Optional[Union[str, Tuple[str, ...], List[str]]] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Replace null values, alias for ``na.fill()``.\n",
      "        :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      "\n",
      "        .. versionadded:: 1.3.1\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        value : int, float, string, bool or dict\n",
      "            Value to replace null values with.\n",
      "            If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      "            from column name (string) to replacement value. The replacement value must be\n",
      "            an int, float, boolean, or string.\n",
      "        subset : str, tuple or list, optional\n",
      "            optional list of column names to consider.\n",
      "            Columns specified in subset that do not have matching data type are ignored.\n",
      "            For example, if `value` is a string, and subset contains a non-string column,\n",
      "            then the non-string column is simply ignored.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df4.na.fill(50).show()\n",
      "        +---+------+-----+\n",
      "        |age|height| name|\n",
      "        +---+------+-----+\n",
      "        | 10|    80|Alice|\n",
      "        |  5|    50|  Bob|\n",
      "        | 50|    50|  Tom|\n",
      "        | 50|    50| null|\n",
      "        +---+------+-----+\n",
      "\n",
      "        >>> df5.na.fill(False).show()\n",
      "        +----+-------+-----+\n",
      "        | age|   name|  spy|\n",
      "        +----+-------+-----+\n",
      "        |  10|  Alice|false|\n",
      "        |   5|    Bob|false|\n",
      "        |null|Mallory| true|\n",
      "        +----+-------+-----+\n",
      "\n",
      "        >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      "        +---+------+-------+\n",
      "        |age|height|   name|\n",
      "        +---+------+-------+\n",
      "        | 10|    80|  Alice|\n",
      "        |  5|  null|    Bob|\n",
      "        | 50|  null|    Tom|\n",
      "        | 50|  null|unknown|\n",
      "        +---+------+-------+\n",
      "        \"\"\"\n",
      "        if not isinstance(value, (float, int, str, bool, dict)):\n",
      "            raise TypeError(\"value should be a float, int, string, bool or dict\")\n",
      "\n",
      "        # Note that bool validates isinstance(int), but we don't want to\n",
      "        # convert bools to floats\n",
      "\n",
      "        if not isinstance(value, bool) and isinstance(value, int):\n",
      "            value = float(value)\n",
      "\n",
      "        if isinstance(value, dict):\n",
      "            return DataFrame(self._jdf.na().fill(value), self.sparkSession)\n",
      "        elif subset is None:\n",
      "            return DataFrame(self._jdf.na().fill(value), self.sparkSession)\n",
      "        else:\n",
      "            if isinstance(subset, str):\n",
      "                subset = [subset]\n",
      "            elif not isinstance(subset, (list, tuple)):\n",
      "                raise TypeError(\"subset should be a list or tuple of column names\")\n",
      "\n",
      "            return DataFrame(self._jdf.na().fill(value, self._jseq(subset)), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: filter\n",
      "Method definition:     def filter(self, condition: \"ColumnOrName\") -> \"DataFrame\":\n",
      "        \"\"\"Filters rows using the given condition.\n",
      "\n",
      "        :func:`where` is an alias for :func:`filter`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        condition : :class:`Column` or str\n",
      "            a :class:`Column` of :class:`types.BooleanType`\n",
      "            or a string of SQL expression.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.filter(df.age > 3).collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        >>> df.where(df.age == 2).collect()\n",
      "        [Row(age=2, name='Alice')]\n",
      "\n",
      "        >>> df.filter(\"age > 3\").collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        >>> df.where(\"age = 2\").collect()\n",
      "        [Row(age=2, name='Alice')]\n",
      "        \"\"\"\n",
      "        if isinstance(condition, str):\n",
      "            jdf = self._jdf.filter(condition)\n",
      "        elif isinstance(condition, Column):\n",
      "            jdf = self._jdf.filter(condition._jc)\n",
      "        else:\n",
      "            raise TypeError(\"condition should be string or Column\")\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: first\n",
      "Method definition:     def first(self) -> Optional[Row]:\n",
      "        \"\"\"Returns the first row as a :class:`Row`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.first()\n",
      "        Row(age=2, name='Alice')\n",
      "        \"\"\"\n",
      "        return self.head()\n",
      "\n",
      "------------\n",
      "Method name: foreach\n",
      "Method definition:     def foreach(self, f: Callable[[Row], None]) -> None:\n",
      "        \"\"\"Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      "\n",
      "        This is a shorthand for ``df.rdd.foreach()``.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(person):\n",
      "        ...     print(person.name)\n",
      "        >>> df.foreach(f)\n",
      "        \"\"\"\n",
      "        self.rdd.foreach(f)\n",
      "\n",
      "------------\n",
      "Method name: foreachPartition\n",
      "Method definition:     def foreachPartition(self, f: Callable[[Iterator[Row]], None]) -> None:\n",
      "        \"\"\"Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      "\n",
      "        This a shorthand for ``df.rdd.foreachPartition()``.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(people):\n",
      "        ...     for person in people:\n",
      "        ...         print(person.name)\n",
      "        >>> df.foreachPartition(f)\n",
      "        \"\"\"\n",
      "        self.rdd.foreachPartition(f)  # type: ignore[arg-type]\n",
      "\n",
      "------------\n",
      "Method name: freqItems\n",
      "Method definition:     def freqItems(\n",
      "        self, cols: Union[List[str], Tuple[str]], support: Optional[float] = None\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Finding frequent items for columns, possibly with false positives. Using the\n",
      "        frequent element count algorithm described in\n",
      "        \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      "        :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : list or tuple\n",
      "            Names of the columns to calculate frequent items for as a list or tuple of\n",
      "            strings.\n",
      "        support : float, optional\n",
      "            The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      "            The support must be greater than 1e-4.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This function is meant for exploratory data analysis, as we make no\n",
      "        guarantee about the backward compatibility of the schema of the resulting\n",
      "        :class:`DataFrame`.\n",
      "        \"\"\"\n",
      "        if isinstance(cols, tuple):\n",
      "            cols = list(cols)\n",
      "        if not isinstance(cols, list):\n",
      "            raise TypeError(\"cols must be a list or tuple of column names as strings.\")\n",
      "        if not support:\n",
      "            support = 0.01\n",
      "        return DataFrame(\n",
      "            self._jdf.stat().freqItems(_to_seq(self._sc, cols), support), self.sparkSession\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: groupBy\n",
      "Method definition:     def groupBy(self, *cols: \"ColumnOrName\") -> \"GroupedData\":  # type: ignore[misc]\n",
      "        \"\"\"Groups the :class:`DataFrame` using the specified columns,\n",
      "        so we can run aggregation on them. See :class:`GroupedData`\n",
      "        for all the available aggregate functions.\n",
      "\n",
      "        :func:`groupby` is an alias for :func:`groupBy`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : list, str or :class:`Column`\n",
      "            columns to group by.\n",
      "            Each element should be a column name (string) or an expression (:class:`Column`).\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.groupBy().avg().collect()\n",
      "        [Row(avg(age)=3.5)]\n",
      "        >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      "        [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "        >>> sorted(df.groupBy(df.name).avg().collect())\n",
      "        [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "        >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      "        [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      "        \"\"\"\n",
      "        jgd = self._jdf.groupBy(self._jcols(*cols))\n",
      "        from pyspark.sql.group import GroupedData\n",
      "\n",
      "        return GroupedData(jgd, self)\n",
      "\n",
      "------------\n",
      "Method name: groupby\n",
      "Method definition:     def groupBy(self, *cols: \"ColumnOrName\") -> \"GroupedData\":  # type: ignore[misc]\n",
      "        \"\"\"Groups the :class:`DataFrame` using the specified columns,\n",
      "        so we can run aggregation on them. See :class:`GroupedData`\n",
      "        for all the available aggregate functions.\n",
      "\n",
      "        :func:`groupby` is an alias for :func:`groupBy`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : list, str or :class:`Column`\n",
      "            columns to group by.\n",
      "            Each element should be a column name (string) or an expression (:class:`Column`).\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.groupBy().avg().collect()\n",
      "        [Row(avg(age)=3.5)]\n",
      "        >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      "        [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "        >>> sorted(df.groupBy(df.name).avg().collect())\n",
      "        [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "        >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      "        [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      "        \"\"\"\n",
      "        jgd = self._jdf.groupBy(self._jcols(*cols))\n",
      "        from pyspark.sql.group import GroupedData\n",
      "\n",
      "        return GroupedData(jgd, self)\n",
      "\n",
      "------------\n",
      "Method name: head\n",
      "Method definition:     def head(self, n: Optional[int] = None) -> Union[Optional[Row], List[Row]]:\n",
      "        \"\"\"Returns the first ``n`` rows.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This method should only be used if the resulting array is expected\n",
      "        to be small, as all the data is loaded into the driver's memory.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int, optional\n",
      "            default 1. Number of rows to return.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        If n is greater than 1, return a list of :class:`Row`.\n",
      "        If n is 1, return a single Row.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.head()\n",
      "        Row(age=2, name='Alice')\n",
      "        >>> df.head(1)\n",
      "        [Row(age=2, name='Alice')]\n",
      "        \"\"\"\n",
      "        if n is None:\n",
      "            rs = self.head(1)\n",
      "            return rs[0] if rs else None\n",
      "        return self.take(n)\n",
      "\n",
      "------------\n",
      "Method name: hint\n",
      "Method definition:     def hint(\n",
      "        self, name: str, *parameters: Union[\"PrimitiveType\", List[\"PrimitiveType\"]]\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Specifies some hint on the current :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 2.2.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        name : str\n",
      "            A name of the hint.\n",
      "        parameters : str, list, float or int\n",
      "            Optional parameters.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`DataFrame`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n",
      "        +----+---+------+\n",
      "        |name|age|height|\n",
      "        +----+---+------+\n",
      "        | Bob|  5|    85|\n",
      "        +----+---+------+\n",
      "        \"\"\"\n",
      "        if len(parameters) == 1 and isinstance(parameters[0], list):\n",
      "            parameters = parameters[0]  # type: ignore[assignment]\n",
      "\n",
      "        if not isinstance(name, str):\n",
      "            raise TypeError(\"name should be provided as str, got {0}\".format(type(name)))\n",
      "\n",
      "        allowed_types = (str, list, float, int)\n",
      "        for p in parameters:\n",
      "            if not isinstance(p, allowed_types):\n",
      "                raise TypeError(\n",
      "                    \"all parameters should be in {0}, got {1} of type {2}\".format(\n",
      "                        allowed_types, p, type(p)\n",
      "                    )\n",
      "                )\n",
      "\n",
      "        jdf = self._jdf.hint(name, self._jseq(parameters))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: inputFiles\n",
      "Method definition:     def inputFiles(self) -> List[str]:\n",
      "        \"\"\"\n",
      "        Returns a best-effort snapshot of the files that compose this :class:`DataFrame`.\n",
      "        This method simply asks each constituent BaseRelation for its respective files and\n",
      "        takes the union of all results. Depending on the source relations, this may not find\n",
      "        all input files. Duplicates are removed.\n",
      "\n",
      "        .. versionadded:: 3.1.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
      "        >>> len(df.inputFiles())\n",
      "        1\n",
      "        \"\"\"\n",
      "        return list(self._jdf.inputFiles())\n",
      "\n",
      "------------\n",
      "Method name: intersect\n",
      "Method definition:     @since(1.3)\n",
      "    def intersect(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` containing rows only in\n",
      "        both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      "\n",
      "        This is equivalent to `INTERSECT` in SQL.\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.intersect(other._jdf), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: intersectAll\n",
      "Method definition:     def intersectAll(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
      "        and another :class:`DataFrame` while preserving duplicates.\n",
      "\n",
      "        This is equivalent to `INTERSECT ALL` in SQL. As standard in SQL, this function\n",
      "        resolves columns by position (not by name).\n",
      "\n",
      "        .. versionadded:: 2.4.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      "        >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      "\n",
      "        >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      "        +---+---+\n",
      "        | C1| C2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  1|\n",
      "        |  b|  3|\n",
      "        +---+---+\n",
      "\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.intersectAll(other._jdf), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: isEmpty\n",
      "Method definition:     def isEmpty(self) -> bool:\n",
      "        \"\"\"Returns ``True`` if this :class:`DataFrame` is empty.\n",
      "\n",
      "        .. versionadded:: 3.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df_empty = spark.createDataFrame([], 'a STRING')\n",
      "        >>> df_non_empty = spark.createDataFrame([(\"a\")], 'STRING')\n",
      "        >>> df_empty.isEmpty()\n",
      "        True\n",
      "        >>> df_non_empty.isEmpty()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self._jdf.isEmpty()\n",
      "\n",
      "------------\n",
      "Method name: isLocal\n",
      "Method definition:     @since(1.3)\n",
      "    def isLocal(self) -> bool:\n",
      "        \"\"\"Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      "        (without any Spark executors).\n",
      "        \"\"\"\n",
      "        return self._jdf.isLocal()\n",
      "\n",
      "------------\n",
      "Method name: join\n",
      "Method definition:     def join(\n",
      "        self,\n",
      "        other: \"DataFrame\",\n",
      "        on: Optional[Union[str, List[str], Column, List[Column]]] = None,\n",
      "        how: Optional[str] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Joins with another :class:`DataFrame`, using the given join expression.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : :class:`DataFrame`\n",
      "            Right side of the join\n",
      "        on : str, list or :class:`Column`, optional\n",
      "            a string for the join column name, a list of column names,\n",
      "            a join expression (Column), or a list of Columns.\n",
      "            If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "            the column(s) must exist on both sides, and this performs an equi-join.\n",
      "        how : str, optional\n",
      "            default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "            ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      "            ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      "            ``anti``, ``leftanti`` and ``left_anti``.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The following performs a full outer join between ``df1`` and ``df2``.\n",
      "\n",
      "        >>> from pyspark.sql.functions import desc\n",
      "        >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height) \\\n",
      "                .sort(desc(\"name\")).collect()\n",
      "        [Row(name='Bob', height=85), Row(name='Alice', height=None), Row(name=None, height=80)]\n",
      "\n",
      "        >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).collect()\n",
      "        [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      "\n",
      "        >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      "        >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      "        [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "\n",
      "        >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      "        [Row(name='Bob', height=85)]\n",
      "\n",
      "        >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      "        [Row(name='Bob', age=5)]\n",
      "        \"\"\"\n",
      "\n",
      "        if on is not None and not isinstance(on, list):\n",
      "            on = [on]  # type: ignore[assignment]\n",
      "\n",
      "        if on is not None:\n",
      "            if isinstance(on[0], str):\n",
      "                on = self._jseq(cast(List[str], on))\n",
      "            else:\n",
      "                assert isinstance(on[0], Column), \"on should be Column or list of Column\"\n",
      "                on = reduce(lambda x, y: x.__and__(y), cast(List[Column], on))\n",
      "                on = on._jc\n",
      "\n",
      "        if on is None and how is None:\n",
      "            jdf = self._jdf.join(other._jdf)\n",
      "        else:\n",
      "            if how is None:\n",
      "                how = \"inner\"\n",
      "            if on is None:\n",
      "                on = self._jseq([])\n",
      "            assert isinstance(how, str), \"how should be a string\"\n",
      "            jdf = self._jdf.join(other._jdf, on, how)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: limit\n",
      "Method definition:     def limit(self, num: int) -> \"DataFrame\":\n",
      "        \"\"\"Limits the result count to the number specified.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.limit(1).collect()\n",
      "        [Row(age=2, name='Alice')]\n",
      "        >>> df.limit(0).collect()\n",
      "        []\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.limit(num)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: localCheckpoint\n",
      "Method definition:     def localCheckpoint(self, eager: bool = True) -> \"DataFrame\":\n",
      "        \"\"\"Returns a locally checkpointed version of this :class:`DataFrame`. Checkpointing can be\n",
      "        used to truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      "        iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
      "        stored in the executors using the caching subsystem and therefore they are not reliable.\n",
      "\n",
      "        .. versionadded:: 2.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        eager : bool, optional\n",
      "            Whether to checkpoint this :class:`DataFrame` immediately\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This API is experimental.\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.localCheckpoint(eager)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: mapInArrow\n",
      "Method definition:     def mapInArrow(\n",
      "        self, func: \"ArrowMapIterFunction\", schema: Union[StructType, str]\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      "        function that takes and outputs a PyArrow's `RecordBatch`, and returns the result as a\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        The function should take an iterator of `pyarrow.RecordBatch`\\\\s and return\n",
      "        another iterator of `pyarrow.RecordBatch`\\\\s. All columns are passed\n",
      "        together as an iterator of `pyarrow.RecordBatch`\\\\s to the function and the\n",
      "        returned iterator of `pyarrow.RecordBatch`\\\\s are combined as a :class:`DataFrame`.\n",
      "        Each `pyarrow.RecordBatch` size can be controlled by\n",
      "        `spark.sql.execution.arrow.maxRecordsPerBatch`.\n",
      "\n",
      "        .. versionadded:: 3.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            a Python native function that takes an iterator of `pyarrow.RecordBatch`\\\\s, and\n",
      "            outputs an iterator of `pyarrow.RecordBatch`\\\\s.\n",
      "        schema : :class:`pyspark.sql.types.DataType` or str\n",
      "            the return type of the `func` in PySpark. The value can be either a\n",
      "            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyarrow  # doctest: +SKIP\n",
      "        >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      "        >>> def filter_func(iterator):\n",
      "        ...     for batch in iterator:\n",
      "        ...         pdf = batch.to_pandas()\n",
      "        ...         yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])\n",
      "        >>> df.mapInArrow(filter_func, df.schema).show()  # doctest: +SKIP\n",
      "        +---+---+\n",
      "        | id|age|\n",
      "        +---+---+\n",
      "        |  1| 21|\n",
      "        +---+---+\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This API is unstable, and for developers.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        pyspark.sql.functions.pandas_udf\n",
      "        pyspark.sql.DataFrame.mapInPandas\n",
      "        \"\"\"\n",
      "        from pyspark.sql import DataFrame\n",
      "        from pyspark.sql.pandas.functions import pandas_udf\n",
      "\n",
      "        assert isinstance(self, DataFrame)\n",
      "\n",
      "        # The usage of the pandas_udf is internal so type checking is disabled.\n",
      "        udf = pandas_udf(\n",
      "            func, returnType=schema, functionType=PythonEvalType.SQL_MAP_ARROW_ITER_UDF\n",
      "        )  # type: ignore[call-overload]\n",
      "        udf_column = udf(*[self[col] for col in self.columns])\n",
      "        jdf = self._jdf.pythonMapInArrow(udf_column._jc.expr())\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: mapInPandas\n",
      "Method definition:     def mapInPandas(\n",
      "        self, func: \"PandasMapIterFunction\", schema: Union[StructType, str]\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      "        function that takes and outputs a pandas DataFrame, and returns the result as a\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        The function should take an iterator of `pandas.DataFrame`\\\\s and return\n",
      "        another iterator of `pandas.DataFrame`\\\\s. All columns are passed\n",
      "        together as an iterator of `pandas.DataFrame`\\\\s to the function and the\n",
      "        returned iterator of `pandas.DataFrame`\\\\s are combined as a :class:`DataFrame`.\n",
      "        Each `pandas.DataFrame` size can be controlled by\n",
      "        `spark.sql.execution.arrow.maxRecordsPerBatch`.\n",
      "\n",
      "        .. versionadded:: 3.0.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            a Python native function that takes an iterator of `pandas.DataFrame`\\\\s, and\n",
      "            outputs an iterator of `pandas.DataFrame`\\\\s.\n",
      "        schema : :class:`pyspark.sql.types.DataType` or str\n",
      "            the return type of the `func` in PySpark. The value can be either a\n",
      "            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import pandas_udf\n",
      "        >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      "        >>> def filter_func(iterator):\n",
      "        ...     for pdf in iterator:\n",
      "        ...         yield pdf[pdf.id == 1]\n",
      "        >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
      "        +---+---+\n",
      "        | id|age|\n",
      "        +---+---+\n",
      "        |  1| 21|\n",
      "        +---+---+\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This API is experimental\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        pyspark.sql.functions.pandas_udf\n",
      "        \"\"\"\n",
      "        from pyspark.sql import DataFrame\n",
      "        from pyspark.sql.pandas.functions import pandas_udf\n",
      "\n",
      "        assert isinstance(self, DataFrame)\n",
      "\n",
      "        # The usage of the pandas_udf is internal so type checking is disabled.\n",
      "        udf = pandas_udf(\n",
      "            func, returnType=schema, functionType=PythonEvalType.SQL_MAP_PANDAS_ITER_UDF\n",
      "        )  # type: ignore[call-overload]\n",
      "        udf_column = udf(*[self[col] for col in self.columns])\n",
      "        jdf = self._jdf.mapInPandas(udf_column._jc.expr())\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: observe\n",
      "Method definition:     @since(3.3)\n",
      "    def observe(self, observation: \"Observation\", *exprs: Column) -> \"DataFrame\":\n",
      "        \"\"\"Observe (named) metrics through an :class:`Observation` instance.\n",
      "\n",
      "        A user can retrieve the metrics by accessing `Observation.get`.\n",
      "\n",
      "        .. versionadded:: 3.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        observation : :class:`Observation`\n",
      "            an :class:`Observation` instance to obtain the metric.\n",
      "        exprs : list of :class:`Column`\n",
      "            column expressions (:class:`Column`).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`DataFrame`\n",
      "            the observed :class:`DataFrame`.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This method does not support streaming datasets.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import col, count, lit, max\n",
      "        >>> from pyspark.sql import Observation\n",
      "        >>> observation = Observation(\"my metrics\")\n",
      "        >>> observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n",
      "        >>> observed_df.count()\n",
      "        2\n",
      "        >>> observation.get\n",
      "        {'count': 2, 'max(age)': 5}\n",
      "        \"\"\"\n",
      "        from pyspark.sql import Observation\n",
      "\n",
      "        assert isinstance(observation, Observation), \"observation should be Observation\"\n",
      "        return observation._on(self, *exprs)\n",
      "\n",
      "------------\n",
      "Method name: orderBy\n",
      "Method definition:     def sort(\n",
      "        self, *cols: Union[str, Column, List[Union[str, Column]]], **kwargs: Any\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : str, list, or :class:`Column`, optional\n",
      "             list of :class:`Column` or column names to sort by.\n",
      "\n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        ascending : bool or list, optional\n",
      "            boolean or list of boolean (default ``True``).\n",
      "            Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "            If a list is specified, length of the list must equal length of the `cols`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.sort(df.age.desc()).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> df.sort(\"age\", ascending=False).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> df.orderBy(df.age.desc()).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> from pyspark.sql.functions import *\n",
      "        >>> df.sort(asc(\"age\")).collect()\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.sort(self._sort_cols(cols, kwargs))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: pandas_api\n",
      "Method definition:     def pandas_api(\n",
      "        self, index_col: Optional[Union[str, List[str]]] = None\n",
      "    ) -> \"PandasOnSparkDataFrame\":\n",
      "        \"\"\"\n",
      "        Converts the existing DataFrame into a pandas-on-Spark DataFrame.\n",
      "\n",
      "        If a pandas-on-Spark DataFrame is converted to a Spark DataFrame and then back\n",
      "        to pandas-on-Spark, it will lose the index information and the original index\n",
      "        will be turned into a normal column.\n",
      "\n",
      "        This is only available if Pandas is installed and available.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        index_col: str or list of str, optional, default: None\n",
      "            Index column of table in Spark.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        pyspark.pandas.frame.DataFrame.to_spark\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.show()  # doctest: +SKIP\n",
      "        +----+----+\n",
      "        |Col1|Col2|\n",
      "        +----+----+\n",
      "        |   a|   1|\n",
      "        |   b|   2|\n",
      "        |   c|   3|\n",
      "        +----+----+\n",
      "\n",
      "        >>> df.pandas_api()  # doctest: +SKIP\n",
      "          Col1  Col2\n",
      "        0    a     1\n",
      "        1    b     2\n",
      "        2    c     3\n",
      "\n",
      "        We can specify the index columns.\n",
      "\n",
      "        >>> df.pandas_api(index_col=\"Col1\"): # doctest: +SKIP\n",
      "              Col2\n",
      "        Col1\n",
      "        a        1\n",
      "        b        2\n",
      "        c        3\n",
      "        \"\"\"\n",
      "        from pyspark.pandas.namespace import _get_index_map\n",
      "        from pyspark.pandas.frame import DataFrame as PandasOnSparkDataFrame\n",
      "        from pyspark.pandas.internal import InternalFrame\n",
      "\n",
      "        index_spark_columns, index_names = _get_index_map(self, index_col)\n",
      "        internal = InternalFrame(\n",
      "            spark_frame=self,\n",
      "            index_spark_columns=index_spark_columns,\n",
      "            index_names=index_names,  # type: ignore[arg-type]\n",
      "        )\n",
      "        return PandasOnSparkDataFrame(internal)\n",
      "\n",
      "------------\n",
      "Method name: persist\n",
      "Method definition:     def persist(\n",
      "        self,\n",
      "        storageLevel: StorageLevel = (StorageLevel.MEMORY_AND_DISK_DESER),\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      "        operations after the first time it is computed. This can only be used to assign\n",
      "        a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      "        If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
      "        \"\"\"\n",
      "        self.is_cached = True\n",
      "        javaStorageLevel = self._sc._getJavaStorageLevel(storageLevel)\n",
      "        self._jdf.persist(javaStorageLevel)\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: printSchema\n",
      "Method definition:     def printSchema(self) -> None:\n",
      "        \"\"\"Prints out the schema in the tree format.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- age: integer (nullable = true)\n",
      "         |-- name: string (nullable = true)\n",
      "        <BLANKLINE>\n",
      "        \"\"\"\n",
      "        print(self._jdf.schema().treeString())\n",
      "\n",
      "------------\n",
      "Method name: randomSplit\n",
      "Method definition:     def randomSplit(self, weights: List[float], seed: Optional[int] = None) -> List[\"DataFrame\"]:\n",
      "        \"\"\"Randomly splits this :class:`DataFrame` with the provided weights.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        weights : list\n",
      "            list of doubles as weights with which to split the :class:`DataFrame`.\n",
      "            Weights will be normalized if they don't sum up to 1.0.\n",
      "        seed : int, optional\n",
      "            The seed for sampling.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> splits = df4.randomSplit([1.0, 2.0], 24)\n",
      "        >>> splits[0].count()\n",
      "        2\n",
      "\n",
      "        >>> splits[1].count()\n",
      "        2\n",
      "        \"\"\"\n",
      "        for w in weights:\n",
      "            if w < 0.0:\n",
      "                raise ValueError(\"Weights must be positive. Found weight value: %s\" % w)\n",
      "        seed = seed if seed is not None else random.randint(0, sys.maxsize)\n",
      "        df_array = self._jdf.randomSplit(\n",
      "            _to_list(self.sparkSession._sc, cast(List[\"ColumnOrName\"], weights)), int(seed)\n",
      "        )\n",
      "        return [DataFrame(df, self.sparkSession) for df in df_array]\n",
      "\n",
      "------------\n",
      "Method name: registerTempTable\n",
      "Method definition:     def registerTempTable(self, name: str) -> None:\n",
      "        \"\"\"Registers this :class:`DataFrame` as a temporary table using the given name.\n",
      "\n",
      "        The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      "        that was used to create this :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "            Use :meth:`DataFrame.createOrReplaceTempView` instead.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.registerTempTable(\"people\")\n",
      "        >>> df2 = spark.sql(\"select * from people\")\n",
      "        >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "        True\n",
      "        >>> spark.catalog.dropTempView(\"people\")\n",
      "        True\n",
      "\n",
      "        \"\"\"\n",
      "        warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n",
      "        self._jdf.createOrReplaceTempView(name)\n",
      "\n",
      "------------\n",
      "Method name: repartition\n",
      "Method definition:     def repartition(  # type: ignore[misc]\n",
      "        self, numPartitions: Union[int, \"ColumnOrName\"], *cols: \"ColumnOrName\"\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      "        resulting :class:`DataFrame` is hash partitioned.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        numPartitions : int\n",
      "            can be an int to specify the target number of partitions or a Column.\n",
      "            If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      "            the default number of partitions is used.\n",
      "        cols : str or :class:`Column`\n",
      "            partitioning columns.\n",
      "\n",
      "            .. versionchanged:: 1.6\n",
      "               Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      "               optional if partitioning columns are specified.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.repartition(10).rdd.getNumPartitions()\n",
      "        10\n",
      "        >>> data = df.union(df).repartition(\"age\")\n",
      "        >>> data.show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        +---+-----+\n",
      "        >>> data = data.repartition(7, \"age\")\n",
      "        >>> data.show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        +---+-----+\n",
      "        >>> data.rdd.getNumPartitions()\n",
      "        7\n",
      "        >>> data = data.repartition(3, \"name\", \"age\")\n",
      "        >>> data.show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  5|  Bob|\n",
      "        |  5|  Bob|\n",
      "        |  2|Alice|\n",
      "        |  2|Alice|\n",
      "        +---+-----+\n",
      "        \"\"\"\n",
      "        if isinstance(numPartitions, int):\n",
      "            if len(cols) == 0:\n",
      "                return DataFrame(self._jdf.repartition(numPartitions), self.sparkSession)\n",
      "            else:\n",
      "                return DataFrame(\n",
      "                    self._jdf.repartition(numPartitions, self._jcols(*cols)),\n",
      "                    self.sparkSession,\n",
      "                )\n",
      "        elif isinstance(numPartitions, (str, Column)):\n",
      "            cols = (numPartitions,) + cols\n",
      "            return DataFrame(self._jdf.repartition(self._jcols(*cols)), self.sparkSession)\n",
      "        else:\n",
      "            raise TypeError(\"numPartitions should be an int or Column\")\n",
      "\n",
      "------------\n",
      "Method name: repartitionByRange\n",
      "Method definition:     def repartitionByRange(  # type: ignore[misc]\n",
      "        self, numPartitions: Union[int, \"ColumnOrName\"], *cols: \"ColumnOrName\"\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      "        resulting :class:`DataFrame` is range partitioned.\n",
      "\n",
      "        At least one partition-by expression must be specified.\n",
      "        When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      "\n",
      "        .. versionadded:: 2.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        numPartitions : int\n",
      "            can be an int to specify the target number of partitions or a Column.\n",
      "            If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      "            the default number of partitions is used.\n",
      "        cols : str or :class:`Column`\n",
      "            partitioning columns.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Due to performance reasons this method uses sampling to estimate the ranges.\n",
      "        Hence, the output may not be consistent, since sampling can return different values.\n",
      "        The sample size can be controlled by the config\n",
      "        `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      "        2\n",
      "        >>> df.show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        +---+-----+\n",
      "        >>> df.repartitionByRange(1, \"age\").rdd.getNumPartitions()\n",
      "        1\n",
      "        >>> data = df.repartitionByRange(\"age\")\n",
      "        >>> df.show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        +---+-----+\n",
      "        \"\"\"\n",
      "        if isinstance(numPartitions, int):\n",
      "            if len(cols) == 0:\n",
      "                raise ValueError(\"At least one partition-by expression must be specified.\")\n",
      "            else:\n",
      "                return DataFrame(\n",
      "                    self._jdf.repartitionByRange(numPartitions, self._jcols(*cols)),\n",
      "                    self.sparkSession,\n",
      "                )\n",
      "        elif isinstance(numPartitions, (str, Column)):\n",
      "            cols = (numPartitions,) + cols\n",
      "            return DataFrame(self._jdf.repartitionByRange(self._jcols(*cols)), self.sparkSession)\n",
      "        else:\n",
      "            raise TypeError(\"numPartitions should be an int, string or Column\")\n",
      "\n",
      "------------\n",
      "Method name: replace\n",
      "Method definition:     def replace(  # type: ignore[misc]\n",
      "        self,\n",
      "        to_replace: Union[\n",
      "            \"LiteralType\", List[\"LiteralType\"], Dict[\"LiteralType\", \"OptionalPrimitiveType\"]\n",
      "        ],\n",
      "        value: Optional[\n",
      "            Union[\"OptionalPrimitiveType\", List[\"OptionalPrimitiveType\"], _NoValueType]\n",
      "        ] = _NoValue,\n",
      "        subset: Optional[List[str]] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` replacing a value with another value.\n",
      "        :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      "        aliases of each other.\n",
      "        Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      "        or strings. Value can have None. When replacing, the new value will be cast\n",
      "        to the type of the existing column.\n",
      "        For numeric replacements all values to be replaced should have unique\n",
      "        floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      "        and arbitrary replacement will be used.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        to_replace : bool, int, float, string, list or dict\n",
      "            Value to be replaced.\n",
      "            If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      "            must be a mapping between a value and a replacement.\n",
      "        value : bool, int, float, string or None, optional\n",
      "            The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      "            list, `value` should be of the same length and type as `to_replace`.\n",
      "            If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      "            used as a replacement for each item in `to_replace`.\n",
      "        subset : list, optional\n",
      "            optional list of column names to consider.\n",
      "            Columns specified in subset that do not have matching data type are ignored.\n",
      "            For example, if `value` is a string, and subset contains a non-string column,\n",
      "            then the non-string column is simply ignored.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df4.na.replace(10, 20).show()\n",
      "        +----+------+-----+\n",
      "        | age|height| name|\n",
      "        +----+------+-----+\n",
      "        |  20|    80|Alice|\n",
      "        |   5|  null|  Bob|\n",
      "        |null|  null|  Tom|\n",
      "        |null|  null| null|\n",
      "        +----+------+-----+\n",
      "\n",
      "        >>> df4.na.replace('Alice', None).show()\n",
      "        +----+------+----+\n",
      "        | age|height|name|\n",
      "        +----+------+----+\n",
      "        |  10|    80|null|\n",
      "        |   5|  null| Bob|\n",
      "        |null|  null| Tom|\n",
      "        |null|  null|null|\n",
      "        +----+------+----+\n",
      "\n",
      "        >>> df4.na.replace({'Alice': None}).show()\n",
      "        +----+------+----+\n",
      "        | age|height|name|\n",
      "        +----+------+----+\n",
      "        |  10|    80|null|\n",
      "        |   5|  null| Bob|\n",
      "        |null|  null| Tom|\n",
      "        |null|  null|null|\n",
      "        +----+------+----+\n",
      "\n",
      "        >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      "        +----+------+----+\n",
      "        | age|height|name|\n",
      "        +----+------+----+\n",
      "        |  10|    80|   A|\n",
      "        |   5|  null|   B|\n",
      "        |null|  null| Tom|\n",
      "        |null|  null|null|\n",
      "        +----+------+----+\n",
      "        \"\"\"\n",
      "        if value is _NoValue:\n",
      "            if isinstance(to_replace, dict):\n",
      "                value = None\n",
      "            else:\n",
      "                raise TypeError(\"value argument is required when to_replace is not a dictionary.\")\n",
      "\n",
      "        # Helper functions\n",
      "        def all_of(types: Union[Type, Tuple[Type, ...]]) -> Callable[[Iterable], bool]:\n",
      "            \"\"\"Given a type or tuple of types and a sequence of xs\n",
      "            check if each x is instance of type(s)\n",
      "\n",
      "            >>> all_of(bool)([True, False])\n",
      "            True\n",
      "            >>> all_of(str)([\"a\", 1])\n",
      "            False\n",
      "            \"\"\"\n",
      "\n",
      "            def all_of_(xs: Iterable) -> bool:\n",
      "                return all(isinstance(x, types) for x in xs)\n",
      "\n",
      "            return all_of_\n",
      "\n",
      "        all_of_bool = all_of(bool)\n",
      "        all_of_str = all_of(str)\n",
      "        all_of_numeric = all_of((float, int))\n",
      "\n",
      "        # Validate input types\n",
      "        valid_types = (bool, float, int, str, list, tuple)\n",
      "        if not isinstance(to_replace, valid_types + (dict,)):\n",
      "            raise TypeError(\n",
      "                \"to_replace should be a bool, float, int, string, list, tuple, or dict. \"\n",
      "                \"Got {0}\".format(type(to_replace))\n",
      "            )\n",
      "\n",
      "        if (\n",
      "            not isinstance(value, valid_types)\n",
      "            and value is not None\n",
      "            and not isinstance(to_replace, dict)\n",
      "        ):\n",
      "            raise TypeError(\n",
      "                \"If to_replace is not a dict, value should be \"\n",
      "                \"a bool, float, int, string, list, tuple or None. \"\n",
      "                \"Got {0}\".format(type(value))\n",
      "            )\n",
      "\n",
      "        if isinstance(to_replace, (list, tuple)) and isinstance(value, (list, tuple)):\n",
      "            if len(to_replace) != len(value):\n",
      "                raise ValueError(\n",
      "                    \"to_replace and value lists should be of the same length. \"\n",
      "                    \"Got {0} and {1}\".format(len(to_replace), len(value))\n",
      "                )\n",
      "\n",
      "        if not (subset is None or isinstance(subset, (list, tuple, str))):\n",
      "            raise TypeError(\n",
      "                \"subset should be a list or tuple of column names, \"\n",
      "                \"column name or None. Got {0}\".format(type(subset))\n",
      "            )\n",
      "\n",
      "        # Reshape input arguments if necessary\n",
      "        if isinstance(to_replace, (float, int, str)):\n",
      "            to_replace = [to_replace]\n",
      "\n",
      "        if isinstance(to_replace, dict):\n",
      "            rep_dict = to_replace\n",
      "            if value is not None:\n",
      "                warnings.warn(\"to_replace is a dict and value is not None. value will be ignored.\")\n",
      "        else:\n",
      "            if isinstance(value, (float, int, str)) or value is None:\n",
      "                value = [value for _ in range(len(to_replace))]\n",
      "            rep_dict = dict(zip(to_replace, cast(\"Iterable[Optional[Union[float, str]]]\", value)))\n",
      "\n",
      "        if isinstance(subset, str):\n",
      "            subset = [subset]\n",
      "\n",
      "        # Verify we were not passed in mixed type generics.\n",
      "        if not any(\n",
      "            all_of_type(rep_dict.keys())\n",
      "            and all_of_type(x for x in rep_dict.values() if x is not None)\n",
      "            for all_of_type in [all_of_bool, all_of_str, all_of_numeric]\n",
      "        ):\n",
      "            raise ValueError(\"Mixed type replacements are not supported\")\n",
      "\n",
      "        if subset is None:\n",
      "            return DataFrame(self._jdf.na().replace(\"*\", rep_dict), self.sparkSession)\n",
      "        else:\n",
      "            return DataFrame(\n",
      "                self._jdf.na().replace(self._jseq(subset), self._jmap(rep_dict)),\n",
      "                self.sparkSession,\n",
      "            )\n",
      "\n",
      "------------\n",
      "Method name: rollup\n",
      "Method definition:     def rollup(self, *cols: \"ColumnOrName\") -> \"GroupedData\":  # type: ignore[misc]\n",
      "        \"\"\"\n",
      "        Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      "        the specified columns, so we can run aggregation on them.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      "        +-----+----+-----+\n",
      "        | name| age|count|\n",
      "        +-----+----+-----+\n",
      "        | null|null|    2|\n",
      "        |Alice|null|    1|\n",
      "        |Alice|   2|    1|\n",
      "        |  Bob|null|    1|\n",
      "        |  Bob|   5|    1|\n",
      "        +-----+----+-----+\n",
      "        \"\"\"\n",
      "        jgd = self._jdf.rollup(self._jcols(*cols))\n",
      "        from pyspark.sql.group import GroupedData\n",
      "\n",
      "        return GroupedData(jgd, self)\n",
      "\n",
      "------------\n",
      "Method name: sameSemantics\n",
      "Method definition:     def sameSemantics(self, other: \"DataFrame\") -> bool:\n",
      "        \"\"\"\n",
      "        Returns `True` when the logical query plans inside both :class:`DataFrame`\\\\s are equal and\n",
      "        therefore return same results.\n",
      "\n",
      "        .. versionadded:: 3.1.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The equality comparison here is simplified by tolerating the cosmetic differences\n",
      "        such as attribute names.\n",
      "\n",
      "        This API can compare both :class:`DataFrame`\\\\s very fast but can still return\n",
      "        `False` on the :class:`DataFrame` that return the same results, for instance, from\n",
      "        different plans. Such false negative semantic can be useful when caching as an example.\n",
      "\n",
      "        This API is a developer API.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.range(10)\n",
      "        >>> df2 = spark.range(10)\n",
      "        >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\n",
      "        True\n",
      "        >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\n",
      "        False\n",
      "        >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n",
      "        True\n",
      "        \"\"\"\n",
      "        if not isinstance(other, DataFrame):\n",
      "            raise TypeError(\"other parameter should be of DataFrame; however, got %s\" % type(other))\n",
      "        return self._jdf.sameSemantics(other._jdf)\n",
      "\n",
      "------------\n",
      "Method name: sample\n",
      "Method definition:     def sample(  # type: ignore[misc]\n",
      "        self,\n",
      "        withReplacement: Optional[Union[float, bool]] = None,\n",
      "        fraction: Optional[Union[int, float]] = None,\n",
      "        seed: Optional[int] = None,\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Returns a sampled subset of this :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        withReplacement : bool, optional\n",
      "            Sample with replacement or not (default ``False``).\n",
      "        fraction : float, optional\n",
      "            Fraction of rows to generate, range [0.0, 1.0].\n",
      "        seed : int, optional\n",
      "            Seed for sampling (default a random seed).\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This is not guaranteed to provide exactly the fraction specified of the total\n",
      "        count of the given :class:`DataFrame`.\n",
      "\n",
      "        `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.sample(0.5, 3).count()\n",
      "        7\n",
      "        >>> df.sample(fraction=0.5, seed=3).count()\n",
      "        7\n",
      "        >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      "        1\n",
      "        >>> df.sample(1.0).count()\n",
      "        10\n",
      "        >>> df.sample(fraction=1.0).count()\n",
      "        10\n",
      "        >>> df.sample(False, fraction=1.0).count()\n",
      "        10\n",
      "        \"\"\"\n",
      "\n",
      "        # For the cases below:\n",
      "        #   sample(True, 0.5 [, seed])\n",
      "        #   sample(True, fraction=0.5 [, seed])\n",
      "        #   sample(withReplacement=False, fraction=0.5 [, seed])\n",
      "        is_withReplacement_set = type(withReplacement) == bool and isinstance(fraction, float)\n",
      "\n",
      "        # For the case below:\n",
      "        #   sample(faction=0.5 [, seed])\n",
      "        is_withReplacement_omitted_kwargs = withReplacement is None and isinstance(fraction, float)\n",
      "\n",
      "        # For the case below:\n",
      "        #   sample(0.5 [, seed])\n",
      "        is_withReplacement_omitted_args = isinstance(withReplacement, float)\n",
      "\n",
      "        if not (\n",
      "            is_withReplacement_set\n",
      "            or is_withReplacement_omitted_kwargs\n",
      "            or is_withReplacement_omitted_args\n",
      "        ):\n",
      "            argtypes = [\n",
      "                str(type(arg)) for arg in [withReplacement, fraction, seed] if arg is not None\n",
      "            ]\n",
      "            raise TypeError(\n",
      "                \"withReplacement (optional), fraction (required) and seed (optional)\"\n",
      "                \" should be a bool, float and number; however, \"\n",
      "                \"got [%s].\" % \", \".join(argtypes)\n",
      "            )\n",
      "\n",
      "        if is_withReplacement_omitted_args:\n",
      "            if fraction is not None:\n",
      "                seed = cast(int, fraction)\n",
      "            fraction = withReplacement\n",
      "            withReplacement = None\n",
      "\n",
      "        seed = int(seed) if seed is not None else None\n",
      "        args = [arg for arg in [withReplacement, fraction, seed] if arg is not None]\n",
      "        jdf = self._jdf.sample(*args)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: sampleBy\n",
      "Method definition:     def sampleBy(\n",
      "        self, col: \"ColumnOrName\", fractions: Dict[Any, float], seed: Optional[int] = None\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Returns a stratified sample without replacement based on the\n",
      "        fraction given on each stratum.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`Column` or str\n",
      "            column that defines strata\n",
      "\n",
      "            .. versionchanged:: 3.0\n",
      "               Added sampling by a column of :class:`Column`\n",
      "        fractions : dict\n",
      "            sampling fraction for each stratum. If a stratum is not\n",
      "            specified, we treat its fraction as zero.\n",
      "        seed : int, optional\n",
      "            random seed\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        a new :class:`DataFrame` that represents the stratified sample\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import col\n",
      "        >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      "        >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      "        >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      "        +---+-----+\n",
      "        |key|count|\n",
      "        +---+-----+\n",
      "        |  0|    3|\n",
      "        |  1|    6|\n",
      "        +---+-----+\n",
      "        >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      "        33\n",
      "        \"\"\"\n",
      "        if isinstance(col, str):\n",
      "            col = Column(col)\n",
      "        elif not isinstance(col, Column):\n",
      "            raise TypeError(\"col must be a string or a column, but got %r\" % type(col))\n",
      "        if not isinstance(fractions, dict):\n",
      "            raise TypeError(\"fractions must be a dict but got %r\" % type(fractions))\n",
      "        for k, v in fractions.items():\n",
      "            if not isinstance(k, (float, int, str)):\n",
      "                raise TypeError(\"key must be float, int, or string, but got %r\" % type(k))\n",
      "            fractions[k] = float(v)\n",
      "        col = col._jc\n",
      "        seed = seed if seed is not None else random.randint(0, sys.maxsize)\n",
      "        return DataFrame(\n",
      "            self._jdf.stat().sampleBy(col, self._jmap(fractions), seed), self.sparkSession\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: select\n",
      "Method definition:     def select(self, *cols: \"ColumnOrName\") -> \"DataFrame\":  # type: ignore[misc]\n",
      "        \"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : str, :class:`Column`, or list\n",
      "            column names (string) or expressions (:class:`Column`).\n",
      "            If one of the column names is '*', that column is expanded to include all columns\n",
      "            in the current :class:`DataFrame`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.select('*').collect()\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        >>> df.select('name', 'age').collect()\n",
      "        [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "        >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n",
      "        [Row(name='Alice', age=12), Row(name='Bob', age=15)]\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.select(self._jcols(*cols))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: selectExpr\n",
      "Method definition:     def selectExpr(self, *expr: Union[str, List[str]]) -> \"DataFrame\":\n",
      "        \"\"\"Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      "\n",
      "        This is a variant of :func:`select` that accepts SQL expressions.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      "        [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      "        \"\"\"\n",
      "        if len(expr) == 1 and isinstance(expr[0], list):\n",
      "            expr = expr[0]  # type: ignore[assignment]\n",
      "        jdf = self._jdf.selectExpr(self._jseq(expr))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: semanticHash\n",
      "Method definition:     def semanticHash(self) -> int:\n",
      "        \"\"\"\n",
      "        Returns a hash code of the logical query plan against this :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 3.1.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Unlike the standard hash code, the hash is calculated against the query plan\n",
      "        simplified by tolerating the cosmetic differences such as attribute names.\n",
      "\n",
      "        This API is a developer API.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  # doctest: +SKIP\n",
      "        1855039936\n",
      "        >>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  # doctest: +SKIP\n",
      "        1855039936\n",
      "        \"\"\"\n",
      "        return self._jdf.semanticHash()\n",
      "\n",
      "------------\n",
      "Method name: show\n",
      "Method definition:     def show(self, n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -> None:\n",
      "        \"\"\"Prints the first ``n`` rows to the console.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int, optional\n",
      "            Number of rows to show.\n",
      "        truncate : bool or int, optional\n",
      "            If set to ``True``, truncate strings longer than 20 chars by default.\n",
      "            If set to a number greater than one, truncates long strings to length ``truncate``\n",
      "            and align cells right.\n",
      "        vertical : bool, optional\n",
      "            If set to ``True``, print output rows vertically (one line\n",
      "            per column value).\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df\n",
      "        DataFrame[age: int, name: string]\n",
      "        >>> df.show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        +---+-----+\n",
      "        >>> df.show(truncate=3)\n",
      "        +---+----+\n",
      "        |age|name|\n",
      "        +---+----+\n",
      "        |  2| Ali|\n",
      "        |  5| Bob|\n",
      "        +---+----+\n",
      "        >>> df.show(vertical=True)\n",
      "        -RECORD 0-----\n",
      "         age  | 2\n",
      "         name | Alice\n",
      "        -RECORD 1-----\n",
      "         age  | 5\n",
      "         name | Bob\n",
      "        \"\"\"\n",
      "\n",
      "        if not isinstance(n, int) or isinstance(n, bool):\n",
      "            raise TypeError(\"Parameter 'n' (number of rows) must be an int\")\n",
      "\n",
      "        if not isinstance(vertical, bool):\n",
      "            raise TypeError(\"Parameter 'vertical' must be a bool\")\n",
      "\n",
      "        if isinstance(truncate, bool) and truncate:\n",
      "            print(self._jdf.showString(n, 20, vertical))\n",
      "        else:\n",
      "            try:\n",
      "                int_truncate = int(truncate)\n",
      "            except ValueError:\n",
      "                raise TypeError(\n",
      "                    \"Parameter 'truncate={}' should be either bool or int.\".format(truncate)\n",
      "                )\n",
      "\n",
      "            print(self._jdf.showString(n, int_truncate, vertical))\n",
      "\n",
      "------------\n",
      "Method name: sort\n",
      "Method definition:     def sort(\n",
      "        self, *cols: Union[str, Column, List[Union[str, Column]]], **kwargs: Any\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : str, list, or :class:`Column`, optional\n",
      "             list of :class:`Column` or column names to sort by.\n",
      "\n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        ascending : bool or list, optional\n",
      "            boolean or list of boolean (default ``True``).\n",
      "            Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "            If a list is specified, length of the list must equal length of the `cols`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.sort(df.age.desc()).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> df.sort(\"age\", ascending=False).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> df.orderBy(df.age.desc()).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> from pyspark.sql.functions import *\n",
      "        >>> df.sort(asc(\"age\")).collect()\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      "        [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.sort(self._sort_cols(cols, kwargs))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: sortWithinPartitions\n",
      "Method definition:     def sortWithinPartitions(\n",
      "        self, *cols: Union[str, Column, List[Union[str, Column]]], **kwargs: Any\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : str, list or :class:`Column`, optional\n",
      "            list of :class:`Column` or column names to sort by.\n",
      "\n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        ascending : bool or list, optional\n",
      "            boolean or list of boolean (default ``True``).\n",
      "            Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      "            If a list is specified, length of the list must equal length of the `cols`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  5|  Bob|\n",
      "        +---+-----+\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.sortWithinPartitions(self._sort_cols(cols, kwargs))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: subtract\n",
      "Method definition:     @since(1.3)\n",
      "    def subtract(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      "        but not in another :class:`DataFrame`.\n",
      "\n",
      "        This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      "\n",
      "        \"\"\"\n",
      "        return DataFrame(getattr(self._jdf, \"except\")(other._jdf), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: summary\n",
      "Method definition:     def summary(self, *statistics: str) -> \"DataFrame\":\n",
      "        \"\"\"Computes specified statistics for numeric and string columns. Available statistics are:\n",
      "        - count\n",
      "        - mean\n",
      "        - stddev\n",
      "        - min\n",
      "        - max\n",
      "        - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
      "\n",
      "        If no statistics are given, this function computes count, mean, stddev, min,\n",
      "        approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      "\n",
      "        .. versionadded:: 2.3.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This function is meant for exploratory data analysis, as we make no\n",
      "        guarantee about the backward compatibility of the schema of the resulting\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      "        ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      "        ... )\n",
      "        >>> df.select(\"age\", \"weight\", \"height\").summary().show()\n",
      "        +-------+----+------------------+-----------------+\n",
      "        |summary| age|            weight|           height|\n",
      "        +-------+----+------------------+-----------------+\n",
      "        |  count|   3|                 3|                3|\n",
      "        |   mean|12.0| 40.73333333333333|            145.0|\n",
      "        | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      "        |    min|  11|              37.8|            142.2|\n",
      "        |    25%|  11|              37.8|            142.2|\n",
      "        |    50%|  12|              40.3|            142.3|\n",
      "        |    75%|  13|              44.1|            150.5|\n",
      "        |    max|  13|              44.1|            150.5|\n",
      "        +-------+----+------------------+-----------------+\n",
      "\n",
      "        >>> df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      "        +-------+---+------+------+\n",
      "        |summary|age|weight|height|\n",
      "        +-------+---+------+------+\n",
      "        |  count|  3|     3|     3|\n",
      "        |    min| 11|  37.8| 142.2|\n",
      "        |    25%| 11|  37.8| 142.2|\n",
      "        |    75%| 13|  44.1| 150.5|\n",
      "        |    max| 13|  44.1| 150.5|\n",
      "        +-------+---+------+------+\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        DataFrame.display\n",
      "        \"\"\"\n",
      "        if len(statistics) == 1 and isinstance(statistics[0], list):\n",
      "            statistics = statistics[0]\n",
      "        jdf = self._jdf.summary(self._jseq(statistics))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: tail\n",
      "Method definition:     def tail(self, num: int) -> List[Row]:\n",
      "        \"\"\"\n",
      "        Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "\n",
      "        Running tail requires moving data into the application's driver process, and doing so with\n",
      "        a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      "\n",
      "        .. versionadded:: 3.0.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.tail(1)\n",
      "        [Row(age=5, name='Bob')]\n",
      "        \"\"\"\n",
      "        with SCCallSiteSync(self._sc):\n",
      "            sock_info = self._jdf.tailToPython(num)\n",
      "        return list(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\n",
      "------------\n",
      "Method name: take\n",
      "Method definition:     def take(self, num: int) -> List[Row]:\n",
      "        \"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.take(2)\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        \"\"\"\n",
      "        return self.limit(num).collect()\n",
      "\n",
      "------------\n",
      "Method name: toDF\n",
      "Method definition:     def toDF(self, *cols: \"ColumnOrName\") -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` that with new specified column names\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : str\n",
      "            new column names\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.toDF('f1', 'f2').collect()\n",
      "        [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n",
      "        \"\"\"\n",
      "        jdf = self._jdf.toDF(self._jseq(cols))\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: toJSON\n",
      "Method definition:     def toJSON(self, use_unicode: bool = True) -> RDD[str]:\n",
      "        \"\"\"Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      "\n",
      "        Each row is turned into a JSON document as one element in the returned RDD.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.toJSON().first()\n",
      "        '{\"age\":2,\"name\":\"Alice\"}'\n",
      "        \"\"\"\n",
      "        rdd = self._jdf.toJSON()\n",
      "        return RDD(rdd.toJavaRDD(), self._sc, UTF8Deserializer(use_unicode))\n",
      "\n",
      "------------\n",
      "Method name: toLocalIterator\n",
      "Method definition:     def toLocalIterator(self, prefetchPartitions: bool = False) -> Iterator[Row]:\n",
      "        \"\"\"\n",
      "        Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      "        The iterator will consume as much memory as the largest partition in this\n",
      "        :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
      "        partitions.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        prefetchPartitions : bool, optional\n",
      "            If Spark should pre-fetch the next partition  before it is needed.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> list(df.toLocalIterator())\n",
      "        [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "        \"\"\"\n",
      "        with SCCallSiteSync(self._sc):\n",
      "            sock_info = self._jdf.toPythonIterator(prefetchPartitions)\n",
      "        return _local_iterator_from_socket(sock_info, BatchedSerializer(CPickleSerializer()))\n",
      "\n",
      "------------\n",
      "Method name: toPandas\n",
      "Method definition:     def toPandas(self) -> \"PandasDataFrameLike\":\n",
      "        \"\"\"\n",
      "        Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      "\n",
      "        This is only available if Pandas is installed and available.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This method should only be used if the resulting Pandas ``pandas.DataFrame`` is\n",
      "        expected to be small, as all the data is loaded into the driver's memory.\n",
      "\n",
      "        Usage with ``spark.sql.execution.arrow.pyspark.enabled=True`` is experimental.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.toPandas()  # doctest: +SKIP\n",
      "           age   name\n",
      "        0    2  Alice\n",
      "        1    5    Bob\n",
      "        \"\"\"\n",
      "        from pyspark.sql.dataframe import DataFrame\n",
      "\n",
      "        assert isinstance(self, DataFrame)\n",
      "\n",
      "        from pyspark.sql.pandas.utils import require_minimum_pandas_version\n",
      "\n",
      "        require_minimum_pandas_version()\n",
      "\n",
      "        import numpy as np\n",
      "        import pandas as pd\n",
      "        from pandas.core.dtypes.common import is_timedelta64_dtype\n",
      "\n",
      "        jconf = self.sparkSession._jconf\n",
      "        timezone = jconf.sessionLocalTimeZone()\n",
      "\n",
      "        if jconf.arrowPySparkEnabled():\n",
      "            use_arrow = True\n",
      "            try:\n",
      "                from pyspark.sql.pandas.types import to_arrow_schema\n",
      "                from pyspark.sql.pandas.utils import require_minimum_pyarrow_version\n",
      "\n",
      "                require_minimum_pyarrow_version()\n",
      "                to_arrow_schema(self.schema)\n",
      "            except Exception as e:\n",
      "\n",
      "                if jconf.arrowPySparkFallbackEnabled():\n",
      "                    msg = (\n",
      "                        \"toPandas attempted Arrow optimization because \"\n",
      "                        \"'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, \"\n",
      "                        \"failed by the reason below:\\n  %s\\n\"\n",
      "                        \"Attempting non-optimization as \"\n",
      "                        \"'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to \"\n",
      "                        \"true.\" % str(e)\n",
      "                    )\n",
      "                    warn(msg)\n",
      "                    use_arrow = False\n",
      "                else:\n",
      "                    msg = (\n",
      "                        \"toPandas attempted Arrow optimization because \"\n",
      "                        \"'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has \"\n",
      "                        \"reached the error below and will not continue because automatic fallback \"\n",
      "                        \"with 'spark.sql.execution.arrow.pyspark.fallback.enabled' has been set to \"\n",
      "                        \"false.\\n  %s\" % str(e)\n",
      "                    )\n",
      "                    warn(msg)\n",
      "                    raise\n",
      "\n",
      "            # Try to use Arrow optimization when the schema is supported and the required version\n",
      "            # of PyArrow is found, if 'spark.sql.execution.arrow.pyspark.enabled' is enabled.\n",
      "            if use_arrow:\n",
      "                try:\n",
      "                    from pyspark.sql.pandas.types import (\n",
      "                        _check_series_localize_timestamps,\n",
      "                        _convert_map_items_to_dict,\n",
      "                    )\n",
      "                    import pyarrow\n",
      "\n",
      "                    # Rename columns to avoid duplicated column names.\n",
      "                    tmp_column_names = [\"col_{}\".format(i) for i in range(len(self.columns))]\n",
      "                    self_destruct = jconf.arrowPySparkSelfDestructEnabled()\n",
      "                    batches = self.toDF(*tmp_column_names)._collect_as_arrow(\n",
      "                        split_batches=self_destruct\n",
      "                    )\n",
      "                    if len(batches) > 0:\n",
      "                        table = pyarrow.Table.from_batches(batches)\n",
      "                        # Ensure only the table has a reference to the batches, so that\n",
      "                        # self_destruct (if enabled) is effective\n",
      "                        del batches\n",
      "                        # Pandas DataFrame created from PyArrow uses datetime64[ns] for date type\n",
      "                        # values, but we should use datetime.date to match the behavior with when\n",
      "                        # Arrow optimization is disabled.\n",
      "                        pandas_options = {\"date_as_object\": True}\n",
      "                        if self_destruct:\n",
      "                            # Configure PyArrow to use as little memory as possible:\n",
      "                            # self_destruct - free columns as they are converted\n",
      "                            # split_blocks - create a separate Pandas block for each column\n",
      "                            # use_threads - convert one column at a time\n",
      "                            pandas_options.update(\n",
      "                                {\n",
      "                                    \"self_destruct\": True,\n",
      "                                    \"split_blocks\": True,\n",
      "                                    \"use_threads\": False,\n",
      "                                }\n",
      "                            )\n",
      "                        pdf = table.to_pandas(**pandas_options)\n",
      "                        # Rename back to the original column names.\n",
      "                        pdf.columns = self.columns\n",
      "                        for field in self.schema:\n",
      "                            if isinstance(field.dataType, TimestampType):\n",
      "                                pdf[field.name] = _check_series_localize_timestamps(\n",
      "                                    pdf[field.name], timezone\n",
      "                                )\n",
      "                            elif isinstance(field.dataType, MapType):\n",
      "                                pdf[field.name] = _convert_map_items_to_dict(pdf[field.name])\n",
      "                        return pdf\n",
      "                    else:\n",
      "                        corrected_panda_types = {}\n",
      "                        for index, field in enumerate(self.schema):\n",
      "                            pandas_type = PandasConversionMixin._to_corrected_pandas_type(\n",
      "                                field.dataType\n",
      "                            )\n",
      "                            corrected_panda_types[tmp_column_names[index]] = (\n",
      "                                np.object0 if pandas_type is None else pandas_type\n",
      "                            )\n",
      "\n",
      "                        pdf = pd.DataFrame(columns=tmp_column_names).astype(\n",
      "                            dtype=corrected_panda_types\n",
      "                        )\n",
      "                        pdf.columns = self.columns\n",
      "                        return pdf\n",
      "                except Exception as e:\n",
      "                    # We might have to allow fallback here as well but multiple Spark jobs can\n",
      "                    # be executed. So, simply fail in this case for now.\n",
      "                    msg = (\n",
      "                        \"toPandas attempted Arrow optimization because \"\n",
      "                        \"'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has \"\n",
      "                        \"reached the error below and can not continue. Note that \"\n",
      "                        \"'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an \"\n",
      "                        \"effect on failures in the middle of \"\n",
      "                        \"computation.\\n  %s\" % str(e)\n",
      "                    )\n",
      "                    warn(msg)\n",
      "                    raise\n",
      "\n",
      "        # Below is toPandas without Arrow optimization.\n",
      "        pdf = pd.DataFrame.from_records(self.collect(), columns=self.columns)\n",
      "        column_counter = Counter(self.columns)\n",
      "\n",
      "        corrected_dtypes: List[Optional[Type]] = [None] * len(self.schema)\n",
      "        for index, field in enumerate(self.schema):\n",
      "            # We use `iloc` to access columns with duplicate column names.\n",
      "            if column_counter[field.name] > 1:\n",
      "                pandas_col = pdf.iloc[:, index]\n",
      "            else:\n",
      "                pandas_col = pdf[field.name]\n",
      "\n",
      "            pandas_type = PandasConversionMixin._to_corrected_pandas_type(field.dataType)\n",
      "            # SPARK-21766: if an integer field is nullable and has null values, it can be\n",
      "            # inferred by pandas as a float column. If we convert the column with NaN back\n",
      "            # to integer type e.g., np.int16, we will hit an exception. So we use the\n",
      "            # pandas-inferred float type, rather than the corrected type from the schema\n",
      "            # in this case.\n",
      "            if pandas_type is not None and not (\n",
      "                isinstance(field.dataType, IntegralType)\n",
      "                and field.nullable\n",
      "                and pandas_col.isnull().any()\n",
      "            ):\n",
      "                corrected_dtypes[index] = pandas_type\n",
      "            # Ensure we fall back to nullable numpy types.\n",
      "            if isinstance(field.dataType, IntegralType) and pandas_col.isnull().any():\n",
      "                corrected_dtypes[index] = np.float64\n",
      "            if isinstance(field.dataType, BooleanType) and pandas_col.isnull().any():\n",
      "                corrected_dtypes[index] = np.object  # type: ignore[attr-defined]\n",
      "\n",
      "        df = pd.DataFrame()\n",
      "        for index, t in enumerate(corrected_dtypes):\n",
      "            column_name = self.schema[index].name\n",
      "\n",
      "            # We use `iloc` to access columns with duplicate column names.\n",
      "            if column_counter[column_name] > 1:\n",
      "                series = pdf.iloc[:, index]\n",
      "            else:\n",
      "                series = pdf[column_name]\n",
      "\n",
      "            # No need to cast for non-empty series for timedelta. The type is already correct.\n",
      "            should_check_timedelta = is_timedelta64_dtype(t) and len(pdf) == 0\n",
      "\n",
      "            if (t is not None and not is_timedelta64_dtype(t)) or should_check_timedelta:\n",
      "                series = series.astype(t, copy=False)\n",
      "\n",
      "            with catch_warnings():\n",
      "                from pandas.errors import PerformanceWarning\n",
      "\n",
      "                simplefilter(action=\"ignore\", category=PerformanceWarning)\n",
      "                # `insert` API makes copy of data,\n",
      "                # we only do it for Series of duplicate column names.\n",
      "                # `pdf.iloc[:, index] = pdf.iloc[:, index]...` doesn't always work\n",
      "                # because `iloc` could return a view or a copy depending by context.\n",
      "                if column_counter[column_name] > 1:\n",
      "                    df.insert(index, column_name, series, allow_duplicates=True)\n",
      "                else:\n",
      "                    df[column_name] = series\n",
      "\n",
      "        if timezone is None:\n",
      "            return df\n",
      "        else:\n",
      "            from pyspark.sql.pandas.types import _check_series_convert_timestamps_local_tz\n",
      "\n",
      "            for field in self.schema:\n",
      "                # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n",
      "                if isinstance(field.dataType, TimestampType):\n",
      "                    df[field.name] = _check_series_convert_timestamps_local_tz(\n",
      "                        df[field.name], timezone\n",
      "                    )\n",
      "            return df\n",
      "\n",
      "------------\n",
      "Method name: to_koalas\n",
      "Method definition:     def to_koalas(\n",
      "        self, index_col: Optional[Union[str, List[str]]] = None\n",
      "    ) -> \"PandasOnSparkDataFrame\":\n",
      "        return self.pandas_api(index_col)\n",
      "\n",
      "------------\n",
      "Method name: to_pandas_on_spark\n",
      "Method definition:     def to_pandas_on_spark(\n",
      "        self, index_col: Optional[Union[str, List[str]]] = None\n",
      "    ) -> \"PandasOnSparkDataFrame\":\n",
      "        warnings.warn(\n",
      "            \"DataFrame.to_pandas_on_spark is deprecated. Use DataFrame.pandas_api instead.\",\n",
      "            FutureWarning,\n",
      "        )\n",
      "        return self.pandas_api(index_col)\n",
      "\n",
      "------------\n",
      "Method name: transform\n",
      "Method definition:     def transform(self, func: Callable[..., \"DataFrame\"], *args: Any, **kwargs: Any) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
      "\n",
      "        .. versionadded:: 3.0.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            a function that takes and returns a :class:`DataFrame`.\n",
      "        *args\n",
      "            Positional arguments to pass to func.\n",
      "\n",
      "            .. versionadded:: 3.3.0\n",
      "        **kwargs\n",
      "            Keyword arguments to pass to func.\n",
      "\n",
      "            .. versionadded:: 3.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import col\n",
      "        >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
      "        >>> def cast_all_to_int(input_df):\n",
      "        ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
      "        >>> def sort_columns_asc(input_df):\n",
      "        ...     return input_df.select(*sorted(input_df.columns))\n",
      "        >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
      "        +-----+---+\n",
      "        |float|int|\n",
      "        +-----+---+\n",
      "        |    1|  1|\n",
      "        |    2|  2|\n",
      "        +-----+---+\n",
      "        >>> def add_n(input_df, n):\n",
      "        ...     return input_df.select([(col(col_name) + n).alias(col_name)\n",
      "        ...                             for col_name in input_df.columns])\n",
      "        >>> df.transform(add_n, 1).transform(add_n, n=10).show()\n",
      "        +---+-----+\n",
      "        |int|float|\n",
      "        +---+-----+\n",
      "        | 12| 12.0|\n",
      "        | 13| 13.0|\n",
      "        +---+-----+\n",
      "        \"\"\"\n",
      "        result = func(self, *args, **kwargs)\n",
      "        assert isinstance(\n",
      "            result, DataFrame\n",
      "        ), \"Func returned an instance of type [%s], \" \"should have been DataFrame.\" % type(result)\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: union\n",
      "Method definition:     @since(2.0)\n",
      "    def union(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      "        (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      "\n",
      "        Also as standard in SQL, this function resolves columns by position (not by name).\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.union(other._jdf), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: unionAll\n",
      "Method definition:     @since(1.3)\n",
      "    def unionAll(self, other: \"DataFrame\") -> \"DataFrame\":\n",
      "        \"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      "        (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      "\n",
      "        Also as standard in SQL, this function resolves columns by position (not by name).\n",
      "        \"\"\"\n",
      "        return self.union(other)\n",
      "\n",
      "------------\n",
      "Method name: unionByName\n",
      "Method definition:     def unionByName(self, other: \"DataFrame\", allowMissingColumns: bool = False) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      "        :class:`DataFrame`.\n",
      "\n",
      "        This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n",
      "        union (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      "\n",
      "        .. versionadded:: 2.3.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        The difference between this function and :func:`union` is that this function\n",
      "        resolves columns by name (not by position):\n",
      "\n",
      "        >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      "        >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      "        >>> df1.unionByName(df2).show()\n",
      "        +----+----+----+\n",
      "        |col0|col1|col2|\n",
      "        +----+----+----+\n",
      "        |   1|   2|   3|\n",
      "        |   6|   4|   5|\n",
      "        +----+----+----+\n",
      "\n",
      "        When the parameter `allowMissingColumns` is ``True``, the set of column names\n",
      "        in this and other :class:`DataFrame` can differ; missing columns will be filled with null.\n",
      "        Further, the missing columns of this :class:`DataFrame` will be added at the end\n",
      "        in the schema of the union result:\n",
      "\n",
      "        >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      "        >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
      "        >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      "        +----+----+----+----+\n",
      "        |col0|col1|col2|col3|\n",
      "        +----+----+----+----+\n",
      "        |   1|   2|   3|null|\n",
      "        |null|   4|   5|   6|\n",
      "        +----+----+----+----+\n",
      "\n",
      "        .. versionchanged:: 3.1.0\n",
      "           Added optional argument `allowMissingColumns` to specify whether to allow\n",
      "           missing columns.\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.unionByName(other._jdf, allowMissingColumns), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: unpersist\n",
      "Method definition:     def unpersist(self, blocking: bool = False) -> \"DataFrame\":\n",
      "        \"\"\"Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      "        memory and disk.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      "        \"\"\"\n",
      "        self.is_cached = False\n",
      "        self._jdf.unpersist(blocking)\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: where\n",
      "Method definition:     def filter(self, condition: \"ColumnOrName\") -> \"DataFrame\":\n",
      "        \"\"\"Filters rows using the given condition.\n",
      "\n",
      "        :func:`where` is an alias for :func:`filter`.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        condition : :class:`Column` or str\n",
      "            a :class:`Column` of :class:`types.BooleanType`\n",
      "            or a string of SQL expression.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.filter(df.age > 3).collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        >>> df.where(df.age == 2).collect()\n",
      "        [Row(age=2, name='Alice')]\n",
      "\n",
      "        >>> df.filter(\"age > 3\").collect()\n",
      "        [Row(age=5, name='Bob')]\n",
      "        >>> df.where(\"age = 2\").collect()\n",
      "        [Row(age=2, name='Alice')]\n",
      "        \"\"\"\n",
      "        if isinstance(condition, str):\n",
      "            jdf = self._jdf.filter(condition)\n",
      "        elif isinstance(condition, Column):\n",
      "            jdf = self._jdf.filter(condition._jc)\n",
      "        else:\n",
      "            raise TypeError(\"condition should be string or Column\")\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: withColumn\n",
      "Method definition:     def withColumn(self, colName: str, col: Column) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      "        existing column that has the same name.\n",
      "\n",
      "        The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      "        a column from some other :class:`DataFrame` will raise an error.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        colName : str\n",
      "            string, name of the new column.\n",
      "        col : :class:`Column`\n",
      "            a :class:`Column` expression for the new column.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This method introduces a projection internally. Therefore, calling it multiple\n",
      "        times, for instance, via loops in order to add multiple columns can generate big\n",
      "        plans which can cause performance issues and even `StackOverflowException`.\n",
      "        To avoid this, use :func:`select` with the multiple columns at once.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.withColumn('age2', df.age + 2).collect()\n",
      "        [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      "\n",
      "        \"\"\"\n",
      "        if not isinstance(col, Column):\n",
      "            raise TypeError(\"col should be Column\")\n",
      "        return DataFrame(self._jdf.withColumn(colName, col._jc), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: withColumnRenamed\n",
      "Method definition:     def withColumnRenamed(self, existing: str, new: str) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` by renaming an existing column.\n",
      "        This is a no-op if schema doesn't contain the given column name.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        existing : str\n",
      "            string, name of the existing column to rename.\n",
      "        new : str\n",
      "            string, new name of the column.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.withColumnRenamed('age', 'age2').collect()\n",
      "        [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jdf.withColumnRenamed(existing, new), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: withColumns\n",
      "Method definition:     def withColumns(self, *colsMap: Dict[str, Column]) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n",
      "        existing columns that has the same names.\n",
      "\n",
      "        The colsMap is a map of column name and column, the column must only refer to attributes\n",
      "        supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n",
      "\n",
      "        .. versionadded:: 3.3.0\n",
      "           Added support for multiple columns adding\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        colsMap : dict\n",
      "            a dict of column name and :class:`Column`. Currently, only single map is supported.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).collect()\n",
      "        [Row(age=2, name='Alice', age2=4, age3=5), Row(age=5, name='Bob', age2=7, age3=8)]\n",
      "        \"\"\"\n",
      "        # Below code is to help enable kwargs in future.\n",
      "        assert len(colsMap) == 1\n",
      "        colsMap = colsMap[0]  # type: ignore[assignment]\n",
      "\n",
      "        if not isinstance(colsMap, dict):\n",
      "            raise TypeError(\"colsMap must be dict of column name and column.\")\n",
      "\n",
      "        col_names = list(colsMap.keys())\n",
      "        cols = list(colsMap.values())\n",
      "\n",
      "        return DataFrame(\n",
      "            self._jdf.withColumns(_to_seq(self._sc, col_names), self._jcols(*cols)),\n",
      "            self.sparkSession,\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: withMetadata\n",
      "Method definition:     def withMetadata(self, columnName: str, metadata: Dict[str, Any]) -> \"DataFrame\":\n",
      "        \"\"\"Returns a new :class:`DataFrame` by updating an existing column with metadata.\n",
      "\n",
      "        .. versionadded:: 3.3.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        columnName : str\n",
      "            string, name of the existing column to update the metadata.\n",
      "        metadata : dict\n",
      "            dict, new metadata to be assigned to df.schema[columnName].metadata\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n",
      "        >>> df_meta.schema['age'].metadata\n",
      "        {'foo': 'bar'}\n",
      "        \"\"\"\n",
      "        if not isinstance(metadata, dict):\n",
      "            raise TypeError(\"metadata should be a dict\")\n",
      "        sc = SparkContext._active_spark_context\n",
      "        assert sc is not None and sc._jvm is not None\n",
      "        jmeta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(json.dumps(metadata))\n",
      "        return DataFrame(self._jdf.withMetadata(columnName, jmeta), self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: withWatermark\n",
      "Method definition:     def withWatermark(self, eventTime: str, delayThreshold: str) -> \"DataFrame\":\n",
      "        \"\"\"Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      "        in time before which we assume no more late data is going to arrive.\n",
      "\n",
      "        Spark will use this watermark for several purposes:\n",
      "          - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      "            when using output modes that do not allow updates.\n",
      "\n",
      "          - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      "\n",
      "        The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      "        all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      "        of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      "        to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      "        process records that arrive more than `delayThreshold` late.\n",
      "\n",
      "        .. versionadded:: 2.1.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        eventTime : str\n",
      "            the name of the column that contains the event time of the row.\n",
      "        delayThreshold : str\n",
      "            the minimum delay to wait to data to arrive late, relative to the\n",
      "            latest record that has been processed in the form of an interval\n",
      "            (e.g. \"1 minute\" or \"5 hours\").\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This API is evolving.\n",
      "\n",
      "        >>> from pyspark.sql.functions import timestamp_seconds\n",
      "        >>> sdf.select(\n",
      "        ...    'name',\n",
      "        ...    timestamp_seconds(sdf.time).alias('time')).withWatermark('time', '10 minutes')\n",
      "        DataFrame[name: string, time: timestamp]\n",
      "        \"\"\"\n",
      "        if not eventTime or type(eventTime) is not str:\n",
      "            raise TypeError(\"eventTime should be provided as a string\")\n",
      "        if not delayThreshold or type(delayThreshold) is not str:\n",
      "            raise TypeError(\"delayThreshold should be provided as a string interval\")\n",
      "        jdf = self._jdf.withWatermark(eventTime, delayThreshold)\n",
      "        return DataFrame(jdf, self.sparkSession)\n",
      "\n",
      "------------\n",
      "Method name: writeTo\n",
      "Method definition:     def writeTo(self, table: str) -> DataFrameWriterV2:\n",
      "        \"\"\"\n",
      "        Create a write configuration builder for v2 sources.\n",
      "\n",
      "        This builder is used to configure and execute write operations.\n",
      "\n",
      "        For example, to append or create or replace existing tables.\n",
      "\n",
      "        .. versionadded:: 3.1.0\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n",
      "        >>> df.writeTo(                              # doctest: +SKIP\n",
      "        ...     \"catalog.db.table\"\n",
      "        ... ).partitionedBy(\"col\").createOrReplace()\n",
      "        \"\"\"\n",
      "        return DataFrameWriterV2(self, table)\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     def __eq__(self, other: Any) -> bool:\n",
      "        return isinstance(other, self.__class__) and self.__dict__ == other.__dict__\n",
      "\n",
      "------------\n",
      "Method name: __hash__\n",
      "Method definition:     def __hash__(self) -> int:\n",
      "        return hash(str(self))\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     def __ne__(self, other: Any) -> bool:\n",
      "        return not self.__eq__(other)\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        return self.__class__.__name__ + \"()\"\n",
      "\n",
      "------------\n",
      "Method name: fromInternal\n",
      "Method definition:     def fromInternal(self, v: int) -> datetime.date:\n",
      "        if v is not None:\n",
      "            return datetime.date.fromordinal(v + self.EPOCH_ORDINAL)\n",
      "\n",
      "------------\n",
      "Method name: json\n",
      "Method definition:     def json(self) -> str:\n",
      "        return json.dumps(self.jsonValue(), separators=(\",\", \":\"), sort_keys=True)\n",
      "\n",
      "------------\n",
      "Method name: jsonValue\n",
      "Method definition:     def jsonValue(self) -> Union[str, Dict[str, Any]]:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: needConversion\n",
      "Method definition:     def needConversion(self) -> bool:\n",
      "        return True\n",
      "\n",
      "------------\n",
      "Method name: simpleString\n",
      "Method definition:     def simpleString(self) -> str:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: toInternal\n",
      "Method definition:     def toInternal(self, d: datetime.date) -> int:\n",
      "        if d is not None:\n",
      "            return d.toordinal() - self.EPOCH_ORDINAL\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     def __eq__(self, other: Any) -> bool:\n",
      "        return isinstance(other, self.__class__) and self.__dict__ == other.__dict__\n",
      "\n",
      "------------\n",
      "Method name: __hash__\n",
      "Method definition:     def __hash__(self) -> int:\n",
      "        return hash(str(self))\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     def __ne__(self, other: Any) -> bool:\n",
      "        return not self.__eq__(other)\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        return self.__class__.__name__ + \"()\"\n",
      "\n",
      "------------\n",
      "Method name: fromInternal\n",
      "Method definition:     def fromInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts an internal SQL object into a native Python object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: json\n",
      "Method definition:     def json(self) -> str:\n",
      "        return json.dumps(self.jsonValue(), separators=(\",\", \":\"), sort_keys=True)\n",
      "\n",
      "------------\n",
      "Method name: jsonValue\n",
      "Method definition:     def jsonValue(self) -> Union[str, Dict[str, Any]]:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: needConversion\n",
      "Method definition:     def needConversion(self) -> bool:\n",
      "        \"\"\"\n",
      "        Does this type needs conversion between Python object and internal SQL object.\n",
      "\n",
      "        This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "        \"\"\"\n",
      "        return False\n",
      "\n",
      "------------\n",
      "Method name: simpleString\n",
      "Method definition:     def simpleString(self) -> str:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: toInternal\n",
      "Method definition:     def toInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts a Python object into an internal SQL object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     def __eq__(self, other: Any) -> bool:\n",
      "        return isinstance(other, self.__class__) and self.__dict__ == other.__dict__\n",
      "\n",
      "------------\n",
      "Method name: __hash__\n",
      "Method definition:     def __hash__(self) -> int:\n",
      "        return hash(str(self))\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     def __ne__(self, other: Any) -> bool:\n",
      "        return not self.__eq__(other)\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        return self.__class__.__name__ + \"()\"\n",
      "\n",
      "------------\n",
      "Method name: fromInternal\n",
      "Method definition:     def fromInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts an internal SQL object into a native Python object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: json\n",
      "Method definition:     def json(self) -> str:\n",
      "        return json.dumps(self.jsonValue(), separators=(\",\", \":\"), sort_keys=True)\n",
      "\n",
      "------------\n",
      "Method name: jsonValue\n",
      "Method definition:     def jsonValue(self) -> Union[str, Dict[str, Any]]:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: needConversion\n",
      "Method definition:     def needConversion(self) -> bool:\n",
      "        \"\"\"\n",
      "        Does this type needs conversion between Python object and internal SQL object.\n",
      "\n",
      "        This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "        \"\"\"\n",
      "        return False\n",
      "\n",
      "------------\n",
      "Method name: simpleString\n",
      "Method definition:     def simpleString(self) -> str:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: toInternal\n",
      "Method definition:     def toInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts a Python object into an internal SQL object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: __abs__\n",
      "Method definition:     def __abs__(self) -> Index:\n",
      "        return self._unary_method(operator.abs)\n",
      "\n",
      "------------\n",
      "Method name: __add__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__add__\")\n",
      "    def __add__(self, other):\n",
      "        return self._arith_method(other, operator.add)\n",
      "\n",
      "------------\n",
      "Method name: __and__\n",
      "Method definition:     @final\n",
      "    def __and__(self, other):\n",
      "        warnings.warn(\n",
      "            \"Index.__and__ operating as a set operation is deprecated, \"\n",
      "            \"in the future this will be a logical operation matching \"\n",
      "            \"Series.__and__.  Use index.intersection(other) instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        return self.intersection(other)\n",
      "\n",
      "------------\n",
      "Method name: __array__\n",
      "Method definition:     def __array__(self, dtype=None) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        The array interface, return my values.\n",
      "        \"\"\"\n",
      "        return np.asarray(self._data, dtype=dtype)\n",
      "\n",
      "------------\n",
      "Method name: __array_ufunc__\n",
      "Method definition:     def __array_ufunc__(self, ufunc: np.ufunc, method: str_t, *inputs, **kwargs):\n",
      "        if any(isinstance(other, (ABCSeries, ABCDataFrame)) for other in inputs):\n",
      "            return NotImplemented\n",
      "\n",
      "        # TODO(2.0) the 'and', 'or' and 'xor' dunder methods are currently set\n",
      "        # operations and not logical operations, so don't dispatch\n",
      "        # This is deprecated, so this full 'if' clause can be removed once\n",
      "        # deprecation is enforced in 2.0\n",
      "        if not (\n",
      "            method == \"__call__\"\n",
      "            and ufunc in (np.bitwise_and, np.bitwise_or, np.bitwise_xor)\n",
      "        ):\n",
      "            result = arraylike.maybe_dispatch_ufunc_to_dunder_op(\n",
      "                self, ufunc, method, *inputs, **kwargs\n",
      "            )\n",
      "            if result is not NotImplemented:\n",
      "                return result\n",
      "\n",
      "        if \"out\" in kwargs:\n",
      "            # e.g. test_dti_isub_tdi\n",
      "            return arraylike.dispatch_ufunc_with_out(\n",
      "                self, ufunc, method, *inputs, **kwargs\n",
      "            )\n",
      "\n",
      "        if method == \"reduce\":\n",
      "            result = arraylike.dispatch_reduction_ufunc(\n",
      "                self, ufunc, method, *inputs, **kwargs\n",
      "            )\n",
      "            if result is not NotImplemented:\n",
      "                return result\n",
      "\n",
      "        new_inputs = [x if x is not self else x._values for x in inputs]\n",
      "        result = getattr(ufunc, method)(*new_inputs, **kwargs)\n",
      "        if ufunc.nout == 2:\n",
      "            # i.e. np.divmod, np.modf, np.frexp\n",
      "            return tuple(self.__array_wrap__(x) for x in result)\n",
      "\n",
      "        return self.__array_wrap__(result)\n",
      "\n",
      "------------\n",
      "Method name: __array_wrap__\n",
      "Method definition:     def __array_wrap__(self, result, context=None):\n",
      "        \"\"\"\n",
      "        Gets called after a ufunc and other functions e.g. np.split.\n",
      "        \"\"\"\n",
      "        result = lib.item_from_zerodim(result)\n",
      "        if is_bool_dtype(result) or lib.is_scalar(result) or np.ndim(result) > 1:\n",
      "            return result\n",
      "\n",
      "        return Index(result, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: __bool__\n",
      "Method definition:     @final\n",
      "    def __nonzero__(self) -> NoReturn:\n",
      "        raise ValueError(\n",
      "            f\"The truth value of a {type(self).__name__} is ambiguous. \"\n",
      "            \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: __contains__\n",
      "Method definition:     def __contains__(self, key: Any) -> bool:\n",
      "        \"\"\"\n",
      "        Return a boolean indicating whether the provided key is in the index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        key : label\n",
      "            The key to check if it is present in the index.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether the key search is in the index.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If the key is not hashable.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.isin : Returns an ndarray of boolean dtype indicating whether the\n",
      "            list-like key is in the index.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx\n",
      "        Int64Index([1, 2, 3, 4], dtype='int64')\n",
      "\n",
      "        >>> 2 in idx\n",
      "        True\n",
      "        >>> 6 in idx\n",
      "        False\n",
      "        \"\"\"\n",
      "        hash(key)\n",
      "        try:\n",
      "            return key in self._engine\n",
      "        except (OverflowError, TypeError, ValueError):\n",
      "            return False\n",
      "\n",
      "------------\n",
      "Method name: __copy__\n",
      "Method definition:     @final\n",
      "    def __copy__(self: _IndexT, **kwargs) -> _IndexT:\n",
      "        return self.copy(**kwargs)\n",
      "\n",
      "------------\n",
      "Method name: __deepcopy__\n",
      "Method definition:     @final\n",
      "    def __deepcopy__(self: _IndexT, memo=None) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Parameters\n",
      "        ----------\n",
      "        memo, default None\n",
      "            Standard signature. Unused\n",
      "        \"\"\"\n",
      "        return self.copy(deep=True)\n",
      "\n",
      "------------\n",
      "Method name: __dir__\n",
      "Method definition:     def __dir__(self) -> list[str]:\n",
      "        \"\"\"\n",
      "        Provide method name lookup and completion.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Only provide 'public' methods.\n",
      "        \"\"\"\n",
      "        rv = set(super().__dir__())\n",
      "        rv = (rv - self._dir_deletions()) | self._dir_additions()\n",
      "        return sorted(rv)\n",
      "\n",
      "------------\n",
      "Method name: __divmod__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__divmod__\")\n",
      "    def __divmod__(self, other):\n",
      "        return self._arith_method(other, divmod)\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__eq__\")\n",
      "    def __eq__(self, other):\n",
      "        return self._cmp_method(other, operator.eq)\n",
      "\n",
      "------------\n",
      "Method name: __floordiv__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__floordiv__\")\n",
      "    def __floordiv__(self, other):\n",
      "        return self._arith_method(other, operator.floordiv)\n",
      "\n",
      "------------\n",
      "Method name: __ge__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__ge__\")\n",
      "    def __ge__(self, other):\n",
      "        return self._cmp_method(other, operator.ge)\n",
      "\n",
      "------------\n",
      "Method name: __getitem__\n",
      "Method definition:     def __getitem__(self, key):\n",
      "        \"\"\"\n",
      "        Override numpy.ndarray's __getitem__ method to work as desired.\n",
      "\n",
      "        This function adds lists and Series as valid boolean indexers\n",
      "        (ndarrays only supports ndarray with dtype=bool).\n",
      "\n",
      "        If resulting ndim != 1, plain ndarray is returned instead of\n",
      "        corresponding `Index` subclass.\n",
      "\n",
      "        \"\"\"\n",
      "        getitem = self._data.__getitem__\n",
      "\n",
      "        if is_integer(key) or is_float(key):\n",
      "            # GH#44051 exclude bool, which would return a 2d ndarray\n",
      "            key = com.cast_scalar_indexer(key, warn_float=True)\n",
      "            return getitem(key)\n",
      "\n",
      "        if isinstance(key, slice):\n",
      "            # This case is separated from the conditional above to avoid\n",
      "            # pessimization com.is_bool_indexer and ndim checks.\n",
      "            result = getitem(key)\n",
      "            # Going through simple_new for performance.\n",
      "            return type(self)._simple_new(result, name=self._name)\n",
      "\n",
      "        if com.is_bool_indexer(key):\n",
      "            # if we have list[bools, length=1e5] then doing this check+convert\n",
      "            #  takes 166 µs + 2.1 ms and cuts the ndarray.__getitem__\n",
      "            #  time below from 3.8 ms to 496 µs\n",
      "            # if we already have ndarray[bool], the overhead is 1.4 µs or .25%\n",
      "            if is_extension_array_dtype(getattr(key, \"dtype\", None)):\n",
      "                key = key.to_numpy(dtype=bool, na_value=False)\n",
      "            else:\n",
      "                key = np.asarray(key, dtype=bool)\n",
      "\n",
      "        result = getitem(key)\n",
      "        # Because we ruled out integer above, we always get an arraylike here\n",
      "        if result.ndim > 1:\n",
      "            deprecate_ndim_indexing(result)\n",
      "            if hasattr(result, \"_ndarray\"):\n",
      "                # i.e. NDArrayBackedExtensionArray\n",
      "                # Unpack to ndarray for MPL compat\n",
      "                # error: Item \"ndarray[Any, Any]\" of\n",
      "                # \"Union[ExtensionArray, ndarray[Any, Any]]\"\n",
      "                # has no attribute \"_ndarray\"\n",
      "                return result._ndarray  # type: ignore[union-attr]\n",
      "            return result\n",
      "\n",
      "        # NB: Using _constructor._simple_new would break if MultiIndex\n",
      "        #  didn't override __getitem__\n",
      "        return self._constructor._simple_new(result, name=self._name)\n",
      "\n",
      "------------\n",
      "Method name: __gt__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__gt__\")\n",
      "    def __gt__(self, other):\n",
      "        return self._cmp_method(other, operator.gt)\n",
      "\n",
      "------------\n",
      "Method name: __iadd__\n",
      "Method definition:     def __iadd__(self, other):\n",
      "        # alias for __add__\n",
      "        return self + other\n",
      "\n",
      "------------\n",
      "Method name: __invert__\n",
      "Method definition:     def __invert__(self) -> Index:\n",
      "        # GH#8875\n",
      "        return self._unary_method(operator.inv)\n",
      "\n",
      "------------\n",
      "Method name: __iter__\n",
      "Method definition:     def __iter__(self):\n",
      "        \"\"\"\n",
      "        Return an iterator of the values.\n",
      "\n",
      "        These are each a scalar type, which is a Python scalar\n",
      "        (for str, int, float) or a pandas scalar\n",
      "        (for Timestamp/Timedelta/Interval/Period)\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        iterator\n",
      "        \"\"\"\n",
      "        # We are explicitly making element iterators.\n",
      "        if not isinstance(self._values, np.ndarray):\n",
      "            # Check type instead of dtype to catch DTA/TDA\n",
      "            return iter(self._values)\n",
      "        else:\n",
      "            return map(self._values.item, range(self._values.size))\n",
      "\n",
      "------------\n",
      "Method name: __le__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__le__\")\n",
      "    def __le__(self, other):\n",
      "        return self._cmp_method(other, operator.le)\n",
      "\n",
      "------------\n",
      "Method name: __len__\n",
      "Method definition:     def __len__(self) -> int:\n",
      "        \"\"\"\n",
      "        Return the length of the Index.\n",
      "        \"\"\"\n",
      "        return len(self._data)\n",
      "\n",
      "------------\n",
      "Method name: __lt__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__lt__\")\n",
      "    def __lt__(self, other):\n",
      "        return self._cmp_method(other, operator.lt)\n",
      "\n",
      "------------\n",
      "Method name: __mod__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__mod__\")\n",
      "    def __mod__(self, other):\n",
      "        return self._arith_method(other, operator.mod)\n",
      "\n",
      "------------\n",
      "Method name: __mul__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__mul__\")\n",
      "    def __mul__(self, other):\n",
      "        return self._arith_method(other, operator.mul)\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__ne__\")\n",
      "    def __ne__(self, other):\n",
      "        return self._cmp_method(other, operator.ne)\n",
      "\n",
      "------------\n",
      "Method name: __neg__\n",
      "Method definition:     def __neg__(self) -> Index:\n",
      "        return self._unary_method(operator.neg)\n",
      "\n",
      "------------\n",
      "Method name: __new__\n",
      "Method definition:     def __new__(\n",
      "        cls, data=None, dtype=None, copy=False, name=None, tupleize_cols=True, **kwargs\n",
      "    ) -> Index:\n",
      "\n",
      "        if kwargs:\n",
      "            warnings.warn(\n",
      "                \"Passing keywords other than 'data', 'dtype', 'copy', 'name', \"\n",
      "                \"'tupleize_cols' is deprecated and will raise TypeError in a \"\n",
      "                \"future version.  Use the specific Index subclass directly instead.\",\n",
      "                FutureWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "\n",
      "        from pandas.core.arrays import PandasArray\n",
      "        from pandas.core.indexes.range import RangeIndex\n",
      "\n",
      "        name = maybe_extract_name(name, data, cls)\n",
      "\n",
      "        if dtype is not None:\n",
      "            dtype = pandas_dtype(dtype)\n",
      "        if \"tz\" in kwargs:\n",
      "            tz = kwargs.pop(\"tz\")\n",
      "            validate_tz_from_dtype(dtype, tz)\n",
      "            dtype = tz_to_dtype(tz)\n",
      "\n",
      "        if type(data) is PandasArray:\n",
      "            # ensure users don't accidentally put a PandasArray in an index,\n",
      "            #  but don't unpack StringArray\n",
      "            data = data.to_numpy()\n",
      "        if isinstance(dtype, PandasDtype):\n",
      "            dtype = dtype.numpy_dtype\n",
      "\n",
      "        data_dtype = getattr(data, \"dtype\", None)\n",
      "\n",
      "        # range\n",
      "        if isinstance(data, (range, RangeIndex)):\n",
      "            result = RangeIndex(start=data, copy=copy, name=name)\n",
      "            if dtype is not None:\n",
      "                return result.astype(dtype, copy=False)\n",
      "            return result\n",
      "\n",
      "        elif is_ea_or_datetimelike_dtype(dtype):\n",
      "            # non-EA dtype indexes have special casting logic, so we punt here\n",
      "            klass = cls._dtype_to_subclass(dtype)\n",
      "            if klass is not Index:\n",
      "                return klass(data, dtype=dtype, copy=copy, name=name, **kwargs)\n",
      "\n",
      "            ea_cls = dtype.construct_array_type()\n",
      "            data = ea_cls._from_sequence(data, dtype=dtype, copy=copy)\n",
      "            disallow_kwargs(kwargs)\n",
      "            return Index._simple_new(data, name=name)\n",
      "\n",
      "        elif is_ea_or_datetimelike_dtype(data_dtype):\n",
      "            data_dtype = cast(DtypeObj, data_dtype)\n",
      "            klass = cls._dtype_to_subclass(data_dtype)\n",
      "            if klass is not Index:\n",
      "                result = klass(data, copy=copy, name=name, **kwargs)\n",
      "                if dtype is not None:\n",
      "                    return result.astype(dtype, copy=False)\n",
      "                return result\n",
      "            elif dtype is not None:\n",
      "                # GH#45206\n",
      "                data = data.astype(dtype, copy=False)\n",
      "\n",
      "            disallow_kwargs(kwargs)\n",
      "            data = extract_array(data, extract_numpy=True)\n",
      "            return Index._simple_new(data, name=name)\n",
      "\n",
      "        # index-like\n",
      "        elif (\n",
      "            isinstance(data, Index)\n",
      "            and data._is_backward_compat_public_numeric_index\n",
      "            and dtype is None\n",
      "        ):\n",
      "            return data._constructor(data, name=name, copy=copy)\n",
      "        elif isinstance(data, (np.ndarray, Index, ABCSeries)):\n",
      "\n",
      "            if isinstance(data, ABCMultiIndex):\n",
      "                data = data._values\n",
      "\n",
      "            if dtype is not None:\n",
      "                # we need to avoid having numpy coerce\n",
      "                # things that look like ints/floats to ints unless\n",
      "                # they are actually ints, e.g. '0' and 0.0\n",
      "                # should not be coerced\n",
      "                # GH 11836\n",
      "                data = sanitize_array(data, None, dtype=dtype, copy=copy)\n",
      "\n",
      "                dtype = data.dtype\n",
      "\n",
      "            if data.dtype.kind in [\"i\", \"u\", \"f\"]:\n",
      "                # maybe coerce to a sub-class\n",
      "                arr = data\n",
      "            elif data.dtype.kind in [\"b\", \"c\"]:\n",
      "                # No special subclass, and Index._ensure_array won't do this\n",
      "                #  for us.\n",
      "                arr = np.asarray(data)\n",
      "            else:\n",
      "                arr = com.asarray_tuplesafe(data, dtype=_dtype_obj)\n",
      "\n",
      "                if dtype is None:\n",
      "                    arr = _maybe_cast_data_without_dtype(\n",
      "                        arr, cast_numeric_deprecated=True\n",
      "                    )\n",
      "                    dtype = arr.dtype\n",
      "\n",
      "                    if kwargs:\n",
      "                        return cls(arr, dtype, copy=copy, name=name, **kwargs)\n",
      "\n",
      "            klass = cls._dtype_to_subclass(arr.dtype)\n",
      "            arr = klass._ensure_array(arr, dtype, copy)\n",
      "            disallow_kwargs(kwargs)\n",
      "            return klass._simple_new(arr, name)\n",
      "\n",
      "        elif is_scalar(data):\n",
      "            raise cls._scalar_data_error(data)\n",
      "        elif hasattr(data, \"__array__\"):\n",
      "            return Index(np.asarray(data), dtype=dtype, copy=copy, name=name, **kwargs)\n",
      "        else:\n",
      "\n",
      "            if tupleize_cols and is_list_like(data):\n",
      "                # GH21470: convert iterable to list before determining if empty\n",
      "                if is_iterator(data):\n",
      "                    data = list(data)\n",
      "\n",
      "                if data and all(isinstance(e, tuple) for e in data):\n",
      "                    # we must be all tuples, otherwise don't construct\n",
      "                    # 10697\n",
      "                    from pandas.core.indexes.multi import MultiIndex\n",
      "\n",
      "                    return MultiIndex.from_tuples(\n",
      "                        data, names=name or kwargs.get(\"names\")\n",
      "                    )\n",
      "            # other iterable of some kind\n",
      "\n",
      "            subarr = com.asarray_tuplesafe(data, dtype=_dtype_obj)\n",
      "            if dtype is None:\n",
      "                # with e.g. a list [1, 2, 3] casting to numeric is _not_ deprecated\n",
      "                subarr = _maybe_cast_data_without_dtype(\n",
      "                    subarr, cast_numeric_deprecated=False\n",
      "                )\n",
      "                dtype = subarr.dtype\n",
      "            return Index(subarr, dtype=dtype, copy=copy, name=name, **kwargs)\n",
      "\n",
      "------------\n",
      "Method name: __nonzero__\n",
      "Method definition:     @final\n",
      "    def __nonzero__(self) -> NoReturn:\n",
      "        raise ValueError(\n",
      "            f\"The truth value of a {type(self).__name__} is ambiguous. \"\n",
      "            \"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: __or__\n",
      "Method definition:     @final\n",
      "    def __or__(self, other):\n",
      "        warnings.warn(\n",
      "            \"Index.__or__ operating as a set operation is deprecated, \"\n",
      "            \"in the future this will be a logical operation matching \"\n",
      "            \"Series.__or__.  Use index.union(other) instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        return self.union(other)\n",
      "\n",
      "------------\n",
      "Method name: __pos__\n",
      "Method definition:     def __pos__(self) -> Index:\n",
      "        return self._unary_method(operator.pos)\n",
      "\n",
      "------------\n",
      "Method name: __pow__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__pow__\")\n",
      "    def __pow__(self, other):\n",
      "        return self._arith_method(other, operator.pow)\n",
      "\n",
      "------------\n",
      "Method name: __radd__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__radd__\")\n",
      "    def __radd__(self, other):\n",
      "        return self._arith_method(other, roperator.radd)\n",
      "\n",
      "------------\n",
      "Method name: __rand__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rand__\")\n",
      "    def __rand__(self, other):\n",
      "        return self._logical_method(other, roperator.rand_)\n",
      "\n",
      "------------\n",
      "Method name: __rdivmod__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rdivmod__\")\n",
      "    def __rdivmod__(self, other):\n",
      "        return self._arith_method(other, roperator.rdivmod)\n",
      "\n",
      "------------\n",
      "Method name: __reduce__\n",
      "Method definition:     def __reduce__(self):\n",
      "        d = {\"data\": self._data, \"name\": self.name}\n",
      "        return _new_Index, (type(self), d), None\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     @final\n",
      "    def __repr__(self) -> str_t:\n",
      "        \"\"\"\n",
      "        Return a string representation for this object.\n",
      "        \"\"\"\n",
      "        klass_name = type(self).__name__\n",
      "        data = self._format_data()\n",
      "        attrs = self._format_attrs()\n",
      "        space = self._format_space()\n",
      "        attrs_str = [f\"{k}={v}\" for k, v in attrs]\n",
      "        prepr = f\",{space}\".join(attrs_str)\n",
      "\n",
      "        # no data provided, just attributes\n",
      "        if data is None:\n",
      "            data = \"\"\n",
      "\n",
      "        return f\"{klass_name}({data}{prepr})\"\n",
      "\n",
      "------------\n",
      "Method name: __rfloordiv__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rfloordiv\")\n",
      "    def __rfloordiv__(self, other):\n",
      "        return self._arith_method(other, roperator.rfloordiv)\n",
      "\n",
      "------------\n",
      "Method name: __rmod__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rmod__\")\n",
      "    def __rmod__(self, other):\n",
      "        return self._arith_method(other, roperator.rmod)\n",
      "\n",
      "------------\n",
      "Method name: __rmul__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rmul__\")\n",
      "    def __rmul__(self, other):\n",
      "        return self._arith_method(other, roperator.rmul)\n",
      "\n",
      "------------\n",
      "Method name: __ror__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__ror__\")\n",
      "    def __ror__(self, other):\n",
      "        return self._logical_method(other, roperator.ror_)\n",
      "\n",
      "------------\n",
      "Method name: __rpow__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rpow__\")\n",
      "    def __rpow__(self, other):\n",
      "        return self._arith_method(other, roperator.rpow)\n",
      "\n",
      "------------\n",
      "Method name: __rsub__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rsub__\")\n",
      "    def __rsub__(self, other):\n",
      "        return self._arith_method(other, roperator.rsub)\n",
      "\n",
      "------------\n",
      "Method name: __rtruediv__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rtruediv__\")\n",
      "    def __rtruediv__(self, other):\n",
      "        return self._arith_method(other, roperator.rtruediv)\n",
      "\n",
      "------------\n",
      "Method name: __rxor__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__rxor__\")\n",
      "    def __rxor__(self, other):\n",
      "        return self._logical_method(other, roperator.rxor)\n",
      "\n",
      "------------\n",
      "Method name: __setitem__\n",
      "Method definition:     @final\n",
      "    def __setitem__(self, key, value):\n",
      "        raise TypeError(\"Index does not support mutable operations\")\n",
      "\n",
      "------------\n",
      "Method name: __sizeof__\n",
      "Method definition:     def __sizeof__(self) -> int:\n",
      "        \"\"\"\n",
      "        Generates the total memory usage for an object that returns\n",
      "        either a value or Series of values\n",
      "        \"\"\"\n",
      "        memory_usage = getattr(self, \"memory_usage\", None)\n",
      "        if memory_usage:\n",
      "            mem = memory_usage(deep=True)\n",
      "            return int(mem if is_scalar(mem) else mem.sum())\n",
      "\n",
      "        # no memory_usage attribute, so fall back to object's 'sizeof'\n",
      "        return super().__sizeof__()\n",
      "\n",
      "------------\n",
      "Method name: __sub__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__sub__\")\n",
      "    def __sub__(self, other):\n",
      "        return self._arith_method(other, operator.sub)\n",
      "\n",
      "------------\n",
      "Method name: __truediv__\n",
      "Method definition:     @unpack_zerodim_and_defer(\"__truediv__\")\n",
      "    def __truediv__(self, other):\n",
      "        return self._arith_method(other, operator.truediv)\n",
      "\n",
      "------------\n",
      "Method name: __xor__\n",
      "Method definition:     @final\n",
      "    def __xor__(self, other):\n",
      "        warnings.warn(\n",
      "            \"Index.__xor__ operating as a set operation is deprecated, \"\n",
      "            \"in the future this will be a logical operation matching \"\n",
      "            \"Series.__xor__.  Use index.symmetric_difference(other) instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        return self.symmetric_difference(other)\n",
      "\n",
      "------------\n",
      "Method name: _arith_method\n",
      "Method definition:     def _arith_method(self, other, op):\n",
      "        if (\n",
      "            isinstance(other, Index)\n",
      "            and is_object_dtype(other.dtype)\n",
      "            and type(other) is not Index\n",
      "        ):\n",
      "            # We return NotImplemented for object-dtype index *subclasses* so they have\n",
      "            # a chance to implement ops before we unwrap them.\n",
      "            # See https://github.com/pandas-dev/pandas/issues/31109\n",
      "            return NotImplemented\n",
      "\n",
      "        return super()._arith_method(other, op)\n",
      "\n",
      "------------\n",
      "Method name: _assert_can_do_setop\n",
      "Method definition:     @final\n",
      "    def _assert_can_do_setop(self, other) -> bool:\n",
      "        if not is_list_like(other):\n",
      "            raise TypeError(\"Input must be Index or array-like\")\n",
      "        return True\n",
      "\n",
      "------------\n",
      "Method name: _can_hold_identifiers_and_holds_name\n",
      "Method definition:     @final\n",
      "    def _can_hold_identifiers_and_holds_name(self, name) -> bool:\n",
      "        \"\"\"\n",
      "        Faster check for ``name in self`` when we know `name` is a Python\n",
      "        identifier (e.g. in NDFrame.__getattr__, which hits this to support\n",
      "        . key lookup). For indexes that can't hold identifiers (everything\n",
      "        but object & categorical) we just return False.\n",
      "\n",
      "        https://github.com/pandas-dev/pandas/issues/19764\n",
      "        \"\"\"\n",
      "        if self.is_object() or is_string_dtype(self.dtype) or self.is_categorical():\n",
      "            return name in self\n",
      "        return False\n",
      "\n",
      "------------\n",
      "Method name: _check_indexing_error\n",
      "Method definition:     def _check_indexing_error(self, key):\n",
      "        if not is_scalar(key):\n",
      "            # if key is not a scalar, directly raise an error (the code below\n",
      "            # would convert to numpy arrays and raise later any way) - GH29926\n",
      "            raise InvalidIndexError(key)\n",
      "\n",
      "------------\n",
      "Method name: _check_indexing_method\n",
      "Method definition:     @final\n",
      "    def _check_indexing_method(\n",
      "        self,\n",
      "        method: str_t | None,\n",
      "        limit: int | None = None,\n",
      "        tolerance=None,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        Raise if we have a get_indexer `method` that is not supported or valid.\n",
      "        \"\"\"\n",
      "        if method not in [None, \"bfill\", \"backfill\", \"pad\", \"ffill\", \"nearest\"]:\n",
      "            # in practice the clean_reindex_fill_method call would raise\n",
      "            #  before we get here\n",
      "            raise ValueError(\"Invalid fill method\")  # pragma: no cover\n",
      "\n",
      "        if self._is_multi:\n",
      "            if method == \"nearest\":\n",
      "                raise NotImplementedError(\n",
      "                    \"method='nearest' not implemented yet \"\n",
      "                    \"for MultiIndex; see GitHub issue 9365\"\n",
      "                )\n",
      "            elif method == \"pad\" or method == \"backfill\":\n",
      "                if tolerance is not None:\n",
      "                    raise NotImplementedError(\n",
      "                        \"tolerance not implemented yet for MultiIndex\"\n",
      "                    )\n",
      "\n",
      "        if is_interval_dtype(self.dtype) or is_categorical_dtype(self.dtype):\n",
      "            # GH#37871 for now this is only for IntervalIndex and CategoricalIndex\n",
      "            if method is not None:\n",
      "                raise NotImplementedError(\n",
      "                    f\"method {method} not yet implemented for {type(self).__name__}\"\n",
      "                )\n",
      "\n",
      "        if method is None:\n",
      "            if tolerance is not None:\n",
      "                raise ValueError(\n",
      "                    \"tolerance argument only valid if doing pad, \"\n",
      "                    \"backfill or nearest reindexing\"\n",
      "                )\n",
      "            if limit is not None:\n",
      "                raise ValueError(\n",
      "                    \"limit argument only valid if doing pad, \"\n",
      "                    \"backfill or nearest reindexing\"\n",
      "                )\n",
      "\n",
      "------------\n",
      "Method name: _cleanup\n",
      "Method definition:     @final\n",
      "    def _cleanup(self) -> None:\n",
      "        self._engine.clear_mapping()\n",
      "\n",
      "------------\n",
      "Method name: _cmp_method\n",
      "Method definition:     def _cmp_method(self, other, op):\n",
      "        \"\"\"\n",
      "        Wrapper used to dispatch comparison operations.\n",
      "        \"\"\"\n",
      "        if self.is_(other):\n",
      "            # fastpath\n",
      "            if op in {operator.eq, operator.le, operator.ge}:\n",
      "                arr = np.ones(len(self), dtype=bool)\n",
      "                if self._can_hold_na and not isinstance(self, ABCMultiIndex):\n",
      "                    # TODO: should set MultiIndex._can_hold_na = False?\n",
      "                    arr[self.isna()] = False\n",
      "                return arr\n",
      "            elif op is operator.ne:\n",
      "                arr = np.zeros(len(self), dtype=bool)\n",
      "                if self._can_hold_na and not isinstance(self, ABCMultiIndex):\n",
      "                    arr[self.isna()] = True\n",
      "                return arr\n",
      "\n",
      "        if isinstance(other, (np.ndarray, Index, ABCSeries, ExtensionArray)) and len(\n",
      "            self\n",
      "        ) != len(other):\n",
      "            raise ValueError(\"Lengths must match to compare\")\n",
      "\n",
      "        if not isinstance(other, ABCMultiIndex):\n",
      "            other = extract_array(other, extract_numpy=True)\n",
      "        else:\n",
      "            other = np.asarray(other)\n",
      "\n",
      "        if is_object_dtype(self.dtype) and isinstance(other, ExtensionArray):\n",
      "            # e.g. PeriodArray, Categorical\n",
      "            with np.errstate(all=\"ignore\"):\n",
      "                result = op(self._values, other)\n",
      "\n",
      "        elif isinstance(self._values, ExtensionArray):\n",
      "            result = op(self._values, other)\n",
      "\n",
      "        elif is_object_dtype(self.dtype) and not isinstance(self, ABCMultiIndex):\n",
      "            # don't pass MultiIndex\n",
      "            with np.errstate(all=\"ignore\"):\n",
      "                result = ops.comp_method_OBJECT_ARRAY(op, self._values, other)\n",
      "\n",
      "        else:\n",
      "            with np.errstate(all=\"ignore\"):\n",
      "                result = ops.comparison_op(self._values, other, op)\n",
      "\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _concat\n",
      "Method definition:     def _concat(self, to_concat: list[Index], name: Hashable) -> Index:\n",
      "        \"\"\"\n",
      "        Concatenate multiple Index objects.\n",
      "        \"\"\"\n",
      "        to_concat_vals = [x._values for x in to_concat]\n",
      "\n",
      "        result = concat_compat(to_concat_vals)\n",
      "\n",
      "        is_numeric = result.dtype.kind in [\"i\", \"u\", \"f\"]\n",
      "        if self._is_backward_compat_public_numeric_index and is_numeric:\n",
      "            return type(self)._simple_new(result, name=name)\n",
      "\n",
      "        return Index._with_infer(result, name=name)\n",
      "\n",
      "------------\n",
      "Method name: _construct_result\n",
      "Method definition:     def _construct_result(self, result, name):\n",
      "        if isinstance(result, tuple):\n",
      "            return (\n",
      "                Index._with_infer(result[0], name=name),\n",
      "                Index._with_infer(result[1], name=name),\n",
      "            )\n",
      "        return Index._with_infer(result, name=name)\n",
      "\n",
      "------------\n",
      "Method name: _convert_can_do_setop\n",
      "Method definition:     def _convert_can_do_setop(self, other) -> tuple[Index, Hashable]:\n",
      "        if not isinstance(other, Index):\n",
      "            # TODO(2.0): no need to special-case here once _with_infer\n",
      "            #  deprecation is enforced\n",
      "            if hasattr(other, \"dtype\"):\n",
      "                other = Index(other, name=self.name, dtype=other.dtype)\n",
      "            else:\n",
      "                # e.g. list\n",
      "                other = Index(other, name=self.name)\n",
      "            result_name = self.name\n",
      "        else:\n",
      "            result_name = get_op_result_name(self, other)\n",
      "        return other, result_name\n",
      "\n",
      "------------\n",
      "Method name: _convert_slice_indexer\n",
      "Method definition:     def _convert_slice_indexer(self, key: slice, kind: str_t):\n",
      "        \"\"\"\n",
      "        Convert a slice indexer.\n",
      "\n",
      "        By definition, these are labels unless 'iloc' is passed in.\n",
      "        Floats are not allowed as the start, step, or stop of the slice.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        key : label of the slice bound\n",
      "        kind : {'loc', 'getitem'}\n",
      "        \"\"\"\n",
      "        assert kind in [\"loc\", \"getitem\"], kind\n",
      "\n",
      "        # potentially cast the bounds to integers\n",
      "        start, stop, step = key.start, key.stop, key.step\n",
      "\n",
      "        # figure out if this is a positional indexer\n",
      "        def is_int(v):\n",
      "            return v is None or is_integer(v)\n",
      "\n",
      "        is_index_slice = is_int(start) and is_int(stop) and is_int(step)\n",
      "\n",
      "        # special case for interval_dtype bc we do not do partial-indexing\n",
      "        #  on integer Intervals when slicing\n",
      "        # TODO: write this in terms of e.g. should_partial_index?\n",
      "        ints_are_positional = self._should_fallback_to_positional or is_interval_dtype(\n",
      "            self.dtype\n",
      "        )\n",
      "        is_positional = is_index_slice and ints_are_positional\n",
      "\n",
      "        if kind == \"getitem\":\n",
      "            # called from the getitem slicers, validate that we are in fact integers\n",
      "            if self.is_integer() or is_index_slice:\n",
      "                # Note: these checks are redundant if we know is_index_slice\n",
      "                self._validate_indexer(\"slice\", key.start, \"getitem\")\n",
      "                self._validate_indexer(\"slice\", key.stop, \"getitem\")\n",
      "                self._validate_indexer(\"slice\", key.step, \"getitem\")\n",
      "                return key\n",
      "\n",
      "        # convert the slice to an indexer here\n",
      "\n",
      "        # if we are mixed and have integers\n",
      "        if is_positional:\n",
      "            try:\n",
      "                # Validate start & stop\n",
      "                if start is not None:\n",
      "                    self.get_loc(start)\n",
      "                if stop is not None:\n",
      "                    self.get_loc(stop)\n",
      "                is_positional = False\n",
      "            except KeyError:\n",
      "                pass\n",
      "\n",
      "        if com.is_null_slice(key):\n",
      "            # It doesn't matter if we are positional or label based\n",
      "            indexer = key\n",
      "        elif is_positional:\n",
      "            if kind == \"loc\":\n",
      "                # GH#16121, GH#24612, GH#31810\n",
      "                warnings.warn(\n",
      "                    \"Slicing a positional slice with .loc is not supported, \"\n",
      "                    \"and will raise TypeError in a future version.  \"\n",
      "                    \"Use .loc with labels or .iloc with positions instead.\",\n",
      "                    FutureWarning,\n",
      "                    stacklevel=find_stack_level(),\n",
      "                )\n",
      "            indexer = key\n",
      "        else:\n",
      "            indexer = self.slice_indexer(start, stop, step)\n",
      "\n",
      "        return indexer\n",
      "\n",
      "------------\n",
      "Method name: _convert_tolerance\n",
      "Method definition:     def _convert_tolerance(self, tolerance, target: np.ndarray | Index) -> np.ndarray:\n",
      "        # override this method on subclasses\n",
      "        tolerance = np.asarray(tolerance)\n",
      "        if target.size != tolerance.size and tolerance.size > 1:\n",
      "            raise ValueError(\"list-like tolerance size must match target index size\")\n",
      "        return tolerance\n",
      "\n",
      "------------\n",
      "Method name: _deprecate_dti_setop\n",
      "Method definition:     @final\n",
      "    def _deprecate_dti_setop(self, other: Index, setop: str_t):\n",
      "        \"\"\"\n",
      "        Deprecate setop behavior between timezone-aware DatetimeIndexes with\n",
      "        mismatched timezones.\n",
      "        \"\"\"\n",
      "        # Caller is responsibelf or checking\n",
      "        #  `not is_dtype_equal(self.dtype, other.dtype)`\n",
      "        if (\n",
      "            isinstance(self, ABCDatetimeIndex)\n",
      "            and isinstance(other, ABCDatetimeIndex)\n",
      "            and self.tz is not None\n",
      "            and other.tz is not None\n",
      "        ):\n",
      "            # GH#39328, GH#45357\n",
      "            warnings.warn(\n",
      "                f\"In a future version, the {setop} of DatetimeIndex objects \"\n",
      "                \"with mismatched timezones will cast both to UTC instead of \"\n",
      "                \"object dtype. To retain the old behavior, \"\n",
      "                f\"use `index.astype(object).{setop}(other)`\",\n",
      "                FutureWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "\n",
      "------------\n",
      "Method name: _deprecated_arg\n",
      "Method definition:     @final\n",
      "    def _deprecated_arg(self, value, name: str_t, methodname: str_t) -> None:\n",
      "        \"\"\"\n",
      "        Issue a FutureWarning if the arg/kwarg is not no_default.\n",
      "        \"\"\"\n",
      "        if value is not no_default:\n",
      "            warnings.warn(\n",
      "                f\"'{name}' argument in {methodname} is deprecated \"\n",
      "                \"and will be removed in a future version.  Do not pass it.\",\n",
      "                FutureWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "\n",
      "------------\n",
      "Method name: _difference\n",
      "Method definition:     def _difference(self, other, sort):\n",
      "        # overridden by RangeIndex\n",
      "\n",
      "        this = self.unique()\n",
      "\n",
      "        indexer = this.get_indexer_for(other)\n",
      "        indexer = indexer.take((indexer != -1).nonzero()[0])\n",
      "\n",
      "        label_diff = np.setdiff1d(np.arange(this.size), indexer, assume_unique=True)\n",
      "        the_diff = this._values.take(label_diff)\n",
      "        the_diff = _maybe_try_sort(the_diff, sort)\n",
      "\n",
      "        return the_diff\n",
      "\n",
      "------------\n",
      "Method name: _difference_compat\n",
      "Method definition:     @final\n",
      "    def _difference_compat(\n",
      "        self, target: Index, indexer: npt.NDArray[np.intp]\n",
      "    ) -> ArrayLike:\n",
      "        # Compatibility for PeriodArray, for which __sub__ returns an ndarray[object]\n",
      "        #  of DateOffset objects, which do not support __abs__ (and would be slow\n",
      "        #  if they did)\n",
      "\n",
      "        if isinstance(self.dtype, PeriodDtype):\n",
      "            # Note: we only get here with matching dtypes\n",
      "            own_values = cast(\"PeriodArray\", self._data)._ndarray\n",
      "            target_values = cast(\"PeriodArray\", target._data)._ndarray\n",
      "            diff = own_values[indexer] - target_values\n",
      "        else:\n",
      "            # error: Unsupported left operand type for - (\"ExtensionArray\")\n",
      "            diff = self._values[indexer] - target._values  # type: ignore[operator]\n",
      "        return abs(diff)\n",
      "\n",
      "------------\n",
      "Method name: _dir_additions\n",
      "Method definition:     def _dir_additions(self) -> set[str]:\n",
      "        \"\"\"\n",
      "        Add additional __dir__ for this object.\n",
      "        \"\"\"\n",
      "        return {accessor for accessor in self._accessors if hasattr(self, accessor)}\n",
      "\n",
      "------------\n",
      "Method name: _dir_deletions\n",
      "Method definition:     def _dir_deletions(self) -> set[str]:\n",
      "        \"\"\"\n",
      "        Delete unwanted __dir__ for this object.\n",
      "        \"\"\"\n",
      "        return self._accessors | self._hidden_attrs\n",
      "\n",
      "------------\n",
      "Method name: _drop_level_numbers\n",
      "Method definition:     @final\n",
      "    def _drop_level_numbers(self, levnums: list[int]):\n",
      "        \"\"\"\n",
      "        Drop MultiIndex levels by level _number_, not name.\n",
      "        \"\"\"\n",
      "\n",
      "        if not levnums and not isinstance(self, ABCMultiIndex):\n",
      "            return self\n",
      "        if len(levnums) >= self.nlevels:\n",
      "            raise ValueError(\n",
      "                f\"Cannot remove {len(levnums)} levels from an index with \"\n",
      "                f\"{self.nlevels} levels: at least one level must be left.\"\n",
      "            )\n",
      "        # The two checks above guarantee that here self is a MultiIndex\n",
      "        self = cast(\"MultiIndex\", self)\n",
      "\n",
      "        new_levels = list(self.levels)\n",
      "        new_codes = list(self.codes)\n",
      "        new_names = list(self.names)\n",
      "\n",
      "        for i in levnums:\n",
      "            new_levels.pop(i)\n",
      "            new_codes.pop(i)\n",
      "            new_names.pop(i)\n",
      "\n",
      "        if len(new_levels) == 1:\n",
      "            lev = new_levels[0]\n",
      "\n",
      "            if len(lev) == 0:\n",
      "                # If lev is empty, lev.take will fail GH#42055\n",
      "                if len(new_codes[0]) == 0:\n",
      "                    # GH#45230 preserve RangeIndex here\n",
      "                    #  see test_reset_index_empty_rangeindex\n",
      "                    result = lev[:0]\n",
      "                else:\n",
      "                    res_values = algos.take(lev._values, new_codes[0], allow_fill=True)\n",
      "                    # _constructor instead of type(lev) for RangeIndex compat GH#35230\n",
      "                    result = lev._constructor._simple_new(res_values, name=new_names[0])\n",
      "            else:\n",
      "                # set nan if needed\n",
      "                mask = new_codes[0] == -1\n",
      "                result = new_levels[0].take(new_codes[0])\n",
      "                if mask.any():\n",
      "                    result = result.putmask(mask, np.nan)\n",
      "\n",
      "                result._name = new_names[0]\n",
      "\n",
      "            return result\n",
      "        else:\n",
      "            from pandas.core.indexes.multi import MultiIndex\n",
      "\n",
      "            return MultiIndex(\n",
      "                levels=new_levels,\n",
      "                codes=new_codes,\n",
      "                names=new_names,\n",
      "                verify_integrity=False,\n",
      "            )\n",
      "\n",
      "------------\n",
      "Method name: _duplicated\n",
      "Method definition:     @final\n",
      "    def _duplicated(\n",
      "        self, keep: Literal[\"first\", \"last\", False] = \"first\"\n",
      "    ) -> npt.NDArray[np.bool_]:\n",
      "        return duplicated(self._values, keep=keep)\n",
      "\n",
      "------------\n",
      "Method name: _filter_indexer_tolerance\n",
      "Method definition:     @final\n",
      "    def _filter_indexer_tolerance(\n",
      "        self,\n",
      "        target: Index,\n",
      "        indexer: npt.NDArray[np.intp],\n",
      "        tolerance,\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "\n",
      "        distance = self._difference_compat(target, indexer)\n",
      "\n",
      "        return np.where(distance <= tolerance, indexer, -1)\n",
      "\n",
      "------------\n",
      "Method name: _find_common_type_compat\n",
      "Method definition:     @final\n",
      "    def _find_common_type_compat(self, target) -> DtypeObj:\n",
      "        \"\"\"\n",
      "        Implementation of find_common_type that adjusts for Index-specific\n",
      "        special cases.\n",
      "        \"\"\"\n",
      "        if is_valid_na_for_dtype(target, self.dtype):\n",
      "            # e.g. setting NA value into IntervalArray[int64]\n",
      "            dtype = ensure_dtype_can_hold_na(self.dtype)\n",
      "            if is_dtype_equal(self.dtype, dtype):\n",
      "                raise NotImplementedError(\n",
      "                    \"This should not be reached. Please report a bug at \"\n",
      "                    \"github.com/pandas-dev/pandas\"\n",
      "                )\n",
      "            return dtype\n",
      "\n",
      "        target_dtype, _ = infer_dtype_from(target, pandas_dtype=True)\n",
      "\n",
      "        # special case: if one dtype is uint64 and the other a signed int, return object\n",
      "        # See https://github.com/pandas-dev/pandas/issues/26778 for discussion\n",
      "        # Now it's:\n",
      "        # * float | [u]int -> float\n",
      "        # * uint64 | signed int  -> object\n",
      "        # We may change union(float | [u]int) to go to object.\n",
      "        if self.dtype == \"uint64\" or target_dtype == \"uint64\":\n",
      "            if is_signed_integer_dtype(self.dtype) or is_signed_integer_dtype(\n",
      "                target_dtype\n",
      "            ):\n",
      "                return _dtype_obj\n",
      "\n",
      "        dtype = find_common_type([self.dtype, target_dtype])\n",
      "        dtype = common_dtype_categorical_compat([self, target], dtype)\n",
      "        return dtype\n",
      "\n",
      "------------\n",
      "Method name: _format_attrs\n",
      "Method definition:     def _format_attrs(self) -> list[tuple[str_t, str_t | int | bool | None]]:\n",
      "        \"\"\"\n",
      "        Return a list of tuples of the (attr,formatted_value).\n",
      "        \"\"\"\n",
      "        attrs: list[tuple[str_t, str_t | int | bool | None]] = []\n",
      "\n",
      "        if not self._is_multi:\n",
      "            attrs.append((\"dtype\", f\"'{self.dtype}'\"))\n",
      "\n",
      "        if self.name is not None:\n",
      "            attrs.append((\"name\", default_pprint(self.name)))\n",
      "        elif self._is_multi and any(x is not None for x in self.names):\n",
      "            attrs.append((\"names\", default_pprint(self.names)))\n",
      "\n",
      "        max_seq_items = get_option(\"display.max_seq_items\") or len(self)\n",
      "        if len(self) > max_seq_items:\n",
      "            attrs.append((\"length\", len(self)))\n",
      "        return attrs\n",
      "\n",
      "------------\n",
      "Method name: _format_data\n",
      "Method definition:     def _format_data(self, name=None) -> str_t:\n",
      "        \"\"\"\n",
      "        Return the formatted data as a unicode string.\n",
      "        \"\"\"\n",
      "        # do we want to justify (only do so for non-objects)\n",
      "        is_justify = True\n",
      "\n",
      "        if self.inferred_type == \"string\":\n",
      "            is_justify = False\n",
      "        elif self.inferred_type == \"categorical\":\n",
      "            self = cast(\"CategoricalIndex\", self)\n",
      "            if is_object_dtype(self.categories):\n",
      "                is_justify = False\n",
      "\n",
      "        return format_object_summary(\n",
      "            self,\n",
      "            self._formatter_func,\n",
      "            is_justify=is_justify,\n",
      "            name=name,\n",
      "            line_break_each_value=self._is_multi,\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: _format_duplicate_message\n",
      "Method definition:     @final\n",
      "    def _format_duplicate_message(self) -> DataFrame:\n",
      "        \"\"\"\n",
      "        Construct the DataFrame for a DuplicateLabelError.\n",
      "\n",
      "        This returns a DataFrame indicating the labels and positions\n",
      "        of duplicates in an index. This should only be called when it's\n",
      "        already known that duplicates are present.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['a', 'b', 'a'])\n",
      "        >>> idx._format_duplicate_message()\n",
      "            positions\n",
      "        label\n",
      "        a        [0, 2]\n",
      "        \"\"\"\n",
      "        from pandas import Series\n",
      "\n",
      "        duplicates = self[self.duplicated(keep=\"first\")].unique()\n",
      "        assert len(duplicates)\n",
      "\n",
      "        out = Series(np.arange(len(self))).groupby(self).agg(list)[duplicates]\n",
      "        if self._is_multi:\n",
      "            # test_format_duplicate_labels_message_multi\n",
      "            # error: \"Type[Index]\" has no attribute \"from_tuples\"  [attr-defined]\n",
      "            out.index = type(self).from_tuples(out.index)  # type: ignore[attr-defined]\n",
      "\n",
      "        if self.nlevels == 1:\n",
      "            out = out.rename_axis(\"label\")\n",
      "        return out.to_frame(name=\"positions\")\n",
      "\n",
      "------------\n",
      "Method name: _format_native_types\n",
      "Method definition:     def _format_native_types(\n",
      "        self, *, na_rep=\"\", quoting=None, **kwargs\n",
      "    ) -> npt.NDArray[np.object_]:\n",
      "        \"\"\"\n",
      "        Actually format specific types of the index.\n",
      "        \"\"\"\n",
      "        mask = isna(self)\n",
      "        if not self.is_object() and not quoting:\n",
      "            values = np.asarray(self).astype(str)\n",
      "        else:\n",
      "            values = np.array(self, dtype=object, copy=True)\n",
      "\n",
      "        values[mask] = na_rep\n",
      "        return values\n",
      "\n",
      "------------\n",
      "Method name: _format_space\n",
      "Method definition:     def _format_space(self) -> str_t:\n",
      "\n",
      "        # using space here controls if the attributes\n",
      "        # are line separated or not (the default)\n",
      "\n",
      "        # max_seq_items = get_option('display.max_seq_items')\n",
      "        # if len(self) > max_seq_items:\n",
      "        #    space = \"\\n%s\" % (' ' * (len(klass) + 1))\n",
      "        return \" \"\n",
      "\n",
      "------------\n",
      "Method name: _format_with_header\n",
      "Method definition:     def _format_with_header(self, header: list[str_t], na_rep: str_t) -> list[str_t]:\n",
      "        from pandas.io.formats.format import format_array\n",
      "\n",
      "        values = self._values\n",
      "\n",
      "        if is_object_dtype(values.dtype):\n",
      "            values = cast(np.ndarray, values)\n",
      "            values = lib.maybe_convert_objects(values, safe=True)\n",
      "\n",
      "            result = [pprint_thing(x, escape_chars=(\"\\t\", \"\\r\", \"\\n\")) for x in values]\n",
      "\n",
      "            # could have nans\n",
      "            mask = is_float_nan(values)\n",
      "            if mask.any():\n",
      "                result_arr = np.array(result)\n",
      "                result_arr[mask] = na_rep\n",
      "                result = result_arr.tolist()\n",
      "        else:\n",
      "            result = trim_front(format_array(values, None, justify=\"left\"))\n",
      "        return header + result\n",
      "\n",
      "------------\n",
      "Method name: _from_join_target\n",
      "Method definition:     def _from_join_target(self, result: np.ndarray) -> ArrayLike:\n",
      "        \"\"\"\n",
      "        Cast the ndarray returned from one of the libjoin.foo_indexer functions\n",
      "        back to type(self)._data.\n",
      "        \"\"\"\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _get_attributes_dict\n",
      "Method definition:     @final\n",
      "    def _get_attributes_dict(self) -> dict[str_t, Any]:\n",
      "        \"\"\"\n",
      "        Return an attributes dict for my class.\n",
      "\n",
      "        Temporarily added back for compatibility issue in dask, see\n",
      "        https://github.com/pandas-dev/pandas/pull/43895\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            \"The Index._get_attributes_dict method is deprecated, and will be \"\n",
      "            \"removed in a future version\",\n",
      "            DeprecationWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        return {k: getattr(self, k, None) for k in self._attributes}\n",
      "\n",
      "------------\n",
      "Method name: _get_default_index_names\n",
      "Method definition:     def _get_default_index_names(\n",
      "        self, names: Hashable | Sequence[Hashable] | None = None, default=None\n",
      "    ) -> list[Hashable]:\n",
      "        \"\"\"\n",
      "        Get names of index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        names : int, str or 1-dimensional list, default None\n",
      "            Index names to set.\n",
      "        default : str\n",
      "            Default name of index.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            if names not str or list-like\n",
      "        \"\"\"\n",
      "        from pandas.core.indexes.multi import MultiIndex\n",
      "\n",
      "        if names is not None:\n",
      "            if isinstance(names, str) or isinstance(names, int):\n",
      "                names = [names]\n",
      "\n",
      "        if not isinstance(names, list) and names is not None:\n",
      "            raise ValueError(\"Index names must be str or 1-dimensional list\")\n",
      "\n",
      "        if not names:\n",
      "            if isinstance(self, MultiIndex):\n",
      "                names = com.fill_missing_names(self.names)\n",
      "            else:\n",
      "                names = [default] if self.name is None else [self.name]\n",
      "\n",
      "        return names\n",
      "\n",
      "------------\n",
      "Method name: _get_engine_target\n",
      "Method definition:     def _get_engine_target(self) -> ArrayLike:\n",
      "        \"\"\"\n",
      "        Get the ndarray or ExtensionArray that we can pass to the IndexEngine\n",
      "        constructor.\n",
      "        \"\"\"\n",
      "        vals = self._values\n",
      "        if isinstance(vals, StringArray):\n",
      "            # GH#45652 much more performant than ExtensionEngine\n",
      "            return vals._ndarray\n",
      "        if type(self) is Index and isinstance(self._values, ExtensionArray):\n",
      "            # TODO(ExtensionIndex): remove special-case, just use self._values\n",
      "            return self._values.astype(object)\n",
      "        return vals\n",
      "\n",
      "------------\n",
      "Method name: _get_fill_indexer\n",
      "Method definition:     @final\n",
      "    def _get_fill_indexer(\n",
      "        self, target: Index, method: str_t, limit: int | None = None, tolerance=None\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "\n",
      "        if self._is_multi:\n",
      "            # TODO: get_indexer_with_fill docstring says values must be _sorted_\n",
      "            #  but that doesn't appear to be enforced\n",
      "            # error: \"IndexEngine\" has no attribute \"get_indexer_with_fill\"\n",
      "            engine = self._engine\n",
      "            return engine.get_indexer_with_fill(  # type: ignore[union-attr]\n",
      "                target=target._values, values=self._values, method=method, limit=limit\n",
      "            )\n",
      "\n",
      "        if self.is_monotonic_increasing and target.is_monotonic_increasing:\n",
      "            target_values = target._get_engine_target()\n",
      "            own_values = self._get_engine_target()\n",
      "            if not isinstance(target_values, np.ndarray) or not isinstance(\n",
      "                own_values, np.ndarray\n",
      "            ):\n",
      "                raise NotImplementedError\n",
      "\n",
      "            if method == \"pad\":\n",
      "                indexer = libalgos.pad(own_values, target_values, limit=limit)\n",
      "            else:\n",
      "                # i.e. \"backfill\"\n",
      "                indexer = libalgos.backfill(own_values, target_values, limit=limit)\n",
      "        else:\n",
      "            indexer = self._get_fill_indexer_searchsorted(target, method, limit)\n",
      "        if tolerance is not None and len(self):\n",
      "            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)\n",
      "        return indexer\n",
      "\n",
      "------------\n",
      "Method name: _get_fill_indexer_searchsorted\n",
      "Method definition:     @final\n",
      "    def _get_fill_indexer_searchsorted(\n",
      "        self, target: Index, method: str_t, limit: int | None = None\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "        \"\"\"\n",
      "        Fallback pad/backfill get_indexer that works for monotonic decreasing\n",
      "        indexes and non-monotonic targets.\n",
      "        \"\"\"\n",
      "        if limit is not None:\n",
      "            raise ValueError(\n",
      "                f\"limit argument for {repr(method)} method only well-defined \"\n",
      "                \"if index and target are monotonic\"\n",
      "            )\n",
      "\n",
      "        side: Literal[\"left\", \"right\"] = \"left\" if method == \"pad\" else \"right\"\n",
      "\n",
      "        # find exact matches first (this simplifies the algorithm)\n",
      "        indexer = self.get_indexer(target)\n",
      "        nonexact = indexer == -1\n",
      "        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact], side)\n",
      "        if side == \"left\":\n",
      "            # searchsorted returns \"indices into a sorted array such that,\n",
      "            # if the corresponding elements in v were inserted before the\n",
      "            # indices, the order of a would be preserved\".\n",
      "            # Thus, we need to subtract 1 to find values to the left.\n",
      "            indexer[nonexact] -= 1\n",
      "            # This also mapped not found values (values of 0 from\n",
      "            # np.searchsorted) to -1, which conveniently is also our\n",
      "            # sentinel for missing values\n",
      "        else:\n",
      "            # Mark indices to the right of the largest value as not found\n",
      "            indexer[indexer == len(self)] = -1\n",
      "        return indexer\n",
      "\n",
      "------------\n",
      "Method name: _get_grouper_for_level\n",
      "Method definition:     def _get_grouper_for_level(\n",
      "        self,\n",
      "        mapper,\n",
      "        *,\n",
      "        level=None,\n",
      "        dropna: bool = True,\n",
      "    ) -> tuple[Index, npt.NDArray[np.signedinteger] | None, Index | None]:\n",
      "        \"\"\"\n",
      "        Get index grouper corresponding to an index level\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        mapper: Group mapping function or None\n",
      "            Function mapping index values to groups\n",
      "        level : int or None\n",
      "            Index level, positional\n",
      "        dropna : bool\n",
      "            dropna from groupby\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        grouper : Index\n",
      "            Index of values to group on.\n",
      "        labels : ndarray of int or None\n",
      "            Array of locations in level_index.\n",
      "        uniques : Index or None\n",
      "            Index of unique values for level.\n",
      "        \"\"\"\n",
      "        assert level is None or level == 0\n",
      "        if mapper is None:\n",
      "            grouper = self\n",
      "        else:\n",
      "            grouper = self.map(mapper)\n",
      "\n",
      "        return grouper, None, None\n",
      "\n",
      "------------\n",
      "Method name: _get_indexer\n",
      "Method definition:     def _get_indexer(\n",
      "        self,\n",
      "        target: Index,\n",
      "        method: str_t | None = None,\n",
      "        limit: int | None = None,\n",
      "        tolerance=None,\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "        if tolerance is not None:\n",
      "            tolerance = self._convert_tolerance(tolerance, target)\n",
      "\n",
      "        if method in [\"pad\", \"backfill\"]:\n",
      "            indexer = self._get_fill_indexer(target, method, limit, tolerance)\n",
      "        elif method == \"nearest\":\n",
      "            indexer = self._get_nearest_indexer(target, limit, tolerance)\n",
      "        else:\n",
      "            if target._is_multi and self._is_multi:\n",
      "                engine = self._engine\n",
      "                # error: Item \"IndexEngine\" of \"Union[IndexEngine, ExtensionEngine]\"\n",
      "                # has no attribute \"_extract_level_codes\"\n",
      "                tgt_values = engine._extract_level_codes(  # type: ignore[union-attr]\n",
      "                    target\n",
      "                )\n",
      "            else:\n",
      "                tgt_values = target._get_engine_target()\n",
      "\n",
      "            indexer = self._engine.get_indexer(tgt_values)\n",
      "\n",
      "        return ensure_platform_int(indexer)\n",
      "\n",
      "------------\n",
      "Method name: _get_indexer_non_comparable\n",
      "Method definition:     @final\n",
      "    def _get_indexer_non_comparable(\n",
      "        self, target: Index, method, unique: bool = True\n",
      "    ) -> npt.NDArray[np.intp] | tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
      "        \"\"\"\n",
      "        Called from get_indexer or get_indexer_non_unique when the target\n",
      "        is of a non-comparable dtype.\n",
      "\n",
      "        For get_indexer lookups with method=None, get_indexer is an _equality_\n",
      "        check, so non-comparable dtypes mean we will always have no matches.\n",
      "\n",
      "        For get_indexer lookups with a method, get_indexer is an _inequality_\n",
      "        check, so non-comparable dtypes mean we will always raise TypeError.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        target : Index\n",
      "        method : str or None\n",
      "        unique : bool, default True\n",
      "            * True if called from get_indexer.\n",
      "            * False if called from get_indexer_non_unique.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If doing an inequality check, i.e. method is not None.\n",
      "        \"\"\"\n",
      "        if method is not None:\n",
      "            other = unpack_nested_dtype(target)\n",
      "            raise TypeError(f\"Cannot compare dtypes {self.dtype} and {other.dtype}\")\n",
      "\n",
      "        no_matches = -1 * np.ones(target.shape, dtype=np.intp)\n",
      "        if unique:\n",
      "            # This is for get_indexer\n",
      "            return no_matches\n",
      "        else:\n",
      "            # This is for get_indexer_non_unique\n",
      "            missing = np.arange(len(target), dtype=np.intp)\n",
      "            return no_matches, missing\n",
      "\n",
      "------------\n",
      "Method name: _get_indexer_strict\n",
      "Method definition:     def _get_indexer_strict(self, key, axis_name: str_t) -> tuple[Index, np.ndarray]:\n",
      "        \"\"\"\n",
      "        Analogue to get_indexer that raises if any elements are missing.\n",
      "        \"\"\"\n",
      "        keyarr = key\n",
      "        if not isinstance(keyarr, Index):\n",
      "            keyarr = com.asarray_tuplesafe(keyarr)\n",
      "\n",
      "        if self._index_as_unique:\n",
      "            indexer = self.get_indexer_for(keyarr)\n",
      "            keyarr = self.reindex(keyarr)[0]\n",
      "        else:\n",
      "            keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr)\n",
      "\n",
      "        self._raise_if_missing(keyarr, indexer, axis_name)\n",
      "\n",
      "        keyarr = self.take(indexer)\n",
      "        if isinstance(key, Index):\n",
      "            # GH 42790 - Preserve name from an Index\n",
      "            keyarr.name = key.name\n",
      "        if keyarr.dtype.kind in [\"m\", \"M\"]:\n",
      "            # DTI/TDI.take can infer a freq in some cases when we dont want one\n",
      "            if isinstance(key, list) or (\n",
      "                isinstance(key, type(self))\n",
      "                # \"Index\" has no attribute \"freq\"\n",
      "                and key.freq is None  # type: ignore[attr-defined]\n",
      "            ):\n",
      "                keyarr = keyarr._with_freq(None)\n",
      "\n",
      "        return keyarr, indexer\n",
      "\n",
      "------------\n",
      "Method name: _get_level_names\n",
      "Method definition:     @final\n",
      "    def _get_level_names(self) -> Hashable | Sequence[Hashable]:\n",
      "        \"\"\"\n",
      "        Return a name or list of names with None replaced by the level number.\n",
      "        \"\"\"\n",
      "        if self._is_multi:\n",
      "            return [\n",
      "                level if name is None else name for level, name in enumerate(self.names)\n",
      "            ]\n",
      "        else:\n",
      "            return 0 if self.name is None else self.name\n",
      "\n",
      "------------\n",
      "Method name: _get_level_number\n",
      "Method definition:     def _get_level_number(self, level) -> int:\n",
      "        self._validate_index_level(level)\n",
      "        return 0\n",
      "\n",
      "------------\n",
      "Method name: _get_level_values\n",
      "Method definition:     def _get_level_values(self, level) -> Index:\n",
      "        \"\"\"\n",
      "        Return an Index of values for requested level.\n",
      "\n",
      "        This is primarily useful to get an individual level of values from a\n",
      "        MultiIndex, but is provided on Index as well for compatibility.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        level : int or str\n",
      "            It is either the integer position or the name of the level.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "            Calling object, as there is only one level in the Index.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        For Index, level should be 0, since there are no multiple levels.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(list('abc'))\n",
      "        >>> idx\n",
      "        Index(['a', 'b', 'c'], dtype='object')\n",
      "\n",
      "        Get level values by supplying `level` as integer:\n",
      "\n",
      "        >>> idx.get_level_values(0)\n",
      "        Index(['a', 'b', 'c'], dtype='object')\n",
      "        \"\"\"\n",
      "        self._validate_index_level(level)\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: _get_names\n",
      "Method definition:     def _get_names(self) -> FrozenList:\n",
      "        return FrozenList((self.name,))\n",
      "\n",
      "------------\n",
      "Method name: _get_nearest_indexer\n",
      "Method definition:     @final\n",
      "    def _get_nearest_indexer(\n",
      "        self, target: Index, limit: int | None, tolerance\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "        \"\"\"\n",
      "        Get the indexer for the nearest index labels; requires an index with\n",
      "        values that can be subtracted from each other (e.g., not strings or\n",
      "        tuples).\n",
      "        \"\"\"\n",
      "        if not len(self):\n",
      "            return self._get_fill_indexer(target, \"pad\")\n",
      "\n",
      "        left_indexer = self.get_indexer(target, \"pad\", limit=limit)\n",
      "        right_indexer = self.get_indexer(target, \"backfill\", limit=limit)\n",
      "\n",
      "        left_distances = self._difference_compat(target, left_indexer)\n",
      "        right_distances = self._difference_compat(target, right_indexer)\n",
      "\n",
      "        op = operator.lt if self.is_monotonic_increasing else operator.le\n",
      "        indexer = np.where(\n",
      "            # error: Argument 1&2 has incompatible type \"Union[ExtensionArray,\n",
      "            # ndarray[Any, Any]]\"; expected \"Union[SupportsDunderLE,\n",
      "            # SupportsDunderGE, SupportsDunderGT, SupportsDunderLT]\"\n",
      "            op(left_distances, right_distances)  # type: ignore[arg-type]\n",
      "            | (right_indexer == -1),\n",
      "            left_indexer,\n",
      "            right_indexer,\n",
      "        )\n",
      "        if tolerance is not None:\n",
      "            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)\n",
      "        return indexer\n",
      "\n",
      "------------\n",
      "Method name: _get_reconciled_name_object\n",
      "Method definition:     def _get_reconciled_name_object(self, other):\n",
      "        \"\"\"\n",
      "        If the result of a set operation will be self,\n",
      "        return self, unless the name changes, in which\n",
      "        case make a shallow copy of self.\n",
      "        \"\"\"\n",
      "        name = get_op_result_name(self, other)\n",
      "        if self.name is not name:\n",
      "            return self.rename(name)\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: _get_string_slice\n",
      "Method definition:     def _get_string_slice(self, key: str_t):\n",
      "        # this is for partial string indexing,\n",
      "        # overridden in DatetimeIndex, TimedeltaIndex and PeriodIndex\n",
      "        raise NotImplementedError\n",
      "\n",
      "------------\n",
      "Method name: _get_values_for_loc\n",
      "Method definition:     def _get_values_for_loc(self, series: Series, loc, key):\n",
      "        \"\"\"\n",
      "        Do a positional lookup on the given Series, returning either a scalar\n",
      "        or a Series.\n",
      "\n",
      "        Assumes that `series.index is self`\n",
      "\n",
      "        key is included for MultiIndex compat.\n",
      "        \"\"\"\n",
      "        if is_integer(loc):\n",
      "            return series._values[loc]\n",
      "\n",
      "        return series.iloc[loc]\n",
      "\n",
      "------------\n",
      "Method name: _getitem_slice\n",
      "Method definition:     def _getitem_slice(self: _IndexT, slobj: slice) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Fastpath for __getitem__ when we know we have a slice.\n",
      "        \"\"\"\n",
      "        res = self._data[slobj]\n",
      "        return type(self)._simple_new(res, name=self._name)\n",
      "\n",
      "------------\n",
      "Method name: _inner_indexer\n",
      "Method definition:     @final\n",
      "    def _inner_indexer(\n",
      "        self: _IndexT, other: _IndexT\n",
      "    ) -> tuple[ArrayLike, npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
      "        # Caller is responsible for ensuring other.dtype == self.dtype\n",
      "        sv = self._get_engine_target()\n",
      "        ov = other._get_engine_target()\n",
      "        # can_use_libjoin assures sv and ov are ndarrays\n",
      "        sv = cast(np.ndarray, sv)\n",
      "        ov = cast(np.ndarray, ov)\n",
      "        joined_ndarray, lidx, ridx = libjoin.inner_join_indexer(sv, ov)\n",
      "        joined = self._from_join_target(joined_ndarray)\n",
      "        return joined, lidx, ridx\n",
      "\n",
      "------------\n",
      "Method name: _intersection\n",
      "Method definition:     def _intersection(self, other: Index, sort=False):\n",
      "        \"\"\"\n",
      "        intersection specialized to the case with matching dtypes.\n",
      "        \"\"\"\n",
      "        if (\n",
      "            self.is_monotonic_increasing\n",
      "            and other.is_monotonic_increasing\n",
      "            and self._can_use_libjoin\n",
      "        ):\n",
      "            try:\n",
      "                result = self._inner_indexer(other)[0]\n",
      "            except TypeError:\n",
      "                # non-comparable; should only be for object dtype\n",
      "                pass\n",
      "            else:\n",
      "                # TODO: algos.unique1d should preserve DTA/TDA\n",
      "                res = algos.unique1d(result)\n",
      "                return ensure_wrapped_if_datetimelike(res)\n",
      "\n",
      "        res_values = self._intersection_via_get_indexer(other, sort=sort)\n",
      "        res_values = _maybe_try_sort(res_values, sort)\n",
      "        return res_values\n",
      "\n",
      "------------\n",
      "Method name: _intersection_via_get_indexer\n",
      "Method definition:     @final\n",
      "    def _intersection_via_get_indexer(self, other: Index, sort) -> ArrayLike:\n",
      "        \"\"\"\n",
      "        Find the intersection of two Indexes using get_indexer.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        np.ndarray or ExtensionArray\n",
      "            The returned array will be unique.\n",
      "        \"\"\"\n",
      "        left_unique = self.unique()\n",
      "        right_unique = other.unique()\n",
      "\n",
      "        # even though we are unique, we need get_indexer_for for IntervalIndex\n",
      "        indexer = left_unique.get_indexer_for(right_unique)\n",
      "\n",
      "        mask = indexer != -1\n",
      "\n",
      "        taker = indexer.take(mask.nonzero()[0])\n",
      "        if sort is False:\n",
      "            # sort bc we want the elements in the same order they are in self\n",
      "            # unnecessary in the case with sort=None bc we will sort later\n",
      "            taker = np.sort(taker)\n",
      "\n",
      "        result = left_unique.take(taker)._values\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _invalid_indexer\n",
      "Method definition:     @final\n",
      "    def _invalid_indexer(self, form: str_t, key) -> TypeError:\n",
      "        \"\"\"\n",
      "        Consistent invalid indexer message.\n",
      "        \"\"\"\n",
      "        return TypeError(\n",
      "            f\"cannot do {form} indexing on {type(self).__name__} with these \"\n",
      "            f\"indexers [{key}] of type {type(key).__name__}\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: _is_comparable_dtype\n",
      "Method definition:     def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:\n",
      "        \"\"\"\n",
      "        Can we compare values of the given dtype to our own?\n",
      "        \"\"\"\n",
      "        if self.dtype.kind == \"b\":\n",
      "            return dtype.kind == \"b\"\n",
      "        elif is_numeric_dtype(self.dtype):\n",
      "            return is_numeric_dtype(dtype)\n",
      "        return True\n",
      "\n",
      "------------\n",
      "Method name: _is_memory_usage_qualified\n",
      "Method definition:     def _is_memory_usage_qualified(self) -> bool:\n",
      "        \"\"\"\n",
      "        Return a boolean if we need a qualified .info display.\n",
      "        \"\"\"\n",
      "        return self.is_object()\n",
      "\n",
      "------------\n",
      "Method name: _join_level\n",
      "Method definition:     @final\n",
      "    def _join_level(\n",
      "        self, other: Index, level, how: str_t = \"left\", keep_order: bool = True\n",
      "    ) -> tuple[MultiIndex, npt.NDArray[np.intp] | None, npt.NDArray[np.intp] | None]:\n",
      "        \"\"\"\n",
      "        The join method *only* affects the level of the resulting\n",
      "        MultiIndex. Otherwise it just exactly aligns the Index data to the\n",
      "        labels of the level in the MultiIndex.\n",
      "\n",
      "        If ```keep_order == True```, the order of the data indexed by the\n",
      "        MultiIndex will not be changed; otherwise, it will tie out\n",
      "        with `other`.\n",
      "        \"\"\"\n",
      "        from pandas.core.indexes.multi import MultiIndex\n",
      "\n",
      "        def _get_leaf_sorter(labels: list[np.ndarray]) -> npt.NDArray[np.intp]:\n",
      "            \"\"\"\n",
      "            Returns sorter for the inner most level while preserving the\n",
      "            order of higher levels.\n",
      "\n",
      "            Parameters\n",
      "            ----------\n",
      "            labels : list[np.ndarray]\n",
      "                Each ndarray has signed integer dtype, not necessarily identical.\n",
      "\n",
      "            Returns\n",
      "            -------\n",
      "            np.ndarray[np.intp]\n",
      "            \"\"\"\n",
      "            if labels[0].size == 0:\n",
      "                return np.empty(0, dtype=np.intp)\n",
      "\n",
      "            if len(labels) == 1:\n",
      "                return get_group_index_sorter(ensure_platform_int(labels[0]))\n",
      "\n",
      "            # find indexers of beginning of each set of\n",
      "            # same-key labels w.r.t all but last level\n",
      "            tic = labels[0][:-1] != labels[0][1:]\n",
      "            for lab in labels[1:-1]:\n",
      "                tic |= lab[:-1] != lab[1:]\n",
      "\n",
      "            starts = np.hstack(([True], tic, [True])).nonzero()[0]\n",
      "            lab = ensure_int64(labels[-1])\n",
      "            return lib.get_level_sorter(lab, ensure_platform_int(starts))\n",
      "\n",
      "        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n",
      "            raise TypeError(\"Join on level between two MultiIndex objects is ambiguous\")\n",
      "\n",
      "        left, right = self, other\n",
      "\n",
      "        flip_order = not isinstance(self, MultiIndex)\n",
      "        if flip_order:\n",
      "            left, right = right, left\n",
      "            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n",
      "\n",
      "        assert isinstance(left, MultiIndex)\n",
      "\n",
      "        level = left._get_level_number(level)\n",
      "        old_level = left.levels[level]\n",
      "\n",
      "        if not right.is_unique:\n",
      "            raise NotImplementedError(\n",
      "                \"Index._join_level on non-unique index is not implemented\"\n",
      "            )\n",
      "\n",
      "        new_level, left_lev_indexer, right_lev_indexer = old_level.join(\n",
      "            right, how=how, return_indexers=True\n",
      "        )\n",
      "\n",
      "        if left_lev_indexer is None:\n",
      "            if keep_order or len(left) == 0:\n",
      "                left_indexer = None\n",
      "                join_index = left\n",
      "            else:  # sort the leaves\n",
      "                left_indexer = _get_leaf_sorter(left.codes[: level + 1])\n",
      "                join_index = left[left_indexer]\n",
      "\n",
      "        else:\n",
      "            left_lev_indexer = ensure_platform_int(left_lev_indexer)\n",
      "            rev_indexer = lib.get_reverse_indexer(left_lev_indexer, len(old_level))\n",
      "            old_codes = left.codes[level]\n",
      "\n",
      "            taker = old_codes[old_codes != -1]\n",
      "            new_lev_codes = rev_indexer.take(taker)\n",
      "\n",
      "            new_codes = list(left.codes)\n",
      "            new_codes[level] = new_lev_codes\n",
      "\n",
      "            new_levels = list(left.levels)\n",
      "            new_levels[level] = new_level\n",
      "\n",
      "            if keep_order:  # just drop missing values. o.w. keep order\n",
      "                left_indexer = np.arange(len(left), dtype=np.intp)\n",
      "                left_indexer = cast(np.ndarray, left_indexer)\n",
      "                mask = new_lev_codes != -1\n",
      "                if not mask.all():\n",
      "                    new_codes = [lab[mask] for lab in new_codes]\n",
      "                    left_indexer = left_indexer[mask]\n",
      "\n",
      "            else:  # tie out the order with other\n",
      "                if level == 0:  # outer most level, take the fast route\n",
      "                    max_new_lev = 0 if len(new_lev_codes) == 0 else new_lev_codes.max()\n",
      "                    ngroups = 1 + max_new_lev\n",
      "                    left_indexer, counts = libalgos.groupsort_indexer(\n",
      "                        new_lev_codes, ngroups\n",
      "                    )\n",
      "\n",
      "                    # missing values are placed first; drop them!\n",
      "                    left_indexer = left_indexer[counts[0] :]\n",
      "                    new_codes = [lab[left_indexer] for lab in new_codes]\n",
      "\n",
      "                else:  # sort the leaves\n",
      "                    mask = new_lev_codes != -1\n",
      "                    mask_all = mask.all()\n",
      "                    if not mask_all:\n",
      "                        new_codes = [lab[mask] for lab in new_codes]\n",
      "\n",
      "                    left_indexer = _get_leaf_sorter(new_codes[: level + 1])\n",
      "                    new_codes = [lab[left_indexer] for lab in new_codes]\n",
      "\n",
      "                    # left_indexers are w.r.t masked frame.\n",
      "                    # reverse to original frame!\n",
      "                    if not mask_all:\n",
      "                        left_indexer = mask.nonzero()[0][left_indexer]\n",
      "\n",
      "            join_index = MultiIndex(\n",
      "                levels=new_levels,\n",
      "                codes=new_codes,\n",
      "                names=left.names,\n",
      "                verify_integrity=False,\n",
      "            )\n",
      "\n",
      "        if right_lev_indexer is not None:\n",
      "            right_indexer = right_lev_indexer.take(join_index.codes[level])\n",
      "        else:\n",
      "            right_indexer = join_index.codes[level]\n",
      "\n",
      "        if flip_order:\n",
      "            left_indexer, right_indexer = right_indexer, left_indexer\n",
      "\n",
      "        left_indexer = (\n",
      "            None if left_indexer is None else ensure_platform_int(left_indexer)\n",
      "        )\n",
      "        right_indexer = (\n",
      "            None if right_indexer is None else ensure_platform_int(right_indexer)\n",
      "        )\n",
      "        return join_index, left_indexer, right_indexer\n",
      "\n",
      "------------\n",
      "Method name: _join_monotonic\n",
      "Method definition:     @final\n",
      "    def _join_monotonic(\n",
      "        self, other: Index, how: str_t = \"left\"\n",
      "    ) -> tuple[Index, npt.NDArray[np.intp] | None, npt.NDArray[np.intp] | None]:\n",
      "        # We only get here with matching dtypes and both monotonic increasing\n",
      "        assert other.dtype == self.dtype\n",
      "\n",
      "        if self.equals(other):\n",
      "            ret_index = other if how == \"right\" else self\n",
      "            return ret_index, None, None\n",
      "\n",
      "        ridx: np.ndarray | None\n",
      "        lidx: np.ndarray | None\n",
      "\n",
      "        if self.is_unique and other.is_unique:\n",
      "            # We can perform much better than the general case\n",
      "            if how == \"left\":\n",
      "                join_index = self\n",
      "                lidx = None\n",
      "                ridx = self._left_indexer_unique(other)\n",
      "            elif how == \"right\":\n",
      "                join_index = other\n",
      "                lidx = other._left_indexer_unique(self)\n",
      "                ridx = None\n",
      "            elif how == \"inner\":\n",
      "                join_array, lidx, ridx = self._inner_indexer(other)\n",
      "                join_index = self._wrap_joined_index(join_array, other)\n",
      "            elif how == \"outer\":\n",
      "                join_array, lidx, ridx = self._outer_indexer(other)\n",
      "                join_index = self._wrap_joined_index(join_array, other)\n",
      "        else:\n",
      "            if how == \"left\":\n",
      "                join_array, lidx, ridx = self._left_indexer(other)\n",
      "            elif how == \"right\":\n",
      "                join_array, ridx, lidx = other._left_indexer(self)\n",
      "            elif how == \"inner\":\n",
      "                join_array, lidx, ridx = self._inner_indexer(other)\n",
      "            elif how == \"outer\":\n",
      "                join_array, lidx, ridx = self._outer_indexer(other)\n",
      "\n",
      "            join_index = self._wrap_joined_index(join_array, other)\n",
      "\n",
      "        lidx = None if lidx is None else ensure_platform_int(lidx)\n",
      "        ridx = None if ridx is None else ensure_platform_int(ridx)\n",
      "        return join_index, lidx, ridx\n",
      "\n",
      "------------\n",
      "Method name: _join_multi\n",
      "Method definition:     @final\n",
      "    def _join_multi(self, other: Index, how: str_t):\n",
      "        from pandas.core.indexes.multi import MultiIndex\n",
      "        from pandas.core.reshape.merge import restore_dropped_levels_multijoin\n",
      "\n",
      "        # figure out join names\n",
      "        self_names_list = list(com.not_none(*self.names))\n",
      "        other_names_list = list(com.not_none(*other.names))\n",
      "        self_names_order = self_names_list.index\n",
      "        other_names_order = other_names_list.index\n",
      "        self_names = set(self_names_list)\n",
      "        other_names = set(other_names_list)\n",
      "        overlap = self_names & other_names\n",
      "\n",
      "        # need at least 1 in common\n",
      "        if not overlap:\n",
      "            raise ValueError(\"cannot join with no overlapping index names\")\n",
      "\n",
      "        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):\n",
      "\n",
      "            # Drop the non-matching levels from left and right respectively\n",
      "            ldrop_names = sorted(self_names - overlap, key=self_names_order)\n",
      "            rdrop_names = sorted(other_names - overlap, key=other_names_order)\n",
      "\n",
      "            # if only the order differs\n",
      "            if not len(ldrop_names + rdrop_names):\n",
      "                self_jnlevels = self\n",
      "                other_jnlevels = other.reorder_levels(self.names)\n",
      "            else:\n",
      "                self_jnlevels = self.droplevel(ldrop_names)\n",
      "                other_jnlevels = other.droplevel(rdrop_names)\n",
      "\n",
      "            # Join left and right\n",
      "            # Join on same leveled multi-index frames is supported\n",
      "            join_idx, lidx, ridx = self_jnlevels.join(\n",
      "                other_jnlevels, how=how, return_indexers=True\n",
      "            )\n",
      "\n",
      "            # Restore the dropped levels\n",
      "            # Returned index level order is\n",
      "            # common levels, ldrop_names, rdrop_names\n",
      "            dropped_names = ldrop_names + rdrop_names\n",
      "\n",
      "            # error: Argument 5/6 to \"restore_dropped_levels_multijoin\" has\n",
      "            # incompatible type \"Optional[ndarray[Any, dtype[signedinteger[Any\n",
      "            # ]]]]\"; expected \"ndarray[Any, dtype[signedinteger[Any]]]\"\n",
      "            levels, codes, names = restore_dropped_levels_multijoin(\n",
      "                self,\n",
      "                other,\n",
      "                dropped_names,\n",
      "                join_idx,\n",
      "                lidx,  # type: ignore[arg-type]\n",
      "                ridx,  # type: ignore[arg-type]\n",
      "            )\n",
      "\n",
      "            # Re-create the multi-index\n",
      "            multi_join_idx = MultiIndex(\n",
      "                levels=levels, codes=codes, names=names, verify_integrity=False\n",
      "            )\n",
      "\n",
      "            multi_join_idx = multi_join_idx.remove_unused_levels()\n",
      "\n",
      "            return multi_join_idx, lidx, ridx\n",
      "\n",
      "        jl = list(overlap)[0]\n",
      "\n",
      "        # Case where only one index is multi\n",
      "        # make the indices into mi's that match\n",
      "        flip_order = False\n",
      "        if isinstance(self, MultiIndex):\n",
      "            self, other = other, self\n",
      "            flip_order = True\n",
      "            # flip if join method is right or left\n",
      "            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n",
      "\n",
      "        level = other.names.index(jl)\n",
      "        result = self._join_level(other, level, how=how)\n",
      "\n",
      "        if flip_order:\n",
      "            return result[0], result[2], result[1]\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _join_non_unique\n",
      "Method definition:     @final\n",
      "    def _join_non_unique(\n",
      "        self, other: Index, how: str_t = \"left\"\n",
      "    ) -> tuple[Index, npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
      "        from pandas.core.reshape.merge import get_join_indexers\n",
      "\n",
      "        # We only get here if dtypes match\n",
      "        assert self.dtype == other.dtype\n",
      "\n",
      "        left_idx, right_idx = get_join_indexers(\n",
      "            [self._values], [other._values], how=how, sort=True\n",
      "        )\n",
      "        mask = left_idx == -1\n",
      "\n",
      "        join_array = self._values.take(left_idx)\n",
      "        right = other._values.take(right_idx)\n",
      "\n",
      "        if isinstance(join_array, np.ndarray):\n",
      "            # error: Argument 3 to \"putmask\" has incompatible type\n",
      "            # \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected\n",
      "            # \"Union[_SupportsArray[dtype[Any]], _NestedSequence[\n",
      "            # _SupportsArray[dtype[Any]]], bool, int, float, complex,\n",
      "            # str, bytes, _NestedSequence[Union[bool, int, float,\n",
      "            # complex, str, bytes]]]\"\n",
      "            np.putmask(join_array, mask, right)  # type: ignore[arg-type]\n",
      "        else:\n",
      "            join_array._putmask(mask, right)\n",
      "\n",
      "        join_index = self._wrap_joined_index(join_array, other)\n",
      "\n",
      "        return join_index, left_idx, right_idx\n",
      "\n",
      "------------\n",
      "Method name: _join_via_get_indexer\n",
      "Method definition:     @final\n",
      "    def _join_via_get_indexer(\n",
      "        self, other: Index, how: str_t, sort: bool\n",
      "    ) -> tuple[Index, npt.NDArray[np.intp] | None, npt.NDArray[np.intp] | None]:\n",
      "        # Fallback if we do not have any fastpaths available based on\n",
      "        #  uniqueness/monotonicity\n",
      "\n",
      "        # Note: at this point we have checked matching dtypes\n",
      "\n",
      "        if how == \"left\":\n",
      "            join_index = self\n",
      "        elif how == \"right\":\n",
      "            join_index = other\n",
      "        elif how == \"inner\":\n",
      "            # TODO: sort=False here for backwards compat. It may\n",
      "            # be better to use the sort parameter passed into join\n",
      "            join_index = self.intersection(other, sort=False)\n",
      "        elif how == \"outer\":\n",
      "            # TODO: sort=True here for backwards compat. It may\n",
      "            # be better to use the sort parameter passed into join\n",
      "            join_index = self.union(other)\n",
      "\n",
      "        if sort:\n",
      "            join_index = join_index.sort_values()\n",
      "\n",
      "        if join_index is self:\n",
      "            lindexer = None\n",
      "        else:\n",
      "            lindexer = self.get_indexer_for(join_index)\n",
      "        if join_index is other:\n",
      "            rindexer = None\n",
      "        else:\n",
      "            rindexer = other.get_indexer_for(join_index)\n",
      "        return join_index, lindexer, rindexer\n",
      "\n",
      "------------\n",
      "Method name: _left_indexer\n",
      "Method definition:     @final\n",
      "    def _left_indexer(\n",
      "        self: _IndexT, other: _IndexT\n",
      "    ) -> tuple[ArrayLike, npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
      "        # Caller is responsible for ensuring other.dtype == self.dtype\n",
      "        sv = self._get_engine_target()\n",
      "        ov = other._get_engine_target()\n",
      "        # can_use_libjoin assures sv and ov are ndarrays\n",
      "        sv = cast(np.ndarray, sv)\n",
      "        ov = cast(np.ndarray, ov)\n",
      "        joined_ndarray, lidx, ridx = libjoin.left_join_indexer(sv, ov)\n",
      "        joined = self._from_join_target(joined_ndarray)\n",
      "        return joined, lidx, ridx\n",
      "\n",
      "------------\n",
      "Method name: _left_indexer_unique\n",
      "Method definition:     @final\n",
      "    def _left_indexer_unique(self: _IndexT, other: _IndexT) -> npt.NDArray[np.intp]:\n",
      "        # Caller is responsible for ensuring other.dtype == self.dtype\n",
      "        sv = self._get_engine_target()\n",
      "        ov = other._get_engine_target()\n",
      "        # can_use_libjoin assures sv and ov are ndarrays\n",
      "        sv = cast(np.ndarray, sv)\n",
      "        ov = cast(np.ndarray, ov)\n",
      "        return libjoin.left_join_indexer_unique(sv, ov)\n",
      "\n",
      "------------\n",
      "Method name: _logical_method\n",
      "Method definition:     def _logical_method(self, other, op):\n",
      "        return NotImplemented\n",
      "\n",
      "------------\n",
      "Method name: _map_values\n",
      "Method definition:     @final\n",
      "    def _map_values(self, mapper, na_action=None):\n",
      "        \"\"\"\n",
      "        An internal function that maps values using the input\n",
      "        correspondence (which can be a dict, Series, or function).\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        mapper : function, dict, or Series\n",
      "            The input correspondence object\n",
      "        na_action : {None, 'ignore'}\n",
      "            If 'ignore', propagate NA values, without passing them to the\n",
      "            mapping function\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Union[Index, MultiIndex], inferred\n",
      "            The output of the mapping function applied to the index.\n",
      "            If the function returns a tuple with more than one element\n",
      "            a MultiIndex will be returned.\n",
      "        \"\"\"\n",
      "        # we can fastpath dict/Series to an efficient map\n",
      "        # as we know that we are not going to have to yield\n",
      "        # python types\n",
      "        if is_dict_like(mapper):\n",
      "            if isinstance(mapper, dict) and hasattr(mapper, \"__missing__\"):\n",
      "                # If a dictionary subclass defines a default value method,\n",
      "                # convert mapper to a lookup function (GH #15999).\n",
      "                dict_with_default = mapper\n",
      "                mapper = lambda x: dict_with_default[x]\n",
      "            else:\n",
      "                # Dictionary does not have a default. Thus it's safe to\n",
      "                # convert to an Series for efficiency.\n",
      "                # we specify the keys here to handle the\n",
      "                # possibility that they are tuples\n",
      "\n",
      "                # The return value of mapping with an empty mapper is\n",
      "                # expected to be pd.Series(np.nan, ...). As np.nan is\n",
      "                # of dtype float64 the return value of this method should\n",
      "                # be float64 as well\n",
      "                mapper = create_series_with_explicit_dtype(\n",
      "                    mapper, dtype_if_empty=np.float64\n",
      "                )\n",
      "\n",
      "        if isinstance(mapper, ABCSeries):\n",
      "            if na_action not in (None, \"ignore\"):\n",
      "                msg = (\n",
      "                    \"na_action must either be 'ignore' or None, \"\n",
      "                    f\"{na_action} was passed\"\n",
      "                )\n",
      "                raise ValueError(msg)\n",
      "\n",
      "            if na_action == \"ignore\":\n",
      "                mapper = mapper[mapper.index.notna()]\n",
      "\n",
      "            # Since values were input this means we came from either\n",
      "            # a dict or a series and mapper should be an index\n",
      "            if is_categorical_dtype(self.dtype):\n",
      "                # use the built in categorical series mapper which saves\n",
      "                # time by mapping the categories instead of all values\n",
      "\n",
      "                cat = cast(\"Categorical\", self._values)\n",
      "                return cat.map(mapper)\n",
      "\n",
      "            values = self._values\n",
      "\n",
      "            indexer = mapper.index.get_indexer(values)\n",
      "            new_values = algorithms.take_nd(mapper._values, indexer)\n",
      "\n",
      "            return new_values\n",
      "\n",
      "        # we must convert to python types\n",
      "        if is_extension_array_dtype(self.dtype) and hasattr(self._values, \"map\"):\n",
      "            # GH#23179 some EAs do not have `map`\n",
      "            values = self._values\n",
      "            if na_action is not None:\n",
      "                raise NotImplementedError\n",
      "            map_f = lambda values, f: values.map(f)\n",
      "        else:\n",
      "            values = self._values.astype(object)\n",
      "            if na_action == \"ignore\":\n",
      "                map_f = lambda values, f: lib.map_infer_mask(\n",
      "                    values, f, isna(values).view(np.uint8)\n",
      "                )\n",
      "            elif na_action is None:\n",
      "                map_f = lib.map_infer\n",
      "            else:\n",
      "                msg = (\n",
      "                    \"na_action must either be 'ignore' or None, \"\n",
      "                    f\"{na_action} was passed\"\n",
      "                )\n",
      "                raise ValueError(msg)\n",
      "\n",
      "        # mapper is a function\n",
      "        new_values = map_f(values, mapper)\n",
      "\n",
      "        return new_values\n",
      "\n",
      "------------\n",
      "Method name: _maybe_cast_indexer\n",
      "Method definition:     def _maybe_cast_indexer(self, key):\n",
      "        \"\"\"\n",
      "        If we have a float key and are not a floating index, then try to cast\n",
      "        to an int if equivalent.\n",
      "        \"\"\"\n",
      "        return key\n",
      "\n",
      "------------\n",
      "Method name: _maybe_cast_listlike_indexer\n",
      "Method definition:     def _maybe_cast_listlike_indexer(self, target) -> Index:\n",
      "        \"\"\"\n",
      "        Analogue to maybe_cast_indexer for get_indexer instead of get_loc.\n",
      "        \"\"\"\n",
      "        return ensure_index(target)\n",
      "\n",
      "------------\n",
      "Method name: _maybe_cast_slice_bound\n",
      "Method definition:     def _maybe_cast_slice_bound(self, label, side: str_t, kind=no_default):\n",
      "        \"\"\"\n",
      "        This function should be overloaded in subclasses that allow non-trivial\n",
      "        casting on label-slice bounds, e.g. datetime-like indices allowing\n",
      "        strings containing formatted datetimes.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        label : object\n",
      "        side : {'left', 'right'}\n",
      "        kind : {'loc', 'getitem'} or None\n",
      "\n",
      "            .. deprecated:: 1.3.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        label : object\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Value of `side` parameter should be validated in caller.\n",
      "        \"\"\"\n",
      "        assert kind in [\"loc\", \"getitem\", None, no_default]\n",
      "        self._deprecated_arg(kind, \"kind\", \"_maybe_cast_slice_bound\")\n",
      "\n",
      "        # We are a plain index here (sub-class override this method if they\n",
      "        # wish to have special treatment for floats/ints, e.g. Float64Index and\n",
      "        # datetimelike Indexes\n",
      "        # reject them, if index does not contain label\n",
      "        if (is_float(label) or is_integer(label)) and label not in self:\n",
      "            raise self._invalid_indexer(\"slice\", label)\n",
      "\n",
      "        return label\n",
      "\n",
      "------------\n",
      "Method name: _maybe_check_unique\n",
      "Method definition:     @final\n",
      "    def _maybe_check_unique(self) -> None:\n",
      "        \"\"\"\n",
      "        Check that an Index has no duplicates.\n",
      "\n",
      "        This is typically only called via\n",
      "        `NDFrame.flags.allows_duplicate_labels.setter` when it's set to\n",
      "        True (duplicates aren't allowed).\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        DuplicateLabelError\n",
      "            When the index is not unique.\n",
      "        \"\"\"\n",
      "        if not self.is_unique:\n",
      "            msg = \"\"\"Index has duplicates.\"\"\"\n",
      "            duplicates = self._format_duplicate_message()\n",
      "            msg += f\"\\n{duplicates}\"\n",
      "\n",
      "            raise DuplicateLabelError(msg)\n",
      "\n",
      "------------\n",
      "Method name: _maybe_disable_logical_methods\n",
      "Method definition:     @final\n",
      "    def _maybe_disable_logical_methods(self, opname: str_t) -> None:\n",
      "        \"\"\"\n",
      "        raise if this Index subclass does not support any or all.\n",
      "        \"\"\"\n",
      "        if (\n",
      "            isinstance(self, ABCMultiIndex)\n",
      "            or needs_i8_conversion(self.dtype)\n",
      "            or is_interval_dtype(self.dtype)\n",
      "            or is_categorical_dtype(self.dtype)\n",
      "            or is_float_dtype(self.dtype)\n",
      "        ):\n",
      "            # This call will raise\n",
      "            make_invalid_op(opname)(self)\n",
      "\n",
      "------------\n",
      "Method name: _maybe_disallow_fill\n",
      "Method definition:     @final\n",
      "    def _maybe_disallow_fill(self, allow_fill: bool, fill_value, indices) -> bool:\n",
      "        \"\"\"\n",
      "        We only use pandas-style take when allow_fill is True _and_\n",
      "        fill_value is not None.\n",
      "        \"\"\"\n",
      "        if allow_fill and fill_value is not None:\n",
      "            # only fill if we are passing a non-None fill_value\n",
      "            if self._can_hold_na:\n",
      "                if (indices < -1).any():\n",
      "                    raise ValueError(\n",
      "                        \"When allow_fill=True and fill_value is not None, \"\n",
      "                        \"all indices must be >= -1\"\n",
      "                    )\n",
      "            else:\n",
      "                cls_name = type(self).__name__\n",
      "                raise ValueError(\n",
      "                    f\"Unable to fill values because {cls_name} cannot contain NA\"\n",
      "                )\n",
      "        else:\n",
      "            allow_fill = False\n",
      "        return allow_fill\n",
      "\n",
      "------------\n",
      "Method name: _maybe_preserve_names\n",
      "Method definition:     def _maybe_preserve_names(self, target: Index, preserve_names: bool):\n",
      "        if preserve_names and target.nlevels == 1 and target.name != self.name:\n",
      "            target = target.copy(deep=False)\n",
      "            target.name = self.name\n",
      "        return target\n",
      "\n",
      "------------\n",
      "Method name: _maybe_promote\n",
      "Method definition:     @final\n",
      "    def _maybe_promote(self, other: Index) -> tuple[Index, Index]:\n",
      "        \"\"\"\n",
      "        When dealing with an object-dtype Index and a non-object Index, see\n",
      "        if we can upcast the object-dtype one to improve performance.\n",
      "        \"\"\"\n",
      "\n",
      "        if isinstance(self, ABCDatetimeIndex) and isinstance(other, ABCDatetimeIndex):\n",
      "            if (\n",
      "                self.tz is not None\n",
      "                and other.tz is not None\n",
      "                and not tz_compare(self.tz, other.tz)\n",
      "            ):\n",
      "                # standardize on UTC\n",
      "                return self.tz_convert(\"UTC\"), other.tz_convert(\"UTC\")\n",
      "\n",
      "        elif self.inferred_type == \"date\" and isinstance(other, ABCDatetimeIndex):\n",
      "            try:\n",
      "                return type(other)(self), other\n",
      "            except OutOfBoundsDatetime:\n",
      "                return self, other\n",
      "        elif self.inferred_type == \"timedelta\" and isinstance(other, ABCTimedeltaIndex):\n",
      "            # TODO: we dont have tests that get here\n",
      "            return type(other)(self), other\n",
      "\n",
      "        elif self.dtype.kind == \"u\" and other.dtype.kind == \"i\":\n",
      "            # GH#41873\n",
      "            if other.min() >= 0:\n",
      "                # lookup min as it may be cached\n",
      "                # TODO: may need itemsize check if we have non-64-bit Indexes\n",
      "                return self, other.astype(self.dtype)\n",
      "\n",
      "        elif self._is_multi and not other._is_multi:\n",
      "            try:\n",
      "                # \"Type[Index]\" has no attribute \"from_tuples\"\n",
      "                other = type(self).from_tuples(other)  # type: ignore[attr-defined]\n",
      "            except (TypeError, ValueError):\n",
      "                # let's instead try with a straight Index\n",
      "                self = Index(self._values)\n",
      "\n",
      "        if not is_object_dtype(self.dtype) and is_object_dtype(other.dtype):\n",
      "            # Reverse op so we dont need to re-implement on the subclasses\n",
      "            other, self = other._maybe_promote(self)\n",
      "\n",
      "        return self, other\n",
      "\n",
      "------------\n",
      "Method name: _memory_usage\n",
      "Method definition:     def _memory_usage(self, deep: bool = False) -> int:\n",
      "        \"\"\"\n",
      "        Memory usage of the values.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        deep : bool, default False\n",
      "            Introspect the data deeply, interrogate\n",
      "            `object` dtypes for system-level memory consumption.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bytes used\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.ndarray.nbytes : Total bytes consumed by the elements of the\n",
      "            array.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Memory usage does not include memory consumed by elements that\n",
      "        are not components of the array if deep=False or if used on PyPy\n",
      "        \"\"\"\n",
      "        if hasattr(self.array, \"memory_usage\"):\n",
      "            # https://github.com/python/mypy/issues/1424\n",
      "            # error: \"ExtensionArray\" has no attribute \"memory_usage\"\n",
      "            return self.array.memory_usage(deep=deep)  # type: ignore[attr-defined]\n",
      "\n",
      "        v = self.array.nbytes\n",
      "        if deep and is_object_dtype(self) and not PYPY:\n",
      "            values = cast(np.ndarray, self._values)\n",
      "            v += lib.memory_usage_of_objects(values)\n",
      "        return v\n",
      "\n",
      "------------\n",
      "Method name: _mpl_repr\n",
      "Method definition:     @final\n",
      "    def _mpl_repr(self) -> np.ndarray:\n",
      "        # how to represent ourselves to matplotlib\n",
      "        if isinstance(self.dtype, np.dtype) and self.dtype.kind != \"M\":\n",
      "            return cast(np.ndarray, self.values)\n",
      "        return self.astype(object, copy=False)._values\n",
      "\n",
      "------------\n",
      "Method name: _outer_indexer\n",
      "Method definition:     @final\n",
      "    def _outer_indexer(\n",
      "        self: _IndexT, other: _IndexT\n",
      "    ) -> tuple[ArrayLike, npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
      "        # Caller is responsible for ensuring other.dtype == self.dtype\n",
      "        sv = self._get_engine_target()\n",
      "        ov = other._get_engine_target()\n",
      "        # can_use_libjoin assures sv and ov are ndarrays\n",
      "        sv = cast(np.ndarray, sv)\n",
      "        ov = cast(np.ndarray, ov)\n",
      "        joined_ndarray, lidx, ridx = libjoin.outer_join_indexer(sv, ov)\n",
      "        joined = self._from_join_target(joined_ndarray)\n",
      "        return joined, lidx, ridx\n",
      "\n",
      "------------\n",
      "Method name: _raise_if_missing\n",
      "Method definition:     def _raise_if_missing(self, key, indexer, axis_name: str_t) -> None:\n",
      "        \"\"\"\n",
      "        Check that indexer can be used to return a result.\n",
      "\n",
      "        e.g. at least one element was found,\n",
      "        unless the list of keys was actually empty.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        key : list-like\n",
      "            Targeted labels (only used to show correct error message).\n",
      "        indexer: array-like of booleans\n",
      "            Indices corresponding to the key,\n",
      "            (with -1 indicating not found).\n",
      "        axis_name : str\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        KeyError\n",
      "            If at least one key was requested but none was found.\n",
      "        \"\"\"\n",
      "        if len(key) == 0:\n",
      "            return\n",
      "\n",
      "        # Count missing values\n",
      "        missing_mask = indexer < 0\n",
      "        nmissing = missing_mask.sum()\n",
      "\n",
      "        if nmissing:\n",
      "\n",
      "            # TODO: remove special-case; this is just to keep exception\n",
      "            #  message tests from raising while debugging\n",
      "            use_interval_msg = is_interval_dtype(self.dtype) or (\n",
      "                is_categorical_dtype(self.dtype)\n",
      "                # \"Index\" has no attribute \"categories\"  [attr-defined]\n",
      "                and is_interval_dtype(\n",
      "                    self.categories.dtype  # type: ignore[attr-defined]\n",
      "                )\n",
      "            )\n",
      "\n",
      "            if nmissing == len(indexer):\n",
      "                if use_interval_msg:\n",
      "                    key = list(key)\n",
      "                raise KeyError(f\"None of [{key}] are in the [{axis_name}]\")\n",
      "\n",
      "            not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())\n",
      "            raise KeyError(f\"{not_found} not in index\")\n",
      "\n",
      "------------\n",
      "Method name: _reduce\n",
      "Method definition:     def _reduce(\n",
      "        self,\n",
      "        op,\n",
      "        name: str,\n",
      "        *,\n",
      "        axis=0,\n",
      "        skipna=True,\n",
      "        numeric_only=None,\n",
      "        filter_type=None,\n",
      "        **kwds,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Perform the reduction type operation if we can.\n",
      "        \"\"\"\n",
      "        func = getattr(self, name, None)\n",
      "        if func is None:\n",
      "            raise TypeError(\n",
      "                f\"{type(self).__name__} cannot perform the operation {name}\"\n",
      "            )\n",
      "        return func(skipna=skipna, **kwds)\n",
      "\n",
      "------------\n",
      "Method name: _reindex_non_unique\n",
      "Method definition:     @final\n",
      "    def _reindex_non_unique(\n",
      "        self, target: Index\n",
      "    ) -> tuple[Index, npt.NDArray[np.intp], npt.NDArray[np.intp] | None]:\n",
      "        \"\"\"\n",
      "        Create a new index with target's values (move/add/delete values as\n",
      "        necessary) use with non-unique Index and a possibly non-unique target.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        target : an iterable\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        new_index : pd.Index\n",
      "            Resulting index.\n",
      "        indexer : np.ndarray[np.intp]\n",
      "            Indices of output values in original index.\n",
      "        new_indexer : np.ndarray[np.intp] or None\n",
      "\n",
      "        \"\"\"\n",
      "        target = ensure_index(target)\n",
      "        if len(target) == 0:\n",
      "            # GH#13691\n",
      "            return self[:0], np.array([], dtype=np.intp), None\n",
      "\n",
      "        indexer, missing = self.get_indexer_non_unique(target)\n",
      "        check = indexer != -1\n",
      "        new_labels = self.take(indexer[check])\n",
      "        new_indexer = None\n",
      "\n",
      "        if len(missing):\n",
      "            length = np.arange(len(indexer), dtype=np.intp)\n",
      "\n",
      "            missing = ensure_platform_int(missing)\n",
      "            missing_labels = target.take(missing)\n",
      "            missing_indexer = length[~check]\n",
      "            cur_labels = self.take(indexer[check]).values\n",
      "            cur_indexer = length[check]\n",
      "\n",
      "            # Index constructor below will do inference\n",
      "            new_labels = np.empty((len(indexer),), dtype=object)\n",
      "            new_labels[cur_indexer] = cur_labels\n",
      "            new_labels[missing_indexer] = missing_labels\n",
      "\n",
      "            # GH#38906\n",
      "            if not len(self):\n",
      "\n",
      "                new_indexer = np.arange(0, dtype=np.intp)\n",
      "\n",
      "            # a unique indexer\n",
      "            elif target.is_unique:\n",
      "\n",
      "                # see GH5553, make sure we use the right indexer\n",
      "                new_indexer = np.arange(len(indexer), dtype=np.intp)\n",
      "                new_indexer[cur_indexer] = np.arange(len(cur_labels))\n",
      "                new_indexer[missing_indexer] = -1\n",
      "\n",
      "            # we have a non_unique selector, need to use the original\n",
      "            # indexer here\n",
      "            else:\n",
      "\n",
      "                # need to retake to have the same size as the indexer\n",
      "                indexer[~check] = -1\n",
      "\n",
      "                # reset the new indexer to account for the new size\n",
      "                new_indexer = np.arange(len(self.take(indexer)), dtype=np.intp)\n",
      "                new_indexer[~check] = -1\n",
      "\n",
      "        if isinstance(self, ABCMultiIndex):\n",
      "            new_index = type(self).from_tuples(new_labels, names=self.names)\n",
      "        else:\n",
      "            new_index = Index._with_infer(new_labels, name=self.name)\n",
      "        return new_index, indexer, new_indexer\n",
      "\n",
      "------------\n",
      "Method name: _rename\n",
      "Method definition:     @final\n",
      "    def _rename(self: _IndexT, name: Hashable) -> _IndexT:\n",
      "        \"\"\"\n",
      "        fastpath for rename if new name is already validated.\n",
      "        \"\"\"\n",
      "        result = self._view()\n",
      "        result._name = name\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _require_scalar\n",
      "Method definition:     @final\n",
      "    def _require_scalar(self, value):\n",
      "        \"\"\"\n",
      "        Check that this is a scalar value that we can use for setitem-like\n",
      "        operations without changing dtype.\n",
      "        \"\"\"\n",
      "        if not is_scalar(value):\n",
      "            raise TypeError(f\"'value' must be a scalar, passed: {type(value).__name__}\")\n",
      "        return value\n",
      "\n",
      "------------\n",
      "Method name: _reset_cache\n",
      "Method definition:     def _reset_cache(self, key: str | None = None) -> None:\n",
      "        \"\"\"\n",
      "        Reset cached properties. If ``key`` is passed, only clears that key.\n",
      "        \"\"\"\n",
      "        if not hasattr(self, \"_cache\"):\n",
      "            return\n",
      "        if key is None:\n",
      "            self._cache.clear()\n",
      "        else:\n",
      "            self._cache.pop(key, None)\n",
      "\n",
      "------------\n",
      "Method name: _reset_identity\n",
      "Method definition:     @final\n",
      "    def _reset_identity(self) -> None:\n",
      "        \"\"\"\n",
      "        Initializes or resets ``_id`` attribute with new object.\n",
      "        \"\"\"\n",
      "        self._id = object()\n",
      "\n",
      "------------\n",
      "Method name: _searchsorted_monotonic\n",
      "Method definition:     def _searchsorted_monotonic(self, label, side: Literal[\"left\", \"right\"] = \"left\"):\n",
      "        if self.is_monotonic_increasing:\n",
      "            return self.searchsorted(label, side=side)\n",
      "        elif self.is_monotonic_decreasing:\n",
      "            # np.searchsorted expects ascending sort order, have to reverse\n",
      "            # everything for it to work (element ordering, search side and\n",
      "            # resulting value).\n",
      "            pos = self[::-1].searchsorted(\n",
      "                label, side=\"right\" if side == \"left\" else \"left\"\n",
      "            )\n",
      "            return len(self) - pos\n",
      "\n",
      "        raise ValueError(\"index must be monotonic increasing or decreasing\")\n",
      "\n",
      "------------\n",
      "Method name: _set_names\n",
      "Method definition:     def _set_names(self, values, *, level=None) -> None:\n",
      "        \"\"\"\n",
      "        Set new names on index. Each name has to be a hashable type.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        values : str or sequence\n",
      "            name(s) to set\n",
      "        level : int, level name, or sequence of int/level names (default None)\n",
      "            If the index is a MultiIndex (hierarchical), level(s) to set (None\n",
      "            for all levels).  Otherwise level must be None\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError if each name is not hashable.\n",
      "        \"\"\"\n",
      "        if not is_list_like(values):\n",
      "            raise ValueError(\"Names must be a list-like\")\n",
      "        if len(values) != 1:\n",
      "            raise ValueError(f\"Length of new names must be 1, got {len(values)}\")\n",
      "\n",
      "        # GH 20527\n",
      "        # All items in 'name' need to be hashable:\n",
      "        validate_all_hashable(*values, error_name=f\"{type(self).__name__}.name\")\n",
      "\n",
      "        self._name = values[0]\n",
      "\n",
      "------------\n",
      "Method name: _shallow_copy\n",
      "Method definition:     def _shallow_copy(self: _IndexT, values, name: Hashable = no_default) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Create a new Index with the same class as the caller, don't copy the\n",
      "        data, use the same object attributes with passed in attributes taking\n",
      "        precedence.\n",
      "\n",
      "        *this is an internal non-public method*\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        values : the values to create the new Index, optional\n",
      "        name : Label, defaults to self.name\n",
      "        \"\"\"\n",
      "        name = self._name if name is no_default else name\n",
      "\n",
      "        return self._simple_new(values, name=name)\n",
      "\n",
      "------------\n",
      "Method name: _should_compare\n",
      "Method definition:     @final\n",
      "    def _should_compare(self, other: Index) -> bool:\n",
      "        \"\"\"\n",
      "        Check if `self == other` can ever have non-False entries.\n",
      "        \"\"\"\n",
      "\n",
      "        if (other.is_boolean() and self.is_numeric()) or (\n",
      "            self.is_boolean() and other.is_numeric()\n",
      "        ):\n",
      "            # GH#16877 Treat boolean labels passed to a numeric index as not\n",
      "            #  found. Without this fix False and True would be treated as 0 and 1\n",
      "            #  respectively.\n",
      "            return False\n",
      "\n",
      "        other = unpack_nested_dtype(other)\n",
      "        dtype = other.dtype\n",
      "        return self._is_comparable_dtype(dtype) or is_object_dtype(dtype)\n",
      "\n",
      "------------\n",
      "Method name: _should_partial_index\n",
      "Method definition:     @final\n",
      "    def _should_partial_index(self, target: Index) -> bool:\n",
      "        \"\"\"\n",
      "        Should we attempt partial-matching indexing?\n",
      "        \"\"\"\n",
      "        if is_interval_dtype(self.dtype):\n",
      "            if is_interval_dtype(target.dtype):\n",
      "                return False\n",
      "            # See https://github.com/pandas-dev/pandas/issues/47772 the commented\n",
      "            # out code can be restored (instead of hardcoding `return True`)\n",
      "            # once that issue if fixed\n",
      "            # \"Index\" has no attribute \"left\"\n",
      "            # return self.left._should_compare(target)  # type: ignore[attr-defined]\n",
      "            return True\n",
      "        return False\n",
      "\n",
      "------------\n",
      "Method name: _sort_levels_monotonic\n",
      "Method definition:     def _sort_levels_monotonic(self: _IndexT) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Compat with MultiIndex.\n",
      "        \"\"\"\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: _summary\n",
      "Method definition:     def _summary(self, name=None) -> str_t:\n",
      "        \"\"\"\n",
      "        Return a summarized representation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        name : str\n",
      "            name to use in the summary representation\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        String with a summarized representation of the index\n",
      "        \"\"\"\n",
      "        if len(self) > 0:\n",
      "            head = self[0]\n",
      "            if hasattr(head, \"format\") and not isinstance(head, str):\n",
      "                head = head.format()\n",
      "            elif needs_i8_conversion(self.dtype):\n",
      "                # e.g. Timedelta, display as values, not quoted\n",
      "                head = self._formatter_func(head).replace(\"'\", \"\")\n",
      "            tail = self[-1]\n",
      "            if hasattr(tail, \"format\") and not isinstance(tail, str):\n",
      "                tail = tail.format()\n",
      "            elif needs_i8_conversion(self.dtype):\n",
      "                # e.g. Timedelta, display as values, not quoted\n",
      "                tail = self._formatter_func(tail).replace(\"'\", \"\")\n",
      "\n",
      "            index_summary = f\", {head} to {tail}\"\n",
      "        else:\n",
      "            index_summary = \"\"\n",
      "\n",
      "        if name is None:\n",
      "            name = type(self).__name__\n",
      "        return f\"{name}: {len(self)} entries{index_summary}\"\n",
      "\n",
      "------------\n",
      "Method name: _transform_index\n",
      "Method definition:     @final\n",
      "    def _transform_index(self, func, *, level=None) -> Index:\n",
      "        \"\"\"\n",
      "        Apply function to all values found in index.\n",
      "\n",
      "        This includes transforming multiindex entries separately.\n",
      "        Only apply function to one level of the MultiIndex if level is specified.\n",
      "        \"\"\"\n",
      "        if isinstance(self, ABCMultiIndex):\n",
      "            if level is not None:\n",
      "                # Caller is responsible for ensuring level is positional.\n",
      "                items = [\n",
      "                    tuple(func(y) if i == level else y for i, y in enumerate(x))\n",
      "                    for x in self\n",
      "                ]\n",
      "            else:\n",
      "                items = [tuple(func(y) for y in x) for x in self]\n",
      "            return type(self).from_tuples(items, names=self.names)\n",
      "        else:\n",
      "            items = [func(x) for x in self]\n",
      "            return Index(items, name=self.name, tupleize_cols=False)\n",
      "\n",
      "------------\n",
      "Method name: _unary_method\n",
      "Method definition:     @final\n",
      "    def _unary_method(self, op):\n",
      "        result = op(self._values)\n",
      "        return Index(result, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: _union\n",
      "Method definition:     def _union(self, other: Index, sort):\n",
      "        \"\"\"\n",
      "        Specific union logic should go here. In subclasses, union behavior\n",
      "        should be overwritten here rather than in `self.union`.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index or array-like\n",
      "        sort : False or None, default False\n",
      "            Whether to sort the resulting index.\n",
      "\n",
      "            * False : do not sort the result.\n",
      "            * None : sort the result, except when `self` and `other` are equal\n",
      "              or when the values cannot be compared.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "        \"\"\"\n",
      "        lvals = self._values\n",
      "        rvals = other._values\n",
      "\n",
      "        if (\n",
      "            sort is None\n",
      "            and self.is_monotonic_increasing\n",
      "            and other.is_monotonic_increasing\n",
      "            and not (self.has_duplicates and other.has_duplicates)\n",
      "            and self._can_use_libjoin\n",
      "        ):\n",
      "            # Both are monotonic and at least one is unique, so can use outer join\n",
      "            #  (actually don't need either unique, but without this restriction\n",
      "            #  test_union_same_value_duplicated_in_both fails)\n",
      "            try:\n",
      "                return self._outer_indexer(other)[0]\n",
      "            except (TypeError, IncompatibleFrequency):\n",
      "                # incomparable objects; should only be for object dtype\n",
      "                value_list = list(lvals)\n",
      "\n",
      "                # worth making this faster? a very unusual case\n",
      "                value_set = set(lvals)\n",
      "                value_list.extend([x for x in rvals if x not in value_set])\n",
      "                # If objects are unorderable, we must have object dtype.\n",
      "                return np.array(value_list, dtype=object)\n",
      "\n",
      "        elif not other.is_unique:\n",
      "            # other has duplicates\n",
      "            result = algos.union_with_duplicates(lvals, rvals)\n",
      "            return _maybe_try_sort(result, sort)\n",
      "\n",
      "        # Self may have duplicates; other already checked as unique\n",
      "        # find indexes of things in \"other\" that are not in \"self\"\n",
      "        if self._index_as_unique:\n",
      "            indexer = self.get_indexer(other)\n",
      "            missing = (indexer == -1).nonzero()[0]\n",
      "        else:\n",
      "            missing = algos.unique1d(self.get_indexer_non_unique(other)[1])\n",
      "\n",
      "        if len(missing) > 0:\n",
      "            other_diff = rvals.take(missing)\n",
      "            result = concat_compat((lvals, other_diff))\n",
      "        else:\n",
      "            result = lvals\n",
      "\n",
      "        if not self.is_monotonic_increasing or not other.is_monotonic_increasing:\n",
      "            # if both are monotonic then result should already be sorted\n",
      "            result = _maybe_try_sort(result, sort)\n",
      "\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _validate_can_reindex\n",
      "Method definition:     @final\n",
      "    def _validate_can_reindex(self, indexer: np.ndarray) -> None:\n",
      "        \"\"\"\n",
      "        Check if we are allowing reindexing with this particular indexer.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        indexer : an integer ndarray\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError if its a duplicate axis\n",
      "        \"\"\"\n",
      "        # trying to reindex on an axis with duplicates\n",
      "        if not self._index_as_unique and len(indexer):\n",
      "            raise ValueError(\"cannot reindex on an axis with duplicate labels\")\n",
      "\n",
      "------------\n",
      "Method name: _validate_fill_value\n",
      "Method definition:     def _validate_fill_value(self, value):\n",
      "        \"\"\"\n",
      "        Check if the value can be inserted into our array without casting,\n",
      "        and convert it to an appropriate native type if necessary.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If the value cannot be inserted into an array of this dtype.\n",
      "        \"\"\"\n",
      "        dtype = self.dtype\n",
      "        if isinstance(dtype, np.dtype) and dtype.kind not in [\"m\", \"M\"]:\n",
      "            # return np_can_hold_element(dtype, value)\n",
      "            try:\n",
      "                return np_can_hold_element(dtype, value)\n",
      "            except LossySetitemError as err:\n",
      "                # re-raise as TypeError for consistency\n",
      "                raise TypeError from err\n",
      "        elif not can_hold_element(self._values, value):\n",
      "            raise TypeError\n",
      "        return value\n",
      "\n",
      "------------\n",
      "Method name: _validate_index_level\n",
      "Method definition:     @final\n",
      "    def _validate_index_level(self, level) -> None:\n",
      "        \"\"\"\n",
      "        Validate index level.\n",
      "\n",
      "        For single-level Index getting level number is a no-op, but some\n",
      "        verification must be done like in MultiIndex.\n",
      "\n",
      "        \"\"\"\n",
      "        if isinstance(level, int):\n",
      "            if level < 0 and level != -1:\n",
      "                raise IndexError(\n",
      "                    \"Too many levels: Index has only 1 level, \"\n",
      "                    f\"{level} is not a valid level number\"\n",
      "                )\n",
      "            elif level > 0:\n",
      "                raise IndexError(\n",
      "                    f\"Too many levels: Index has only 1 level, not {level + 1}\"\n",
      "                )\n",
      "        elif level != self.name:\n",
      "            raise KeyError(\n",
      "                f\"Requested level ({level}) does not match index name ({self.name})\"\n",
      "            )\n",
      "\n",
      "------------\n",
      "Method name: _validate_indexer\n",
      "Method definition:     @final\n",
      "    def _validate_indexer(self, form: str_t, key, kind: str_t):\n",
      "        \"\"\"\n",
      "        If we are positional indexer, validate that we have appropriate\n",
      "        typed bounds must be an integer.\n",
      "        \"\"\"\n",
      "        assert kind in [\"getitem\", \"iloc\"]\n",
      "\n",
      "        if key is not None and not is_integer(key):\n",
      "            raise self._invalid_indexer(form, key)\n",
      "\n",
      "------------\n",
      "Method name: _validate_names\n",
      "Method definition:     @final\n",
      "    def _validate_names(\n",
      "        self, name=None, names=None, deep: bool = False\n",
      "    ) -> list[Hashable]:\n",
      "        \"\"\"\n",
      "        Handles the quirks of having a singular 'name' parameter for general\n",
      "        Index and plural 'names' parameter for MultiIndex.\n",
      "        \"\"\"\n",
      "        from copy import deepcopy\n",
      "\n",
      "        if names is not None and name is not None:\n",
      "            raise TypeError(\"Can only provide one of `names` and `name`\")\n",
      "        elif names is None and name is None:\n",
      "            new_names = deepcopy(self.names) if deep else self.names\n",
      "        elif names is not None:\n",
      "            if not is_list_like(names):\n",
      "                raise TypeError(\"Must pass list-like as `names`.\")\n",
      "            new_names = names\n",
      "        elif not is_list_like(name):\n",
      "            new_names = [name]\n",
      "        else:\n",
      "            new_names = name\n",
      "\n",
      "        if len(new_names) != len(self.names):\n",
      "            raise ValueError(\n",
      "                f\"Length of new names must be {len(self.names)}, got {len(new_names)}\"\n",
      "            )\n",
      "\n",
      "        # All items in 'new_names' need to be hashable\n",
      "        validate_all_hashable(*new_names, error_name=f\"{type(self).__name__}.name\")\n",
      "\n",
      "        return new_names\n",
      "\n",
      "------------\n",
      "Method name: _validate_positional_slice\n",
      "Method definition:     @final\n",
      "    def _validate_positional_slice(self, key: slice) -> None:\n",
      "        \"\"\"\n",
      "        For positional indexing, a slice must have either int or None\n",
      "        for each of start, stop, and step.\n",
      "        \"\"\"\n",
      "        self._validate_indexer(\"positional\", key.start, \"iloc\")\n",
      "        self._validate_indexer(\"positional\", key.stop, \"iloc\")\n",
      "        self._validate_indexer(\"positional\", key.step, \"iloc\")\n",
      "\n",
      "------------\n",
      "Method name: _validate_sort_keyword\n",
      "Method definition:     @final\n",
      "    def _validate_sort_keyword(self, sort):\n",
      "        if sort not in [None, False]:\n",
      "            raise ValueError(\n",
      "                \"The 'sort' keyword only takes the values of \"\n",
      "                f\"None or False; {sort} was passed.\"\n",
      "            )\n",
      "\n",
      "------------\n",
      "Method name: _view\n",
      "Method definition:     def _view(self: _IndexT) -> _IndexT:\n",
      "        \"\"\"\n",
      "        fastpath to make a shallow copy, i.e. new object with same data.\n",
      "        \"\"\"\n",
      "        result = self._simple_new(self._values, name=self._name)\n",
      "\n",
      "        result._cache = self._cache\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: _wrap_difference_result\n",
      "Method definition:     def _wrap_difference_result(self, other, result):\n",
      "        # We will override for MultiIndex to handle empty results\n",
      "        return self._wrap_setop_result(other, result)\n",
      "\n",
      "------------\n",
      "Method name: _wrap_intersection_result\n",
      "Method definition:     def _wrap_intersection_result(self, other, result):\n",
      "        # We will override for MultiIndex to handle empty results\n",
      "        return self._wrap_setop_result(other, result)\n",
      "\n",
      "------------\n",
      "Method name: _wrap_joined_index\n",
      "Method definition:     def _wrap_joined_index(self: _IndexT, joined: ArrayLike, other: _IndexT) -> _IndexT:\n",
      "        assert other.dtype == self.dtype\n",
      "\n",
      "        if isinstance(self, ABCMultiIndex):\n",
      "            name = self.names if self.names == other.names else None\n",
      "            # error: Incompatible return value type (got \"MultiIndex\",\n",
      "            # expected \"_IndexT\")\n",
      "            return self._constructor(joined, name=name)  # type: ignore[return-value]\n",
      "        else:\n",
      "            name = get_op_result_name(self, other)\n",
      "            return self._constructor._with_infer(joined, name=name, dtype=self.dtype)\n",
      "\n",
      "------------\n",
      "Method name: _wrap_reindex_result\n",
      "Method definition:     def _wrap_reindex_result(self, target, indexer, preserve_names: bool):\n",
      "        target = self._maybe_preserve_names(target, preserve_names)\n",
      "        return target\n",
      "\n",
      "------------\n",
      "Method name: _wrap_setop_result\n",
      "Method definition:     @final\n",
      "    def _wrap_setop_result(self, other: Index, result) -> Index:\n",
      "        name = get_op_result_name(self, other)\n",
      "        if isinstance(result, Index):\n",
      "            if result.name != name:\n",
      "                result = result.rename(name)\n",
      "        else:\n",
      "            result = self._shallow_copy(result, name=name)\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: all\n",
      "Method definition:     def all(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Return whether all elements are Truthy.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        *args\n",
      "            Required for compatibility with numpy.\n",
      "        **kwargs\n",
      "            Required for compatibility with numpy.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        all : bool or array-like (if axis is specified)\n",
      "            A single element array-like may be converted to bool.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.any : Return whether any element in an Index is True.\n",
      "        Series.any : Return whether any element in a Series is True.\n",
      "        Series.all : Return whether all elements in a Series are True.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Not a Number (NaN), positive infinity and negative infinity\n",
      "        evaluate to True because these are not equal to zero.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        True, because nonzero integers are considered True.\n",
      "\n",
      "        >>> pd.Index([1, 2, 3]).all()\n",
      "        True\n",
      "\n",
      "        False, because ``0`` is considered False.\n",
      "\n",
      "        >>> pd.Index([0, 1, 2]).all()\n",
      "        False\n",
      "        \"\"\"\n",
      "        nv.validate_all(args, kwargs)\n",
      "        self._maybe_disable_logical_methods(\"all\")\n",
      "        # error: Argument 1 to \"all\" has incompatible type \"ArrayLike\"; expected\n",
      "        # \"Union[Union[int, float, complex, str, bytes, generic], Sequence[Union[int,\n",
      "        # float, complex, str, bytes, generic]], Sequence[Sequence[Any]],\n",
      "        # _SupportsArray]\"\n",
      "        return np.all(self.values)  # type: ignore[arg-type]\n",
      "\n",
      "------------\n",
      "Method name: any\n",
      "Method definition:     def any(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Return whether any element is Truthy.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        *args\n",
      "            Required for compatibility with numpy.\n",
      "        **kwargs\n",
      "            Required for compatibility with numpy.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        any : bool or array-like (if axis is specified)\n",
      "            A single element array-like may be converted to bool.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.all : Return whether all elements are True.\n",
      "        Series.all : Return whether all elements are True.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Not a Number (NaN), positive infinity and negative infinity\n",
      "        evaluate to True because these are not equal to zero.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> index = pd.Index([0, 1, 2])\n",
      "        >>> index.any()\n",
      "        True\n",
      "\n",
      "        >>> index = pd.Index([0, 0, 0])\n",
      "        >>> index.any()\n",
      "        False\n",
      "        \"\"\"\n",
      "        nv.validate_any(args, kwargs)\n",
      "        self._maybe_disable_logical_methods(\"any\")\n",
      "        # error: Argument 1 to \"any\" has incompatible type \"ArrayLike\"; expected\n",
      "        # \"Union[Union[int, float, complex, str, bytes, generic], Sequence[Union[int,\n",
      "        # float, complex, str, bytes, generic]], Sequence[Sequence[Any]],\n",
      "        # _SupportsArray]\"\n",
      "        return np.any(self.values)  # type: ignore[arg-type]\n",
      "\n",
      "------------\n",
      "Method name: append\n",
      "Method definition:     def append(self, other: Index | Sequence[Index]) -> Index:\n",
      "        \"\"\"\n",
      "        Append a collection of Index options together.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index or list/tuple of indices\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "        \"\"\"\n",
      "        to_concat = [self]\n",
      "\n",
      "        if isinstance(other, (list, tuple)):\n",
      "            to_concat += list(other)\n",
      "        else:\n",
      "            # error: Argument 1 to \"append\" of \"list\" has incompatible type\n",
      "            # \"Union[Index, Sequence[Index]]\"; expected \"Index\"\n",
      "            to_concat.append(other)  # type: ignore[arg-type]\n",
      "\n",
      "        for obj in to_concat:\n",
      "            if not isinstance(obj, Index):\n",
      "                raise TypeError(\"all inputs must be Index\")\n",
      "\n",
      "        names = {obj.name for obj in to_concat}\n",
      "        name = None if len(names) > 1 else self.name\n",
      "\n",
      "        return self._concat(to_concat, name)\n",
      "\n",
      "------------\n",
      "Method name: argmax\n",
      "Method definition:     @Appender(IndexOpsMixin.argmax.__doc__)\n",
      "    def argmax(self, axis=None, skipna=True, *args, **kwargs) -> int:\n",
      "        nv.validate_argmax(args, kwargs)\n",
      "        nv.validate_minmax_axis(axis)\n",
      "\n",
      "        if not self._is_multi and self.hasnans:\n",
      "            # Take advantage of cache\n",
      "            mask = self._isnan\n",
      "            if not skipna or mask.all():\n",
      "                return -1\n",
      "        return super().argmax(skipna=skipna)\n",
      "\n",
      "------------\n",
      "Method name: argmin\n",
      "Method definition:     @Appender(IndexOpsMixin.argmin.__doc__)\n",
      "    def argmin(self, axis=None, skipna=True, *args, **kwargs) -> int:\n",
      "        nv.validate_argmin(args, kwargs)\n",
      "        nv.validate_minmax_axis(axis)\n",
      "\n",
      "        if not self._is_multi and self.hasnans:\n",
      "            # Take advantage of cache\n",
      "            mask = self._isnan\n",
      "            if not skipna or mask.all():\n",
      "                return -1\n",
      "        return super().argmin(skipna=skipna)\n",
      "\n",
      "------------\n",
      "Method name: argsort\n",
      "Method definition:     def argsort(self, *args, **kwargs) -> npt.NDArray[np.intp]:\n",
      "        \"\"\"\n",
      "        Return the integer indices that would sort the index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        *args\n",
      "            Passed to `numpy.ndarray.argsort`.\n",
      "        **kwargs\n",
      "            Passed to `numpy.ndarray.argsort`.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        np.ndarray[np.intp]\n",
      "            Integer indices that would sort the index if used as\n",
      "            an indexer.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.argsort : Similar method for NumPy arrays.\n",
      "        Index.sort_values : Return sorted copy of Index.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['b', 'a', 'd', 'c'])\n",
      "        >>> idx\n",
      "        Index(['b', 'a', 'd', 'c'], dtype='object')\n",
      "\n",
      "        >>> order = idx.argsort()\n",
      "        >>> order\n",
      "        array([1, 0, 3, 2])\n",
      "\n",
      "        >>> idx[order]\n",
      "        Index(['a', 'b', 'c', 'd'], dtype='object')\n",
      "        \"\"\"\n",
      "        # This works for either ndarray or EA, is overridden\n",
      "        #  by RangeIndex, MultIIndex\n",
      "        return self._data.argsort(*args, **kwargs)\n",
      "\n",
      "------------\n",
      "Method name: asof\n",
      "Method definition:     @final\n",
      "    def asof(self, label):\n",
      "        \"\"\"\n",
      "        Return the label from the index, or, if not present, the previous one.\n",
      "\n",
      "        Assuming that the index is sorted, return the passed index label if it\n",
      "        is in the index, or return the previous index label if the passed one\n",
      "        is not in the index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        label : object\n",
      "            The label up to which the method returns the latest index label.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        object\n",
      "            The passed label if it is in the index. The previous label if the\n",
      "            passed label is not in the sorted index or `NaN` if there is no\n",
      "            such label.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.asof : Return the latest value in a Series up to the\n",
      "            passed index.\n",
      "        merge_asof : Perform an asof merge (similar to left join but it\n",
      "            matches on nearest key rather than equal key).\n",
      "        Index.get_loc : An `asof` is a thin wrapper around `get_loc`\n",
      "            with method='pad'.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        `Index.asof` returns the latest index label up to the passed label.\n",
      "\n",
      "        >>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n",
      "        >>> idx.asof('2014-01-01')\n",
      "        '2013-12-31'\n",
      "\n",
      "        If the label is in the index, the method returns the passed label.\n",
      "\n",
      "        >>> idx.asof('2014-01-02')\n",
      "        '2014-01-02'\n",
      "\n",
      "        If all of the labels in the index are later than the passed label,\n",
      "        NaN is returned.\n",
      "\n",
      "        >>> idx.asof('1999-01-02')\n",
      "        nan\n",
      "\n",
      "        If the index is not sorted, an error is raised.\n",
      "\n",
      "        >>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n",
      "        ...                            '2014-01-03'])\n",
      "        >>> idx_not_sorted.asof('2013-12-31')\n",
      "        Traceback (most recent call last):\n",
      "        ValueError: index must be monotonic increasing or decreasing\n",
      "        \"\"\"\n",
      "        self._searchsorted_monotonic(label)  # validate sortedness\n",
      "        try:\n",
      "            loc = self.get_loc(label)\n",
      "        except (KeyError, TypeError):\n",
      "            # KeyError -> No exact match, try for padded\n",
      "            # TypeError -> passed e.g. non-hashable, fall through to get\n",
      "            #  the tested exception message\n",
      "            indexer = self.get_indexer([label], method=\"pad\")\n",
      "            if indexer.ndim > 1 or indexer.size > 1:\n",
      "                raise TypeError(\"asof requires scalar valued input\")\n",
      "            loc = indexer.item()\n",
      "            if loc == -1:\n",
      "                return self._na_value\n",
      "        else:\n",
      "            if isinstance(loc, slice):\n",
      "                loc = loc.indices(len(self))[-1]\n",
      "\n",
      "        return self[loc]\n",
      "\n",
      "------------\n",
      "Method name: asof_locs\n",
      "Method definition:     def asof_locs(\n",
      "        self, where: Index, mask: npt.NDArray[np.bool_]\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "        \"\"\"\n",
      "        Return the locations (indices) of labels in the index.\n",
      "\n",
      "        As in the `asof` function, if the label (a particular entry in\n",
      "        `where`) is not in the index, the latest index label up to the\n",
      "        passed label is chosen and its index returned.\n",
      "\n",
      "        If all of the labels in the index are later than a label in `where`,\n",
      "        -1 is returned.\n",
      "\n",
      "        `mask` is used to ignore NA values in the index during calculation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        where : Index\n",
      "            An Index consisting of an array of timestamps.\n",
      "        mask : np.ndarray[bool]\n",
      "            Array of booleans denoting where values in the original\n",
      "            data are not NA.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        np.ndarray[np.intp]\n",
      "            An array of locations (indices) of the labels from the Index\n",
      "            which correspond to the return values of the `asof` function\n",
      "            for every element in `where`.\n",
      "        \"\"\"\n",
      "        # error: No overload variant of \"searchsorted\" of \"ndarray\" matches argument\n",
      "        # types \"Union[ExtensionArray, ndarray[Any, Any]]\", \"str\"\n",
      "        # TODO: will be fixed when ExtensionArray.searchsorted() is fixed\n",
      "        locs = self._values[mask].searchsorted(\n",
      "            where._values, side=\"right\"  # type: ignore[call-overload]\n",
      "        )\n",
      "        locs = np.where(locs > 0, locs - 1, 0)\n",
      "\n",
      "        result = np.arange(len(self), dtype=np.intp)[mask].take(locs)\n",
      "\n",
      "        first_value = self._values[mask.argmax()]\n",
      "        result[(locs == 0) & (where._values < first_value)] = -1\n",
      "\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: astype\n",
      "Method definition:     def astype(self, dtype, copy: bool = True):\n",
      "        \"\"\"\n",
      "        Create an Index with values cast to dtypes.\n",
      "\n",
      "        The class of a new Index is determined by dtype. When conversion is\n",
      "        impossible, a TypeError exception is raised.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        dtype : numpy dtype or pandas type\n",
      "            Note that any signed integer `dtype` is treated as ``'int64'``,\n",
      "            and any unsigned integer `dtype` is treated as ``'uint64'``,\n",
      "            regardless of the size.\n",
      "        copy : bool, default True\n",
      "            By default, astype always returns a newly allocated object.\n",
      "            If copy is set to False and internal requirements on dtype are\n",
      "            satisfied, the original data is used to create a new Index\n",
      "            or the original Index is returned.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "            Index with values cast to specified dtype.\n",
      "        \"\"\"\n",
      "        if dtype is not None:\n",
      "            dtype = pandas_dtype(dtype)\n",
      "\n",
      "        if is_dtype_equal(self.dtype, dtype):\n",
      "            # Ensure that self.astype(self.dtype) is self\n",
      "            return self.copy() if copy else self\n",
      "\n",
      "        values = self._data\n",
      "        if isinstance(values, ExtensionArray):\n",
      "            if isinstance(dtype, np.dtype) and dtype.kind == \"M\" and is_unitless(dtype):\n",
      "                # TODO(2.0): remove this special-casing once this is enforced\n",
      "                #  in DTA.astype\n",
      "                raise TypeError(f\"Cannot cast {type(self).__name__} to dtype\")\n",
      "\n",
      "            with rewrite_exception(type(values).__name__, type(self).__name__):\n",
      "                new_values = values.astype(dtype, copy=copy)\n",
      "\n",
      "        elif is_float_dtype(self.dtype) and needs_i8_conversion(dtype):\n",
      "            # NB: this must come before the ExtensionDtype check below\n",
      "            # TODO: this differs from Series behavior; can/should we align them?\n",
      "            raise TypeError(\n",
      "                f\"Cannot convert Float64Index to dtype {dtype}; integer \"\n",
      "                \"values are required for conversion\"\n",
      "            )\n",
      "\n",
      "        elif isinstance(dtype, ExtensionDtype):\n",
      "            cls = dtype.construct_array_type()\n",
      "            # Note: for RangeIndex and CategoricalDtype self vs self._values\n",
      "            #  behaves differently here.\n",
      "            new_values = cls._from_sequence(self, dtype=dtype, copy=copy)\n",
      "\n",
      "        else:\n",
      "            try:\n",
      "                if dtype == str:\n",
      "                    # GH#38607\n",
      "                    new_values = values.astype(dtype, copy=copy)\n",
      "                else:\n",
      "                    # GH#13149 specifically use astype_nansafe instead of astype\n",
      "                    new_values = astype_nansafe(values, dtype=dtype, copy=copy)\n",
      "            except IntCastingNaNError:\n",
      "                raise\n",
      "            except (TypeError, ValueError) as err:\n",
      "                if dtype.kind == \"u\" and \"losslessly\" in str(err):\n",
      "                    # keep the message from _astype_float_to_int_nansafe\n",
      "                    raise\n",
      "                raise TypeError(\n",
      "                    f\"Cannot cast {type(self).__name__} to dtype {dtype}\"\n",
      "                ) from err\n",
      "\n",
      "        # pass copy=False because any copying will be done in the astype above\n",
      "        if self._is_backward_compat_public_numeric_index:\n",
      "            # this block is needed so e.g. NumericIndex[int8].astype(\"int32\") returns\n",
      "            # NumericIndex[int32] and not Int64Index with dtype int64.\n",
      "            # When Int64Index etc. are removed from the code base, removed this also.\n",
      "            if isinstance(dtype, np.dtype) and is_numeric_dtype(dtype):\n",
      "                return self._constructor(\n",
      "                    new_values, name=self.name, dtype=dtype, copy=False\n",
      "                )\n",
      "        return Index(new_values, name=self.name, dtype=new_values.dtype, copy=False)\n",
      "\n",
      "------------\n",
      "Method name: copy\n",
      "Method definition:     def copy(\n",
      "        self: _IndexT,\n",
      "        name: Hashable | None = None,\n",
      "        deep: bool = False,\n",
      "        dtype: Dtype | None = None,\n",
      "        names: Sequence[Hashable] | None = None,\n",
      "    ) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Make a copy of this object.\n",
      "\n",
      "        Name and dtype sets those attributes on the new object.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        name : Label, optional\n",
      "            Set name for new object.\n",
      "        deep : bool, default False\n",
      "        dtype : numpy dtype or pandas type, optional\n",
      "            Set dtype for new object.\n",
      "\n",
      "            .. deprecated:: 1.2.0\n",
      "                use ``astype`` method instead.\n",
      "        names : list-like, optional\n",
      "            Kept for compatibility with MultiIndex. Should not be used.\n",
      "\n",
      "            .. deprecated:: 1.4.0\n",
      "                use ``name`` instead.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "            Index refer to new object which is a copy of this object.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        In most cases, there should be no functional difference from using\n",
      "        ``deep``, but if ``deep`` is passed it will attempt to deepcopy.\n",
      "        \"\"\"\n",
      "        if names is not None:\n",
      "            warnings.warn(\n",
      "                \"parameter names is deprecated and will be removed in a future \"\n",
      "                \"version. Use the name parameter instead.\",\n",
      "                FutureWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "\n",
      "        name = self._validate_names(name=name, names=names, deep=deep)[0]\n",
      "        if deep:\n",
      "            new_data = self._data.copy()\n",
      "            new_index = type(self)._simple_new(new_data, name=name)\n",
      "        else:\n",
      "            new_index = self._rename(name=name)\n",
      "\n",
      "        if dtype:\n",
      "            warnings.warn(\n",
      "                \"parameter dtype is deprecated and will be removed in a future \"\n",
      "                \"version. Use the astype method instead.\",\n",
      "                FutureWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "            new_index = new_index.astype(dtype)\n",
      "        return new_index\n",
      "\n",
      "------------\n",
      "Method name: delete\n",
      "Method definition:     def delete(self: _IndexT, loc) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Make new Index with passed location(-s) deleted.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        loc : int or list of int\n",
      "            Location of item(-s) which will be deleted.\n",
      "            Use a list of locations to delete more than one value at the same time.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "            Will be same type as self, except for RangeIndex.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.delete : Delete any rows and column from NumPy array (ndarray).\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['a', 'b', 'c'])\n",
      "        >>> idx.delete(1)\n",
      "        Index(['a', 'c'], dtype='object')\n",
      "\n",
      "        >>> idx = pd.Index(['a', 'b', 'c'])\n",
      "        >>> idx.delete([0, 2])\n",
      "        Index(['b'], dtype='object')\n",
      "        \"\"\"\n",
      "        values = self._values\n",
      "        res_values: ArrayLike\n",
      "        if isinstance(values, np.ndarray):\n",
      "            # TODO(__array_function__): special casing will be unnecessary\n",
      "            res_values = np.delete(values, loc)\n",
      "        else:\n",
      "            res_values = values.delete(loc)\n",
      "\n",
      "        # _constructor so RangeIndex->Int64Index\n",
      "        return self._constructor._simple_new(res_values, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: difference\n",
      "Method definition:     @final\n",
      "    def difference(self, other, sort=None):\n",
      "        \"\"\"\n",
      "        Return a new Index with elements of index not in `other`.\n",
      "\n",
      "        This is the set difference of two Index objects.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index or array-like\n",
      "        sort : False or None, default None\n",
      "            Whether to sort the resulting index. By default, the\n",
      "            values are attempted to be sorted, but any TypeError from\n",
      "            incomparable elements is caught by pandas.\n",
      "\n",
      "            * None : Attempt to sort the result, but catch any TypeErrors\n",
      "              from comparing incomparable elements.\n",
      "            * False : Do not sort the result.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        difference : Index\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx1 = pd.Index([2, 1, 3, 4])\n",
      "        >>> idx2 = pd.Index([3, 4, 5, 6])\n",
      "        >>> idx1.difference(idx2)\n",
      "        Int64Index([1, 2], dtype='int64')\n",
      "        >>> idx1.difference(idx2, sort=False)\n",
      "        Int64Index([2, 1], dtype='int64')\n",
      "        \"\"\"\n",
      "        self._validate_sort_keyword(sort)\n",
      "        self._assert_can_do_setop(other)\n",
      "        other, result_name = self._convert_can_do_setop(other)\n",
      "\n",
      "        # Note: we do NOT call _deprecate_dti_setop here, as there\n",
      "        #  is no requirement that .difference be commutative, so it does\n",
      "        #  not cast to object.\n",
      "\n",
      "        if self.equals(other):\n",
      "            # Note: we do not (yet) sort even if sort=None GH#24959\n",
      "            return self[:0].rename(result_name)\n",
      "\n",
      "        if len(other) == 0:\n",
      "            # Note: we do not (yet) sort even if sort=None GH#24959\n",
      "            return self.rename(result_name)\n",
      "\n",
      "        if not self._should_compare(other):\n",
      "            # Nothing matches -> difference is everything\n",
      "            return self.rename(result_name)\n",
      "\n",
      "        result = self._difference(other, sort=sort)\n",
      "        return self._wrap_difference_result(other, result)\n",
      "\n",
      "------------\n",
      "Method name: drop\n",
      "Method definition:     def drop(\n",
      "        self,\n",
      "        labels: Index | np.ndarray | Iterable[Hashable],\n",
      "        errors: IgnoreRaise = \"raise\",\n",
      "    ) -> Index:\n",
      "        \"\"\"\n",
      "        Make new Index with passed list of labels deleted.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        labels : array-like or scalar\n",
      "        errors : {'ignore', 'raise'}, default 'raise'\n",
      "            If 'ignore', suppress error and existing labels are dropped.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        dropped : Index\n",
      "            Will be same type as self, except for RangeIndex.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        KeyError\n",
      "            If not all of the labels are found in the selected axis\n",
      "        \"\"\"\n",
      "        if not isinstance(labels, Index):\n",
      "            # avoid materializing e.g. RangeIndex\n",
      "            arr_dtype = \"object\" if self.dtype == \"object\" else None\n",
      "            labels = com.index_labels_to_array(labels, dtype=arr_dtype)\n",
      "\n",
      "        indexer = self.get_indexer_for(labels)\n",
      "        mask = indexer == -1\n",
      "        if mask.any():\n",
      "            if errors != \"ignore\":\n",
      "                raise KeyError(f\"{list(labels[mask])} not found in axis\")\n",
      "            indexer = indexer[~mask]\n",
      "        return self.delete(indexer)\n",
      "\n",
      "------------\n",
      "Method name: drop_duplicates\n",
      "Method definition:     @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n",
      "    def drop_duplicates(self: _IndexT, keep: str_t | bool = \"first\") -> _IndexT:\n",
      "        \"\"\"\n",
      "        Return Index with duplicate values removed.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        keep : {'first', 'last', ``False``}, default 'first'\n",
      "            - 'first' : Drop duplicates except for the first occurrence.\n",
      "            - 'last' : Drop duplicates except for the last occurrence.\n",
      "            - ``False`` : Drop all duplicates.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        deduplicated : Index\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.drop_duplicates : Equivalent method on Series.\n",
      "        DataFrame.drop_duplicates : Equivalent method on DataFrame.\n",
      "        Index.duplicated : Related method on Index, indicating duplicate\n",
      "            Index values.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Generate an pandas.Index with duplicate values.\n",
      "\n",
      "        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])\n",
      "\n",
      "        The `keep` parameter controls  which duplicate values are removed.\n",
      "        The value 'first' keeps the first occurrence for each\n",
      "        set of duplicated entries. The default value of keep is 'first'.\n",
      "\n",
      "        >>> idx.drop_duplicates(keep='first')\n",
      "        Index(['lama', 'cow', 'beetle', 'hippo'], dtype='object')\n",
      "\n",
      "        The value 'last' keeps the last occurrence for each set of duplicated\n",
      "        entries.\n",
      "\n",
      "        >>> idx.drop_duplicates(keep='last')\n",
      "        Index(['cow', 'beetle', 'lama', 'hippo'], dtype='object')\n",
      "\n",
      "        The value ``False`` discards all sets of duplicated entries.\n",
      "\n",
      "        >>> idx.drop_duplicates(keep=False)\n",
      "        Index(['cow', 'beetle', 'hippo'], dtype='object')\n",
      "        \"\"\"\n",
      "        if self.is_unique:\n",
      "            return self._view()\n",
      "\n",
      "        return super().drop_duplicates(keep=keep)\n",
      "\n",
      "------------\n",
      "Method name: droplevel\n",
      "Method definition:     @final\n",
      "    def droplevel(self, level=0):\n",
      "        \"\"\"\n",
      "        Return index with requested level(s) removed.\n",
      "\n",
      "        If resulting index has only 1 level left, the result will be\n",
      "        of Index type, not MultiIndex.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        level : int, str, or list-like, default 0\n",
      "            If a string is given, must be the name of a level\n",
      "            If list-like, elements must be names or indexes of levels.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index or MultiIndex\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> mi = pd.MultiIndex.from_arrays(\n",
      "        ... [[1, 2], [3, 4], [5, 6]], names=['x', 'y', 'z'])\n",
      "        >>> mi\n",
      "        MultiIndex([(1, 3, 5),\n",
      "                    (2, 4, 6)],\n",
      "                   names=['x', 'y', 'z'])\n",
      "\n",
      "        >>> mi.droplevel()\n",
      "        MultiIndex([(3, 5),\n",
      "                    (4, 6)],\n",
      "                   names=['y', 'z'])\n",
      "\n",
      "        >>> mi.droplevel(2)\n",
      "        MultiIndex([(1, 3),\n",
      "                    (2, 4)],\n",
      "                   names=['x', 'y'])\n",
      "\n",
      "        >>> mi.droplevel('z')\n",
      "        MultiIndex([(1, 3),\n",
      "                    (2, 4)],\n",
      "                   names=['x', 'y'])\n",
      "\n",
      "        >>> mi.droplevel(['x', 'y'])\n",
      "        Int64Index([5, 6], dtype='int64', name='z')\n",
      "        \"\"\"\n",
      "        if not isinstance(level, (tuple, list)):\n",
      "            level = [level]\n",
      "\n",
      "        levnums = sorted(self._get_level_number(lev) for lev in level)[::-1]\n",
      "\n",
      "        return self._drop_level_numbers(levnums)\n",
      "\n",
      "------------\n",
      "Method name: dropna\n",
      "Method definition:     def dropna(self: _IndexT, how: str_t = \"any\") -> _IndexT:\n",
      "        \"\"\"\n",
      "        Return Index without NA/NaN values.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        how : {'any', 'all'}, default 'any'\n",
      "            If the Index is a MultiIndex, drop the value when any or all levels\n",
      "            are NaN.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "        \"\"\"\n",
      "        if how not in (\"any\", \"all\"):\n",
      "            raise ValueError(f\"invalid how option: {how}\")\n",
      "\n",
      "        if self.hasnans:\n",
      "            res_values = self._values[~self._isnan]\n",
      "            return type(self)._simple_new(res_values, name=self.name)\n",
      "        return self._view()\n",
      "\n",
      "------------\n",
      "Method name: duplicated\n",
      "Method definition:     def duplicated(\n",
      "        self, keep: Literal[\"first\", \"last\", False] = \"first\"\n",
      "    ) -> npt.NDArray[np.bool_]:\n",
      "        \"\"\"\n",
      "        Indicate duplicate index values.\n",
      "\n",
      "        Duplicated values are indicated as ``True`` values in the resulting\n",
      "        array. Either all duplicates, all except the first, or all except the\n",
      "        last occurrence of duplicates can be indicated.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        keep : {'first', 'last', False}, default 'first'\n",
      "            The value or values in a set of duplicates to mark as missing.\n",
      "\n",
      "            - 'first' : Mark duplicates as ``True`` except for the first\n",
      "              occurrence.\n",
      "            - 'last' : Mark duplicates as ``True`` except for the last\n",
      "              occurrence.\n",
      "            - ``False`` : Mark all duplicates as ``True``.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        np.ndarray[bool]\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.duplicated : Equivalent method on pandas.Series.\n",
      "        DataFrame.duplicated : Equivalent method on pandas.DataFrame.\n",
      "        Index.drop_duplicates : Remove duplicate values from Index.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        By default, for each set of duplicated values, the first occurrence is\n",
      "        set to False and all others to True:\n",
      "\n",
      "        >>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n",
      "        >>> idx.duplicated()\n",
      "        array([False, False,  True, False,  True])\n",
      "\n",
      "        which is equivalent to\n",
      "\n",
      "        >>> idx.duplicated(keep='first')\n",
      "        array([False, False,  True, False,  True])\n",
      "\n",
      "        By using 'last', the last occurrence of each set of duplicated values\n",
      "        is set on False and all others on True:\n",
      "\n",
      "        >>> idx.duplicated(keep='last')\n",
      "        array([ True, False,  True, False, False])\n",
      "\n",
      "        By setting keep on ``False``, all duplicates are True:\n",
      "\n",
      "        >>> idx.duplicated(keep=False)\n",
      "        array([ True, False,  True, False,  True])\n",
      "        \"\"\"\n",
      "        if self.is_unique:\n",
      "            # fastpath available bc we are immutable\n",
      "            return np.zeros(len(self), dtype=bool)\n",
      "        return self._duplicated(keep=keep)\n",
      "\n",
      "------------\n",
      "Method name: equals\n",
      "Method definition:     def equals(self, other: Any) -> bool:\n",
      "        \"\"\"\n",
      "        Determine if two Index object are equal.\n",
      "\n",
      "        The things that are being compared are:\n",
      "\n",
      "        * The elements inside the Index object.\n",
      "        * The order of the elements inside the Index object.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Any\n",
      "            The other object to compare against.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            True if \"other\" is an Index and it has the same elements and order\n",
      "            as the calling index; False otherwise.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx1 = pd.Index([1, 2, 3])\n",
      "        >>> idx1\n",
      "        Int64Index([1, 2, 3], dtype='int64')\n",
      "        >>> idx1.equals(pd.Index([1, 2, 3]))\n",
      "        True\n",
      "\n",
      "        The elements inside are compared\n",
      "\n",
      "        >>> idx2 = pd.Index([\"1\", \"2\", \"3\"])\n",
      "        >>> idx2\n",
      "        Index(['1', '2', '3'], dtype='object')\n",
      "\n",
      "        >>> idx1.equals(idx2)\n",
      "        False\n",
      "\n",
      "        The order is compared\n",
      "\n",
      "        >>> ascending_idx = pd.Index([1, 2, 3])\n",
      "        >>> ascending_idx\n",
      "        Int64Index([1, 2, 3], dtype='int64')\n",
      "        >>> descending_idx = pd.Index([3, 2, 1])\n",
      "        >>> descending_idx\n",
      "        Int64Index([3, 2, 1], dtype='int64')\n",
      "        >>> ascending_idx.equals(descending_idx)\n",
      "        False\n",
      "\n",
      "        The dtype is *not* compared\n",
      "\n",
      "        >>> int64_idx = pd.Index([1, 2, 3], dtype='int64')\n",
      "        >>> int64_idx\n",
      "        Int64Index([1, 2, 3], dtype='int64')\n",
      "        >>> uint64_idx = pd.Index([1, 2, 3], dtype='uint64')\n",
      "        >>> uint64_idx\n",
      "        UInt64Index([1, 2, 3], dtype='uint64')\n",
      "        >>> int64_idx.equals(uint64_idx)\n",
      "        True\n",
      "        \"\"\"\n",
      "        if self.is_(other):\n",
      "            return True\n",
      "\n",
      "        if not isinstance(other, Index):\n",
      "            return False\n",
      "\n",
      "        if is_object_dtype(self.dtype) and not is_object_dtype(other.dtype):\n",
      "            # if other is not object, use other's logic for coercion\n",
      "            return other.equals(self)\n",
      "\n",
      "        if isinstance(other, ABCMultiIndex):\n",
      "            # d-level MultiIndex can equal d-tuple Index\n",
      "            return other.equals(self)\n",
      "\n",
      "        if isinstance(self._values, ExtensionArray):\n",
      "            # Dispatch to the ExtensionArray's .equals method.\n",
      "            if not isinstance(other, type(self)):\n",
      "                return False\n",
      "\n",
      "            earr = cast(ExtensionArray, self._data)\n",
      "            return earr.equals(other._data)\n",
      "\n",
      "        if is_extension_array_dtype(other.dtype):\n",
      "            # All EA-backed Index subclasses override equals\n",
      "            return other.equals(self)\n",
      "\n",
      "        return array_equivalent(self._values, other._values)\n",
      "\n",
      "------------\n",
      "Method name: factorize\n",
      "Method definition:     @doc(\n",
      "        algorithms.factorize,\n",
      "        values=\"\",\n",
      "        order=\"\",\n",
      "        size_hint=\"\",\n",
      "        sort=textwrap.dedent(\n",
      "            \"\"\"\\\n",
      "            sort : bool, default False\n",
      "                Sort `uniques` and shuffle `codes` to maintain the\n",
      "                relationship.\n",
      "            \"\"\"\n",
      "        ),\n",
      "    )\n",
      "    def factorize(\n",
      "        self,\n",
      "        sort: bool = False,\n",
      "        na_sentinel: int | lib.NoDefault = lib.no_default,\n",
      "        use_na_sentinel: bool | lib.NoDefault = lib.no_default,\n",
      "    ):\n",
      "        return algorithms.factorize(\n",
      "            self, sort=sort, na_sentinel=na_sentinel, use_na_sentinel=use_na_sentinel\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: fillna\n",
      "Method definition:     def fillna(self, value=None, downcast=None):\n",
      "        \"\"\"\n",
      "        Fill NA/NaN values with the specified value.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        value : scalar\n",
      "            Scalar value to use to fill holes (e.g. 0).\n",
      "            This value cannot be a list-likes.\n",
      "        downcast : dict, default is None\n",
      "            A dict of item->dtype of what to downcast if possible,\n",
      "            or the string 'infer' which will try to downcast to an appropriate\n",
      "            equal type (e.g. float64 to int64 if possible).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        DataFrame.fillna : Fill NaN values of a DataFrame.\n",
      "        Series.fillna : Fill NaN Values of a Series.\n",
      "        \"\"\"\n",
      "\n",
      "        value = self._require_scalar(value)\n",
      "        if self.hasnans:\n",
      "            result = self.putmask(self._isnan, value)\n",
      "            if downcast is None:\n",
      "                # no need to care metadata other than name\n",
      "                # because it can't have freq if it has NaTs\n",
      "                return Index._with_infer(result, name=self.name)\n",
      "            raise NotImplementedError(\n",
      "                f\"{type(self).__name__}.fillna does not support 'downcast' \"\n",
      "                \"argument values other than 'None'.\"\n",
      "            )\n",
      "        return self._view()\n",
      "\n",
      "------------\n",
      "Method name: format\n",
      "Method definition:     def format(\n",
      "        self,\n",
      "        name: bool = False,\n",
      "        formatter: Callable | None = None,\n",
      "        na_rep: str_t = \"NaN\",\n",
      "    ) -> list[str_t]:\n",
      "        \"\"\"\n",
      "        Render a string representation of the Index.\n",
      "        \"\"\"\n",
      "        header = []\n",
      "        if name:\n",
      "            header.append(\n",
      "                pprint_thing(self.name, escape_chars=(\"\\t\", \"\\r\", \"\\n\"))\n",
      "                if self.name is not None\n",
      "                else \"\"\n",
      "            )\n",
      "\n",
      "        if formatter is not None:\n",
      "            return header + list(self.map(formatter))\n",
      "\n",
      "        return self._format_with_header(header, na_rep=na_rep)\n",
      "\n",
      "------------\n",
      "Method name: get_indexer\n",
      "Method definition:     @Appender(_index_shared_docs[\"get_indexer\"] % _index_doc_kwargs)\n",
      "    @final\n",
      "    def get_indexer(\n",
      "        self,\n",
      "        target,\n",
      "        method: str_t | None = None,\n",
      "        limit: int | None = None,\n",
      "        tolerance=None,\n",
      "    ) -> npt.NDArray[np.intp]:\n",
      "        method = missing.clean_reindex_fill_method(method)\n",
      "        orig_target = target\n",
      "        target = self._maybe_cast_listlike_indexer(target)\n",
      "\n",
      "        self._check_indexing_method(method, limit, tolerance)\n",
      "\n",
      "        if not self._index_as_unique:\n",
      "            raise InvalidIndexError(self._requires_unique_msg)\n",
      "\n",
      "        if len(target) == 0:\n",
      "            return np.array([], dtype=np.intp)\n",
      "\n",
      "        if not self._should_compare(target) and not self._should_partial_index(target):\n",
      "            # IntervalIndex get special treatment bc numeric scalars can be\n",
      "            #  matched to Interval scalars\n",
      "            return self._get_indexer_non_comparable(target, method=method, unique=True)\n",
      "\n",
      "        if is_categorical_dtype(self.dtype):\n",
      "            # _maybe_cast_listlike_indexer ensures target has our dtype\n",
      "            #  (could improve perf by doing _should_compare check earlier?)\n",
      "            assert is_dtype_equal(self.dtype, target.dtype)\n",
      "\n",
      "            indexer = self._engine.get_indexer(target.codes)\n",
      "            if self.hasnans and target.hasnans:\n",
      "                # After _maybe_cast_listlike_indexer, target elements which do not\n",
      "                # belong to some category are changed to NaNs\n",
      "                # Mask to track actual NaN values compared to inserted NaN values\n",
      "                # GH#45361\n",
      "                target_nans = isna(orig_target)\n",
      "                loc = self.get_loc(np.nan)\n",
      "                mask = target.isna()\n",
      "                indexer[target_nans] = loc\n",
      "                indexer[mask & ~target_nans] = -1\n",
      "            return indexer\n",
      "\n",
      "        if is_categorical_dtype(target.dtype):\n",
      "            # potential fastpath\n",
      "            # get an indexer for unique categories then propagate to codes via take_nd\n",
      "            # get_indexer instead of _get_indexer needed for MultiIndex cases\n",
      "            #  e.g. test_append_different_columns_types\n",
      "            categories_indexer = self.get_indexer(target.categories)\n",
      "\n",
      "            indexer = algos.take_nd(categories_indexer, target.codes, fill_value=-1)\n",
      "\n",
      "            if (not self._is_multi and self.hasnans) and target.hasnans:\n",
      "                # Exclude MultiIndex because hasnans raises NotImplementedError\n",
      "                # we should only get here if we are unique, so loc is an integer\n",
      "                # GH#41934\n",
      "                loc = self.get_loc(np.nan)\n",
      "                mask = target.isna()\n",
      "                indexer[mask] = loc\n",
      "\n",
      "            return ensure_platform_int(indexer)\n",
      "\n",
      "        pself, ptarget = self._maybe_promote(target)\n",
      "        if pself is not self or ptarget is not target:\n",
      "            return pself.get_indexer(\n",
      "                ptarget, method=method, limit=limit, tolerance=tolerance\n",
      "            )\n",
      "\n",
      "        if is_dtype_equal(self.dtype, target.dtype) and self.equals(target):\n",
      "            # Only call equals if we have same dtype to avoid inference/casting\n",
      "            return np.arange(len(target), dtype=np.intp)\n",
      "\n",
      "        if not is_dtype_equal(self.dtype, target.dtype) and not is_interval_dtype(\n",
      "            self.dtype\n",
      "        ):\n",
      "            # IntervalIndex gets special treatment for partial-indexing\n",
      "            dtype = self._find_common_type_compat(target)\n",
      "\n",
      "            this = self.astype(dtype, copy=False)\n",
      "            target = target.astype(dtype, copy=False)\n",
      "            return this._get_indexer(\n",
      "                target, method=method, limit=limit, tolerance=tolerance\n",
      "            )\n",
      "\n",
      "        return self._get_indexer(target, method, limit, tolerance)\n",
      "\n",
      "------------\n",
      "Method name: get_indexer_for\n",
      "Method definition:     @final\n",
      "    def get_indexer_for(self, target) -> npt.NDArray[np.intp]:\n",
      "        \"\"\"\n",
      "        Guaranteed return of an indexer even when non-unique.\n",
      "\n",
      "        This dispatches to get_indexer or get_indexer_non_unique\n",
      "        as appropriate.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        np.ndarray[np.intp]\n",
      "            List of indices.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([np.nan, 'var1', np.nan])\n",
      "        >>> idx.get_indexer_for([np.nan])\n",
      "        array([0, 2])\n",
      "        \"\"\"\n",
      "        if self._index_as_unique:\n",
      "            return self.get_indexer(target)\n",
      "        indexer, _ = self.get_indexer_non_unique(target)\n",
      "        return indexer\n",
      "\n",
      "------------\n",
      "Method name: get_indexer_non_unique\n",
      "Method definition:     @Appender(_index_shared_docs[\"get_indexer_non_unique\"] % _index_doc_kwargs)\n",
      "    def get_indexer_non_unique(\n",
      "        self, target\n",
      "    ) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
      "        target = ensure_index(target)\n",
      "        target = self._maybe_cast_listlike_indexer(target)\n",
      "\n",
      "        if not self._should_compare(target) and not is_interval_dtype(self.dtype):\n",
      "            # IntervalIndex get special treatment bc numeric scalars can be\n",
      "            #  matched to Interval scalars\n",
      "            return self._get_indexer_non_comparable(target, method=None, unique=False)\n",
      "\n",
      "        pself, ptarget = self._maybe_promote(target)\n",
      "        if pself is not self or ptarget is not target:\n",
      "            return pself.get_indexer_non_unique(ptarget)\n",
      "\n",
      "        if not is_dtype_equal(self.dtype, target.dtype):\n",
      "            # TODO: if object, could use infer_dtype to preempt costly\n",
      "            #  conversion if still non-comparable?\n",
      "            dtype = self._find_common_type_compat(target)\n",
      "\n",
      "            this = self.astype(dtype, copy=False)\n",
      "            that = target.astype(dtype, copy=False)\n",
      "            return this.get_indexer_non_unique(that)\n",
      "\n",
      "        # Note: _maybe_promote ensures we never get here with MultiIndex\n",
      "        #  self and non-Multi target\n",
      "        tgt_values = target._get_engine_target()\n",
      "        if self._is_multi and target._is_multi:\n",
      "            engine = self._engine\n",
      "            # Item \"IndexEngine\" of \"Union[IndexEngine, ExtensionEngine]\" has\n",
      "            # no attribute \"_extract_level_codes\"\n",
      "            tgt_values = engine._extract_level_codes(target)  # type: ignore[union-attr]\n",
      "\n",
      "        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n",
      "        return ensure_platform_int(indexer), ensure_platform_int(missing)\n",
      "\n",
      "------------\n",
      "Method name: get_level_values\n",
      "Method definition:     def _get_level_values(self, level) -> Index:\n",
      "        \"\"\"\n",
      "        Return an Index of values for requested level.\n",
      "\n",
      "        This is primarily useful to get an individual level of values from a\n",
      "        MultiIndex, but is provided on Index as well for compatibility.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        level : int or str\n",
      "            It is either the integer position or the name of the level.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "            Calling object, as there is only one level in the Index.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        MultiIndex.get_level_values : Get values for a level of a MultiIndex.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        For Index, level should be 0, since there are no multiple levels.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(list('abc'))\n",
      "        >>> idx\n",
      "        Index(['a', 'b', 'c'], dtype='object')\n",
      "\n",
      "        Get level values by supplying `level` as integer:\n",
      "\n",
      "        >>> idx.get_level_values(0)\n",
      "        Index(['a', 'b', 'c'], dtype='object')\n",
      "        \"\"\"\n",
      "        self._validate_index_level(level)\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: get_loc\n",
      "Method definition:     def get_loc(self, key, method=None, tolerance=None):\n",
      "        \"\"\"\n",
      "        Get integer location, slice or boolean mask for requested label.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        key : label\n",
      "        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n",
      "            * default: exact matches only.\n",
      "            * pad / ffill: find the PREVIOUS index value if no exact match.\n",
      "            * backfill / bfill: use NEXT index value if no exact match\n",
      "            * nearest: use the NEAREST index value if no exact match. Tied\n",
      "              distances are broken by preferring the larger index value.\n",
      "\n",
      "            .. deprecated:: 1.4\n",
      "                Use index.get_indexer([item], method=...) instead.\n",
      "\n",
      "        tolerance : int or float, optional\n",
      "            Maximum distance from index value for inexact matches. The value of\n",
      "            the index at the matching location must satisfy the equation\n",
      "            ``abs(index[loc] - key) <= tolerance``.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        loc : int if unique index, slice if monotonic index, else mask\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> unique_index = pd.Index(list('abc'))\n",
      "        >>> unique_index.get_loc('b')\n",
      "        1\n",
      "\n",
      "        >>> monotonic_index = pd.Index(list('abbc'))\n",
      "        >>> monotonic_index.get_loc('b')\n",
      "        slice(1, 3, None)\n",
      "\n",
      "        >>> non_monotonic_index = pd.Index(list('abcb'))\n",
      "        >>> non_monotonic_index.get_loc('b')\n",
      "        array([False,  True, False,  True])\n",
      "        \"\"\"\n",
      "        if method is None:\n",
      "            if tolerance is not None:\n",
      "                raise ValueError(\n",
      "                    \"tolerance argument only valid if using pad, \"\n",
      "                    \"backfill or nearest lookups\"\n",
      "                )\n",
      "            casted_key = self._maybe_cast_indexer(key)\n",
      "            try:\n",
      "                return self._engine.get_loc(casted_key)\n",
      "            except KeyError as err:\n",
      "                raise KeyError(key) from err\n",
      "            except TypeError:\n",
      "                # If we have a listlike key, _check_indexing_error will raise\n",
      "                #  InvalidIndexError. Otherwise we fall through and re-raise\n",
      "                #  the TypeError.\n",
      "                self._check_indexing_error(key)\n",
      "                raise\n",
      "\n",
      "        # GH#42269\n",
      "        warnings.warn(\n",
      "            f\"Passing method to {type(self).__name__}.get_loc is deprecated \"\n",
      "            \"and will raise in a future version. Use \"\n",
      "            \"index.get_indexer([item], method=...) instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "\n",
      "        if is_scalar(key) and isna(key) and not self.hasnans:\n",
      "            raise KeyError(key)\n",
      "\n",
      "        if tolerance is not None:\n",
      "            tolerance = self._convert_tolerance(tolerance, np.asarray(key))\n",
      "\n",
      "        indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n",
      "        if indexer.ndim > 1 or indexer.size > 1:\n",
      "            raise TypeError(\"get_loc requires scalar valued input\")\n",
      "        loc = indexer.item()\n",
      "        if loc == -1:\n",
      "            raise KeyError(key)\n",
      "        return loc\n",
      "\n",
      "------------\n",
      "Method name: get_slice_bound\n",
      "Method definition:     def get_slice_bound(\n",
      "        self, label, side: Literal[\"left\", \"right\"], kind=no_default\n",
      "    ) -> int:\n",
      "        \"\"\"\n",
      "        Calculate slice bound that corresponds to given label.\n",
      "\n",
      "        Returns leftmost (one-past-the-rightmost if ``side=='right'``) position\n",
      "        of given label.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        label : object\n",
      "        side : {'left', 'right'}\n",
      "        kind : {'loc', 'getitem'} or None\n",
      "\n",
      "            .. deprecated:: 1.4.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        int\n",
      "            Index of label.\n",
      "        \"\"\"\n",
      "        assert kind in [\"loc\", \"getitem\", None, no_default]\n",
      "        self._deprecated_arg(kind, \"kind\", \"get_slice_bound\")\n",
      "\n",
      "        if side not in (\"left\", \"right\"):\n",
      "            raise ValueError(\n",
      "                \"Invalid value for side kwarg, must be either \"\n",
      "                f\"'left' or 'right': {side}\"\n",
      "            )\n",
      "\n",
      "        original_label = label\n",
      "\n",
      "        # For datetime indices label may be a string that has to be converted\n",
      "        # to datetime boundary according to its resolution.\n",
      "        label = self._maybe_cast_slice_bound(label, side)\n",
      "\n",
      "        # we need to look up the label\n",
      "        try:\n",
      "            slc = self.get_loc(label)\n",
      "        except KeyError as err:\n",
      "            try:\n",
      "                return self._searchsorted_monotonic(label, side)\n",
      "            except ValueError:\n",
      "                # raise the original KeyError\n",
      "                raise err\n",
      "\n",
      "        if isinstance(slc, np.ndarray):\n",
      "            # get_loc may return a boolean array, which\n",
      "            # is OK as long as they are representable by a slice.\n",
      "            assert is_bool_dtype(slc.dtype)\n",
      "            slc = lib.maybe_booleans_to_slice(slc.view(\"u1\"))\n",
      "            if isinstance(slc, np.ndarray):\n",
      "                raise KeyError(\n",
      "                    f\"Cannot get {side} slice bound for non-unique \"\n",
      "                    f\"label: {repr(original_label)}\"\n",
      "                )\n",
      "\n",
      "        if isinstance(slc, slice):\n",
      "            if side == \"left\":\n",
      "                return slc.start\n",
      "            else:\n",
      "                return slc.stop\n",
      "        else:\n",
      "            if side == \"right\":\n",
      "                return slc + 1\n",
      "            else:\n",
      "                return slc\n",
      "\n",
      "------------\n",
      "Method name: get_value\n",
      "Method definition:     @final\n",
      "    def get_value(self, series: Series, key):\n",
      "        \"\"\"\n",
      "        Fast lookup of value from 1-dimensional ndarray.\n",
      "\n",
      "        Only use this if you know what you're doing.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        scalar or Series\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            \"get_value is deprecated and will be removed in a future version. \"\n",
      "            \"Use Series[key] instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "\n",
      "        self._check_indexing_error(key)\n",
      "\n",
      "        try:\n",
      "            # GH 20882, 21257\n",
      "            # First try to convert the key to a location\n",
      "            # If that fails, raise a KeyError if an integer\n",
      "            # index, otherwise, see if key is an integer, and\n",
      "            # try that\n",
      "            loc = self.get_loc(key)\n",
      "        except KeyError:\n",
      "            if not self._should_fallback_to_positional:\n",
      "                raise\n",
      "            elif is_integer(key):\n",
      "                # If the Index cannot hold integer, then this is unambiguously\n",
      "                #  a locational lookup.\n",
      "                loc = key\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        return self._get_values_for_loc(series, loc, key)\n",
      "\n",
      "------------\n",
      "Method name: groupby\n",
      "Method definition:     @final\n",
      "    def groupby(self, values) -> PrettyDict[Hashable, np.ndarray]:\n",
      "        \"\"\"\n",
      "        Group the index labels by a given array of values.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        values : array\n",
      "            Values used to determine the groups.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "            {group name -> group labels}\n",
      "        \"\"\"\n",
      "        # TODO: if we are a MultiIndex, we can do better\n",
      "        # that converting to tuples\n",
      "        if isinstance(values, ABCMultiIndex):\n",
      "            values = values._values\n",
      "        values = Categorical(values)\n",
      "        result = values._reverse_indexer()\n",
      "\n",
      "        # map to the label\n",
      "        result = {k: self.take(v) for k, v in result.items()}\n",
      "\n",
      "        return PrettyDict(result)\n",
      "\n",
      "------------\n",
      "Method name: holds_integer\n",
      "Method definition:     @final\n",
      "    def holds_integer(self) -> bool:\n",
      "        \"\"\"\n",
      "        Whether the type is an integer type.\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"integer\", \"mixed-integer\"]\n",
      "\n",
      "------------\n",
      "Method name: identical\n",
      "Method definition:     @final\n",
      "    def identical(self, other) -> bool:\n",
      "        \"\"\"\n",
      "        Similar to equals, but checks that object attributes and types are also equal.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            If two Index objects have equal elements and same type True,\n",
      "            otherwise False.\n",
      "        \"\"\"\n",
      "        return (\n",
      "            self.equals(other)\n",
      "            and all(\n",
      "                getattr(self, c, None) == getattr(other, c, None)\n",
      "                for c in self._comparables\n",
      "            )\n",
      "            and type(self) == type(other)\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: insert\n",
      "Method definition:     def insert(self, loc: int, item) -> Index:\n",
      "        \"\"\"\n",
      "        Make new Index inserting new item at location.\n",
      "\n",
      "        Follows Python numpy.insert semantics for negative values.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        loc : int\n",
      "        item : object\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        new_index : Index\n",
      "        \"\"\"\n",
      "        item = lib.item_from_zerodim(item)\n",
      "        if is_valid_na_for_dtype(item, self.dtype) and self.dtype != object:\n",
      "            item = self._na_value\n",
      "\n",
      "        arr = self._values\n",
      "\n",
      "        try:\n",
      "            if isinstance(arr, ExtensionArray):\n",
      "                res_values = arr.insert(loc, item)\n",
      "                return type(self)._simple_new(res_values, name=self.name)\n",
      "            else:\n",
      "                item = self._validate_fill_value(item)\n",
      "        except (TypeError, ValueError, LossySetitemError):\n",
      "            # e.g. trying to insert an integer into a DatetimeIndex\n",
      "            #  We cannot keep the same dtype, so cast to the (often object)\n",
      "            #  minimal shared dtype before doing the insert.\n",
      "            dtype = self._find_common_type_compat(item)\n",
      "            return self.astype(dtype).insert(loc, item)\n",
      "\n",
      "        if arr.dtype != object or not isinstance(\n",
      "            item, (tuple, np.datetime64, np.timedelta64)\n",
      "        ):\n",
      "            # with object-dtype we need to worry about numpy incorrectly casting\n",
      "            # dt64/td64 to integer, also about treating tuples as sequences\n",
      "            # special-casing dt64/td64 https://github.com/numpy/numpy/issues/12550\n",
      "            casted = arr.dtype.type(item)\n",
      "            new_values = np.insert(arr, loc, casted)\n",
      "\n",
      "        else:\n",
      "            # error: No overload variant of \"insert\" matches argument types\n",
      "            # \"ndarray[Any, Any]\", \"int\", \"None\"\n",
      "            new_values = np.insert(arr, loc, None)  # type: ignore[call-overload]\n",
      "            loc = loc if loc >= 0 else loc - 1\n",
      "            new_values[loc] = item\n",
      "\n",
      "        if self._typ == \"numericindex\":\n",
      "            # Use self._constructor instead of Index to retain NumericIndex GH#43921\n",
      "            # TODO(2.0) can use Index instead of self._constructor\n",
      "            return self._constructor._with_infer(new_values, name=self.name)\n",
      "        else:\n",
      "            return Index._with_infer(new_values, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: intersection\n",
      "Method definition:     @final\n",
      "    def intersection(self, other, sort=False):\n",
      "        \"\"\"\n",
      "        Form the intersection of two Index objects.\n",
      "\n",
      "        This returns a new Index with elements common to the index and `other`.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index or array-like\n",
      "        sort : False or None, default False\n",
      "            Whether to sort the resulting index.\n",
      "\n",
      "            * False : do not sort the result.\n",
      "            * None : sort the result, except when `self` and `other` are equal\n",
      "              or when the values cannot be compared.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        intersection : Index\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx1 = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx2 = pd.Index([3, 4, 5, 6])\n",
      "        >>> idx1.intersection(idx2)\n",
      "        Int64Index([3, 4], dtype='int64')\n",
      "        \"\"\"\n",
      "        self._validate_sort_keyword(sort)\n",
      "        self._assert_can_do_setop(other)\n",
      "        other, result_name = self._convert_can_do_setop(other)\n",
      "\n",
      "        if not is_dtype_equal(self.dtype, other.dtype):\n",
      "            self._deprecate_dti_setop(other, \"intersection\")\n",
      "\n",
      "        if self.equals(other):\n",
      "            if self.has_duplicates:\n",
      "                return self.unique()._get_reconciled_name_object(other)\n",
      "            return self._get_reconciled_name_object(other)\n",
      "\n",
      "        if len(self) == 0 or len(other) == 0:\n",
      "            # fastpath; we need to be careful about having commutativity\n",
      "\n",
      "            if self._is_multi or other._is_multi:\n",
      "                # _convert_can_do_setop ensures that we have both or neither\n",
      "                # We retain self.levels\n",
      "                return self[:0].rename(result_name)\n",
      "\n",
      "            dtype = self._find_common_type_compat(other)\n",
      "            if is_dtype_equal(self.dtype, dtype):\n",
      "                # Slicing allows us to retain DTI/TDI.freq, RangeIndex\n",
      "\n",
      "                # Note: self[:0] vs other[:0] affects\n",
      "                #  1) which index's `freq` we get in DTI/TDI cases\n",
      "                #     This may be a historical artifact, i.e. no documented\n",
      "                #     reason for this choice.\n",
      "                #  2) The `step` we get in RangeIndex cases\n",
      "                if len(self) == 0:\n",
      "                    return self[:0].rename(result_name)\n",
      "                else:\n",
      "                    return other[:0].rename(result_name)\n",
      "\n",
      "            return Index([], dtype=dtype, name=result_name)\n",
      "\n",
      "        elif not self._should_compare(other):\n",
      "            # We can infer that the intersection is empty.\n",
      "            if isinstance(self, ABCMultiIndex):\n",
      "                return self[:0].rename(result_name)\n",
      "            return Index([], name=result_name)\n",
      "\n",
      "        elif not is_dtype_equal(self.dtype, other.dtype):\n",
      "            dtype = self._find_common_type_compat(other)\n",
      "            this = self.astype(dtype, copy=False)\n",
      "            other = other.astype(dtype, copy=False)\n",
      "            return this.intersection(other, sort=sort)\n",
      "\n",
      "        result = self._intersection(other, sort=sort)\n",
      "        return self._wrap_intersection_result(other, result)\n",
      "\n",
      "------------\n",
      "Method name: is_\n",
      "Method definition:     @final\n",
      "    def is_(self, other) -> bool:\n",
      "        \"\"\"\n",
      "        More flexible, faster check like ``is`` but that works through views.\n",
      "\n",
      "        Note: this is *not* the same as ``Index.identical()``, which checks\n",
      "        that metadata is also the same.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : object\n",
      "            Other object to compare against.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            True if both have same underlying data, False otherwise.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.identical : Works like ``Index.is_`` but also checks metadata.\n",
      "        \"\"\"\n",
      "        if self is other:\n",
      "            return True\n",
      "        elif not hasattr(other, \"_id\"):\n",
      "            return False\n",
      "        elif self._id is None or other._id is None:\n",
      "            return False\n",
      "        else:\n",
      "            return self._id is other._id\n",
      "\n",
      "------------\n",
      "Method name: is_boolean\n",
      "Method definition:     @final\n",
      "    def is_boolean(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index only consists of booleans.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index only consists of booleans.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([True, False, True])\n",
      "        >>> idx.is_boolean()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([\"True\", \"False\", \"True\"])\n",
      "        >>> idx.is_boolean()\n",
      "        False\n",
      "\n",
      "        >>> idx = pd.Index([True, False, \"True\"])\n",
      "        >>> idx.is_boolean()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"boolean\"]\n",
      "\n",
      "------------\n",
      "Method name: is_categorical\n",
      "Method definition:     @final\n",
      "    def is_categorical(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index holds categorical data.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            True if the Index is categorical.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        CategoricalIndex : Index for categorical data.\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n",
      "        ...                 \"Watermelon\"]).astype(\"category\")\n",
      "        >>> idx.is_categorical()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 3, 5, 7])\n",
      "        >>> idx.is_categorical()\n",
      "        False\n",
      "\n",
      "        >>> s = pd.Series([\"Peter\", \"Victor\", \"Elisabeth\", \"Mar\"])\n",
      "        >>> s\n",
      "        0        Peter\n",
      "        1       Victor\n",
      "        2    Elisabeth\n",
      "        3          Mar\n",
      "        dtype: object\n",
      "        >>> s.index.is_categorical()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"categorical\"]\n",
      "\n",
      "------------\n",
      "Method name: is_floating\n",
      "Method definition:     @final\n",
      "    def is_floating(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index is a floating type.\n",
      "\n",
      "        The Index may consist of only floats, NaNs, or a mix of floats,\n",
      "        integers, or NaNs.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index only consists of only consists of floats, NaNs, or\n",
      "            a mix of floats, integers, or NaNs.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n",
      "        >>> idx.is_floating()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1.0, 2.0, np.nan, 4.0])\n",
      "        >>> idx.is_floating()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 2, 3, 4, np.nan])\n",
      "        >>> idx.is_floating()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx.is_floating()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"floating\", \"mixed-integer-float\", \"integer-na\"]\n",
      "\n",
      "------------\n",
      "Method name: is_integer\n",
      "Method definition:     @final\n",
      "    def is_integer(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index only consists of integers.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index only consists of integers.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx.is_integer()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n",
      "        >>> idx.is_integer()\n",
      "        False\n",
      "\n",
      "        >>> idx = pd.Index([\"Apple\", \"Mango\", \"Watermelon\"])\n",
      "        >>> idx.is_integer()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"integer\"]\n",
      "\n",
      "------------\n",
      "Method name: is_interval\n",
      "Method definition:     @final\n",
      "    def is_interval(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index holds Interval objects.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index holds Interval objects.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        IntervalIndex : Index for Interval objects.\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([pd.Interval(left=0, right=5),\n",
      "        ...                 pd.Interval(left=5, right=10)])\n",
      "        >>> idx.is_interval()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 3, 5, 7])\n",
      "        >>> idx.is_interval()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"interval\"]\n",
      "\n",
      "------------\n",
      "Method name: is_mixed\n",
      "Method definition:     @final\n",
      "    def is_mixed(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index holds data with mixed data types.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['a', np.nan, 'b'])\n",
      "        >>> idx.is_mixed()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1.0, 2.0, 3.0, 5.0])\n",
      "        >>> idx.is_mixed()\n",
      "        False\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            \"Index.is_mixed is deprecated and will be removed in a future version. \"\n",
      "            \"Check index.inferred_type directly instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        return self.inferred_type in [\"mixed\"]\n",
      "\n",
      "------------\n",
      "Method name: is_numeric\n",
      "Method definition:     @final\n",
      "    def is_numeric(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index only consists of numeric data.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index only consists of numeric data.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_object : Check if the Index is of the object dtype.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n",
      "        >>> idx.is_numeric()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 2, 3, 4.0])\n",
      "        >>> idx.is_numeric()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx.is_numeric()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 2, 3, 4.0, np.nan])\n",
      "        >>> idx.is_numeric()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([1, 2, 3, 4.0, np.nan, \"Apple\"])\n",
      "        >>> idx.is_numeric()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return self.inferred_type in [\"integer\", \"floating\"]\n",
      "\n",
      "------------\n",
      "Method name: is_object\n",
      "Method definition:     @final\n",
      "    def is_object(self) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the Index is of the object dtype.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        bool\n",
      "            Whether or not the Index is of the object dtype.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        is_boolean : Check if the Index only consists of booleans.\n",
      "        is_integer : Check if the Index only consists of integers.\n",
      "        is_floating : Check if the Index is a floating type.\n",
      "        is_numeric : Check if the Index only consists of numeric data.\n",
      "        is_categorical : Check if the Index holds categorical data.\n",
      "        is_interval : Check if the Index holds Interval objects.\n",
      "        is_mixed : Check if the Index holds data with mixed data types.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([\"Apple\", \"Mango\", \"Watermelon\"])\n",
      "        >>> idx.is_object()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([\"Apple\", \"Mango\", 2.0])\n",
      "        >>> idx.is_object()\n",
      "        True\n",
      "\n",
      "        >>> idx = pd.Index([\"Watermelon\", \"Orange\", \"Apple\",\n",
      "        ...                 \"Watermelon\"]).astype(\"category\")\n",
      "        >>> idx.is_object()\n",
      "        False\n",
      "\n",
      "        >>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n",
      "        >>> idx.is_object()\n",
      "        False\n",
      "        \"\"\"\n",
      "        return is_object_dtype(self.dtype)\n",
      "\n",
      "------------\n",
      "Method name: is_type_compatible\n",
      "Method definition:     def is_type_compatible(self, kind: str_t) -> bool:\n",
      "        \"\"\"\n",
      "        Whether the index type is compatible with the provided type.\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            \"Index.is_type_compatible is deprecated and will be removed in a \"\n",
      "            \"future version.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        return kind == self.inferred_type\n",
      "\n",
      "------------\n",
      "Method name: isin\n",
      "Method definition:     def isin(self, values, level=None) -> npt.NDArray[np.bool_]:\n",
      "        \"\"\"\n",
      "        Return a boolean array where the index values are in `values`.\n",
      "\n",
      "        Compute boolean array of whether each index value is found in the\n",
      "        passed set of values. The length of the returned boolean array matches\n",
      "        the length of the index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        values : set or list-like\n",
      "            Sought values.\n",
      "        level : str or int, optional\n",
      "            Name or position of the index level to use (if the index is a\n",
      "            `MultiIndex`).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        np.ndarray[bool]\n",
      "            NumPy array of boolean values.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.isin : Same for Series.\n",
      "        DataFrame.isin : Same method for DataFrames.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        In the case of `MultiIndex` you must either specify `values` as a\n",
      "        list-like object containing tuples that are the same length as the\n",
      "        number of levels, or specify `level`. Otherwise it will raise a\n",
      "        ``ValueError``.\n",
      "\n",
      "        If `level` is specified:\n",
      "\n",
      "        - if it is the name of one *and only one* index level, use that level;\n",
      "        - otherwise it should be a number indicating level position.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([1,2,3])\n",
      "        >>> idx\n",
      "        Int64Index([1, 2, 3], dtype='int64')\n",
      "\n",
      "        Check whether each index value in a list of values.\n",
      "\n",
      "        >>> idx.isin([1, 4])\n",
      "        array([ True, False, False])\n",
      "\n",
      "        >>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n",
      "        ...                                  ['red', 'blue', 'green']],\n",
      "        ...                                  names=('number', 'color'))\n",
      "        >>> midx\n",
      "        MultiIndex([(1,   'red'),\n",
      "                    (2,  'blue'),\n",
      "                    (3, 'green')],\n",
      "                   names=['number', 'color'])\n",
      "\n",
      "        Check whether the strings in the 'color' level of the MultiIndex\n",
      "        are in a list of colors.\n",
      "\n",
      "        >>> midx.isin(['red', 'orange', 'yellow'], level='color')\n",
      "        array([ True, False, False])\n",
      "\n",
      "        To check across the levels of a MultiIndex, pass a list of tuples:\n",
      "\n",
      "        >>> midx.isin([(1, 'red'), (3, 'red')])\n",
      "        array([ True, False, False])\n",
      "\n",
      "        For a DatetimeIndex, string values in `values` are converted to\n",
      "        Timestamps.\n",
      "\n",
      "        >>> dates = ['2000-03-11', '2000-03-12', '2000-03-13']\n",
      "        >>> dti = pd.to_datetime(dates)\n",
      "        >>> dti\n",
      "        DatetimeIndex(['2000-03-11', '2000-03-12', '2000-03-13'],\n",
      "        dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "        >>> dti.isin(['2000-03-11'])\n",
      "        array([ True, False, False])\n",
      "        \"\"\"\n",
      "        if level is not None:\n",
      "            self._validate_index_level(level)\n",
      "        return algos.isin(self._values, values)\n",
      "\n",
      "------------\n",
      "Method name: isna\n",
      "Method definition:     @final\n",
      "    def isna(self) -> npt.NDArray[np.bool_]:\n",
      "        \"\"\"\n",
      "        Detect missing values.\n",
      "\n",
      "        Return a boolean same-sized object indicating if the values are NA.\n",
      "        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n",
      "        mapped to ``True`` values.\n",
      "        Everything else get mapped to ``False`` values. Characters such as\n",
      "        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n",
      "        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray[bool]\n",
      "            A boolean array of whether my values are NA.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.notna : Boolean inverse of isna.\n",
      "        Index.dropna : Omit entries with missing values.\n",
      "        isna : Top-level isna.\n",
      "        Series.isna : Detect missing values in Series object.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Show which entries in a pandas.Index are NA. The result is an\n",
      "        array.\n",
      "\n",
      "        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n",
      "        >>> idx\n",
      "        Float64Index([5.2, 6.0, nan], dtype='float64')\n",
      "        >>> idx.isna()\n",
      "        array([False, False,  True])\n",
      "\n",
      "        Empty strings are not considered NA values. None is considered an NA\n",
      "        value.\n",
      "\n",
      "        >>> idx = pd.Index(['black', '', 'red', None])\n",
      "        >>> idx\n",
      "        Index(['black', '', 'red', None], dtype='object')\n",
      "        >>> idx.isna()\n",
      "        array([False, False, False,  True])\n",
      "\n",
      "        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n",
      "\n",
      "        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n",
      "        ...                         pd.Timestamp(''), None, pd.NaT])\n",
      "        >>> idx\n",
      "        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n",
      "                      dtype='datetime64[ns]', freq=None)\n",
      "        >>> idx.isna()\n",
      "        array([False,  True,  True,  True])\n",
      "        \"\"\"\n",
      "        return self._isnan\n",
      "\n",
      "------------\n",
      "Method name: isnull\n",
      "Method definition:     @final\n",
      "    def isna(self) -> npt.NDArray[np.bool_]:\n",
      "        \"\"\"\n",
      "        Detect missing values.\n",
      "\n",
      "        Return a boolean same-sized object indicating if the values are NA.\n",
      "        NA values, such as ``None``, :attr:`numpy.NaN` or :attr:`pd.NaT`, get\n",
      "        mapped to ``True`` values.\n",
      "        Everything else get mapped to ``False`` values. Characters such as\n",
      "        empty strings `''` or :attr:`numpy.inf` are not considered NA values\n",
      "        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray[bool]\n",
      "            A boolean array of whether my values are NA.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.notna : Boolean inverse of isna.\n",
      "        Index.dropna : Omit entries with missing values.\n",
      "        isna : Top-level isna.\n",
      "        Series.isna : Detect missing values in Series object.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Show which entries in a pandas.Index are NA. The result is an\n",
      "        array.\n",
      "\n",
      "        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n",
      "        >>> idx\n",
      "        Float64Index([5.2, 6.0, nan], dtype='float64')\n",
      "        >>> idx.isna()\n",
      "        array([False, False,  True])\n",
      "\n",
      "        Empty strings are not considered NA values. None is considered an NA\n",
      "        value.\n",
      "\n",
      "        >>> idx = pd.Index(['black', '', 'red', None])\n",
      "        >>> idx\n",
      "        Index(['black', '', 'red', None], dtype='object')\n",
      "        >>> idx.isna()\n",
      "        array([False, False, False,  True])\n",
      "\n",
      "        For datetimes, `NaT` (Not a Time) is considered as an NA value.\n",
      "\n",
      "        >>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n",
      "        ...                         pd.Timestamp(''), None, pd.NaT])\n",
      "        >>> idx\n",
      "        DatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n",
      "                      dtype='datetime64[ns]', freq=None)\n",
      "        >>> idx.isna()\n",
      "        array([False,  True,  True,  True])\n",
      "        \"\"\"\n",
      "        return self._isnan\n",
      "\n",
      "------------\n",
      "Method name: item\n",
      "Method definition:     def item(self):\n",
      "        \"\"\"\n",
      "        Return the first element of the underlying data as a Python scalar.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        scalar\n",
      "            The first element of %(klass)s.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If the data is not length-1.\n",
      "        \"\"\"\n",
      "        if len(self) == 1:\n",
      "            return next(iter(self))\n",
      "        raise ValueError(\"can only convert an array of size 1 to a Python scalar\")\n",
      "\n",
      "------------\n",
      "Method name: join\n",
      "Method definition:     @final\n",
      "    @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"other\"])\n",
      "    @_maybe_return_indexers\n",
      "    def join(\n",
      "        self,\n",
      "        other: Index,\n",
      "        how: str_t = \"left\",\n",
      "        level: Level = None,\n",
      "        return_indexers: bool = False,\n",
      "        sort: bool = False,\n",
      "    ) -> Index | tuple[Index, npt.NDArray[np.intp] | None, npt.NDArray[np.intp] | None]:\n",
      "        \"\"\"\n",
      "        Compute join_index and indexers to conform data structures to the new index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index\n",
      "        how : {'left', 'right', 'inner', 'outer'}\n",
      "        level : int or level name, default None\n",
      "        return_indexers : bool, default False\n",
      "        sort : bool, default False\n",
      "            Sort the join keys lexicographically in the result Index. If False,\n",
      "            the order of the join keys depends on the join type (how keyword).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        join_index, (left_indexer, right_indexer)\n",
      "        \"\"\"\n",
      "        other = ensure_index(other)\n",
      "\n",
      "        if isinstance(self, ABCDatetimeIndex) and isinstance(other, ABCDatetimeIndex):\n",
      "            if (self.tz is None) ^ (other.tz is None):\n",
      "                # Raise instead of casting to object below.\n",
      "                raise TypeError(\"Cannot join tz-naive with tz-aware DatetimeIndex\")\n",
      "\n",
      "        if not self._is_multi and not other._is_multi:\n",
      "            # We have specific handling for MultiIndex below\n",
      "            pself, pother = self._maybe_promote(other)\n",
      "            if pself is not self or pother is not other:\n",
      "                return pself.join(\n",
      "                    pother, how=how, level=level, return_indexers=True, sort=sort\n",
      "                )\n",
      "\n",
      "        lindexer: np.ndarray | None\n",
      "        rindexer: np.ndarray | None\n",
      "\n",
      "        # try to figure out the join level\n",
      "        # GH3662\n",
      "        if level is None and (self._is_multi or other._is_multi):\n",
      "\n",
      "            # have the same levels/names so a simple join\n",
      "            if self.names == other.names:\n",
      "                pass\n",
      "            else:\n",
      "                return self._join_multi(other, how=how)\n",
      "\n",
      "        # join on the level\n",
      "        if level is not None and (self._is_multi or other._is_multi):\n",
      "            return self._join_level(other, level, how=how)\n",
      "\n",
      "        if len(other) == 0:\n",
      "            if how in (\"left\", \"outer\"):\n",
      "                join_index = self._view()\n",
      "                rindexer = np.broadcast_to(np.intp(-1), len(join_index))\n",
      "                return join_index, None, rindexer\n",
      "            elif how in (\"right\", \"inner\", \"cross\"):\n",
      "                join_index = other._view()\n",
      "                lindexer = np.array([])\n",
      "                return join_index, lindexer, None\n",
      "\n",
      "        if len(self) == 0:\n",
      "            if how in (\"right\", \"outer\"):\n",
      "                join_index = other._view()\n",
      "                lindexer = np.broadcast_to(np.intp(-1), len(join_index))\n",
      "                return join_index, lindexer, None\n",
      "            elif how in (\"left\", \"inner\", \"cross\"):\n",
      "                join_index = self._view()\n",
      "                rindexer = np.array([])\n",
      "                return join_index, None, rindexer\n",
      "\n",
      "        if self._join_precedence < other._join_precedence:\n",
      "            how = {\"right\": \"left\", \"left\": \"right\"}.get(how, how)\n",
      "            join_index, lidx, ridx = other.join(\n",
      "                self, how=how, level=level, return_indexers=True\n",
      "            )\n",
      "            lidx, ridx = ridx, lidx\n",
      "            return join_index, lidx, ridx\n",
      "\n",
      "        if not is_dtype_equal(self.dtype, other.dtype):\n",
      "            dtype = self._find_common_type_compat(other)\n",
      "            this = self.astype(dtype, copy=False)\n",
      "            other = other.astype(dtype, copy=False)\n",
      "            return this.join(other, how=how, return_indexers=True)\n",
      "\n",
      "        _validate_join_method(how)\n",
      "\n",
      "        if not self.is_unique and not other.is_unique:\n",
      "            return self._join_non_unique(other, how=how)\n",
      "        elif not self.is_unique or not other.is_unique:\n",
      "            if self.is_monotonic_increasing and other.is_monotonic_increasing:\n",
      "                if not is_interval_dtype(self.dtype):\n",
      "                    # otherwise we will fall through to _join_via_get_indexer\n",
      "                    # GH#39133\n",
      "                    # go through object dtype for ea till engine is supported properly\n",
      "                    return self._join_monotonic(other, how=how)\n",
      "            else:\n",
      "                return self._join_non_unique(other, how=how)\n",
      "        elif (\n",
      "            self.is_monotonic_increasing\n",
      "            and other.is_monotonic_increasing\n",
      "            and self._can_use_libjoin\n",
      "            and (\n",
      "                not isinstance(self, ABCMultiIndex)\n",
      "                or not any(is_categorical_dtype(dtype) for dtype in self.dtypes)\n",
      "            )\n",
      "            and not is_categorical_dtype(self.dtype)\n",
      "        ):\n",
      "            # Categorical is monotonic if data are ordered as categories, but join can\n",
      "            #  not handle this in case of not lexicographically monotonic GH#38502\n",
      "            try:\n",
      "                return self._join_monotonic(other, how=how)\n",
      "            except TypeError:\n",
      "                # object dtype; non-comparable objects\n",
      "                pass\n",
      "\n",
      "        return self._join_via_get_indexer(other, how, sort)\n",
      "\n",
      "------------\n",
      "Method name: map\n",
      "Method definition:     def map(self, mapper, na_action=None):\n",
      "        \"\"\"\n",
      "        Map values using an input mapping or function.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        mapper : function, dict, or Series\n",
      "            Mapping correspondence.\n",
      "        na_action : {None, 'ignore'}\n",
      "            If 'ignore', propagate NA values, without passing them to the\n",
      "            mapping correspondence.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        applied : Union[Index, MultiIndex], inferred\n",
      "            The output of the mapping function applied to the index.\n",
      "            If the function returns a tuple with more than one element\n",
      "            a MultiIndex will be returned.\n",
      "        \"\"\"\n",
      "        from pandas.core.indexes.multi import MultiIndex\n",
      "\n",
      "        new_values = self._map_values(mapper, na_action=na_action)\n",
      "\n",
      "        # we can return a MultiIndex\n",
      "        if new_values.size and isinstance(new_values[0], tuple):\n",
      "            if isinstance(self, MultiIndex):\n",
      "                names = self.names\n",
      "            elif self.name:\n",
      "                names = [self.name] * len(new_values[0])\n",
      "            else:\n",
      "                names = None\n",
      "            return MultiIndex.from_tuples(new_values, names=names)\n",
      "\n",
      "        dtype = None\n",
      "        if not new_values.size:\n",
      "            # empty\n",
      "            dtype = self.dtype\n",
      "\n",
      "        # e.g. if we are floating and new_values is all ints, then we\n",
      "        #  don't want to cast back to floating.  But if we are UInt64\n",
      "        #  and new_values is all ints, we want to try.\n",
      "        same_dtype = lib.infer_dtype(new_values, skipna=False) == self.inferred_type\n",
      "        if same_dtype:\n",
      "            new_values = maybe_cast_pointwise_result(\n",
      "                new_values, self.dtype, same_dtype=same_dtype\n",
      "            )\n",
      "\n",
      "        if self._is_backward_compat_public_numeric_index and is_numeric_dtype(\n",
      "            new_values.dtype\n",
      "        ):\n",
      "            return self._constructor(\n",
      "                new_values, dtype=dtype, copy=False, name=self.name\n",
      "            )\n",
      "\n",
      "        return Index._with_infer(new_values, dtype=dtype, copy=False, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: max\n",
      "Method definition:     @doc(IndexOpsMixin.max)\n",
      "    def max(self, axis=None, skipna=True, *args, **kwargs):\n",
      "        nv.validate_max(args, kwargs)\n",
      "        nv.validate_minmax_axis(axis)\n",
      "\n",
      "        if not len(self):\n",
      "            return self._na_value\n",
      "\n",
      "        if len(self) and self.is_monotonic_increasing:\n",
      "            # quick check\n",
      "            last = self[-1]\n",
      "            if not isna(last):\n",
      "                return last\n",
      "\n",
      "        if not self._is_multi and self.hasnans:\n",
      "            # Take advantage of cache\n",
      "            mask = self._isnan\n",
      "            if not skipna or mask.all():\n",
      "                return self._na_value\n",
      "\n",
      "        if not self._is_multi and not isinstance(self._values, np.ndarray):\n",
      "            # \"ExtensionArray\" has no attribute \"max\"\n",
      "            return self._values.max(skipna=skipna)  # type: ignore[attr-defined]\n",
      "\n",
      "        return super().max(skipna=skipna)\n",
      "\n",
      "------------\n",
      "Method name: memory_usage\n",
      "Method definition:     @doc(IndexOpsMixin._memory_usage)\n",
      "    def memory_usage(self, deep: bool = False) -> int:\n",
      "        result = self._memory_usage(deep=deep)\n",
      "\n",
      "        # include our engine hashtable\n",
      "        result += self._engine.sizeof(deep=deep)\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: min\n",
      "Method definition:     @doc(IndexOpsMixin.min)\n",
      "    def min(self, axis=None, skipna=True, *args, **kwargs):\n",
      "        nv.validate_min(args, kwargs)\n",
      "        nv.validate_minmax_axis(axis)\n",
      "\n",
      "        if not len(self):\n",
      "            return self._na_value\n",
      "\n",
      "        if len(self) and self.is_monotonic_increasing:\n",
      "            # quick check\n",
      "            first = self[0]\n",
      "            if not isna(first):\n",
      "                return first\n",
      "\n",
      "        if not self._is_multi and self.hasnans:\n",
      "            # Take advantage of cache\n",
      "            mask = self._isnan\n",
      "            if not skipna or mask.all():\n",
      "                return self._na_value\n",
      "\n",
      "        if not self._is_multi and not isinstance(self._values, np.ndarray):\n",
      "            # \"ExtensionArray\" has no attribute \"min\"\n",
      "            return self._values.min(skipna=skipna)  # type: ignore[attr-defined]\n",
      "\n",
      "        return super().min(skipna=skipna)\n",
      "\n",
      "------------\n",
      "Method name: notna\n",
      "Method definition:     @final\n",
      "    def notna(self) -> npt.NDArray[np.bool_]:\n",
      "        \"\"\"\n",
      "        Detect existing (non-missing) values.\n",
      "\n",
      "        Return a boolean same-sized object indicating if the values are not NA.\n",
      "        Non-missing values get mapped to ``True``. Characters such as empty\n",
      "        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n",
      "        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      "        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n",
      "        values.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray[bool]\n",
      "            Boolean array to indicate which entries are not NA.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.notnull : Alias of notna.\n",
      "        Index.isna: Inverse of notna.\n",
      "        notna : Top-level notna.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Show which entries in an Index are not NA. The result is an\n",
      "        array.\n",
      "\n",
      "        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n",
      "        >>> idx\n",
      "        Float64Index([5.2, 6.0, nan], dtype='float64')\n",
      "        >>> idx.notna()\n",
      "        array([ True,  True, False])\n",
      "\n",
      "        Empty strings are not considered NA values. None is considered a NA\n",
      "        value.\n",
      "\n",
      "        >>> idx = pd.Index(['black', '', 'red', None])\n",
      "        >>> idx\n",
      "        Index(['black', '', 'red', None], dtype='object')\n",
      "        >>> idx.notna()\n",
      "        array([ True,  True,  True, False])\n",
      "        \"\"\"\n",
      "        return ~self.isna()\n",
      "\n",
      "------------\n",
      "Method name: notnull\n",
      "Method definition:     @final\n",
      "    def notna(self) -> npt.NDArray[np.bool_]:\n",
      "        \"\"\"\n",
      "        Detect existing (non-missing) values.\n",
      "\n",
      "        Return a boolean same-sized object indicating if the values are not NA.\n",
      "        Non-missing values get mapped to ``True``. Characters such as empty\n",
      "        strings ``''`` or :attr:`numpy.inf` are not considered NA values\n",
      "        (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n",
      "        NA values, such as None or :attr:`numpy.NaN`, get mapped to ``False``\n",
      "        values.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray[bool]\n",
      "            Boolean array to indicate which entries are not NA.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.notnull : Alias of notna.\n",
      "        Index.isna: Inverse of notna.\n",
      "        notna : Top-level notna.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Show which entries in an Index are not NA. The result is an\n",
      "        array.\n",
      "\n",
      "        >>> idx = pd.Index([5.2, 6.0, np.NaN])\n",
      "        >>> idx\n",
      "        Float64Index([5.2, 6.0, nan], dtype='float64')\n",
      "        >>> idx.notna()\n",
      "        array([ True,  True, False])\n",
      "\n",
      "        Empty strings are not considered NA values. None is considered a NA\n",
      "        value.\n",
      "\n",
      "        >>> idx = pd.Index(['black', '', 'red', None])\n",
      "        >>> idx\n",
      "        Index(['black', '', 'red', None], dtype='object')\n",
      "        >>> idx.notna()\n",
      "        array([ True,  True,  True, False])\n",
      "        \"\"\"\n",
      "        return ~self.isna()\n",
      "\n",
      "------------\n",
      "Method name: nunique\n",
      "Method definition:     def nunique(self, dropna: bool = True) -> int:\n",
      "        \"\"\"\n",
      "        Return number of unique elements in the object.\n",
      "\n",
      "        Excludes NA values by default.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        dropna : bool, default True\n",
      "            Don't include NaN in the count.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        int\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        DataFrame.nunique: Method nunique for DataFrame.\n",
      "        Series.count: Count non-NA/null observations in the Series.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> s = pd.Series([1, 3, 5, 7, 7])\n",
      "        >>> s\n",
      "        0    1\n",
      "        1    3\n",
      "        2    5\n",
      "        3    7\n",
      "        4    7\n",
      "        dtype: int64\n",
      "\n",
      "        >>> s.nunique()\n",
      "        4\n",
      "        \"\"\"\n",
      "        uniqs = self.unique()\n",
      "        if dropna:\n",
      "            uniqs = remove_na_arraylike(uniqs)\n",
      "        return len(uniqs)\n",
      "\n",
      "------------\n",
      "Method name: putmask\n",
      "Method definition:     @final\n",
      "    def putmask(self, mask, value) -> Index:\n",
      "        \"\"\"\n",
      "        Return a new Index of the values set with the mask.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.ndarray.putmask : Changes elements of an array\n",
      "            based on conditional and input values.\n",
      "        \"\"\"\n",
      "        mask, noop = validate_putmask(self._values, mask)\n",
      "        if noop:\n",
      "            return self.copy()\n",
      "\n",
      "        if self.dtype != object and is_valid_na_for_dtype(value, self.dtype):\n",
      "            # e.g. None -> np.nan, see also Block._standardize_fill_value\n",
      "            value = self._na_value\n",
      "        try:\n",
      "            converted = self._validate_fill_value(value)\n",
      "        except (LossySetitemError, ValueError, TypeError) as err:\n",
      "            if is_object_dtype(self):  # pragma: no cover\n",
      "                raise err\n",
      "\n",
      "            dtype = self._find_common_type_compat(value)\n",
      "            return self.astype(dtype).putmask(mask, value)\n",
      "\n",
      "        values = self._values.copy()\n",
      "\n",
      "        if isinstance(values, np.ndarray):\n",
      "            converted = setitem_datetimelike_compat(values, mask.sum(), converted)\n",
      "            np.putmask(values, mask, converted)\n",
      "\n",
      "        else:\n",
      "            # Note: we use the original value here, not converted, as\n",
      "            #  _validate_fill_value is not idempotent\n",
      "            values._putmask(mask, value)\n",
      "\n",
      "        return self._shallow_copy(values)\n",
      "\n",
      "------------\n",
      "Method name: ravel\n",
      "Method definition:     @final\n",
      "    def ravel(self, order=\"C\"):\n",
      "        \"\"\"\n",
      "        Return an ndarray of the flattened values of the underlying data.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "            Flattened array.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.ndarray.ravel : Return a flattened array.\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            \"Index.ravel returning ndarray is deprecated; in a future version \"\n",
      "            \"this will return a view on self.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        if needs_i8_conversion(self.dtype):\n",
      "            # Item \"ndarray[Any, Any]\" of \"Union[ExtensionArray, ndarray[Any, Any]]\"\n",
      "            # has no attribute \"_ndarray\"\n",
      "            values = self._data._ndarray  # type: ignore[union-attr]\n",
      "        elif is_interval_dtype(self.dtype):\n",
      "            values = np.asarray(self._data)\n",
      "        else:\n",
      "            values = self._get_engine_target()\n",
      "        return values.ravel(order=order)\n",
      "\n",
      "------------\n",
      "Method name: reindex\n",
      "Method definition:     def reindex(\n",
      "        self, target, method=None, level=None, limit=None, tolerance=None\n",
      "    ) -> tuple[Index, npt.NDArray[np.intp] | None]:\n",
      "        \"\"\"\n",
      "        Create index with target's values.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        target : an iterable\n",
      "        method : {None, 'pad'/'ffill', 'backfill'/'bfill', 'nearest'}, optional\n",
      "            * default: exact matches only.\n",
      "            * pad / ffill: find the PREVIOUS index value if no exact match.\n",
      "            * backfill / bfill: use NEXT index value if no exact match\n",
      "            * nearest: use the NEAREST index value if no exact match. Tied\n",
      "              distances are broken by preferring the larger index value.\n",
      "        level : int, optional\n",
      "            Level of multiindex.\n",
      "        limit : int, optional\n",
      "            Maximum number of consecutive labels in ``target`` to match for\n",
      "            inexact matches.\n",
      "        tolerance : int or float, optional\n",
      "            Maximum distance between original and new labels for inexact\n",
      "            matches. The values of the index at the matching locations must\n",
      "            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      "\n",
      "            Tolerance may be a scalar value, which applies the same tolerance\n",
      "            to all values, or list-like, which applies variable tolerance per\n",
      "            element. List-like includes list, tuple, array, Series, and must be\n",
      "            the same size as the index and its dtype must exactly match the\n",
      "            index's type.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        new_index : pd.Index\n",
      "            Resulting index.\n",
      "        indexer : np.ndarray[np.intp] or None\n",
      "            Indices of output values in original index.\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If ``method`` passed along with ``level``.\n",
      "        ValueError\n",
      "            If non-unique multi-index\n",
      "        ValueError\n",
      "            If non-unique index and ``method`` or ``limit`` passed.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.reindex : Conform Series to new index with optional filling logic.\n",
      "        DataFrame.reindex : Conform DataFrame to new index with optional filling logic.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['car', 'bike', 'train', 'tractor'])\n",
      "        >>> idx\n",
      "        Index(['car', 'bike', 'train', 'tractor'], dtype='object')\n",
      "        >>> idx.reindex(['car', 'bike'])\n",
      "        (Index(['car', 'bike'], dtype='object'), array([0, 1]))\n",
      "        \"\"\"\n",
      "        # GH6552: preserve names when reindexing to non-named target\n",
      "        # (i.e. neither Index nor Series).\n",
      "        preserve_names = not hasattr(target, \"name\")\n",
      "\n",
      "        # GH7774: preserve dtype/tz if target is empty and not an Index.\n",
      "        target = ensure_has_len(target)  # target may be an iterator\n",
      "\n",
      "        if not isinstance(target, Index) and len(target) == 0:\n",
      "            if level is not None and self._is_multi:\n",
      "                # \"Index\" has no attribute \"levels\"; maybe \"nlevels\"?\n",
      "                idx = self.levels[level]  # type: ignore[attr-defined]\n",
      "            else:\n",
      "                idx = self\n",
      "            target = idx[:0]\n",
      "        else:\n",
      "            target = ensure_index(target)\n",
      "\n",
      "        if level is not None and (\n",
      "            isinstance(self, ABCMultiIndex) or isinstance(target, ABCMultiIndex)\n",
      "        ):\n",
      "            if method is not None:\n",
      "                raise TypeError(\"Fill method not supported if level passed\")\n",
      "\n",
      "            # TODO: tests where passing `keep_order=not self._is_multi`\n",
      "            #  makes a difference for non-MultiIndex case\n",
      "            target, indexer, _ = self._join_level(\n",
      "                target, level, how=\"right\", keep_order=not self._is_multi\n",
      "            )\n",
      "\n",
      "        else:\n",
      "            if self.equals(target):\n",
      "                indexer = None\n",
      "            else:\n",
      "                if self._index_as_unique:\n",
      "                    indexer = self.get_indexer(\n",
      "                        target, method=method, limit=limit, tolerance=tolerance\n",
      "                    )\n",
      "                elif self._is_multi:\n",
      "                    raise ValueError(\"cannot handle a non-unique multi-index!\")\n",
      "                else:\n",
      "                    if method is not None or limit is not None:\n",
      "                        raise ValueError(\n",
      "                            \"cannot reindex a non-unique index \"\n",
      "                            \"with a method or limit\"\n",
      "                        )\n",
      "                    indexer, _ = self.get_indexer_non_unique(target)\n",
      "\n",
      "                if not self.is_unique:\n",
      "                    # GH#42568\n",
      "                    warnings.warn(\n",
      "                        \"reindexing with a non-unique Index is deprecated and \"\n",
      "                        \"will raise in a future version.\",\n",
      "                        FutureWarning,\n",
      "                        stacklevel=find_stack_level(),\n",
      "                    )\n",
      "\n",
      "        target = self._wrap_reindex_result(target, indexer, preserve_names)\n",
      "        return target, indexer\n",
      "\n",
      "------------\n",
      "Method name: rename\n",
      "Method definition:     def rename(self, name, inplace=False):\n",
      "        \"\"\"\n",
      "        Alter Index or MultiIndex name.\n",
      "\n",
      "        Able to set new names without level. Defaults to returning new index.\n",
      "        Length of names must match number of levels in MultiIndex.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        name : label or list of labels\n",
      "            Name(s) to set.\n",
      "        inplace : bool, default False\n",
      "            Modifies the object directly, instead of creating a new Index or\n",
      "            MultiIndex.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index or None\n",
      "            The same type as the caller or None if ``inplace=True``.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.set_names : Able to set new names partially and by level.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['A', 'C', 'A', 'B'], name='score')\n",
      "        >>> idx.rename('grade')\n",
      "        Index(['A', 'C', 'A', 'B'], dtype='object', name='grade')\n",
      "\n",
      "        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n",
      "        ...                                   [2018, 2019]],\n",
      "        ...                                   names=['kind', 'year'])\n",
      "        >>> idx\n",
      "        MultiIndex([('python', 2018),\n",
      "                    ('python', 2019),\n",
      "                    ( 'cobra', 2018),\n",
      "                    ( 'cobra', 2019)],\n",
      "                   names=['kind', 'year'])\n",
      "        >>> idx.rename(['species', 'year'])\n",
      "        MultiIndex([('python', 2018),\n",
      "                    ('python', 2019),\n",
      "                    ( 'cobra', 2018),\n",
      "                    ( 'cobra', 2019)],\n",
      "                   names=['species', 'year'])\n",
      "        >>> idx.rename('species')\n",
      "        Traceback (most recent call last):\n",
      "        TypeError: Must pass list-like as `names`.\n",
      "        \"\"\"\n",
      "        return self.set_names([name], inplace=inplace)\n",
      "\n",
      "------------\n",
      "Method name: repeat\n",
      "Method definition:     @Appender(_index_shared_docs[\"repeat\"] % _index_doc_kwargs)\n",
      "    def repeat(self, repeats, axis=None):\n",
      "        repeats = ensure_platform_int(repeats)\n",
      "        nv.validate_repeat((), {\"axis\": axis})\n",
      "        res_values = self._values.repeat(repeats)\n",
      "\n",
      "        # _constructor so RangeIndex->Int64Index\n",
      "        return self._constructor._simple_new(res_values, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: searchsorted\n",
      "Method definition:     @doc(_shared_docs[\"searchsorted\"], klass=\"Index\")\n",
      "    def searchsorted(\n",
      "        self,\n",
      "        value: NumpyValueArrayLike | ExtensionArray,\n",
      "        side: Literal[\"left\", \"right\"] = \"left\",\n",
      "        sorter: NumpySorter = None,\n",
      "    ) -> npt.NDArray[np.intp] | np.intp:\n",
      "\n",
      "        values = self._values\n",
      "        if not isinstance(values, np.ndarray):\n",
      "            # Going through EA.searchsorted directly improves performance GH#38083\n",
      "            return values.searchsorted(value, side=side, sorter=sorter)\n",
      "\n",
      "        return algorithms.searchsorted(\n",
      "            values,\n",
      "            value,\n",
      "            side=side,\n",
      "            sorter=sorter,\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: set_names\n",
      "Method definition:     @deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\", \"names\"])\n",
      "    def set_names(self, names, level=None, inplace: bool = False):\n",
      "        \"\"\"\n",
      "        Set Index or MultiIndex name.\n",
      "\n",
      "        Able to set new names partially and by level.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "\n",
      "        names : label or list of label or dict-like for MultiIndex\n",
      "            Name(s) to set.\n",
      "\n",
      "            .. versionchanged:: 1.3.0\n",
      "\n",
      "        level : int, label or list of int or label, optional\n",
      "            If the index is a MultiIndex and names is not dict-like, level(s) to set\n",
      "            (None for all levels). Otherwise level must be None.\n",
      "\n",
      "            .. versionchanged:: 1.3.0\n",
      "\n",
      "        inplace : bool, default False\n",
      "            Modifies the object directly, instead of creating a new Index or\n",
      "            MultiIndex.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index or None\n",
      "            The same type as the caller or None if ``inplace=True``.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.rename : Able to set new names without level.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx\n",
      "        Int64Index([1, 2, 3, 4], dtype='int64')\n",
      "        >>> idx.set_names('quarter')\n",
      "        Int64Index([1, 2, 3, 4], dtype='int64', name='quarter')\n",
      "\n",
      "        >>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n",
      "        ...                                   [2018, 2019]])\n",
      "        >>> idx\n",
      "        MultiIndex([('python', 2018),\n",
      "                    ('python', 2019),\n",
      "                    ( 'cobra', 2018),\n",
      "                    ( 'cobra', 2019)],\n",
      "                   )\n",
      "        >>> idx.set_names(['kind', 'year'], inplace=True)\n",
      "        >>> idx\n",
      "        MultiIndex([('python', 2018),\n",
      "                    ('python', 2019),\n",
      "                    ( 'cobra', 2018),\n",
      "                    ( 'cobra', 2019)],\n",
      "                   names=['kind', 'year'])\n",
      "        >>> idx.set_names('species', level=0)\n",
      "        MultiIndex([('python', 2018),\n",
      "                    ('python', 2019),\n",
      "                    ( 'cobra', 2018),\n",
      "                    ( 'cobra', 2019)],\n",
      "                   names=['species', 'year'])\n",
      "\n",
      "        When renaming levels with a dict, levels can not be passed.\n",
      "\n",
      "        >>> idx.set_names({'kind': 'snake'})\n",
      "        MultiIndex([('python', 2018),\n",
      "                    ('python', 2019),\n",
      "                    ( 'cobra', 2018),\n",
      "                    ( 'cobra', 2019)],\n",
      "                   names=['snake', 'year'])\n",
      "        \"\"\"\n",
      "        if level is not None and not isinstance(self, ABCMultiIndex):\n",
      "            raise ValueError(\"Level must be None for non-MultiIndex\")\n",
      "\n",
      "        elif level is not None and not is_list_like(level) and is_list_like(names):\n",
      "            raise TypeError(\"Names must be a string when a single level is provided.\")\n",
      "\n",
      "        elif not is_list_like(names) and level is None and self.nlevels > 1:\n",
      "            raise TypeError(\"Must pass list-like as `names`.\")\n",
      "\n",
      "        elif is_dict_like(names) and not isinstance(self, ABCMultiIndex):\n",
      "            raise TypeError(\"Can only pass dict-like as `names` for MultiIndex.\")\n",
      "\n",
      "        elif is_dict_like(names) and level is not None:\n",
      "            raise TypeError(\"Can not pass level for dictlike `names`.\")\n",
      "\n",
      "        if isinstance(self, ABCMultiIndex) and is_dict_like(names) and level is None:\n",
      "            # Transform dict to list of new names and corresponding levels\n",
      "            level, names_adjusted = [], []\n",
      "            for i, name in enumerate(self.names):\n",
      "                if name in names.keys():\n",
      "                    level.append(i)\n",
      "                    names_adjusted.append(names[name])\n",
      "            names = names_adjusted\n",
      "\n",
      "        if not is_list_like(names):\n",
      "            names = [names]\n",
      "        if level is not None and not is_list_like(level):\n",
      "            level = [level]\n",
      "\n",
      "        if inplace:\n",
      "            idx = self\n",
      "        else:\n",
      "            idx = self._view()\n",
      "\n",
      "        idx._set_names(names, level=level)\n",
      "        if not inplace:\n",
      "            return idx\n",
      "\n",
      "------------\n",
      "Method name: set_value\n",
      "Method definition:     @final\n",
      "    def set_value(self, arr, key, value) -> None:\n",
      "        \"\"\"\n",
      "        Fast lookup of value from 1-dimensional ndarray.\n",
      "\n",
      "        .. deprecated:: 1.0\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Only use this if you know what you're doing.\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            (\n",
      "                \"The 'set_value' method is deprecated, and \"\n",
      "                \"will be removed in a future version.\"\n",
      "            ),\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        loc = self._engine.get_loc(key)\n",
      "        if not can_hold_element(arr, value):\n",
      "            raise ValueError\n",
      "        arr[loc] = value\n",
      "\n",
      "------------\n",
      "Method name: shift\n",
      "Method definition:     def shift(self, periods=1, freq=None):\n",
      "        \"\"\"\n",
      "        Shift index by desired number of time frequency increments.\n",
      "\n",
      "        This method is for shifting the values of datetime-like indexes\n",
      "        by a specified time increment a given number of times.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        periods : int, default 1\n",
      "            Number of periods (or increments) to shift by,\n",
      "            can be positive or negative.\n",
      "        freq : pandas.DateOffset, pandas.Timedelta or str, optional\n",
      "            Frequency increment to shift by.\n",
      "            If None, the index is shifted by its own `freq` attribute.\n",
      "            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        pandas.Index\n",
      "            Shifted index.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.shift : Shift values of Series.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This method is only implemented for datetime-like index classes,\n",
      "        i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Put the first 5 month starts of 2011 into an index.\n",
      "\n",
      "        >>> month_starts = pd.date_range('1/1/2011', periods=5, freq='MS')\n",
      "        >>> month_starts\n",
      "        DatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n",
      "                       '2011-05-01'],\n",
      "                      dtype='datetime64[ns]', freq='MS')\n",
      "\n",
      "        Shift the index by 10 days.\n",
      "\n",
      "        >>> month_starts.shift(10, freq='D')\n",
      "        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n",
      "                       '2011-05-11'],\n",
      "                      dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "        The default value of `freq` is the `freq` attribute of the index,\n",
      "        which is 'MS' (month start) in this example.\n",
      "\n",
      "        >>> month_starts.shift(10)\n",
      "        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n",
      "                       '2012-03-01'],\n",
      "                      dtype='datetime64[ns]', freq='MS')\n",
      "        \"\"\"\n",
      "        raise NotImplementedError(\n",
      "            f\"This method is only implemented for DatetimeIndex, PeriodIndex and \"\n",
      "            f\"TimedeltaIndex; Got type {type(self).__name__}\"\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: slice_indexer\n",
      "Method definition:     def slice_indexer(\n",
      "        self,\n",
      "        start: Hashable | None = None,\n",
      "        end: Hashable | None = None,\n",
      "        step: int | None = None,\n",
      "        kind=no_default,\n",
      "    ) -> slice:\n",
      "        \"\"\"\n",
      "        Compute the slice indexer for input labels and step.\n",
      "\n",
      "        Index needs to be ordered and unique.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        start : label, default None\n",
      "            If None, defaults to the beginning.\n",
      "        end : label, default None\n",
      "            If None, defaults to the end.\n",
      "        step : int, default None\n",
      "        kind : str, default None\n",
      "\n",
      "            .. deprecated:: 1.4.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        indexer : slice\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        KeyError : If key does not exist, or key is not unique and index is\n",
      "            not ordered.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This function assumes that the data is sorted, so use at your own peril\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        This is a method on all index types. For example you can do:\n",
      "\n",
      "        >>> idx = pd.Index(list('abcd'))\n",
      "        >>> idx.slice_indexer(start='b', end='c')\n",
      "        slice(1, 3, None)\n",
      "\n",
      "        >>> idx = pd.MultiIndex.from_arrays([list('abcd'), list('efgh')])\n",
      "        >>> idx.slice_indexer(start='b', end=('c', 'g'))\n",
      "        slice(1, 3, None)\n",
      "        \"\"\"\n",
      "        self._deprecated_arg(kind, \"kind\", \"slice_indexer\")\n",
      "\n",
      "        start_slice, end_slice = self.slice_locs(start, end, step=step)\n",
      "\n",
      "        # return a slice\n",
      "        if not is_scalar(start_slice):\n",
      "            raise AssertionError(\"Start slice bound is non-scalar\")\n",
      "        if not is_scalar(end_slice):\n",
      "            raise AssertionError(\"End slice bound is non-scalar\")\n",
      "\n",
      "        return slice(start_slice, end_slice, step)\n",
      "\n",
      "------------\n",
      "Method name: slice_locs\n",
      "Method definition:     def slice_locs(\n",
      "        self, start=None, end=None, step=None, kind=no_default\n",
      "    ) -> tuple[int, int]:\n",
      "        \"\"\"\n",
      "        Compute slice locations for input labels.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        start : label, default None\n",
      "            If None, defaults to the beginning.\n",
      "        end : label, default None\n",
      "            If None, defaults to the end.\n",
      "        step : int, defaults None\n",
      "            If None, defaults to 1.\n",
      "        kind : {'loc', 'getitem'} or None\n",
      "\n",
      "            .. deprecated:: 1.4.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        start, end : int\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.get_loc : Get location for a single label.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        This method only works if the index is monotonic or unique.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(list('abcd'))\n",
      "        >>> idx.slice_locs(start='b', end='c')\n",
      "        (1, 3)\n",
      "        \"\"\"\n",
      "        self._deprecated_arg(kind, \"kind\", \"slice_locs\")\n",
      "        inc = step is None or step >= 0\n",
      "\n",
      "        if not inc:\n",
      "            # If it's a reverse slice, temporarily swap bounds.\n",
      "            start, end = end, start\n",
      "\n",
      "        # GH 16785: If start and end happen to be date strings with UTC offsets\n",
      "        # attempt to parse and check that the offsets are the same\n",
      "        if isinstance(start, (str, datetime)) and isinstance(end, (str, datetime)):\n",
      "            try:\n",
      "                ts_start = Timestamp(start)\n",
      "                ts_end = Timestamp(end)\n",
      "            except (ValueError, TypeError):\n",
      "                pass\n",
      "            else:\n",
      "                if not tz_compare(ts_start.tzinfo, ts_end.tzinfo):\n",
      "                    raise ValueError(\"Both dates must have the same UTC offset\")\n",
      "\n",
      "        start_slice = None\n",
      "        if start is not None:\n",
      "            start_slice = self.get_slice_bound(start, \"left\")\n",
      "        if start_slice is None:\n",
      "            start_slice = 0\n",
      "\n",
      "        end_slice = None\n",
      "        if end is not None:\n",
      "            end_slice = self.get_slice_bound(end, \"right\")\n",
      "        if end_slice is None:\n",
      "            end_slice = len(self)\n",
      "\n",
      "        if not inc:\n",
      "            # Bounds at this moment are swapped, swap them back and shift by 1.\n",
      "            #\n",
      "            # slice_locs('B', 'A', step=-1): s='B', e='A'\n",
      "            #\n",
      "            #              s='A'                 e='B'\n",
      "            # AFTER SWAP:    |                     |\n",
      "            #                v ------------------> V\n",
      "            #           -----------------------------------\n",
      "            #           | | |A|A|A|A| | | | | |B|B| | | | |\n",
      "            #           -----------------------------------\n",
      "            #              ^ <------------------ ^\n",
      "            # SHOULD BE:   |                     |\n",
      "            #           end=s-1              start=e-1\n",
      "            #\n",
      "            end_slice, start_slice = start_slice - 1, end_slice - 1\n",
      "\n",
      "            # i == -1 triggers ``len(self) + i`` selection that points to the\n",
      "            # last element, not before-the-first one, subtracting len(self)\n",
      "            # compensates that.\n",
      "            if end_slice == -1:\n",
      "                end_slice -= len(self)\n",
      "            if start_slice == -1:\n",
      "                start_slice -= len(self)\n",
      "\n",
      "        return start_slice, end_slice\n",
      "\n",
      "------------\n",
      "Method name: sort\n",
      "Method definition:     @final\n",
      "    def sort(self, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Use sort_values instead.\n",
      "        \"\"\"\n",
      "        raise TypeError(\"cannot sort an Index object in-place, use sort_values instead\")\n",
      "\n",
      "------------\n",
      "Method name: sort_values\n",
      "Method definition:     def sort_values(\n",
      "        self,\n",
      "        return_indexer: bool = False,\n",
      "        ascending: bool = True,\n",
      "        na_position: str_t = \"last\",\n",
      "        key: Callable | None = None,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Return a sorted copy of the index.\n",
      "\n",
      "        Return a sorted copy of the index, and optionally return the indices\n",
      "        that sorted the index itself.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        return_indexer : bool, default False\n",
      "            Should the indices that would sort the index be returned.\n",
      "        ascending : bool, default True\n",
      "            Should the index values be sorted in an ascending order.\n",
      "        na_position : {'first' or 'last'}, default 'last'\n",
      "            Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n",
      "            the end.\n",
      "\n",
      "            .. versionadded:: 1.2.0\n",
      "\n",
      "        key : callable, optional\n",
      "            If not None, apply the key function to the index values\n",
      "            before sorting. This is similar to the `key` argument in the\n",
      "            builtin :meth:`sorted` function, with the notable difference that\n",
      "            this `key` function should be *vectorized*. It should expect an\n",
      "            ``Index`` and return an ``Index`` of the same shape.\n",
      "\n",
      "            .. versionadded:: 1.1.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        sorted_index : pandas.Index\n",
      "            Sorted copy of the index.\n",
      "        indexer : numpy.ndarray, optional\n",
      "            The indices that the index itself was sorted by.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.sort_values : Sort values of a Series.\n",
      "        DataFrame.sort_values : Sort values in a DataFrame.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index([10, 100, 1, 1000])\n",
      "        >>> idx\n",
      "        Int64Index([10, 100, 1, 1000], dtype='int64')\n",
      "\n",
      "        Sort values in ascending order (default behavior).\n",
      "\n",
      "        >>> idx.sort_values()\n",
      "        Int64Index([1, 10, 100, 1000], dtype='int64')\n",
      "\n",
      "        Sort values in descending order, and also get the indices `idx` was\n",
      "        sorted by.\n",
      "\n",
      "        >>> idx.sort_values(ascending=False, return_indexer=True)\n",
      "        (Int64Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))\n",
      "        \"\"\"\n",
      "        idx = ensure_key_mapped(self, key)\n",
      "\n",
      "        # GH 35584. Sort missing values according to na_position kwarg\n",
      "        # ignore na_position for MultiIndex\n",
      "        if not isinstance(self, ABCMultiIndex):\n",
      "            _as = nargsort(\n",
      "                items=idx, ascending=ascending, na_position=na_position, key=key\n",
      "            )\n",
      "        else:\n",
      "            _as = idx.argsort()\n",
      "            if not ascending:\n",
      "                _as = _as[::-1]\n",
      "\n",
      "        sorted_index = self.take(_as)\n",
      "\n",
      "        if return_indexer:\n",
      "            return sorted_index, _as\n",
      "        else:\n",
      "            return sorted_index\n",
      "\n",
      "------------\n",
      "Method name: sortlevel\n",
      "Method definition:     def sortlevel(self, level=None, ascending=True, sort_remaining=None):\n",
      "        \"\"\"\n",
      "        For internal compatibility with the Index API.\n",
      "\n",
      "        Sort the Index. This is for compat with MultiIndex\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        ascending : bool, default True\n",
      "            False to sort in descending order\n",
      "\n",
      "        level, sort_remaining are compat parameters\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "        \"\"\"\n",
      "        if not isinstance(ascending, (list, bool)):\n",
      "            raise TypeError(\n",
      "                \"ascending must be a single bool value or\"\n",
      "                \"a list of bool values of length 1\"\n",
      "            )\n",
      "\n",
      "        if isinstance(ascending, list):\n",
      "            if len(ascending) != 1:\n",
      "                raise TypeError(\"ascending must be a list of bool values of length 1\")\n",
      "            ascending = ascending[0]\n",
      "\n",
      "        if not isinstance(ascending, bool):\n",
      "            raise TypeError(\"ascending must be a bool value\")\n",
      "\n",
      "        return self.sort_values(return_indexer=True, ascending=ascending)\n",
      "\n",
      "------------\n",
      "Method name: symmetric_difference\n",
      "Method definition:     def symmetric_difference(self, other, result_name=None, sort=None):\n",
      "        \"\"\"\n",
      "        Compute the symmetric difference of two Index objects.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index or array-like\n",
      "        result_name : str\n",
      "        sort : False or None, default None\n",
      "            Whether to sort the resulting index. By default, the\n",
      "            values are attempted to be sorted, but any TypeError from\n",
      "            incomparable elements is caught by pandas.\n",
      "\n",
      "            * None : Attempt to sort the result, but catch any TypeErrors\n",
      "              from comparing incomparable elements.\n",
      "            * False : Do not sort the result.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        symmetric_difference : Index\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        ``symmetric_difference`` contains elements that appear in either\n",
      "        ``idx1`` or ``idx2`` but not both. Equivalent to the Index created by\n",
      "        ``idx1.difference(idx2) | idx2.difference(idx1)`` with duplicates\n",
      "        dropped.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx1 = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx2 = pd.Index([2, 3, 4, 5])\n",
      "        >>> idx1.symmetric_difference(idx2)\n",
      "        Int64Index([1, 5], dtype='int64')\n",
      "        \"\"\"\n",
      "        self._validate_sort_keyword(sort)\n",
      "        self._assert_can_do_setop(other)\n",
      "        other, result_name_update = self._convert_can_do_setop(other)\n",
      "        if result_name is None:\n",
      "            result_name = result_name_update\n",
      "\n",
      "        if not is_dtype_equal(self.dtype, other.dtype):\n",
      "            self._deprecate_dti_setop(other, \"symmetric_difference\")\n",
      "\n",
      "        if not self._should_compare(other):\n",
      "            return self.union(other, sort=sort).rename(result_name)\n",
      "\n",
      "        elif not is_dtype_equal(self.dtype, other.dtype):\n",
      "            dtype = self._find_common_type_compat(other)\n",
      "            this = self.astype(dtype, copy=False)\n",
      "            that = other.astype(dtype, copy=False)\n",
      "            return this.symmetric_difference(that, sort=sort).rename(result_name)\n",
      "\n",
      "        this = self.unique()\n",
      "        other = other.unique()\n",
      "        indexer = this.get_indexer_for(other)\n",
      "\n",
      "        # {this} minus {other}\n",
      "        common_indexer = indexer.take((indexer != -1).nonzero()[0])\n",
      "        left_indexer = np.setdiff1d(\n",
      "            np.arange(this.size), common_indexer, assume_unique=True\n",
      "        )\n",
      "        left_diff = this._values.take(left_indexer)\n",
      "\n",
      "        # {other} minus {this}\n",
      "        right_indexer = (indexer == -1).nonzero()[0]\n",
      "        right_diff = other._values.take(right_indexer)\n",
      "\n",
      "        res_values = concat_compat([left_diff, right_diff])\n",
      "        res_values = _maybe_try_sort(res_values, sort)\n",
      "\n",
      "        # pass dtype so we retain object dtype\n",
      "        result = Index(res_values, name=result_name, dtype=res_values.dtype)\n",
      "\n",
      "        if self._is_multi:\n",
      "            self = cast(\"MultiIndex\", self)\n",
      "            if len(result) == 0:\n",
      "                # On equal symmetric_difference MultiIndexes the difference is empty.\n",
      "                # Therefore, an empty MultiIndex is returned GH#13490\n",
      "                return type(self)(\n",
      "                    levels=[[] for _ in range(self.nlevels)],\n",
      "                    codes=[[] for _ in range(self.nlevels)],\n",
      "                    names=result.name,\n",
      "                )\n",
      "            return type(self).from_tuples(result, names=result.name)\n",
      "\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: take\n",
      "Method definition:     @Appender(_index_shared_docs[\"take\"] % _index_doc_kwargs)\n",
      "    def take(\n",
      "        self, indices, axis: int = 0, allow_fill: bool = True, fill_value=None, **kwargs\n",
      "    ):\n",
      "        if kwargs:\n",
      "            nv.validate_take((), kwargs)\n",
      "        if is_scalar(indices):\n",
      "            raise TypeError(\"Expected indices to be array-like\")\n",
      "        indices = ensure_platform_int(indices)\n",
      "        allow_fill = self._maybe_disallow_fill(allow_fill, fill_value, indices)\n",
      "\n",
      "        # Note: we discard fill_value and use self._na_value, only relevant\n",
      "        #  in the case where allow_fill is True and fill_value is not None\n",
      "        values = self._values\n",
      "        if isinstance(values, np.ndarray):\n",
      "            taken = algos.take(\n",
      "                values, indices, allow_fill=allow_fill, fill_value=self._na_value\n",
      "            )\n",
      "        else:\n",
      "            # algos.take passes 'axis' keyword which not all EAs accept\n",
      "            taken = values.take(\n",
      "                indices, allow_fill=allow_fill, fill_value=self._na_value\n",
      "            )\n",
      "        # _constructor so RangeIndex->Int64Index\n",
      "        return self._constructor._simple_new(taken, name=self.name)\n",
      "\n",
      "------------\n",
      "Method name: to_flat_index\n",
      "Method definition:     def to_flat_index(self: _IndexT) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Identity method.\n",
      "\n",
      "        This is implemented for compatibility with subclass implementations\n",
      "        when chaining.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        pd.Index\n",
      "            Caller.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        MultiIndex.to_flat_index : Subclass implementation.\n",
      "        \"\"\"\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: to_frame\n",
      "Method definition:     def to_frame(\n",
      "        self, index: bool = True, name: Hashable = lib.no_default\n",
      "    ) -> DataFrame:\n",
      "        \"\"\"\n",
      "        Create a DataFrame with a column containing the Index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        index : bool, default True\n",
      "            Set the index of the returned DataFrame as the original Index.\n",
      "\n",
      "        name : object, default None\n",
      "            The passed name should substitute for the index name (if it has\n",
      "            one).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        DataFrame\n",
      "            DataFrame containing the original Index data.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.to_series : Convert an Index to a Series.\n",
      "        Series.to_frame : Convert Series to DataFrame.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n",
      "        >>> idx.to_frame()\n",
      "               animal\n",
      "        animal\n",
      "        Ant       Ant\n",
      "        Bear     Bear\n",
      "        Cow       Cow\n",
      "\n",
      "        By default, the original Index is reused. To enforce a new Index:\n",
      "\n",
      "        >>> idx.to_frame(index=False)\n",
      "            animal\n",
      "        0   Ant\n",
      "        1  Bear\n",
      "        2   Cow\n",
      "\n",
      "        To override the name of the resulting column, specify `name`:\n",
      "\n",
      "        >>> idx.to_frame(index=False, name='zoo')\n",
      "            zoo\n",
      "        0   Ant\n",
      "        1  Bear\n",
      "        2   Cow\n",
      "        \"\"\"\n",
      "        from pandas import DataFrame\n",
      "\n",
      "        if name is None:\n",
      "            warnings.warn(\n",
      "                \"Explicitly passing `name=None` currently preserves the Index's name \"\n",
      "                \"or uses a default name of 0. This behaviour is deprecated, and in \"\n",
      "                \"the future `None` will be used as the name of the resulting \"\n",
      "                \"DataFrame column.\",\n",
      "                FutureWarning,\n",
      "                stacklevel=find_stack_level(),\n",
      "            )\n",
      "            name = lib.no_default\n",
      "\n",
      "        if name is lib.no_default:\n",
      "            name = self._get_level_names()\n",
      "        result = DataFrame({name: self._values.copy()})\n",
      "\n",
      "        if index:\n",
      "            result.index = self\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: to_list\n",
      "Method definition:     def tolist(self):\n",
      "        \"\"\"\n",
      "        Return a list of the values.\n",
      "\n",
      "        These are each a scalar type, which is a Python scalar\n",
      "        (for str, int, float) or a pandas scalar\n",
      "        (for Timestamp/Timedelta/Interval/Period)\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        list\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.ndarray.tolist : Return the array as an a.ndim-levels deep\n",
      "            nested list of Python scalars.\n",
      "        \"\"\"\n",
      "        return self._values.tolist()\n",
      "\n",
      "------------\n",
      "Method name: to_native_types\n",
      "Method definition:     @final\n",
      "    def to_native_types(self, slicer=None, **kwargs) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Format specified values of `self` and return them.\n",
      "\n",
      "        .. deprecated:: 1.2.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        slicer : int, array-like\n",
      "            An indexer into `self` that specifies which values\n",
      "            are used in the formatting process.\n",
      "        kwargs : dict\n",
      "            Options for specifying how the values should be formatted.\n",
      "            These options include the following:\n",
      "\n",
      "            1) na_rep : str\n",
      "                The value that serves as a placeholder for NULL values\n",
      "            2) quoting : bool or None\n",
      "                Whether or not there are quoted values in `self`\n",
      "            3) date_format : str\n",
      "                The format used to represent date-like values.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "            Formatted values.\n",
      "        \"\"\"\n",
      "        warnings.warn(\n",
      "            \"The 'to_native_types' method is deprecated and will be removed in \"\n",
      "            \"a future version. Use 'astype(str)' instead.\",\n",
      "            FutureWarning,\n",
      "            stacklevel=find_stack_level(),\n",
      "        )\n",
      "        values = self\n",
      "        if slicer is not None:\n",
      "            values = values[slicer]\n",
      "        return values._format_native_types(**kwargs)\n",
      "\n",
      "------------\n",
      "Method name: to_numpy\n",
      "Method definition:     def to_numpy(\n",
      "        self,\n",
      "        dtype: npt.DTypeLike | None = None,\n",
      "        copy: bool = False,\n",
      "        na_value: object = lib.no_default,\n",
      "        **kwargs,\n",
      "    ) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        A NumPy ndarray representing the values in this Series or Index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        dtype : str or numpy.dtype, optional\n",
      "            The dtype to pass to :meth:`numpy.asarray`.\n",
      "        copy : bool, default False\n",
      "            Whether to ensure that the returned value is not a view on\n",
      "            another array. Note that ``copy=False`` does not *ensure* that\n",
      "            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n",
      "            a copy is made, even if not strictly necessary.\n",
      "        na_value : Any, optional\n",
      "            The value to use for missing values. The default value depends\n",
      "            on `dtype` and the type of the array.\n",
      "\n",
      "            .. versionadded:: 1.0.0\n",
      "\n",
      "        **kwargs\n",
      "            Additional keywords passed through to the ``to_numpy`` method\n",
      "            of the underlying array (for extension arrays).\n",
      "\n",
      "            .. versionadded:: 1.0.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.array : Get the actual data stored within.\n",
      "        Index.array : Get the actual data stored within.\n",
      "        DataFrame.to_numpy : Similar method for DataFrame.\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        The returned array will be the same up to equality (values equal\n",
      "        in `self` will be equal in the returned array; likewise for values\n",
      "        that are not equal). When `self` contains an ExtensionArray, the\n",
      "        dtype may be different. For example, for a category-dtype Series,\n",
      "        ``to_numpy()`` will return a NumPy array and the categorical dtype\n",
      "        will be lost.\n",
      "\n",
      "        For NumPy dtypes, this will be a reference to the actual data stored\n",
      "        in this Series or Index (assuming ``copy=False``). Modifying the result\n",
      "        in place will modify the data stored in the Series or Index (not that\n",
      "        we recommend doing that).\n",
      "\n",
      "        For extension types, ``to_numpy()`` *may* require copying data and\n",
      "        coercing the result to a NumPy type (possibly object), which may be\n",
      "        expensive. When you need a no-copy reference to the underlying data,\n",
      "        :attr:`Series.array` should be used instead.\n",
      "\n",
      "        This table lays out the different dtypes and default return types of\n",
      "        ``to_numpy()`` for various dtypes within pandas.\n",
      "\n",
      "        ================== ================================\n",
      "        dtype              array type\n",
      "        ================== ================================\n",
      "        category[T]        ndarray[T] (same dtype as input)\n",
      "        period             ndarray[object] (Periods)\n",
      "        interval           ndarray[object] (Intervals)\n",
      "        IntegerNA          ndarray[object]\n",
      "        datetime64[ns]     datetime64[ns]\n",
      "        datetime64[ns, tz] ndarray[object] (Timestamps)\n",
      "        ================== ================================\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\n",
      "        >>> ser.to_numpy()\n",
      "        array(['a', 'b', 'a'], dtype=object)\n",
      "\n",
      "        Specify the `dtype` to control how datetime-aware data is represented.\n",
      "        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\n",
      "        objects, each with the correct ``tz``.\n",
      "\n",
      "        >>> ser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\n",
      "        >>> ser.to_numpy(dtype=object)\n",
      "        array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\n",
      "               Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\n",
      "              dtype=object)\n",
      "\n",
      "        Or ``dtype='datetime64[ns]'`` to return an ndarray of native\n",
      "        datetime64 values. The values are converted to UTC and the timezone\n",
      "        info is dropped.\n",
      "\n",
      "        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\n",
      "        ... # doctest: +ELLIPSIS\n",
      "        array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00...'],\n",
      "              dtype='datetime64[ns]')\n",
      "        \"\"\"\n",
      "        if is_extension_array_dtype(self.dtype):\n",
      "            return self.array.to_numpy(dtype, copy=copy, na_value=na_value, **kwargs)\n",
      "        elif kwargs:\n",
      "            bad_keys = list(kwargs.keys())[0]\n",
      "            raise TypeError(\n",
      "                f\"to_numpy() got an unexpected keyword argument '{bad_keys}'\"\n",
      "            )\n",
      "\n",
      "        result = np.asarray(self._values, dtype=dtype)\n",
      "        # TODO(GH-24345): Avoid potential double copy\n",
      "        if copy or na_value is not lib.no_default:\n",
      "            result = result.copy()\n",
      "            if na_value is not lib.no_default:\n",
      "                result[np.asanyarray(self.isna())] = na_value\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: to_series\n",
      "Method definition:     def to_series(self, index=None, name: Hashable = None) -> Series:\n",
      "        \"\"\"\n",
      "        Create a Series with both index and values equal to the index keys.\n",
      "\n",
      "        Useful with map for returning an indexer based on an index.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        index : Index, optional\n",
      "            Index of resulting Series. If None, defaults to original index.\n",
      "        name : str, optional\n",
      "            Name of resulting Series. If None, defaults to name of original\n",
      "            index.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Series\n",
      "            The dtype will be based on the type of the Index values.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Index.to_frame : Convert an Index to a DataFrame.\n",
      "        Series.to_frame : Convert Series to DataFrame.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n",
      "\n",
      "        By default, the original Index and original name is reused.\n",
      "\n",
      "        >>> idx.to_series()\n",
      "        animal\n",
      "        Ant      Ant\n",
      "        Bear    Bear\n",
      "        Cow      Cow\n",
      "        Name: animal, dtype: object\n",
      "\n",
      "        To enforce a new Index, specify new labels to ``index``:\n",
      "\n",
      "        >>> idx.to_series(index=[0, 1, 2])\n",
      "        0     Ant\n",
      "        1    Bear\n",
      "        2     Cow\n",
      "        Name: animal, dtype: object\n",
      "\n",
      "        To override the name of the resulting column, specify `name`:\n",
      "\n",
      "        >>> idx.to_series(name='zoo')\n",
      "        animal\n",
      "        Ant      Ant\n",
      "        Bear    Bear\n",
      "        Cow      Cow\n",
      "        Name: zoo, dtype: object\n",
      "        \"\"\"\n",
      "        from pandas import Series\n",
      "\n",
      "        if index is None:\n",
      "            index = self._view()\n",
      "        if name is None:\n",
      "            name = self.name\n",
      "\n",
      "        return Series(self._values.copy(), index=index, name=name)\n",
      "\n",
      "------------\n",
      "Method name: tolist\n",
      "Method definition:     def tolist(self):\n",
      "        \"\"\"\n",
      "        Return a list of the values.\n",
      "\n",
      "        These are each a scalar type, which is a Python scalar\n",
      "        (for str, int, float) or a pandas scalar\n",
      "        (for Timestamp/Timedelta/Interval/Period)\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        list\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        numpy.ndarray.tolist : Return the array as an a.ndim-levels deep\n",
      "            nested list of Python scalars.\n",
      "        \"\"\"\n",
      "        return self._values.tolist()\n",
      "\n",
      "------------\n",
      "Method name: transpose\n",
      "Method definition:     def transpose(self: _T, *args, **kwargs) -> _T:\n",
      "        \"\"\"\n",
      "        Return the transpose, which is by definition self.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        %(klass)s\n",
      "        \"\"\"\n",
      "        nv.validate_transpose(args, kwargs)\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: union\n",
      "Method definition:     @final\n",
      "    def union(self, other, sort=None):\n",
      "        \"\"\"\n",
      "        Form the union of two Index objects.\n",
      "\n",
      "        If the Index objects are incompatible, both Index objects will be\n",
      "        cast to dtype('object') first.\n",
      "\n",
      "            .. versionchanged:: 0.25.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Index or array-like\n",
      "        sort : bool or None, default None\n",
      "            Whether to sort the resulting Index.\n",
      "\n",
      "            * None : Sort the result, except when\n",
      "\n",
      "              1. `self` and `other` are equal.\n",
      "              2. `self` or `other` has length 0.\n",
      "              3. Some values in `self` or `other` cannot be compared.\n",
      "                 A RuntimeWarning is issued in this case.\n",
      "\n",
      "            * False : do not sort the result.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        union : Index\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Union matching dtypes\n",
      "\n",
      "        >>> idx1 = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx2 = pd.Index([3, 4, 5, 6])\n",
      "        >>> idx1.union(idx2)\n",
      "        Int64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n",
      "\n",
      "        Union mismatched dtypes\n",
      "\n",
      "        >>> idx1 = pd.Index(['a', 'b', 'c', 'd'])\n",
      "        >>> idx2 = pd.Index([1, 2, 3, 4])\n",
      "        >>> idx1.union(idx2)\n",
      "        Index(['a', 'b', 'c', 'd', 1, 2, 3, 4], dtype='object')\n",
      "\n",
      "        MultiIndex case\n",
      "\n",
      "        >>> idx1 = pd.MultiIndex.from_arrays(\n",
      "        ...     [[1, 1, 2, 2], [\"Red\", \"Blue\", \"Red\", \"Blue\"]]\n",
      "        ... )\n",
      "        >>> idx1\n",
      "        MultiIndex([(1,  'Red'),\n",
      "            (1, 'Blue'),\n",
      "            (2,  'Red'),\n",
      "            (2, 'Blue')],\n",
      "           )\n",
      "        >>> idx2 = pd.MultiIndex.from_arrays(\n",
      "        ...     [[3, 3, 2, 2], [\"Red\", \"Green\", \"Red\", \"Green\"]]\n",
      "        ... )\n",
      "        >>> idx2\n",
      "        MultiIndex([(3,   'Red'),\n",
      "            (3, 'Green'),\n",
      "            (2,   'Red'),\n",
      "            (2, 'Green')],\n",
      "           )\n",
      "        >>> idx1.union(idx2)\n",
      "        MultiIndex([(1,  'Blue'),\n",
      "            (1,   'Red'),\n",
      "            (2,  'Blue'),\n",
      "            (2, 'Green'),\n",
      "            (2,   'Red'),\n",
      "            (3, 'Green'),\n",
      "            (3,   'Red')],\n",
      "           )\n",
      "        >>> idx1.union(idx2, sort=False)\n",
      "        MultiIndex([(1,   'Red'),\n",
      "            (1,  'Blue'),\n",
      "            (2,   'Red'),\n",
      "            (2,  'Blue'),\n",
      "            (3,   'Red'),\n",
      "            (3, 'Green'),\n",
      "            (2, 'Green')],\n",
      "           )\n",
      "        \"\"\"\n",
      "        self._validate_sort_keyword(sort)\n",
      "        self._assert_can_do_setop(other)\n",
      "        other, result_name = self._convert_can_do_setop(other)\n",
      "\n",
      "        if not is_dtype_equal(self.dtype, other.dtype):\n",
      "            if (\n",
      "                isinstance(self, ABCMultiIndex)\n",
      "                and not is_object_dtype(unpack_nested_dtype(other))\n",
      "                and len(other) > 0\n",
      "            ):\n",
      "                raise NotImplementedError(\n",
      "                    \"Can only union MultiIndex with MultiIndex or Index of tuples, \"\n",
      "                    \"try mi.to_flat_index().union(other) instead.\"\n",
      "                )\n",
      "            self._deprecate_dti_setop(other, \"union\")\n",
      "\n",
      "            dtype = self._find_common_type_compat(other)\n",
      "            left = self.astype(dtype, copy=False)\n",
      "            right = other.astype(dtype, copy=False)\n",
      "            return left.union(right, sort=sort)\n",
      "\n",
      "        elif not len(other) or self.equals(other):\n",
      "            # NB: whether this (and the `if not len(self)` check below) come before\n",
      "            #  or after the is_dtype_equal check above affects the returned dtype\n",
      "            return self._get_reconciled_name_object(other)\n",
      "\n",
      "        elif not len(self):\n",
      "            return other._get_reconciled_name_object(self)\n",
      "\n",
      "        result = self._union(other, sort=sort)\n",
      "\n",
      "        return self._wrap_setop_result(other, result)\n",
      "\n",
      "------------\n",
      "Method name: unique\n",
      "Method definition:     def unique(self: _IndexT, level: Hashable | None = None) -> _IndexT:\n",
      "        \"\"\"\n",
      "        Return unique values in the index.\n",
      "\n",
      "        Unique values are returned in order of appearance, this does NOT sort.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        level : int or hashable, optional\n",
      "            Only return values from specified level (for MultiIndex).\n",
      "            If int, gets the level by integer position, else by level name.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Index\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        unique : Numpy array of unique values in that column.\n",
      "        Series.unique : Return unique values of Series object.\n",
      "        \"\"\"\n",
      "        if level is not None:\n",
      "            self._validate_index_level(level)\n",
      "\n",
      "        if self.is_unique:\n",
      "            return self._view()\n",
      "\n",
      "        result = super().unique()\n",
      "        return self._shallow_copy(result)\n",
      "\n",
      "------------\n",
      "Method name: value_counts\n",
      "Method definition:     def value_counts(\n",
      "        self,\n",
      "        normalize: bool = False,\n",
      "        sort: bool = True,\n",
      "        ascending: bool = False,\n",
      "        bins=None,\n",
      "        dropna: bool = True,\n",
      "    ) -> Series:\n",
      "        \"\"\"\n",
      "        Return a Series containing counts of unique values.\n",
      "\n",
      "        The resulting object will be in descending order so that the\n",
      "        first element is the most frequently-occurring element.\n",
      "        Excludes NA values by default.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        normalize : bool, default False\n",
      "            If True then the object returned will contain the relative\n",
      "            frequencies of the unique values.\n",
      "        sort : bool, default True\n",
      "            Sort by frequencies.\n",
      "        ascending : bool, default False\n",
      "            Sort in ascending order.\n",
      "        bins : int, optional\n",
      "            Rather than count values, group them into half-open bins,\n",
      "            a convenience for ``pd.cut``, only works with numeric data.\n",
      "        dropna : bool, default True\n",
      "            Don't include counts of NaN.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Series\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.count: Number of non-NA elements in a Series.\n",
      "        DataFrame.count: Number of non-NA elements in a DataFrame.\n",
      "        DataFrame.value_counts: Equivalent method on DataFrames.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> index = pd.Index([3, 1, 2, 3, 4, np.nan])\n",
      "        >>> index.value_counts()\n",
      "        3.0    2\n",
      "        1.0    1\n",
      "        2.0    1\n",
      "        4.0    1\n",
      "        dtype: int64\n",
      "\n",
      "        With `normalize` set to `True`, returns the relative frequency by\n",
      "        dividing all values by the sum of values.\n",
      "\n",
      "        >>> s = pd.Series([3, 1, 2, 3, 4, np.nan])\n",
      "        >>> s.value_counts(normalize=True)\n",
      "        3.0    0.4\n",
      "        1.0    0.2\n",
      "        2.0    0.2\n",
      "        4.0    0.2\n",
      "        dtype: float64\n",
      "\n",
      "        **bins**\n",
      "\n",
      "        Bins can be useful for going from a continuous variable to a\n",
      "        categorical variable; instead of counting unique\n",
      "        apparitions of values, divide the index in the specified\n",
      "        number of half-open bins.\n",
      "\n",
      "        >>> s.value_counts(bins=3)\n",
      "        (0.996, 2.0]    2\n",
      "        (2.0, 3.0]      2\n",
      "        (3.0, 4.0]      1\n",
      "        dtype: int64\n",
      "\n",
      "        **dropna**\n",
      "\n",
      "        With `dropna` set to `False` we can also see NaN index values.\n",
      "\n",
      "        >>> s.value_counts(dropna=False)\n",
      "        3.0    2\n",
      "        1.0    1\n",
      "        2.0    1\n",
      "        4.0    1\n",
      "        NaN    1\n",
      "        dtype: int64\n",
      "        \"\"\"\n",
      "        return value_counts(\n",
      "            self,\n",
      "            sort=sort,\n",
      "            ascending=ascending,\n",
      "            normalize=normalize,\n",
      "            bins=bins,\n",
      "            dropna=dropna,\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: view\n",
      "Method definition:     def view(self, cls=None):\n",
      "\n",
      "        # we need to see if we are subclassing an\n",
      "        # index type here\n",
      "        if cls is not None and not hasattr(cls, \"_typ\"):\n",
      "            dtype = cls\n",
      "            if isinstance(cls, str):\n",
      "                dtype = pandas_dtype(cls)\n",
      "\n",
      "            if isinstance(dtype, (np.dtype, ExtensionDtype)) and needs_i8_conversion(\n",
      "                dtype\n",
      "            ):\n",
      "                if dtype.kind == \"m\" and dtype != \"m8[ns]\":\n",
      "                    # e.g. m8[s]\n",
      "                    return self._data.view(cls)\n",
      "\n",
      "                idx_cls = self._dtype_to_subclass(dtype)\n",
      "                # NB: we only get here for subclasses that override\n",
      "                #  _data_cls such that it is a type and not a tuple\n",
      "                #  of types.\n",
      "                arr_cls = idx_cls._data_cls\n",
      "                arr = arr_cls(self._data.view(\"i8\"), dtype=dtype)\n",
      "                return idx_cls._simple_new(arr, name=self.name)\n",
      "\n",
      "            result = self._data.view(cls)\n",
      "        else:\n",
      "            result = self._view()\n",
      "        if isinstance(result, Index):\n",
      "            result._id = self._id\n",
      "        return result\n",
      "\n",
      "------------\n",
      "Method name: where\n",
      "Method definition:     @final\n",
      "    def where(self, cond, other=None) -> Index:\n",
      "        \"\"\"\n",
      "        Replace values where the condition is False.\n",
      "\n",
      "        The replacement is taken from other.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        cond : bool array-like with the same length as self\n",
      "            Condition to select the values on.\n",
      "        other : scalar, or array-like, default None\n",
      "            Replacement if the condition is False.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        pandas.Index\n",
      "            A copy of self with values replaced from other\n",
      "            where the condition is False.\n",
      "\n",
      "        See Also\n",
      "        --------\n",
      "        Series.where : Same method for Series.\n",
      "        DataFrame.where : Same method for DataFrame.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> idx = pd.Index(['car', 'bike', 'train', 'tractor'])\n",
      "        >>> idx\n",
      "        Index(['car', 'bike', 'train', 'tractor'], dtype='object')\n",
      "        >>> idx.where(idx.isin(['car', 'train']), 'other')\n",
      "        Index(['car', 'other', 'train', 'other'], dtype='object')\n",
      "        \"\"\"\n",
      "        if isinstance(self, ABCMultiIndex):\n",
      "            raise NotImplementedError(\n",
      "                \".where is not supported for MultiIndex operations\"\n",
      "            )\n",
      "        cond = np.asarray(cond, dtype=bool)\n",
      "        return self.putmask(~cond, other)\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     def __eq__(self, other: Any) -> bool:\n",
      "        return isinstance(other, self.__class__) and self.__dict__ == other.__dict__\n",
      "\n",
      "------------\n",
      "Method name: __hash__\n",
      "Method definition:     def __hash__(self) -> int:\n",
      "        return hash(str(self))\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     def __ne__(self, other: Any) -> bool:\n",
      "        return not self.__eq__(other)\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        return self.__class__.__name__ + \"()\"\n",
      "\n",
      "------------\n",
      "Method name: fromInternal\n",
      "Method definition:     def fromInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts an internal SQL object into a native Python object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: json\n",
      "Method definition:     def json(self) -> str:\n",
      "        return json.dumps(self.jsonValue(), separators=(\",\", \":\"), sort_keys=True)\n",
      "\n",
      "------------\n",
      "Method name: jsonValue\n",
      "Method definition:     def jsonValue(self) -> Union[str, Dict[str, Any]]:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: needConversion\n",
      "Method definition:     def needConversion(self) -> bool:\n",
      "        \"\"\"\n",
      "        Does this type needs conversion between Python object and internal SQL object.\n",
      "\n",
      "        This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "        \"\"\"\n",
      "        return False\n",
      "\n",
      "------------\n",
      "Method name: simpleString\n",
      "Method definition:     def simpleString(self) -> str:\n",
      "        return \"int\"\n",
      "\n",
      "------------\n",
      "Method name: toInternal\n",
      "Method definition:     def toInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts a Python object into an internal SQL object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: __enter__\n",
      "Method definition:     @since(2.0)\n",
      "    def __enter__(self) -> \"SparkSession\":\n",
      "        \"\"\"\n",
      "        Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      "        \"\"\"\n",
      "        return self\n",
      "\n",
      "------------\n",
      "Method name: __exit__\n",
      "Method definition:     @since(2.0)\n",
      "    def __exit__(\n",
      "        self,\n",
      "        exc_type: Optional[Type[BaseException]],\n",
      "        exc_val: Optional[BaseException],\n",
      "        exc_tb: Optional[TracebackType],\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      "\n",
      "        Specifically stop the SparkSession on exit of the with block.\n",
      "        \"\"\"\n",
      "        self.stop()\n",
      "\n",
      "------------\n",
      "Method name: __init__\n",
      "Method definition:     def __init__(\n",
      "        self,\n",
      "        sparkContext: SparkContext,\n",
      "        jsparkSession: Optional[JavaObject] = None,\n",
      "        options: Dict[str, Any] = {},\n",
      "    ):\n",
      "        self._sc = sparkContext\n",
      "        self._jsc = self._sc._jsc\n",
      "        self._jvm = self._sc._jvm\n",
      "\n",
      "        assert self._jvm is not None\n",
      "\n",
      "        if jsparkSession is None:\n",
      "            if (\n",
      "                self._jvm.SparkSession.getDefaultSession().isDefined()\n",
      "                and not self._jvm.SparkSession.getDefaultSession().get().sparkContext().isStopped()\n",
      "            ):\n",
      "                jsparkSession = self._jvm.SparkSession.getDefaultSession().get()\n",
      "                getattr(getattr(self._jvm, \"SparkSession$\"), \"MODULE$\").applyModifiableSettings(\n",
      "                    jsparkSession, options\n",
      "                )\n",
      "            else:\n",
      "                jsparkSession = self._jvm.SparkSession(self._jsc.sc(), options)\n",
      "        else:\n",
      "            getattr(getattr(self._jvm, \"SparkSession$\"), \"MODULE$\").applyModifiableSettings(\n",
      "                jsparkSession, options\n",
      "            )\n",
      "        self._jsparkSession = jsparkSession\n",
      "        _monkey_patch_RDD(self)\n",
      "        install_exception_handler()\n",
      "        # If we had an instantiated SparkSession attached with a SparkContext\n",
      "        # which is stopped now, we need to renew the instantiated SparkSession.\n",
      "        # Otherwise, we will use invalid SparkSession when we call Builder.getOrCreate.\n",
      "        if (\n",
      "            SparkSession._instantiatedSession is None\n",
      "            or SparkSession._instantiatedSession._sc._jsc is None\n",
      "        ):\n",
      "            SparkSession._instantiatedSession = self\n",
      "            SparkSession._activeSession = self\n",
      "            assert self._jvm is not None\n",
      "            self._jvm.SparkSession.setDefaultSession(self._jsparkSession)\n",
      "            self._jvm.SparkSession.setActiveSession(self._jsparkSession)\n",
      "\n",
      "------------\n",
      "Method name: _convert_from_pandas\n",
      "Method definition:     def _convert_from_pandas(\n",
      "        self, pdf: \"PandasDataFrameLike\", schema: Union[StructType, str, List[str]], timezone: str\n",
      "    ) -> List:\n",
      "        \"\"\"\n",
      "        Convert a pandas.DataFrame to list of records that can be used to make a DataFrame\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        list\n",
      "            list of records\n",
      "        \"\"\"\n",
      "        import pandas as pd\n",
      "        from pyspark.sql import SparkSession\n",
      "\n",
      "        assert isinstance(self, SparkSession)\n",
      "\n",
      "        if timezone is not None:\n",
      "            from pyspark.sql.pandas.types import _check_series_convert_timestamps_tz_local\n",
      "            from pandas.core.dtypes.common import is_datetime64tz_dtype, is_timedelta64_dtype\n",
      "\n",
      "            copied = False\n",
      "            if isinstance(schema, StructType):\n",
      "                for field in schema:\n",
      "                    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?\n",
      "                    if isinstance(field.dataType, TimestampType):\n",
      "                        s = _check_series_convert_timestamps_tz_local(pdf[field.name], timezone)\n",
      "                        if s is not pdf[field.name]:\n",
      "                            if not copied:\n",
      "                                # Copy once if the series is modified to prevent the original\n",
      "                                # Pandas DataFrame from being updated\n",
      "                                pdf = pdf.copy()\n",
      "                                copied = True\n",
      "                            pdf[field.name] = s\n",
      "            else:\n",
      "                should_localize = not is_timestamp_ntz_preferred()\n",
      "                for column, series in pdf.iteritems():\n",
      "                    s = series\n",
      "                    if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n",
      "                        s = _check_series_convert_timestamps_tz_local(series, timezone)\n",
      "                    if s is not series:\n",
      "                        if not copied:\n",
      "                            # Copy once if the series is modified to prevent the original\n",
      "                            # Pandas DataFrame from being updated\n",
      "                            pdf = pdf.copy()\n",
      "                            copied = True\n",
      "                        pdf[column] = s\n",
      "\n",
      "            for column, series in pdf.iteritems():\n",
      "                if is_timedelta64_dtype(series):\n",
      "                    if not copied:\n",
      "                        pdf = pdf.copy()\n",
      "                        copied = True\n",
      "                    # Explicitly set the timedelta as object so the output of numpy records can\n",
      "                    # hold the timedelta instances as are. Otherwise, it converts to the internal\n",
      "                    # numeric values.\n",
      "                    ser = pdf[column]\n",
      "                    pdf[column] = pd.Series(\n",
      "                        ser.dt.to_pytimedelta(), index=ser.index, dtype=\"object\", name=ser.name\n",
      "                    )\n",
      "\n",
      "        # Convert pandas.DataFrame to list of numpy records\n",
      "        np_records = pdf.to_records(index=False)\n",
      "\n",
      "        # Check if any columns need to be fixed for Spark to infer properly\n",
      "        if len(np_records) > 0:\n",
      "            record_dtype = self._get_numpy_record_dtype(np_records[0])\n",
      "            if record_dtype is not None:\n",
      "                return [r.astype(record_dtype).tolist() for r in np_records]\n",
      "\n",
      "        # Convert list of numpy records to python lists\n",
      "        return [r.tolist() for r in np_records]\n",
      "\n",
      "------------\n",
      "Method name: _createFromLocal\n",
      "Method definition:     def _createFromLocal(\n",
      "        self, data: Iterable[Any], schema: Optional[Union[DataType, List[str]]]\n",
      "    ) -> Tuple[RDD[Tuple], StructType]:\n",
      "        \"\"\"\n",
      "        Create an RDD for DataFrame from a list or pandas.DataFrame, returns\n",
      "        the RDD and schema.\n",
      "        \"\"\"\n",
      "        # make sure data could consumed multiple times\n",
      "        if not isinstance(data, list):\n",
      "            data = list(data)\n",
      "\n",
      "        if schema is None or isinstance(schema, (list, tuple)):\n",
      "            struct = self._inferSchemaFromList(data, names=schema)\n",
      "            converter = _create_converter(struct)\n",
      "            tupled_data: Iterable[Tuple] = map(converter, data)\n",
      "            if isinstance(schema, (list, tuple)):\n",
      "                for i, name in enumerate(schema):\n",
      "                    struct.fields[i].name = name\n",
      "                    struct.names[i] = name\n",
      "\n",
      "        elif isinstance(schema, StructType):\n",
      "            struct = schema\n",
      "            tupled_data = data\n",
      "\n",
      "        else:\n",
      "            raise TypeError(\"schema should be StructType or list or None, but got: %s\" % schema)\n",
      "\n",
      "        # convert python objects to sql data\n",
      "        internal_data = [struct.toInternal(row) for row in tupled_data]\n",
      "        return self._sc.parallelize(internal_data), struct\n",
      "\n",
      "------------\n",
      "Method name: _createFromRDD\n",
      "Method definition:     def _createFromRDD(\n",
      "        self,\n",
      "        rdd: RDD[Any],\n",
      "        schema: Optional[Union[DataType, List[str]]],\n",
      "        samplingRatio: Optional[float],\n",
      "    ) -> Tuple[RDD[Tuple], StructType]:\n",
      "        \"\"\"\n",
      "        Create an RDD for DataFrame from an existing RDD, returns the RDD and schema.\n",
      "        \"\"\"\n",
      "        if schema is None or isinstance(schema, (list, tuple)):\n",
      "            struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "            converter = _create_converter(struct)\n",
      "            tupled_rdd = rdd.map(converter)\n",
      "            if isinstance(schema, (list, tuple)):\n",
      "                for i, name in enumerate(schema):\n",
      "                    struct.fields[i].name = name\n",
      "                    struct.names[i] = name\n",
      "\n",
      "        elif isinstance(schema, StructType):\n",
      "            struct = schema\n",
      "            tupled_rdd = rdd\n",
      "\n",
      "        else:\n",
      "            raise TypeError(\"schema should be StructType or list or None, but got: %s\" % schema)\n",
      "\n",
      "        # convert python objects to sql data\n",
      "        internal_rdd = tupled_rdd.map(struct.toInternal)\n",
      "        return internal_rdd, struct\n",
      "\n",
      "------------\n",
      "Method name: _create_dataframe\n",
      "Method definition:     def _create_dataframe(\n",
      "        self,\n",
      "        data: Union[RDD[Any], Iterable[Any]],\n",
      "        schema: Optional[Union[DataType, List[str]]],\n",
      "        samplingRatio: Optional[float],\n",
      "        verifySchema: bool,\n",
      "    ) -> DataFrame:\n",
      "        if isinstance(schema, StructType):\n",
      "            verify_func = _make_type_verifier(schema) if verifySchema else lambda _: True\n",
      "\n",
      "            @no_type_check\n",
      "            def prepare(obj):\n",
      "                verify_func(obj)\n",
      "                return obj\n",
      "\n",
      "        elif isinstance(schema, DataType):\n",
      "            dataType = schema\n",
      "            schema = StructType().add(\"value\", schema)\n",
      "\n",
      "            verify_func = (\n",
      "                _make_type_verifier(dataType, name=\"field value\")\n",
      "                if verifySchema\n",
      "                else lambda _: True\n",
      "            )\n",
      "\n",
      "            @no_type_check\n",
      "            def prepare(obj):\n",
      "                verify_func(obj)\n",
      "                return (obj,)\n",
      "\n",
      "        else:\n",
      "\n",
      "            def prepare(obj: Any) -> Any:\n",
      "                return obj\n",
      "\n",
      "        if isinstance(data, RDD):\n",
      "            rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "        else:\n",
      "            rdd, struct = self._createFromLocal(map(prepare, data), schema)\n",
      "        assert self._jvm is not None\n",
      "        jrdd = self._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())\n",
      "        jdf = self._jsparkSession.applySchemaToPythonRDD(jrdd.rdd(), struct.json())\n",
      "        df = DataFrame(jdf, self)\n",
      "        df._schema = struct\n",
      "        return df\n",
      "\n",
      "------------\n",
      "Method name: _create_from_pandas_with_arrow\n",
      "Method definition:     def _create_from_pandas_with_arrow(\n",
      "        self, pdf: \"PandasDataFrameLike\", schema: Union[StructType, List[str]], timezone: str\n",
      "    ) -> \"DataFrame\":\n",
      "        \"\"\"\n",
      "        Create a DataFrame from a given pandas.DataFrame by slicing it into partitions, converting\n",
      "        to Arrow data, then sending to the JVM to parallelize. If a schema is passed in, the\n",
      "        data types will be used to coerce the data in Pandas to Arrow conversion.\n",
      "        \"\"\"\n",
      "        from pyspark.sql import SparkSession\n",
      "        from pyspark.sql.dataframe import DataFrame\n",
      "\n",
      "        assert isinstance(self, SparkSession)\n",
      "\n",
      "        from pyspark.sql.pandas.serializers import ArrowStreamPandasSerializer\n",
      "        from pyspark.sql.types import TimestampType\n",
      "        from pyspark.sql.pandas.types import from_arrow_type, to_arrow_type\n",
      "        from pyspark.sql.pandas.utils import (\n",
      "            require_minimum_pandas_version,\n",
      "            require_minimum_pyarrow_version,\n",
      "        )\n",
      "\n",
      "        require_minimum_pandas_version()\n",
      "        require_minimum_pyarrow_version()\n",
      "\n",
      "        from pandas.api.types import (  # type: ignore[attr-defined]\n",
      "            is_datetime64_dtype,\n",
      "            is_datetime64tz_dtype,\n",
      "        )\n",
      "        import pyarrow as pa\n",
      "\n",
      "        # Create the Spark schema from list of names passed in with Arrow types\n",
      "        if isinstance(schema, (list, tuple)):\n",
      "            arrow_schema = pa.Schema.from_pandas(pdf, preserve_index=False)\n",
      "            struct = StructType()\n",
      "            prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n",
      "            for name, field in zip(schema, arrow_schema):\n",
      "                struct.add(\n",
      "                    name, from_arrow_type(field.type, prefer_timestamp_ntz), nullable=field.nullable\n",
      "                )\n",
      "            schema = struct\n",
      "\n",
      "        # Determine arrow types to coerce data when creating batches\n",
      "        if isinstance(schema, StructType):\n",
      "            arrow_types = [to_arrow_type(f.dataType) for f in schema.fields]\n",
      "        elif isinstance(schema, DataType):\n",
      "            raise ValueError(\"Single data type %s is not supported with Arrow\" % str(schema))\n",
      "        else:\n",
      "            # Any timestamps must be coerced to be compatible with Spark\n",
      "            arrow_types = [\n",
      "                to_arrow_type(TimestampType())\n",
      "                if is_datetime64_dtype(t) or is_datetime64tz_dtype(t)\n",
      "                else None\n",
      "                for t in pdf.dtypes\n",
      "            ]\n",
      "\n",
      "        # Slice the DataFrame to be batched\n",
      "        step = -(-len(pdf) // self.sparkContext.defaultParallelism)  # round int up\n",
      "        pdf_slices = (pdf.iloc[start : start + step] for start in range(0, len(pdf), step))\n",
      "\n",
      "        # Create list of Arrow (columns, type) for serializer dump_stream\n",
      "        arrow_data = [\n",
      "            [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n",
      "            for pdf_slice in pdf_slices\n",
      "        ]\n",
      "\n",
      "        jsparkSession = self._jsparkSession\n",
      "\n",
      "        safecheck = self._jconf.arrowSafeTypeConversion()\n",
      "        col_by_name = True  # col by name only applies to StructType columns, can't happen here\n",
      "        ser = ArrowStreamPandasSerializer(timezone, safecheck, col_by_name)\n",
      "\n",
      "        @no_type_check\n",
      "        def reader_func(temp_filename):\n",
      "            return self._jvm.PythonSQLUtils.readArrowStreamFromFile(jsparkSession, temp_filename)\n",
      "\n",
      "        @no_type_check\n",
      "        def create_RDD_server():\n",
      "            return self._jvm.ArrowRDDServer(jsparkSession)\n",
      "\n",
      "        # Create Spark DataFrame from Arrow stream file, using one batch per partition\n",
      "        jrdd = self._sc._serialize_to_jvm(arrow_data, ser, reader_func, create_RDD_server)\n",
      "        assert self._jvm is not None\n",
      "        jdf = self._jvm.PythonSQLUtils.toDataFrame(jrdd, schema.json(), jsparkSession)\n",
      "        df = DataFrame(jdf, self)\n",
      "        df._schema = schema\n",
      "        return df\n",
      "\n",
      "------------\n",
      "Method name: _create_shell_session\n",
      "Method definition:     @staticmethod\n",
      "    def _create_shell_session() -> \"SparkSession\":\n",
      "        \"\"\"\n",
      "        Initialize a :class:`SparkSession` for a pyspark shell session. This is called from\n",
      "        shell.py to make error handling simpler without needing to declare local variables in\n",
      "        that script, which would expose those to users.\n",
      "        \"\"\"\n",
      "        import py4j\n",
      "        from pyspark.conf import SparkConf\n",
      "        from pyspark.context import SparkContext\n",
      "\n",
      "        try:\n",
      "            # Try to access HiveConf, it will raise exception if Hive is not added\n",
      "            conf = SparkConf()\n",
      "            assert SparkContext._jvm is not None\n",
      "            if conf.get(\"spark.sql.catalogImplementation\", \"hive\").lower() == \"hive\":\n",
      "                SparkContext._jvm.org.apache.hadoop.hive.conf.HiveConf()\n",
      "                return SparkSession.builder.enableHiveSupport().getOrCreate()\n",
      "            else:\n",
      "                return SparkSession._getActiveSessionOrCreate()\n",
      "        except (py4j.protocol.Py4JError, TypeError):\n",
      "            if conf.get(\"spark.sql.catalogImplementation\", \"\").lower() == \"hive\":\n",
      "                warnings.warn(\n",
      "                    \"Fall back to non-hive support because failing to access HiveConf, \"\n",
      "                    \"please make sure you build spark with hive\"\n",
      "                )\n",
      "\n",
      "        return SparkSession._getActiveSessionOrCreate()\n",
      "\n",
      "------------\n",
      "Method name: _getActiveSessionOrCreate\n",
      "Method definition:     @staticmethod\n",
      "    def _getActiveSessionOrCreate(**static_conf: Any) -> \"SparkSession\":\n",
      "        \"\"\"\n",
      "        Returns the active :class:`SparkSession` for the current thread, returned by the builder,\n",
      "        or if there is no existing one, creates a new one based on the options set in the builder.\n",
      "\n",
      "        NOTE that 'static_conf' might not be set if there's an active or default Spark session\n",
      "        running.\n",
      "        \"\"\"\n",
      "        spark = SparkSession.getActiveSession()\n",
      "        if spark is None:\n",
      "            builder = SparkSession.builder\n",
      "            for k, v in static_conf.items():\n",
      "                builder = builder.config(k, v)\n",
      "            spark = builder.getOrCreate()\n",
      "        return spark\n",
      "\n",
      "------------\n",
      "Method name: _get_numpy_record_dtype\n",
      "Method definition:     def _get_numpy_record_dtype(self, rec: \"np.recarray\") -> Optional[\"np.dtype\"]:\n",
      "        \"\"\"\n",
      "        Used when converting a pandas.DataFrame to Spark using to_records(), this will correct\n",
      "        the dtypes of fields in a record so they can be properly loaded into Spark.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        rec : numpy.record\n",
      "            a numpy record to check field dtypes\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.dtype\n",
      "            corrected dtype for a numpy.record or None if no correction needed\n",
      "        \"\"\"\n",
      "        import numpy as np\n",
      "\n",
      "        cur_dtypes = rec.dtype\n",
      "        col_names = cur_dtypes.names\n",
      "        record_type_list = []\n",
      "        has_rec_fix = False\n",
      "        for i in range(len(cur_dtypes)):\n",
      "            curr_type = cur_dtypes[i]\n",
      "            # If type is a datetime64 timestamp, convert to microseconds\n",
      "            # NOTE: if dtype is datetime[ns] then np.record.tolist() will output values as longs,\n",
      "            # conversion from [us] or lower will lead to py datetime objects, see SPARK-22417\n",
      "            if curr_type == np.dtype(\"datetime64[ns]\"):\n",
      "                curr_type = \"datetime64[us]\"\n",
      "                has_rec_fix = True\n",
      "            record_type_list.append((str(col_names[i]), curr_type))\n",
      "        return np.dtype(record_type_list) if has_rec_fix else None\n",
      "\n",
      "------------\n",
      "Method name: _inferSchema\n",
      "Method definition:     def _inferSchema(\n",
      "        self,\n",
      "        rdd: RDD[Any],\n",
      "        samplingRatio: Optional[float] = None,\n",
      "        names: Optional[List[str]] = None,\n",
      "    ) -> StructType:\n",
      "        \"\"\"\n",
      "        Infer schema from an RDD of Row, dict, or tuple.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        rdd : :class:`RDD`\n",
      "            an RDD of Row, dict, or tuple\n",
      "        samplingRatio : float, optional\n",
      "            sampling ratio, or no sampling (default)\n",
      "        names : list, optional\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`pyspark.sql.types.StructType`\n",
      "        \"\"\"\n",
      "        first = rdd.first()\n",
      "        if not first:\n",
      "            raise ValueError(\"The first row in RDD is empty, \" \"can not infer schema\")\n",
      "\n",
      "        infer_dict_as_struct = self._jconf.inferDictAsStruct()\n",
      "        prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n",
      "        if samplingRatio is None:\n",
      "            schema = _infer_schema(\n",
      "                first,\n",
      "                names=names,\n",
      "                infer_dict_as_struct=infer_dict_as_struct,\n",
      "                prefer_timestamp_ntz=prefer_timestamp_ntz,\n",
      "            )\n",
      "            if _has_nulltype(schema):\n",
      "                for row in rdd.take(100)[1:]:\n",
      "                    schema = _merge_type(\n",
      "                        schema,\n",
      "                        _infer_schema(\n",
      "                            row,\n",
      "                            names=names,\n",
      "                            infer_dict_as_struct=infer_dict_as_struct,\n",
      "                            prefer_timestamp_ntz=prefer_timestamp_ntz,\n",
      "                        ),\n",
      "                    )\n",
      "                    if not _has_nulltype(schema):\n",
      "                        break\n",
      "                else:\n",
      "                    raise ValueError(\n",
      "                        \"Some of types cannot be determined by the \"\n",
      "                        \"first 100 rows, please try again with sampling\"\n",
      "                    )\n",
      "        else:\n",
      "            if samplingRatio < 0.99:\n",
      "                rdd = rdd.sample(False, float(samplingRatio))\n",
      "            schema = rdd.map(\n",
      "                lambda row: _infer_schema(\n",
      "                    row,\n",
      "                    names,\n",
      "                    infer_dict_as_struct=infer_dict_as_struct,\n",
      "                    prefer_timestamp_ntz=prefer_timestamp_ntz,\n",
      "                )\n",
      "            ).reduce(_merge_type)\n",
      "        return schema\n",
      "\n",
      "------------\n",
      "Method name: _inferSchemaFromList\n",
      "Method definition:     def _inferSchemaFromList(\n",
      "        self, data: Iterable[Any], names: Optional[List[str]] = None\n",
      "    ) -> StructType:\n",
      "        \"\"\"\n",
      "        Infer schema from list of Row, dict, or tuple.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        data : iterable\n",
      "            list of Row, dict, or tuple\n",
      "        names : list, optional\n",
      "            list of column names\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`pyspark.sql.types.StructType`\n",
      "        \"\"\"\n",
      "        if not data:\n",
      "            raise ValueError(\"can not infer schema from empty dataset\")\n",
      "        infer_dict_as_struct = self._jconf.inferDictAsStruct()\n",
      "        prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n",
      "        schema = reduce(\n",
      "            _merge_type,\n",
      "            (_infer_schema(row, names, infer_dict_as_struct, prefer_timestamp_ntz) for row in data),\n",
      "        )\n",
      "        if _has_nulltype(schema):\n",
      "            raise ValueError(\"Some of types cannot be determined after inferring\")\n",
      "        return schema\n",
      "\n",
      "------------\n",
      "Method name: _repr_html_\n",
      "Method definition:     def _repr_html_(self) -> str:\n",
      "        return \"\"\"\n",
      "            <div>\n",
      "                <p><b>SparkSession - {catalogImplementation}</b></p>\n",
      "                {sc_HTML}\n",
      "            </div>\n",
      "        \"\"\".format(\n",
      "            catalogImplementation=self.conf.get(\"spark.sql.catalogImplementation\"),\n",
      "            sc_HTML=self.sparkContext._repr_html_(),\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: createDataFrame\n",
      "Method definition:     def createDataFrame(  # type: ignore[misc]\n",
      "        self,\n",
      "        data: Union[RDD[Any], Iterable[Any], \"PandasDataFrameLike\"],\n",
      "        schema: Optional[Union[AtomicType, StructType, str]] = None,\n",
      "        samplingRatio: Optional[float] = None,\n",
      "        verifySchema: bool = True,\n",
      "    ) -> DataFrame:\n",
      "        \"\"\"\n",
      "        Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "\n",
      "        When ``schema`` is a list of column names, the type of each column\n",
      "        will be inferred from ``data``.\n",
      "\n",
      "        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "        from ``data``, which should be an RDD of either :class:`Row`,\n",
      "        :class:`namedtuple`, or :class:`dict`.\n",
      "\n",
      "        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      "        the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "        :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      "        Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "\n",
      "        If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      "        rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        .. versionchanged:: 2.1.0\n",
      "           Added verifySchema.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        data : :class:`RDD` or iterable\n",
      "            an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "            :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      "            :class:`pandas.DataFrame`.\n",
      "        schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "            a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "            column names, default is None.  The data type string format equals to\n",
      "            :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "            omit the ``struct<>``.\n",
      "        samplingRatio : float, optional\n",
      "            the sample ratio of rows used for inferring\n",
      "        verifySchema : bool, optional\n",
      "            verify data types of every row against schema. Enabled by default.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`DataFrame`\n",
      "\n",
      "        Notes\n",
      "        -----\n",
      "        Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> l = [('Alice', 1)]\n",
      "        >>> spark.createDataFrame(l).collect()\n",
      "        [Row(_1='Alice', _2=1)]\n",
      "        >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      "        [Row(name='Alice', age=1)]\n",
      "\n",
      "        >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "        >>> spark.createDataFrame(d).collect()\n",
      "        [Row(age=1, name='Alice')]\n",
      "\n",
      "        >>> rdd = sc.parallelize(l)\n",
      "        >>> spark.createDataFrame(rdd).collect()\n",
      "        [Row(_1='Alice', _2=1)]\n",
      "        >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      "        >>> df.collect()\n",
      "        [Row(name='Alice', age=1)]\n",
      "\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> Person = Row('name', 'age')\n",
      "        >>> person = rdd.map(lambda r: Person(*r))\n",
      "        >>> df2 = spark.createDataFrame(person)\n",
      "        >>> df2.collect()\n",
      "        [Row(name='Alice', age=1)]\n",
      "\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> schema = StructType([\n",
      "        ...    StructField(\"name\", StringType(), True),\n",
      "        ...    StructField(\"age\", IntegerType(), True)])\n",
      "        >>> df3 = spark.createDataFrame(rdd, schema)\n",
      "        >>> df3.collect()\n",
      "        [Row(name='Alice', age=1)]\n",
      "\n",
      "        >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "        [Row(name='Alice', age=1)]\n",
      "        >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "        [Row(0=1, 1=2)]\n",
      "\n",
      "        >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "        [Row(a='Alice', b=1)]\n",
      "        >>> rdd = rdd.map(lambda row: row[1])\n",
      "        >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      "        [Row(value=1)]\n",
      "        >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "        Traceback (most recent call last):\n",
      "            ...\n",
      "        Py4JJavaError: ...\n",
      "        \"\"\"\n",
      "        SparkSession._activeSession = self\n",
      "        assert self._jvm is not None\n",
      "        self._jvm.SparkSession.setActiveSession(self._jsparkSession)\n",
      "        if isinstance(data, DataFrame):\n",
      "            raise TypeError(\"data is already a DataFrame\")\n",
      "\n",
      "        if isinstance(schema, str):\n",
      "            schema = cast(Union[AtomicType, StructType, str], _parse_datatype_string(schema))\n",
      "        elif isinstance(schema, (list, tuple)):\n",
      "            # Must re-encode any unicode strings to be consistent with StructField names\n",
      "            schema = [x.encode(\"utf-8\") if not isinstance(x, str) else x for x in schema]\n",
      "\n",
      "        try:\n",
      "            import pandas\n",
      "\n",
      "            has_pandas = True\n",
      "        except Exception:\n",
      "            has_pandas = False\n",
      "        if has_pandas and isinstance(data, pandas.DataFrame):\n",
      "            # Create a DataFrame from pandas DataFrame.\n",
      "            return super(SparkSession, self).createDataFrame(  # type: ignore[call-overload]\n",
      "                data, schema, samplingRatio, verifySchema\n",
      "            )\n",
      "        return self._create_dataframe(\n",
      "            data, schema, samplingRatio, verifySchema  # type: ignore[arg-type]\n",
      "        )\n",
      "\n",
      "------------\n",
      "Method name: newSession\n",
      "Method definition:     @since(2.0)\n",
      "    def newSession(self) -> \"SparkSession\":\n",
      "        \"\"\"\n",
      "        Returns a new :class:`SparkSession` as new session, that has separate SQLConf,\n",
      "        registered temporary views and UDFs, but shared :class:`SparkContext` and\n",
      "        table cache.\n",
      "        \"\"\"\n",
      "        return self.__class__(self._sc, self._jsparkSession.newSession())\n",
      "\n",
      "------------\n",
      "Method name: range\n",
      "Method definition:     def range(\n",
      "        self,\n",
      "        start: int,\n",
      "        end: Optional[int] = None,\n",
      "        step: int = 1,\n",
      "        numPartitions: Optional[int] = None,\n",
      "    ) -> DataFrame:\n",
      "        \"\"\"\n",
      "        Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      "        ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      "        step value ``step``.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        start : int\n",
      "            the start value\n",
      "        end : int, optional\n",
      "            the end value (exclusive)\n",
      "        step : int, optional\n",
      "            the incremental step (default: 1)\n",
      "        numPartitions : int, optional\n",
      "            the number of partitions of the DataFrame\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`DataFrame`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(1, 7, 2).collect()\n",
      "        [Row(id=1), Row(id=3), Row(id=5)]\n",
      "\n",
      "        If only one argument is specified, it will be used as the end value.\n",
      "\n",
      "        >>> spark.range(3).collect()\n",
      "        [Row(id=0), Row(id=1), Row(id=2)]\n",
      "        \"\"\"\n",
      "        if numPartitions is None:\n",
      "            numPartitions = self._sc.defaultParallelism\n",
      "\n",
      "        if end is None:\n",
      "            jdf = self._jsparkSession.range(0, int(start), int(step), int(numPartitions))\n",
      "        else:\n",
      "            jdf = self._jsparkSession.range(int(start), int(end), int(step), int(numPartitions))\n",
      "\n",
      "        return DataFrame(jdf, self)\n",
      "\n",
      "------------\n",
      "Method name: sql\n",
      "Method definition:     def sql(self, sqlQuery: str, **kwargs: Any) -> DataFrame:\n",
      "        \"\"\"Returns a :class:`DataFrame` representing the result of the given query.\n",
      "        When ``kwargs`` is specified, this method formats the given string by using the Python\n",
      "        standard formatter.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        sqlQuery : str\n",
      "            SQL query string.\n",
      "        kwargs : dict\n",
      "            Other variables that the user wants to set that can be referenced in the query\n",
      "\n",
      "            .. versionchanged:: 3.3.0\n",
      "               Added optional argument ``kwargs`` to specify the mapping of variables in the query.\n",
      "               This feature is experimental and unstable.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`DataFrame`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        Executing a SQL query.\n",
      "\n",
      "        >>> spark.sql(\"SELECT * FROM range(10) where id > 7\").show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  8|\n",
      "        |  9|\n",
      "        +---+\n",
      "\n",
      "        Executing a SQL query with variables as Python formatter standard.\n",
      "\n",
      "        >>> spark.sql(\n",
      "        ...     \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\", bound1=7, bound2=9\n",
      "        ... ).show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  8|\n",
      "        +---+\n",
      "\n",
      "        >>> mydf = spark.range(10)\n",
      "        >>> spark.sql(\n",
      "        ...     \"SELECT {col} FROM {mydf} WHERE id IN {x}\",\n",
      "        ...     col=mydf.id, mydf=mydf, x=tuple(range(4))).show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  0|\n",
      "        |  1|\n",
      "        |  2|\n",
      "        |  3|\n",
      "        +---+\n",
      "\n",
      "        >>> spark.sql('''\n",
      "        ...   SELECT m1.a, m2.b\n",
      "        ...   FROM {table1} m1 INNER JOIN {table2} m2\n",
      "        ...   ON m1.key = m2.key\n",
      "        ...   ORDER BY m1.a, m2.b''',\n",
      "        ...   table1=spark.createDataFrame([(1, \"a\"), (2, \"b\")], [\"a\", \"key\"]),\n",
      "        ...   table2=spark.createDataFrame([(3, \"a\"), (4, \"b\"), (5, \"b\")], [\"b\", \"key\"])).show()\n",
      "        +---+---+\n",
      "        |  a|  b|\n",
      "        +---+---+\n",
      "        |  1|  3|\n",
      "        |  2|  4|\n",
      "        |  2|  5|\n",
      "        +---+---+\n",
      "\n",
      "        Also, it is possible to query using class:`Column` from :class:`DataFrame`.\n",
      "\n",
      "        >>> mydf = spark.createDataFrame([(1, 4), (2, 4), (3, 6)], [\"A\", \"B\"])\n",
      "        >>> spark.sql(\"SELECT {df.A}, {df[B]} FROM {df}\", df=mydf).show()\n",
      "        +---+---+\n",
      "        |  A|  B|\n",
      "        +---+---+\n",
      "        |  1|  4|\n",
      "        |  2|  4|\n",
      "        |  3|  6|\n",
      "        +---+---+\n",
      "        \"\"\"\n",
      "\n",
      "        formatter = SQLStringFormatter(self)\n",
      "        if len(kwargs) > 0:\n",
      "            sqlQuery = formatter.format(sqlQuery, **kwargs)\n",
      "        try:\n",
      "            return DataFrame(self._jsparkSession.sql(sqlQuery), self)\n",
      "        finally:\n",
      "            if len(kwargs) > 0:\n",
      "                formatter.clear()\n",
      "\n",
      "------------\n",
      "Method name: stop\n",
      "Method definition:     @since(2.0)\n",
      "    def stop(self) -> None:\n",
      "        \"\"\"Stop the underlying :class:`SparkContext`.\"\"\"\n",
      "        from pyspark.sql.context import SQLContext\n",
      "\n",
      "        self._sc.stop()\n",
      "        # We should clean the default session up. See SPARK-23228.\n",
      "        assert self._jvm is not None\n",
      "        self._jvm.SparkSession.clearDefaultSession()\n",
      "        self._jvm.SparkSession.clearActiveSession()\n",
      "        SparkSession._instantiatedSession = None\n",
      "        SparkSession._activeSession = None\n",
      "        SQLContext._instantiatedContext = None\n",
      "\n",
      "------------\n",
      "Method name: table\n",
      "Method definition:     def table(self, tableName: str) -> DataFrame:\n",
      "        \"\"\"Returns the specified table as a :class:`DataFrame`.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        :class:`DataFrame`\n",
      "\n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.createOrReplaceTempView(\"table1\")\n",
      "        >>> df2 = spark.table(\"table1\")\n",
      "        >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "        True\n",
      "        \"\"\"\n",
      "        return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "\n",
      "------------\n",
      "Method name: __eq__\n",
      "Method definition:     def __eq__(self, other: Any) -> bool:\n",
      "        return isinstance(other, self.__class__) and self.__dict__ == other.__dict__\n",
      "\n",
      "------------\n",
      "Method name: __hash__\n",
      "Method definition:     def __hash__(self) -> int:\n",
      "        return hash(str(self))\n",
      "\n",
      "------------\n",
      "Method name: __ne__\n",
      "Method definition:     def __ne__(self, other: Any) -> bool:\n",
      "        return not self.__eq__(other)\n",
      "\n",
      "------------\n",
      "Method name: __repr__\n",
      "Method definition:     def __repr__(self) -> str:\n",
      "        return self.__class__.__name__ + \"()\"\n",
      "\n",
      "------------\n",
      "Method name: fromInternal\n",
      "Method definition:     def fromInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts an internal SQL object into a native Python object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n",
      "Method name: json\n",
      "Method definition:     def json(self) -> str:\n",
      "        return json.dumps(self.jsonValue(), separators=(\",\", \":\"), sort_keys=True)\n",
      "\n",
      "------------\n",
      "Method name: jsonValue\n",
      "Method definition:     def jsonValue(self) -> Union[str, Dict[str, Any]]:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: needConversion\n",
      "Method definition:     def needConversion(self) -> bool:\n",
      "        \"\"\"\n",
      "        Does this type needs conversion between Python object and internal SQL object.\n",
      "\n",
      "        This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "        \"\"\"\n",
      "        return False\n",
      "\n",
      "------------\n",
      "Method name: simpleString\n",
      "Method definition:     def simpleString(self) -> str:\n",
      "        return self.typeName()\n",
      "\n",
      "------------\n",
      "Method name: toInternal\n",
      "Method definition:     def toInternal(self, obj: Any) -> Any:\n",
      "        \"\"\"\n",
      "        Converts a Python object into an internal SQL object.\n",
      "        \"\"\"\n",
      "        return obj\n",
      "\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "import inspect\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "def getmembers(item: Any, predicate=None) -> List[Tuple[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get all members of an object.\n",
    "\n",
    "    :param item: An object to get the members of.\n",
    "    :param predicate: A callable used to filter the members.\n",
    "    :return: A list of tuples containing the name and value of each member.\n",
    "    \"\"\"\n",
    "    if predicate is None:\n",
    "        predicate = inspect.ismemberdescriptor\n",
    "    return inspect.getmembers(item, predicate)\n",
    "\n",
    "def import_all_modules(directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Import all modules from a directory.\n",
    "\n",
    "    :param directory: The directory to import modules from.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for file in os.listdir(directory):\n",
    "        func_dict={}\n",
    "        if file.endswith(\".py\") and file != '__init__.py':\n",
    "            module_name = file[:-3]\n",
    "            spec = importlib.util.spec_from_file_location(module_name, os.path.join(directory, file))\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(module)\n",
    "            members = getmembers(module, inspect.isfunction)\n",
    "            for member in members:\n",
    "                print(f\"Function name: {member[0]}\")\n",
    "                print(f\"Function definition: {inspect.getsource(member[1])}\")\n",
    "                print(\"------------\")\n",
    "                func_dict[member[0]]=member[1]\n",
    "            classes = [m[1] for m in getmembers(module, inspect.isclass)]\n",
    "            for class_ in classes:\n",
    "                members = getmembers(class_, inspect.isfunction)\n",
    "                for member in members:\n",
    "                    print(f\"Method name: {member[0]}\")\n",
    "                    print(f\"Method definition: {inspect.getsource(member[1])}\")\n",
    "                    print(\"------------\")\n",
    "                    func_dict[member[0]]=member[1]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import_all_modules(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dq_check.py\n",
      "Skipping imported function: broadcast\n",
      "Skipping imported function: urlparse\n",
      "Skipping imported method: __init__\n",
      "Skipping imported method: __str__\n",
      "Skipping imported method: getErrorClass\n",
      "Skipping imported method: getSqlState\n",
      "Skipping imported method: __init__\n",
      "Skipping imported method: __reduce__\n",
      "Skipping imported method: _get_retry_info\n",
      "Skipping imported method: __add__\n",
      "Skipping imported method: __and__\n",
      "Skipping imported method: __bool__\n",
      "Skipping imported method: __contains__\n",
      "Skipping imported method: __div__\n",
      "Skipping imported method: __eq__\n",
      "Skipping imported method: __ge__\n",
      "Skipping imported method: __getattr__\n",
      "Skipping imported method: __getitem__\n",
      "Skipping imported method: __gt__\n",
      "Skipping imported method: __init__\n",
      "Skipping imported method: __invert__\n",
      "Skipping imported method: __iter__\n",
      "Skipping imported method: __le__\n",
      "Skipping imported method: __lt__\n",
      "Skipping imported method: __mod__\n",
      "Skipping imported method: __mul__\n",
      "Skipping imported method: __ne__\n",
      "Skipping imported method: __neg__\n",
      "Skipping imported method: __nonzero__\n",
      "Skipping imported method: __or__\n",
      "Skipping imported method: __pow__\n",
      "Skipping imported method: __radd__\n",
      "Skipping imported method: __rand__\n",
      "Skipping imported method: __rdiv__\n",
      "Skipping imported method: __repr__\n",
      "Skipping imported method: __rmod__\n",
      "Skipping imported method: __rmul__\n",
      "Skipping imported method: __ror__\n",
      "Skipping imported method: __rpow__\n",
      "Skipping imported method: __rsub__\n",
      "Skipping imported method: __rtruediv__\n",
      "Skipping imported method: __sub__\n",
      "Skipping imported method: __truediv__\n",
      "Skipping imported method: alias\n",
      "Skipping imported method: asc\n",
      "Skipping imported method: asc_nulls_first\n",
      "Skipping imported method: asc_nulls_last\n",
      "Skipping imported method: astype\n",
      "Skipping imported method: between\n",
      "Skipping imported method: bitwiseAND\n",
      "Skipping imported method: bitwiseOR\n",
      "Skipping imported method: bitwiseXOR\n",
      "Skipping imported method: cast\n",
      "Skipping imported method: contains\n",
      "Skipping imported method: desc\n",
      "Skipping imported method: desc_nulls_first\n",
      "Skipping imported method: desc_nulls_last\n",
      "Skipping imported method: dropFields\n",
      "Skipping imported method: endswith\n",
      "Skipping imported method: eqNullSafe\n",
      "Skipping imported method: getField\n",
      "Skipping imported method: getItem\n",
      "Skipping imported method: ilike\n",
      "Skipping imported method: isNotNull\n",
      "Skipping imported method: isNull\n",
      "Skipping imported method: isin\n",
      "Skipping imported method: like\n",
      "Skipping imported method: name\n",
      "Skipping imported method: otherwise\n",
      "Skipping imported method: over\n",
      "Skipping imported method: rlike\n",
      "Skipping imported method: startswith\n",
      "Skipping imported method: substr\n",
      "Skipping imported method: when\n",
      "Skipping imported method: withField\n",
      "Method name: __init__\n",
      "Method definition:     def __init__(\n",
      "        self,\n",
      "        source_df: DataFrame,\n",
      "        spark_context: SparkSession.builder.getOrCreate,\n",
      "        config_path: str,\n",
      "        file_name: str,\n",
      "        src_system: str,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        A class checking the quality of a source data frame based on the given criteria.\n",
      "        :param source_df (DataFrame): the source data frame to apply the data quality checks on.\n",
      "        :param spark_context (SparkSession.builder.getOrCreate): the spark session to run the data quality check.\n",
      "        :param config_path (str): the path to config csv file.\n",
      "        :param file_name (str): the name of the file to check the data quality check on\n",
      "        :param src_system (str): the source of the file where it comes from (vendor)\n",
      "        :raise KeyError: raise a key error if the columns in the source data frame is not in the config file.\n",
      "        \"\"\"\n",
      "        # Set variables\n",
      "        self.spark = spark_context\n",
      "        self.source_df = source_df\n",
      "\n",
      "        self.error_df = None\n",
      "        self.error_columns = []\n",
      "        self.error_counter = 0\n",
      "        self.schema_dict = {\n",
      "            \"StringType\": StringType,\n",
      "            \"DateType\": DateType,\n",
      "            \"IntegerType\": IntegerType,\n",
      "            \"FloatType\": FloatType,\n",
      "            \"DoubleType\": DoubleType,\n",
      "        }\n",
      "\n",
      "        # Initial configuration\n",
      "        config_content = self.read_s3_file(config_path).decode()\n",
      "        self.config = self.resolve_config(config_path.replace(\"config.json\", \"env.json\"), json.loads(config_content))\n",
      "\n",
      "        dq_rule_path = self.config[src_system][\"dq_rule_path\"]\n",
      "        # dq_rule_content = self.read_s3_file(dq_rule_path)\n",
      "        self.rule_df = pd.read_csv(dq_rule_path, index_col=\"column_name\")\n",
      "        self.file_name = file_name\n",
      "        self.rule_df = self.rule_df[(self.rule_df[\"file_name\"] == self.file_name)]\n",
      "        self.rule_df = self.rule_df.applymap(lambda x: x if x else np.nan)\n",
      "        self.rule_df.sort_index(inplace=True)\n",
      "        self.sns_message = []\n",
      "\n",
      "        self.input_columns = self.source_df.columns\n",
      "        self.output_columns = self.config[src_system][\"sources\"][self.file_name][\"dq_output_columns\"]\n",
      "        for index in range(len(self.input_columns)):\n",
      "            if \".\" in self.input_columns[index]:\n",
      "                self.input_columns[index] = \"`\" + self.input_columns[index] + \"`\"\n",
      "\n",
      "        missed_columns = set(self.input_columns) - set(self.rule_df.index)\n",
      "        if len(missed_columns) > 0:\n",
      "            logger.warning(f\"[{missed_columns}] are not found in the rule file.\")\n",
      "\n",
      "        self.spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
      "        # self.spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\n",
      "        self.spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
      "        self.spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
      "\n",
      "------------\n",
      "Method name: add_error_col\n",
      "Method definition:     def add_error_col(self, error_msg: str, condition: Column, error_col_name: str) -> None:\n",
      "        \"\"\"\n",
      "        Add an error column based on the condition to filter and the error message\n",
      "        :param error_msg: the error message to be added if the condition is met\n",
      "        :param condition: the condition of the error to be met in order to return the error column\n",
      "        :param error_col_name: the name of the error column\n",
      "        \"\"\"\n",
      "        if condition is not None and error_col_name and error_msg:\n",
      "            col_condition = f.when(condition, f.lit(error_msg)).otherwise(f.lit(None))\n",
      "            error_col_name = error_col_name + str(self.error_counter)\n",
      "            self.source_df = self.source_df.withColumn(error_col_name, col_condition)\n",
      "            self.error_columns.append(f.col(error_col_name))\n",
      "            self.error_counter += 1\n",
      "\n",
      "------------\n",
      "Method name: category_check\n",
      "Method definition:     def category_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        Method checks input_col with a category and add an error column to self.source_df.\n",
      "        :param input_col: the column to check.\n",
      "        \"\"\"\n",
      "        print(\"start category check\")\n",
      "        valuelist_type = self.rule_df.loc[input_col, \"reference_valuelist\"].upper()\n",
      "        if valuelist_type[0:2] == \"__\":\n",
      "            category_cond, category_error_msg = self.file_check(input_col)\n",
      "        else:\n",
      "            category_list = valuelist_type.split(\",\")\n",
      "            self.source_df = self.source_df.withColumn(input_col, f.trim(f.upper(f.col(input_col))))\n",
      "            category_cond = (f.col(input_col).isin(category_list) == False) & (\n",
      "                (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\n",
      "            )\n",
      "            category_error_msg = f\"category_FAIL: Column [{input_col}] accepted values are ({category_list}])\"\n",
      "        self.add_error_col(\n",
      "            error_msg=category_error_msg, condition=category_cond, error_col_name=input_col + \" category_check\"\n",
      "        )\n",
      "        logger.info(f\"[{input_col}] category check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: columns_to_check\n",
      "Method definition:     def columns_to_check(self, criteria: str) -> Index:\n",
      "        \"\"\"\n",
      "        Returns the indexes to be used while working on the check rules.\n",
      "        :param criteria: whether it is data type, nullable, etc.\n",
      "        :return Index: the index of the columns that this condition should be met.\n",
      "        \"\"\"\n",
      "        return self.rule_df[(self.rule_df[criteria]).notna()].index\n",
      "\n",
      "------------\n",
      "Method name: conditional_check\n",
      "Method definition:     def conditional_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        Checks all the conditional columns in a single row in a config csv file.\n",
      "        :param input_col: The column to apply the check on.\n",
      "        \"\"\"\n",
      "        multiple_conditional_list = self.rule_df.loc[input_col, \"conditional_columns\"].split(\";\")\n",
      "        for cond_ind, condition_columns in enumerate(multiple_conditional_list):\n",
      "            current_additional_cond_value = self.rule_df.loc[input_col, \"conditional_column_value\"].split(\";\")[cond_ind]\n",
      "            current_conditional_valuelist = self.rule_df.loc[input_col, \"conditional_valuelist\"].split(\";\")[cond_ind]\n",
      "\n",
      "            first_col_cond, first_col_msg = self.conditional_cond_syntax(\n",
      "                input_col=input_col,\n",
      "                condition_column=condition_columns,\n",
      "                conditional_variables=current_conditional_valuelist,\n",
      "            )\n",
      "            for condition_column in condition_columns.split(\",\"):\n",
      "                if condition_column in self.input_columns:\n",
      "                    second_cal_cond, second_col_msg = self.conditional_cond_syntax(\n",
      "                        input_col=condition_column,\n",
      "                        condition_column=input_col,\n",
      "                        conditional_variables=current_additional_cond_value,\n",
      "                    )\n",
      "                    conditional_cond = first_col_cond & (~second_cal_cond)\n",
      "                    conditional_error_msg = f\"Cond_fail: {first_col_msg}+{second_col_msg}\"\n",
      "                    self.add_error_col(\n",
      "                        error_msg=conditional_error_msg,\n",
      "                        condition=conditional_cond,\n",
      "                        error_col_name=input_col + \" conditional_check\",\n",
      "                    )\n",
      "                else:\n",
      "                    self.sns_message.append(\n",
      "                        f\"Column {condition_column} is not in report {self.file_name} while it is needed for conditional check\"\n",
      "                    )\n",
      "        logger.info(f\"[{input_col}] conditional check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: conditional_cond_syntax\n",
      "Method definition:     def conditional_cond_syntax(self, input_col: str, condition_column: str, conditional_variables: str) -> Column:\n",
      "        \"\"\"\n",
      "        Generates a Column condition given input column and the condition column.\n",
      "        :param input_col: The column in which the current syntax is applied on\n",
      "        :param condition_column: the second column which might be used in this conditional check.\n",
      "        :param conditional_variables: The variables of the check. It could be __NOT__NULL__, a string of comma separated variables or a single number.\n",
      "        :return Column: The conditional column condition.\n",
      "        \"\"\"\n",
      "        not_category = []\n",
      "        if conditional_variables == \"__NOT__NULL__\":\n",
      "            category_cond = ~self.null_cond_syntax(input_col)\n",
      "            conditional_msg = f\"{input_col} is not null,\"\n",
      "            return category_cond, conditional_msg\n",
      "        elif self.is_float(conditional_variables):\n",
      "            category_cond = self.sum_check_syntax(\n",
      "                input_col + \" schema\", condition_column + \" schema\", conditional_variables\n",
      "            )\n",
      "            conditional_msg = f\"[{input_col}] and [{condition_column}] sum is not equal to {conditional_variables}\"\n",
      "            return category_cond, conditional_msg\n",
      "        else:\n",
      "            category_list = conditional_variables.split(\",\")\n",
      "            for ind, value in enumerate(category_list):\n",
      "                if \"__NOT__\" == value[: len(\"__NOT__\")]:\n",
      "                    not_category.append(category_list.pop(ind)[len(\"__NOT__\") :])\n",
      "\n",
      "            category_cond = f.col(input_col).isin(category_list) == True\n",
      "            if not_category:\n",
      "                category_cond = category_cond & (f.col(input_col).isin(not_category) == False)\n",
      "            conditional_msg = f\"[{input_col}] is in category ({category_list}),)\"\n",
      "            return category_cond, conditional_msg\n",
      "\n",
      "------------\n",
      "Method name: data_type_check\n",
      "Method definition:     def data_type_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        Checks the data type of all columns and give an error column for each column\n",
      "        :param input_col: the column to apply this check.\n",
      "        \"\"\"\n",
      "        print(\"start data type check\")\n",
      "        dtype_key = self.rule_df.loc[input_col, \"type\"]\n",
      "        if dtype_key == \"DateType\":\n",
      "            date_format = self.rule_df.loc[input_col, \"date_format\"]\n",
      "            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.to_date(f.col(input_col), date_format))\n",
      "            # self.source_df.select(input_col + ' schema').cache()\n",
      "        else:\n",
      "            dtype = self.schema_dict[dtype_key]()\n",
      "            self.source_df = self.source_df.withColumn(input_col + \" schema\", f.col(input_col).cast(dtype))\n",
      "            # self.source_df.select(input_col + ' schema').cache()\n",
      "        # dtype_cond should check if the given input_col is not null and the one with schema is null.\n",
      "        dtype_cond = ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")) & (\n",
      "            f.col(input_col + \" schema\").isNull()\n",
      "        )\n",
      "        type_error_msg = f\"data_type_FAIL: Column [{input_col}] should be {dtype_key}\"\n",
      "        self.add_error_col(error_msg=type_error_msg, condition=dtype_cond, error_col_name=input_col + \" type_check\")\n",
      "        logger.info(f\"[{input_col}] dtype check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: duplicate_check\n",
      "Method definition:     def duplicate_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        This method checks input_col with should be unique and add an error column to self.source_df.\n",
      "        :param input_col: the column to check.\n",
      "        \"\"\"\n",
      "        print(\"start duplicate_check\")\n",
      "        schema_col = input_col + \" schema\"\n",
      "        duplicate_cond = (self.duplicate_cond_syntax(schema_col)) & (\n",
      "            (f.col(input_col).isNotNull()) | (f.col(input_col) != \"\")\n",
      "        )\n",
      "        duplicate_error_msg = f\"unique_FAIL: Column [{input_col}] is not unique.])\"\n",
      "        self.add_error_col(\n",
      "            error_msg=duplicate_error_msg, condition=duplicate_cond, error_col_name=input_col + \" duplicate_check\"\n",
      "        )\n",
      "        self.source_df = self.source_df.drop(f.col(\"Duplicate_indicator\"))\n",
      "        logger.info(f\"[{input_col}] duplicate check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: duplicate_cond_syntax\n",
      "Method definition:     def duplicate_cond_syntax(self, input_col: str) -> Column:\n",
      "        \"\"\"\n",
      "        The duplicate check column condition\n",
      "        :param input_col: The column to apply the check on.\n",
      "        :raise Column: the duplicate check column condition.\n",
      "        \"\"\"\n",
      "        self.source_df = self.source_df.join(\n",
      "            broadcast(self.source_df.groupBy(input_col).agg((f.count(\"*\")).alias(\"Duplicate_indicator\"))),\n",
      "            on=input_col,\n",
      "            how=\"inner\",\n",
      "        )\n",
      "        return f.col(\"Duplicate_indicator\") > 1\n",
      "\n",
      "------------\n",
      "Method name: file_check\n",
      "Method definition:     def file_check(self, input_col):\n",
      "        # finding source side columns\n",
      "        source_df_columns_list = [input_col]\n",
      "        reference_columns_str = self.rule_df.loc[input_col, \"reference_columns\"]\n",
      "        if type(reference_columns_str) == str:\n",
      "            additional_columns_list = reference_columns_str.replace(\", \", \",\").replace(\" ,\", \",\").split(\",\")\n",
      "            source_df_columns_list.extend(additional_columns_list)\n",
      "\n",
      "        # Find reference side columns\n",
      "        file_check_type = self.rule_df.loc[input_col, \"reference_valuelist\"]\n",
      "        print(\"file check type\", file_check_type)\n",
      "        current_file_config = self.config[\"dq_referential_files\"][file_check_type]\n",
      "        reference_columns = current_file_config[\"reference_columns\"]\n",
      "        print(\"ref cols\", reference_columns)\n",
      "\n",
      "        # Read reference file\n",
      "        file_path = current_file_config[\"path\"]\n",
      "        file_df = self.spark.read.option(\"header\", \"true\").csv(file_path)\n",
      "        # changing columns names on reference to be the same as source\n",
      "        print(\"src df cols list\", source_df_columns_list)\n",
      "        for col_ind, ref_col in enumerate(reference_columns):\n",
      "            source_col = source_df_columns_list[col_ind]\n",
      "            try:\n",
      "                file_df = file_df.withColumnRenamed(ref_col, source_col)\n",
      "                file_df = file_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\n",
      "                file_df = file_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\n",
      "                self.source_df = self.source_df.withColumn(source_col, f.trim(f.upper(f.col(source_col))))\n",
      "                self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \"-\", \" \"))\n",
      "                if \"Postal Code\".upper() in source_col.upper():\n",
      "                    self.source_df = self.source_df.withColumn(source_col, f.regexp_replace(source_col, \" \", \"\"))\n",
      "            except AnalysisException:\n",
      "                self.sns_message.append(f\"Column {ref_col} is not found in the RDW file needed for DQ check\")\n",
      "                return None, None\n",
      "\n",
      "        pre_join_columns = set(self.source_df.columns)\n",
      "        self.source_df = self.source_df.join(file_df, source_df_columns_list, how=\"left\")\n",
      "        post_join_columns = set(self.source_df.columns)\n",
      "        join_col = tuple(post_join_columns - pre_join_columns)[0]\n",
      "        file_cond = (f.col(join_col).isNull()) & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\n",
      "        file_error_msg = (\n",
      "            f\"{file_check_type}_FAIL: Column [{source_df_columns_list}] did not pass the {file_check_type}.\"\n",
      "        )\n",
      "        return file_cond, file_error_msg\n",
      "\n",
      "------------\n",
      "Method name: is_float\n",
      "Method definition:     @staticmethod\n",
      "    def is_float(element) -> bool:\n",
      "        \"\"\"\n",
      "        Check if the given input can be returned as float or not.\n",
      "        :param element: The input which can be anything (_type_).\n",
      "        :return bool: whether it is float (True) or not (False).\n",
      "        \"\"\"\n",
      "        try:\n",
      "            float(element)\n",
      "            return True\n",
      "        except ValueError:\n",
      "            return False\n",
      "\n",
      "------------\n",
      "Method name: limit_finder\n",
      "Method definition:     def limit_finder(self, input_col: str, rule_value: Union[str, int, float]) -> Union[float, Column, None]:\n",
      "        \"\"\"\n",
      "        Finds the limit based on the given column. If it is a number it returns the number or if it is a column it returns the f.col.\n",
      "        :param input_col: the column to check this condition on\n",
      "        :param rule_value: value of the limit no matter the datatype\n",
      "        :return Union[str, Column, None]: whether a float or a column in order to check for range check.\n",
      "        :raise KeyError: if the input_col needs other columns that is not in the dataset it will raise an error.\n",
      "        \"\"\"\n",
      "        if self.is_float(rule_value):\n",
      "            rule_value = float(rule_value)\n",
      "            if math.isnan(rule_value):\n",
      "                return None\n",
      "            else:\n",
      "                return rule_value\n",
      "        elif type(rule_value) == str:\n",
      "            if rule_value not in self.input_columns:\n",
      "                print(rule_value)\n",
      "                self.sns_message.append(\n",
      "                    f\"column {rule_value} is not in report {self.file_name} while it is {input_col} needed for range check\"\n",
      "                )\n",
      "                return None\n",
      "            return f.col(rule_value)\n",
      "\n",
      "------------\n",
      "Method name: main_pipeline\n",
      "Method definition:     def main_pipeline(self) -> DataFrame:\n",
      "        \"\"\"\n",
      "        The main pipeline to do all the checks on the given data frame.\n",
      "        :return DataFrame: The consolidated error dataframe.\n",
      "        \"\"\"\n",
      "\n",
      "        columns_to_check_dict = {}\n",
      "        columns_to_check_dict[self.data_type_check] = self.columns_to_check(\"type\")\n",
      "        columns_to_check_dict[self.null_check] = self.rule_df[(self.rule_df[\"nullable\"]).isna()].index\n",
      "        columns_to_check_dict[self.duplicate_check] = self.columns_to_check(\"unique\")\n",
      "        columns_to_check_dict[self.category_check] = self.columns_to_check(\"reference_valuelist\")\n",
      "        columns_to_check_dict[self.range_check] = list(\n",
      "            set(self.columns_to_check(\"min\")) | set(self.columns_to_check(\"max\"))\n",
      "        )\n",
      "\n",
      "        for index in range(len(self.input_columns)):\n",
      "            print(\"inpul col\", self.input_columns[index])\n",
      "            for check_type in columns_to_check_dict.keys():\n",
      "                if self.input_columns[index] in columns_to_check_dict[check_type]:\n",
      "                    check_type(self.input_columns[index])\n",
      "\n",
      "        # conditional column in a different loop since they rely on schema of the multiple columns.\n",
      "        columns_to_check_dict[self.conditional_check] = self.columns_to_check(\"conditional_columns\")\n",
      "        for conditional_col in columns_to_check_dict[self.conditional_check]:\n",
      "            if conditional_col in self.input_columns:\n",
      "                self.conditional_check(conditional_col)\n",
      "            else:\n",
      "                self.sns_message.append(\n",
      "                    f\"column {conditional_col} is not in report {self.file_name} while it is needed for conditional check\"\n",
      "                )\n",
      "\n",
      "        # combining all error columns to one column.\n",
      "        self.source_df = (\n",
      "            self.source_df.withColumn(\"temp\", f.array(self.error_columns))\n",
      "            .withColumn(\"final\", f.expr(\"FILTER(temp, x -> x is not null)\"))\n",
      "            .drop(\"temp\")\n",
      "        )\n",
      "\n",
      "        # exploding the error column into multiple rows.\n",
      "        print(\"exploding error df\")\n",
      "        self.error_df = self.source_df.select(self.output_columns[0], f.explode(\"final\").alias(\"Error\")).filter(\n",
      "            f.col(\"Error\").isNotNull()\n",
      "        )\n",
      "\n",
      "        if self.error_df and self.error_df.rdd.isEmpty():\n",
      "            self.error_df = None\n",
      "        return self.error_df, self.sns_message\n",
      "\n",
      "------------\n",
      "Method name: null_check\n",
      "Method definition:     def null_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        Checks not nullable columns and give an error column for input_col\n",
      "        :param input_col: The column to apply the null check.\n",
      "        \"\"\"\n",
      "        print(\"start null_check\")\n",
      "        if not math.isnan(self.rule_df.loc[input_col, \"nullable\"]):\n",
      "            return\n",
      "        null_condition = self.null_cond_syntax(input_col)\n",
      "        null_error_msg = f\"null_FAIL: Column [{input_col}] cannot be null\"\n",
      "        self.add_error_col(error_msg=null_error_msg, condition=null_condition, error_col_name=input_col + \" null_check\")\n",
      "        logger.info(f\"[{input_col}] null check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: null_cond_syntax\n",
      "Method definition:     def null_cond_syntax(self, input_col: str) -> Column:\n",
      "        \"\"\"\n",
      "        The condition for a null check.\n",
      "        :param input_col: the column to apply the check on.\n",
      "        :raise Column: The not null condition column.\n",
      "        \"\"\"\n",
      "        return (f.col(input_col) == \"\") | (f.col(input_col).isNull())\n",
      "\n",
      "------------\n",
      "Method name: range_check\n",
      "Method definition:     def range_check(self, input_col: str) -> None:\n",
      "        \"\"\"\n",
      "        This method checks input_col with a range check and add an error column to self.source_df.\n",
      "        Args:\n",
      "            input_col (str): the column to check.\n",
      "        \"\"\"\n",
      "        print(\"start range_check\")\n",
      "        min_str, max_str, range_error_cond = self.range_cond_syntax(input_col)\n",
      "        range_error_cond = range_error_cond & ((f.col(input_col).isNotNull()) | (f.col(input_col) != \"\"))\n",
      "        range_error_msg = f\"range_FAIL: Column [{input_col}] must be in range ([{min_str}]<{input_col}<[{max_str}])\"\n",
      "        self.add_error_col(\n",
      "            error_msg=range_error_msg, condition=range_error_cond, error_col_name=input_col + \" range_check\"\n",
      "        )\n",
      "        logger.info(f\"[{input_col}] range check is done.\")\n",
      "\n",
      "------------\n",
      "Method name: range_cond_syntax\n",
      "Method definition:     def range_cond_syntax(self, input_col: str) -> Column:\n",
      "        \"\"\"\n",
      "        The range check column condition\n",
      "        :param input_col: The column to apply the check on\n",
      "        :return Column: The range check column condition.\n",
      "        \"\"\"\n",
      "        schema_col = input_col + \" schema\"\n",
      "        output_cond = None\n",
      "\n",
      "        min_str = self.rule_df.loc[input_col, \"min\"]\n",
      "        min_value = self.limit_finder(input_col, min_str)\n",
      "        if min_value is not None:\n",
      "            output_cond = output_cond | (f.col(schema_col) < min_value)\n",
      "        max_str = self.rule_df.loc[input_col, \"max\"]\n",
      "        max_value = self.limit_finder(input_col, max_str)\n",
      "        if max_value is not None:\n",
      "            output_cond = output_cond | (f.col(schema_col) > max_value)\n",
      "        return min_str, max_str, output_cond\n",
      "\n",
      "------------\n",
      "Method name: read_s3_file\n",
      "Method definition:     def read_s3_file(self, file_path) -> bytes:\n",
      "        \"\"\"\n",
      "        Read s3 file content and return it in byte format\n",
      "        :param file_path: full s3 object path\n",
      "        :return byte content of the file\n",
      "        \"\"\"\n",
      "        file_res = urlparse(file_path)\n",
      "        try:\n",
      "            file_obj = s3_resource.Object(file_res.netloc, file_res.path.lstrip(\"/\"))\n",
      "            return file_obj.get()[\"Body\"].read()\n",
      "        except ClientError:\n",
      "            raise FileNotFoundError(f\"File cannot be found in S3 given path '{file_path}'\")\n",
      "\n",
      "------------\n",
      "Method name: resolve_config\n",
      "Method definition:     def resolve_config(self, env_path, config_content):\n",
      "        \"\"\"\n",
      "        Read config content and resolve env variables\n",
      "        :param env_path: environment file path\n",
      "        :param config_content: environment agnostic config file\n",
      "        :return environmentally resolved config\n",
      "        \"\"\"\n",
      "        env_content = self.read_s3_file(env_path).decode()\n",
      "        env_sub = json.loads(env_content)[\"subs\"]\n",
      "\n",
      "        config_content_str = (\n",
      "            str(config_content)\n",
      "            .replace(\"<env>\", env_sub[\"<env>\"])\n",
      "            .replace(\"<_env>\", env_sub[\"<_env>\"])\n",
      "            .replace(\"<bucket_name>\", env_sub[\"<bucket_name>\"])\n",
      "            .replace(\"<account>\", env_sub[\"<account>\"])\n",
      "        )\n",
      "\n",
      "        return ast.literal_eval(config_content_str)\n",
      "\n",
      "------------\n",
      "Method name: sum_check_syntax\n",
      "Method definition:     def sum_check_syntax(self, input_col1: str, input_col2: str, syntax_value: float) -> Column:\n",
      "        \"\"\"\n",
      "        The syntax to check for sum of 2 columns to equal a syntax value, to be used in conditional checks.\n",
      "        :param input_col1: column 1 to check\n",
      "        :param input_col2: column 2 to check\n",
      "        :param syntax_value: column value that column 1 and 2 should equal to.\n",
      "        :return Column: The sum_check Column condition.\n",
      "        \"\"\"\n",
      "        return ~(f.col(input_col1) + f.col(input_col2) != syntax_value)\n",
      "\n",
      "------------\n",
      "Skipping imported method: __getattr__\n",
      "Skipping imported method: __getitem__\n",
      "Skipping imported method: __init__\n",
      "Skipping imported method: __repr__\n",
      "Skipping imported method: _collect_as_arrow\n",
      "Skipping imported method: _jcols\n",
      "Skipping imported method: _jmap\n",
      "Skipping imported method: _joinAsOf\n",
      "Skipping imported method: _jseq\n",
      "Skipping imported method: _repr_html_\n",
      "Skipping imported method: _sort_cols\n",
      "Skipping imported method: _to_corrected_pandas_type\n",
      "Skipping imported method: agg\n",
      "Skipping imported method: alias\n",
      "Skipping imported method: approxQuantile\n",
      "Skipping imported method: cache\n",
      "Skipping imported method: checkpoint\n",
      "Skipping imported method: coalesce\n",
      "Skipping imported method: colRegex\n",
      "Skipping imported method: collect\n",
      "Skipping imported method: corr\n",
      "Skipping imported method: count\n",
      "Skipping imported method: cov\n",
      "Skipping imported method: createGlobalTempView\n",
      "Skipping imported method: createOrReplaceGlobalTempView\n",
      "Skipping imported method: createOrReplaceTempView\n",
      "Skipping imported method: createTempView\n",
      "Skipping imported method: crossJoin\n",
      "Skipping imported method: crosstab\n",
      "Skipping imported method: cube\n",
      "Skipping imported method: describe\n",
      "Skipping imported method: distinct\n",
      "Skipping imported method: drop\n",
      "Skipping imported method: dropDuplicates\n",
      "Skipping imported method: drop_duplicates\n",
      "Skipping imported method: dropna\n",
      "Skipping imported method: exceptAll\n",
      "Skipping imported method: explain\n",
      "Skipping imported method: fillna\n",
      "Skipping imported method: filter\n",
      "Skipping imported method: first\n",
      "Skipping imported method: foreach\n",
      "Skipping imported method: foreachPartition\n",
      "Skipping imported method: freqItems\n",
      "Skipping imported method: groupBy\n",
      "Skipping imported method: groupby\n",
      "Skipping imported method: head\n",
      "Skipping imported method: hint\n",
      "Skipping imported method: inputFiles\n",
      "Skipping imported method: intersect\n",
      "Skipping imported method: intersectAll\n",
      "Skipping imported method: isEmpty\n",
      "Skipping imported method: isLocal\n",
      "Skipping imported method: join\n",
      "Skipping imported method: limit\n",
      "Skipping imported method: localCheckpoint\n",
      "Skipping imported method: mapInArrow\n",
      "Skipping imported method: mapInPandas\n",
      "Skipping imported method: observe\n",
      "Skipping imported method: orderBy\n",
      "Skipping imported method: pandas_api\n",
      "Skipping imported method: persist\n",
      "Skipping imported method: printSchema\n",
      "Skipping imported method: randomSplit\n",
      "Skipping imported method: registerTempTable\n",
      "Skipping imported method: repartition\n",
      "Skipping imported method: repartitionByRange\n",
      "Skipping imported method: replace\n",
      "Skipping imported method: rollup\n",
      "Skipping imported method: sameSemantics\n",
      "Skipping imported method: sample\n",
      "Skipping imported method: sampleBy\n",
      "Skipping imported method: select\n",
      "Skipping imported method: selectExpr\n",
      "Skipping imported method: semanticHash\n",
      "Skipping imported method: show\n",
      "Skipping imported method: sort\n",
      "Skipping imported method: sortWithinPartitions\n",
      "Skipping imported method: subtract\n",
      "Skipping imported method: summary\n",
      "Skipping imported method: tail\n",
      "Skipping imported method: take\n",
      "Skipping imported method: toDF\n",
      "Skipping imported method: toJSON\n",
      "Skipping imported method: toLocalIterator\n",
      "Skipping imported method: toPandas\n",
      "Skipping imported method: to_koalas\n",
      "Skipping imported method: to_pandas_on_spark\n",
      "Skipping imported method: transform\n",
      "Skipping imported method: union\n",
      "Skipping imported method: unionAll\n",
      "Skipping imported method: unionByName\n",
      "Skipping imported method: unpersist\n",
      "Skipping imported method: where\n",
      "Skipping imported method: withColumn\n",
      "Skipping imported method: withColumnRenamed\n",
      "Skipping imported method: withColumns\n",
      "Skipping imported method: withMetadata\n",
      "Skipping imported method: withWatermark\n",
      "Skipping imported method: writeTo\n",
      "Skipping imported method: __eq__\n",
      "Skipping imported method: __hash__\n",
      "Skipping imported method: __ne__\n",
      "Skipping imported method: __repr__\n",
      "Skipping imported method: fromInternal\n",
      "Skipping imported method: json\n",
      "Skipping imported method: jsonValue\n",
      "Skipping imported method: needConversion\n",
      "Skipping imported method: simpleString\n",
      "Skipping imported method: toInternal\n",
      "Skipping imported method: __eq__\n",
      "Skipping imported method: __hash__\n",
      "Skipping imported method: __ne__\n",
      "Skipping imported method: __repr__\n",
      "Skipping imported method: fromInternal\n",
      "Skipping imported method: json\n",
      "Skipping imported method: jsonValue\n",
      "Skipping imported method: needConversion\n",
      "Skipping imported method: simpleString\n",
      "Skipping imported method: toInternal\n",
      "Skipping imported method: __eq__\n",
      "Skipping imported method: __hash__\n",
      "Skipping imported method: __ne__\n",
      "Skipping imported method: __repr__\n",
      "Skipping imported method: fromInternal\n",
      "Skipping imported method: json\n",
      "Skipping imported method: jsonValue\n",
      "Skipping imported method: needConversion\n",
      "Skipping imported method: simpleString\n",
      "Skipping imported method: toInternal\n",
      "Skipping imported method: __abs__\n",
      "Skipping imported method: __add__\n",
      "Skipping imported method: __and__\n",
      "Skipping imported method: __array__\n",
      "Skipping imported method: __array_ufunc__\n",
      "Skipping imported method: __array_wrap__\n",
      "Skipping imported method: __bool__\n",
      "Skipping imported method: __contains__\n",
      "Skipping imported method: __copy__\n",
      "Skipping imported method: __deepcopy__\n",
      "Skipping imported method: __dir__\n",
      "Skipping imported method: __divmod__\n",
      "Skipping imported method: __eq__\n",
      "Skipping imported method: __floordiv__\n",
      "Skipping imported method: __ge__\n",
      "Skipping imported method: __getitem__\n",
      "Skipping imported method: __gt__\n",
      "Skipping imported method: __iadd__\n",
      "Skipping imported method: __invert__\n",
      "Skipping imported method: __iter__\n",
      "Skipping imported method: __le__\n",
      "Skipping imported method: __len__\n",
      "Skipping imported method: __lt__\n",
      "Skipping imported method: __mod__\n",
      "Skipping imported method: __mul__\n",
      "Skipping imported method: __ne__\n",
      "Skipping imported method: __neg__\n",
      "Skipping imported method: __new__\n",
      "Skipping imported method: __nonzero__\n",
      "Skipping imported method: __or__\n",
      "Skipping imported method: __pos__\n",
      "Skipping imported method: __pow__\n",
      "Skipping imported method: __radd__\n",
      "Skipping imported method: __rand__\n",
      "Skipping imported method: __rdivmod__\n",
      "Skipping imported method: __reduce__\n",
      "Skipping imported method: __repr__\n",
      "Skipping imported method: __rfloordiv__\n",
      "Skipping imported method: __rmod__\n",
      "Skipping imported method: __rmul__\n",
      "Skipping imported method: __ror__\n",
      "Skipping imported method: __rpow__\n",
      "Skipping imported method: __rsub__\n",
      "Skipping imported method: __rtruediv__\n",
      "Skipping imported method: __rxor__\n",
      "Skipping imported method: __setitem__\n",
      "Skipping imported method: __sizeof__\n",
      "Skipping imported method: __sub__\n",
      "Skipping imported method: __truediv__\n",
      "Skipping imported method: __xor__\n",
      "Skipping imported method: _arith_method\n",
      "Skipping imported method: _assert_can_do_setop\n",
      "Skipping imported method: _can_hold_identifiers_and_holds_name\n",
      "Skipping imported method: _check_indexing_error\n",
      "Skipping imported method: _check_indexing_method\n",
      "Skipping imported method: _cleanup\n",
      "Skipping imported method: _cmp_method\n",
      "Skipping imported method: _concat\n",
      "Skipping imported method: _construct_result\n",
      "Skipping imported method: _convert_can_do_setop\n",
      "Skipping imported method: _convert_slice_indexer\n",
      "Skipping imported method: _convert_tolerance\n",
      "Skipping imported method: _deprecate_dti_setop\n",
      "Skipping imported method: _deprecated_arg\n",
      "Skipping imported method: _difference\n",
      "Skipping imported method: _difference_compat\n",
      "Skipping imported method: _dir_additions\n",
      "Skipping imported method: _dir_deletions\n",
      "Skipping imported method: _drop_level_numbers\n",
      "Skipping imported method: _duplicated\n",
      "Skipping imported method: _filter_indexer_tolerance\n",
      "Skipping imported method: _find_common_type_compat\n",
      "Skipping imported method: _format_attrs\n",
      "Skipping imported method: _format_data\n",
      "Skipping imported method: _format_duplicate_message\n",
      "Skipping imported method: _format_native_types\n",
      "Skipping imported method: _format_space\n",
      "Skipping imported method: _format_with_header\n",
      "Skipping imported method: _from_join_target\n",
      "Skipping imported method: _get_attributes_dict\n",
      "Skipping imported method: _get_default_index_names\n",
      "Skipping imported method: _get_engine_target\n",
      "Skipping imported method: _get_fill_indexer\n",
      "Skipping imported method: _get_fill_indexer_searchsorted\n",
      "Skipping imported method: _get_grouper_for_level\n",
      "Skipping imported method: _get_indexer\n",
      "Skipping imported method: _get_indexer_non_comparable\n",
      "Skipping imported method: _get_indexer_strict\n",
      "Skipping imported method: _get_level_names\n",
      "Skipping imported method: _get_level_number\n",
      "Skipping imported method: _get_level_values\n",
      "Skipping imported method: _get_names\n",
      "Skipping imported method: _get_nearest_indexer\n",
      "Skipping imported method: _get_reconciled_name_object\n",
      "Skipping imported method: _get_string_slice\n",
      "Skipping imported method: _get_values_for_loc\n",
      "Skipping imported method: _getitem_slice\n",
      "Skipping imported method: _inner_indexer\n",
      "Skipping imported method: _intersection\n",
      "Skipping imported method: _intersection_via_get_indexer\n",
      "Skipping imported method: _invalid_indexer\n",
      "Skipping imported method: _is_comparable_dtype\n",
      "Skipping imported method: _is_memory_usage_qualified\n",
      "Skipping imported method: _join_level\n",
      "Skipping imported method: _join_monotonic\n",
      "Skipping imported method: _join_multi\n",
      "Skipping imported method: _join_non_unique\n",
      "Skipping imported method: _join_via_get_indexer\n",
      "Skipping imported method: _left_indexer\n",
      "Skipping imported method: _left_indexer_unique\n",
      "Skipping imported method: _logical_method\n",
      "Skipping imported method: _map_values\n",
      "Skipping imported method: _maybe_cast_indexer\n",
      "Skipping imported method: _maybe_cast_listlike_indexer\n",
      "Skipping imported method: _maybe_cast_slice_bound\n",
      "Skipping imported method: _maybe_check_unique\n",
      "Skipping imported method: _maybe_disable_logical_methods\n",
      "Skipping imported method: _maybe_disallow_fill\n",
      "Skipping imported method: _maybe_preserve_names\n",
      "Skipping imported method: _maybe_promote\n",
      "Skipping imported method: _memory_usage\n",
      "Skipping imported method: _mpl_repr\n",
      "Skipping imported method: _outer_indexer\n",
      "Skipping imported method: _raise_if_missing\n",
      "Skipping imported method: _reduce\n",
      "Skipping imported method: _reindex_non_unique\n",
      "Skipping imported method: _rename\n",
      "Skipping imported method: _require_scalar\n",
      "Skipping imported method: _reset_cache\n",
      "Skipping imported method: _reset_identity\n",
      "Skipping imported method: _searchsorted_monotonic\n",
      "Skipping imported method: _set_names\n",
      "Skipping imported method: _shallow_copy\n",
      "Skipping imported method: _should_compare\n",
      "Skipping imported method: _should_partial_index\n",
      "Skipping imported method: _sort_levels_monotonic\n",
      "Skipping imported method: _summary\n",
      "Skipping imported method: _transform_index\n",
      "Skipping imported method: _unary_method\n",
      "Skipping imported method: _union\n",
      "Skipping imported method: _validate_can_reindex\n",
      "Skipping imported method: _validate_fill_value\n",
      "Skipping imported method: _validate_index_level\n",
      "Skipping imported method: _validate_indexer\n",
      "Skipping imported method: _validate_names\n",
      "Skipping imported method: _validate_positional_slice\n",
      "Skipping imported method: _validate_sort_keyword\n",
      "Skipping imported method: _view\n",
      "Skipping imported method: _wrap_difference_result\n",
      "Skipping imported method: _wrap_intersection_result\n",
      "Skipping imported method: _wrap_joined_index\n",
      "Skipping imported method: _wrap_reindex_result\n",
      "Skipping imported method: _wrap_setop_result\n",
      "Skipping imported method: all\n",
      "Skipping imported method: any\n",
      "Skipping imported method: append\n",
      "Skipping imported method: argmax\n",
      "Skipping imported method: argmin\n",
      "Skipping imported method: argsort\n",
      "Skipping imported method: asof\n",
      "Skipping imported method: asof_locs\n",
      "Skipping imported method: astype\n",
      "Skipping imported method: copy\n",
      "Skipping imported method: delete\n",
      "Skipping imported method: difference\n",
      "Skipping imported method: drop\n",
      "Skipping imported method: drop_duplicates\n",
      "Skipping imported method: droplevel\n",
      "Skipping imported method: dropna\n",
      "Skipping imported method: duplicated\n",
      "Skipping imported method: equals\n",
      "Skipping imported method: factorize\n",
      "Skipping imported method: fillna\n",
      "Skipping imported method: format\n",
      "Skipping imported method: get_indexer\n",
      "Skipping imported method: get_indexer_for\n",
      "Skipping imported method: get_indexer_non_unique\n",
      "Skipping imported method: get_level_values\n",
      "Skipping imported method: get_loc\n",
      "Skipping imported method: get_slice_bound\n",
      "Skipping imported method: get_value\n",
      "Skipping imported method: groupby\n",
      "Skipping imported method: holds_integer\n",
      "Skipping imported method: identical\n",
      "Skipping imported method: insert\n",
      "Skipping imported method: intersection\n",
      "Skipping imported method: is_\n",
      "Skipping imported method: is_boolean\n",
      "Skipping imported method: is_categorical\n",
      "Skipping imported method: is_floating\n",
      "Skipping imported method: is_integer\n",
      "Skipping imported method: is_interval\n",
      "Skipping imported method: is_mixed\n",
      "Skipping imported method: is_numeric\n",
      "Skipping imported method: is_object\n",
      "Skipping imported method: is_type_compatible\n",
      "Skipping imported method: isin\n",
      "Skipping imported method: isna\n",
      "Skipping imported method: isnull\n",
      "Skipping imported method: item\n",
      "Skipping imported method: join\n",
      "Skipping imported method: map\n",
      "Skipping imported method: max\n",
      "Skipping imported method: memory_usage\n",
      "Skipping imported method: min\n",
      "Skipping imported method: notna\n",
      "Skipping imported method: notnull\n",
      "Skipping imported method: nunique\n",
      "Skipping imported method: putmask\n",
      "Skipping imported method: ravel\n",
      "Skipping imported method: reindex\n",
      "Skipping imported method: rename\n",
      "Skipping imported method: repeat\n",
      "Skipping imported method: searchsorted\n",
      "Skipping imported method: set_names\n",
      "Skipping imported method: set_value\n",
      "Skipping imported method: shift\n",
      "Skipping imported method: slice_indexer\n",
      "Skipping imported method: slice_locs\n",
      "Skipping imported method: sort\n",
      "Skipping imported method: sort_values\n",
      "Skipping imported method: sortlevel\n",
      "Skipping imported method: symmetric_difference\n",
      "Skipping imported method: take\n",
      "Skipping imported method: to_flat_index\n",
      "Skipping imported method: to_frame\n",
      "Skipping imported method: to_list\n",
      "Skipping imported method: to_native_types\n",
      "Skipping imported method: to_numpy\n",
      "Skipping imported method: to_series\n",
      "Skipping imported method: tolist\n",
      "Skipping imported method: transpose\n",
      "Skipping imported method: union\n",
      "Skipping imported method: unique\n",
      "Skipping imported method: value_counts\n",
      "Skipping imported method: view\n",
      "Skipping imported method: where\n",
      "Skipping imported method: __eq__\n",
      "Skipping imported method: __hash__\n",
      "Skipping imported method: __ne__\n",
      "Skipping imported method: __repr__\n",
      "Skipping imported method: fromInternal\n",
      "Skipping imported method: json\n",
      "Skipping imported method: jsonValue\n",
      "Skipping imported method: needConversion\n",
      "Skipping imported method: simpleString\n",
      "Skipping imported method: toInternal\n",
      "Skipping imported method: __enter__\n",
      "Skipping imported method: __exit__\n",
      "Skipping imported method: __init__\n",
      "Skipping imported method: _convert_from_pandas\n",
      "Skipping imported method: _createFromLocal\n",
      "Skipping imported method: _createFromRDD\n",
      "Skipping imported method: _create_dataframe\n",
      "Skipping imported method: _create_from_pandas_with_arrow\n",
      "Skipping imported method: _create_shell_session\n",
      "Skipping imported method: _getActiveSessionOrCreate\n",
      "Skipping imported method: _get_numpy_record_dtype\n",
      "Skipping imported method: _inferSchema\n",
      "Skipping imported method: _inferSchemaFromList\n",
      "Skipping imported method: _repr_html_\n",
      "Skipping imported method: createDataFrame\n",
      "Skipping imported method: newSession\n",
      "Skipping imported method: range\n",
      "Skipping imported method: sql\n",
      "Skipping imported method: stop\n",
      "Skipping imported method: table\n",
      "Skipping imported method: __eq__\n",
      "Skipping imported method: __hash__\n",
      "Skipping imported method: __ne__\n",
      "Skipping imported method: __repr__\n",
      "Skipping imported method: fromInternal\n",
      "Skipping imported method: json\n",
      "Skipping imported method: jsonValue\n",
      "Skipping imported method: needConversion\n",
      "Skipping imported method: simpleString\n",
      "Skipping imported method: toInternal\n",
      "main.py\n",
      "Function name: unit_test_from_function\n",
      "Function definition: def unit_test_from_function(\n",
      "    function_to_test: str,  # Python function to test, as a string\n",
      "    unit_test_package: str = \"pytest\",  # unit testing package; use the name as it appears in the import statement\n",
      "    approx_min_cases_to_cover: int = 7,  # minimum number of test case categories to cover (approximate)\n",
      "    print_text: bool = False,  # optionally prints text; helpful for understanding the function & debugging\n",
      "    engine: str = \"GPT4\",  # engine used to generate text plans in steps 1, 2, and 2b\n",
      "    max_tokens: int = 1000,  # can set this high, as generations should be stopped earlier by stop sequences\n",
      "    temperature: float = 0.4,  # temperature = 0 can sometimes get stuck in repetitive loops, so we use 0.4\n",
      "    reruns_if_fail: int = 1,  # if the output code cannot be parsed, this will re-run the function up to N times\n",
      ") -> str:\n",
      "    \"\"\"Outputs a unit test for a given Python function, using a 3-step GPT-3 prompt.\"\"\"\n",
      "\n",
      "    # Step 1: Generate an explanation of the function\n",
      "\n",
      "    # create a markdown-formatted prompt that asks GPT-3 to complete an explanation of the function, formatted as a bullet list\n",
      "    prompt_to_explain_the_function = f\"\"\"# How to write great unit tests with {unit_test_package}\n",
      "\n",
      "In this advanced tutorial for experts, we'll use Python 3.9 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function.\n",
      "```python\n",
      "{function_to_test}\n",
      "```\n",
      "\n",
      "Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
      "- First,\"\"\"\n",
      "    if print_text:\n",
      "        text_color_prefix = \"\\033[30m\"  # black; if you read against a dark background \\033[97m is white\n",
      "        print(\n",
      "            text_color_prefix + prompt_to_explain_the_function, end=\"\"\n",
      "        )  # end='' prevents a newline from being printed\n",
      "\n",
      "    # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n",
      "    explanation_response = openai.ChatCompletion.create(\n",
      "        engine=engine,\n",
      "        messages=[{\n",
      "            \"role\":\"system\",\n",
      "            \"content\": prompt_to_explain_the_function\n",
      "        }],\n",
      "        # stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n",
      "        # max_tokens=max_tokens,\n",
      "        # temperature=temperature,\n",
      "        # stream=True,\n",
      "    )\n",
      "    # explanation_completion = \"\"\n",
      "    if print_text:\n",
      "        completion_color_prefix = \"\\033[92m\"  # green\n",
      "        print(completion_color_prefix, end=\"\")\n",
      "    explanation_completion = explanation_response.choices[0].message.content\n",
      "    if print_text:\n",
      "        print(explanation_completion, end=\"\")\n",
      "\n",
      "    # Step 2: Generate a plan to write a unit test\n",
      "\n",
      "    # create a markdown-formatted prompt that asks GPT-3 to complete a plan for writing unit tests, formatted as a bullet list\n",
      "    prompt_to_explain_a_plan = f\"\"\"\n",
      "\n",
      "A good unit test suite should aim to:\n",
      "- Test the function's behavior for a wide range of possible inputs\n",
      "- Test edge cases that the author may not have foreseen\n",
      "- Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain\n",
      "- Be easy to read and understand, with clean code and descriptive names\n",
      "- Be deterministic, so that the tests always pass or fail in the same way\n",
      "\n",
      "`{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n",
      "\n",
      "For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n",
      "-\"\"\"\n",
      "    if print_text:\n",
      "        print(text_color_prefix + prompt_to_explain_a_plan, end=\"\")\n",
      "\n",
      "    # append this planning prompt to the results from step 1\n",
      "    prior_text = prompt_to_explain_the_function + explanation_completion\n",
      "    full_plan_prompt = prior_text + prompt_to_explain_a_plan\n",
      "\n",
      "    # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n",
      "    plan_response = openai.ChatCompletion.create(\n",
      "        engine=engine,\n",
      "        messages=[{\n",
      "            \"role\":\"system\",\n",
      "            \"content\": full_plan_prompt\n",
      "        }],\n",
      "        # stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n",
      "        # max_tokens=max_tokens,\n",
      "        # temperature=temperature,\n",
      "        # stream=True,\n",
      "    )\n",
      "    # plan_completion = \"\"\n",
      "    if print_text:\n",
      "        print(completion_color_prefix, end=\"\")\n",
      "    plan_completion = plan_response.choices[0].message.content\n",
      "    if print_text:\n",
      "        print(plan_completion, end=\"\")\n",
      "\n",
      "    # Step 2b: If the plan is short, ask GPT-3 to elaborate further\n",
      "    # this counts top-level bullets (e.g., categories), but not sub-bullets (e.g., test cases)\n",
      "    elaboration_needed = (\n",
      "        plan_completion.count(\"\\n-\") + 1 < approx_min_cases_to_cover\n",
      "    )  # adds 1 because the first bullet is not counted\n",
      "    if elaboration_needed:\n",
      "        prompt_to_elaborate_on_the_plan = f\"\"\"\n",
      "\n",
      "In addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):\n",
      "-\"\"\"\n",
      "        if print_text:\n",
      "            print(text_color_prefix + prompt_to_elaborate_on_the_plan, end=\"\")\n",
      "\n",
      "        # append this elaboration prompt to the results from step 2\n",
      "        prior_text = full_plan_prompt + plan_completion\n",
      "        full_elaboration_prompt = prior_text + prompt_to_elaborate_on_the_plan\n",
      "\n",
      "        # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n",
      "        elaboration_response = openai.ChatCompletion.create(\n",
      "            engine=engine,\n",
      "            messages=[{\n",
      "                \"role\":\"system\",\n",
      "                \"content\": full_elaboration_prompt\n",
      "            }],\n",
      "            # stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n",
      "            # max_tokens=max_tokens,\n",
      "            # temperature=temperature,\n",
      "            # stream=True,\n",
      "        )\n",
      "        # elaboration_completion = \"\"\n",
      "        if print_text:\n",
      "            print(completion_color_prefix, end=\"\")\n",
      "        elaboration_completion = elaboration_response.choices[0].message.content\n",
      "        if print_text:\n",
      "            print(elaboration_completion, end=\"\")\n",
      "\n",
      "    # Step 3: Generate the unit test\n",
      "\n",
      "    # create a markdown-formatted prompt that asks GPT-3 to complete a unit test\n",
      "    starter_comment = \"\"\n",
      "    if unit_test_package == \"pytest\":\n",
      "        starter_comment = (\n",
      "            \"Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\"\n",
      "        )\n",
      "    prompt_to_generate_the_unit_test = f\"\"\"\n",
      "\n",
      "Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n",
      "```python\n",
      "import {unit_test_package}  # used for our unit tests\n",
      "\n",
      "{function_to_test}\n",
      "\n",
      "#{starter_comment}\"\"\"\n",
      "    if print_text:\n",
      "        print(text_color_prefix + prompt_to_generate_the_unit_test, end=\"\")\n",
      "\n",
      "    # append this unit test prompt to the results from step 3\n",
      "    if elaboration_needed:\n",
      "        prior_text = full_elaboration_prompt + elaboration_completion\n",
      "    else:\n",
      "        prior_text = full_plan_prompt + plan_completion\n",
      "    full_unit_test_prompt = prior_text + prompt_to_generate_the_unit_test\n",
      "\n",
      "    # send the prompt to the API, using ``` as a stop sequence to stop at the end of the code block\n",
      "    unit_test_response = openai.ChatCompletion.create(\n",
      "        engine=engine,\n",
      "        messages=[{\n",
      "            \"role\":\"system\",\n",
      "            \"content\": full_unit_test_prompt\n",
      "        }],\n",
      "        # stop=\"```\",\n",
      "        # max_tokens=max_tokens,\n",
      "        # temperature=temperature,\n",
      "        # stream=True,\n",
      "    )\n",
      "    # unit_test_completion = \"\"\n",
      "    if print_text:\n",
      "        print(completion_color_prefix, end=\"\")\n",
      "    unit_test_completion = unit_test_response.choices[0].message.content\n",
      "    if print_text:\n",
      "        print(unit_test_completion, end=\"\")\n",
      "\n",
      "    # check the output for errors\n",
      "    code_start_index = prompt_to_generate_the_unit_test.find(\"```python\\n\") + len(\"```python\\n\")\n",
      "    code_output = prompt_to_generate_the_unit_test[code_start_index:] + unit_test_completion\n",
      "    try:\n",
      "        ast.parse(code_output)\n",
      "    except SyntaxError as e:\n",
      "        print(f\"Syntax error in generated code: {e}\")\n",
      "        if reruns_if_fail > 0:\n",
      "            print(\"Rerunning...\")\n",
      "            return unit_test_from_function(\n",
      "                function_to_test=function_to_test,\n",
      "                unit_test_package=unit_test_package,\n",
      "                approx_min_cases_to_cover=approx_min_cases_to_cover,\n",
      "                print_text=print_text,\n",
      "                engine=engine,\n",
      "                max_tokens=max_tokens,\n",
      "                temperature=temperature,\n",
      "                reruns_if_fail=reruns_if_fail - 1,  # decrement rerun counter when calling again\n",
      "            )\n",
      "\n",
      "    # return the unit test as a string\n",
      "    return unit_test_completion\n",
      "\n",
      "------------\n",
      "get_functions.py\n",
      "Function name: getmembers\n",
      "Function definition: def getmembers(item: Any, predicate=None) -> List[Tuple[str, Any]]:\n",
      "    \"\"\"\n",
      "    Get all members of an object.\n",
      "\n",
      "    :param item: An object to get the members of.\n",
      "    :param predicate: A callable used to filter the members.\n",
      "    :return: A list of tuples containing the name and value of each member.\n",
      "    \"\"\"\n",
      "    if predicate is None:\n",
      "        predicate = inspect.ismemberdescriptor\n",
      "    return inspect.getmembers(item, predicate)\n",
      "\n",
      "------------\n",
      "Function name: import_all_modules\n",
      "Function definition: def import_all_modules(directory: str) -> None:\n",
      "    \"\"\"\n",
      "    Import all modules from a directory.\n",
      "\n",
      "    :param directory: The directory to import modules from.\n",
      "    :return: None\n",
      "    \"\"\"\n",
      "    output_dict={}\n",
      "    for file in os.listdir(directory):\n",
      "        if file.endswith(\".py\") and file != '__init__.py':\n",
      "            module_name = file[:-3]\n",
      "            spec = importlib.util.spec_from_file_location(module_name, os.path.join(directory, file))\n",
      "            module = importlib.util.module_from_spec(spec)\n",
      "            spec.loader.exec_module(module)\n",
      "            members = getmembers(module, inspect.isfunction)\n",
      "            for member in members:\n",
      "                if member[1].__module__ == module.__name__:\n",
      "                    output_dict[member[0]]=member[1]\n",
      "            classes = [m[1] for m in getmembers(module, inspect.isclass)]\n",
      "            for class_ in classes:\n",
      "                members = getmembers(class_, inspect.isfunction)\n",
      "                for member in members:\n",
      "                    if member[1].__module__ == module.__name__:\n",
      "                        output_dict[member[0]]=member[1]\n",
      "    return output_dict\n",
      "\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "import inspect\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "def getmembers(item: Any, predicate=None) -> List[Tuple[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get all members of an object.\n",
    "\n",
    "    :param item: An object to get the members of.\n",
    "    :param predicate: A callable used to filter the members.\n",
    "    :return: A list of tuples containing the name and value of each member.\n",
    "    \"\"\"\n",
    "    if predicate is None:\n",
    "        predicate = inspect.ismemberdescriptor\n",
    "    return inspect.getmembers(item, predicate)\n",
    "\n",
    "def import_all_modules(directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Import all modules from a directory.\n",
    "\n",
    "    :param directory: The directory to import modules from.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        output_dict={}\n",
    "        filepath = os.path.join(directory, file)\n",
    "        if os.path.isdir(filepath):\n",
    "            # Recursively call the function for subdirectories\n",
    "            import_all_modules(filepath)\n",
    "        elif file.endswith(\".py\") and file != '__init__.py':\n",
    "            print(file)\n",
    "            module_name = file[:-3]\n",
    "            spec = importlib.util.spec_from_file_location(module_name, os.path.join(directory, file))\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(module)\n",
    "            members = getmembers(module, inspect.isfunction)\n",
    "            for member in members:\n",
    "                if member[1].__module__ == module.__name__:\n",
    "                    print(f\"Function name: {member[0]}\")\n",
    "                    print(f\"Function definition: {inspect.getsource(member[1])}\")\n",
    "                    print(\"------------\")\n",
    "                    output_dict[member[0]]=member[1]\n",
    "                else:\n",
    "                    print(f\"Skipping imported function: {member[0]}\")\n",
    "            classes = [m[1] for m in getmembers(module, inspect.isclass)]\n",
    "            for class_ in classes:\n",
    "                members = getmembers(class_, inspect.isfunction)\n",
    "                for member in members:\n",
    "                    if member[1].__module__ == module.__name__:\n",
    "                        print(f\"Method name: {member[0]}\")\n",
    "                        print(f\"Method definition: {inspect.getsource(member[1])}\")\n",
    "                        print(\"------------\")\n",
    "                        output_dict[member[0]]=member[1]\n",
    "                    else:\n",
    "                        print(f\"Skipping imported method: {member[0]}\")\n",
    "    return output_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_dict=import_all_modules(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['unit_test_from_function', '__init__', 'add_error_col', 'category_check', 'columns_to_check', 'conditional_check', 'conditional_cond_syntax', 'data_type_check', 'duplicate_check', 'duplicate_cond_syntax', 'file_check', 'is_float', 'limit_finder', 'main_pipeline', 'null_check', 'null_cond_syntax', 'range_check', 'range_cond_syntax', 'read_s3_file', 'resolve_config', 'sum_check_syntax'])\n"
     ]
    }
   ],
   "source": [
    "print(output_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests failed:\n",
      " ============================= test session starts ==============================\n",
      "platform linux -- Python 3.10.9, pytest-7.2.2, pluggy-1.0.0\n",
      "rootdir: /home/ubuntu/openai-cookbook\n",
      "plugins: anyio-3.6.2, typeguard-2.13.3\n",
      "collected 3 items\n",
      "\n",
      "test_code/unit_test/test___init__.py FFF                                 [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_ test_init[source_df0-mock_config_path-mock_file_name-mock_src_system-rule_df0-None] _\n",
      "test_code/unit_test/test___init__.py:57: in test_init\n",
      "    assert dq.schema_dict == {\n",
      "E   NameError: name 'StringType' is not defined\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "23/05/08 23:12:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "----------------------------- Captured stderr call -----------------------------\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "_ test_init[source_df1-mock_config_path-mock_file_name-mock_src_system-rule_df1-None] _\n",
      "test_code/unit_test/test___init__.py:57: in test_init\n",
      "    assert dq.schema_dict == {\n",
      "E   NameError: name 'StringType' is not defined\n",
      "_ test_init[source_df2-mock_config_path-mock_file_name-mock_src_system-rule_df2-None] _\n",
      "test_code/unit_test/test___init__.py:49: in test_init\n",
      "    dq = DataCheck(source_df, mock_spark, config_path, file_name, src_system)\n",
      "test_code/to_test/dq_check.py:80: in __init__\n",
      "    self.input_columns[index] = \"`\" + self.input_columns[index] + \"`\"\n",
      "../anaconda3/envs/mathnesium/lib/python3.10/site-packages/pandas/core/indexes/base.py:5302: in __setitem__\n",
      "    raise TypeError(\"Index does not support mutable operations\")\n",
      "E   TypeError: Index does not support mutable operations\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_code/unit_test/test___init__.py::test_init[source_df0-mock_config_path-mock_file_name-mock_src_system-rule_df0-None]\n",
      "FAILED test_code/unit_test/test___init__.py::test_init[source_df1-mock_config_path-mock_file_name-mock_src_system-rule_df1-None]\n",
      "FAILED test_code/unit_test/test___init__.py::test_init[source_df2-mock_config_path-mock_file_name-mock_src_system-rule_df2-None]\n",
      "============================== 3 failed in 7.43s ===============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_tests(path):\n",
    "    # Run pytest as a subprocess from the main directory\n",
    "    result = subprocess.run(['pytest', path, '--tb=short'], capture_output=True, text=True)\n",
    "    \n",
    "    # If the return code is non-zero, there was an error\n",
    "    if result.returncode != 0:\n",
    "        # Return the error string\n",
    "        return result.stdout\n",
    "    \n",
    "    # Otherwise, return the output (which should be empty if there are no errors)\n",
    "    return result.stdout\n",
    "\n",
    "# Example usage:\n",
    "output = run_tests(\"test_code/unit_test/test___init__.py\")\n",
    "if output:\n",
    "    print(\"Tests failed:\\n\", output)\n",
    "else:\n",
    "    print(\"Tests passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import json\n",
      "import pandas\n",
      "import numpy\n",
      "from pyspark.sql import SparkSession\n",
      "from unittest.mock import MagicMock\n",
      "import pytest\n",
      "from test_code.to_test.dq_check import DataCheck\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "with open('test_code/unit_test/test___init__.py', 'r') as f:\n",
    "    code = f.read()\n",
    "\n",
    "tree = ast.parse(code)\n",
    "\n",
    "for node in tree.body:\n",
    "    if isinstance(node, ast.Import):\n",
    "        print(f\"import {', '.join([alias.name for alias in node.names])}\")\n",
    "    elif isinstance(node, ast.ImportFrom):\n",
    "        print(f\"from {node.module} import {', '.join([alias.name for alias in node.names])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_code/to_test/\\n├── TEST_DATA (1).csv\\n├── __init__.py\\n├── __pycache__\\n│\\xa0\\xa0 ├── __init__.cpython-310.pyc\\n│\\xa0\\xa0 ├── dq_check.cpython-310.pyc\\n│\\xa0\\xa0 ├── dq_etl_glue.cpython-310.pyc\\n│\\xa0\\xa0 ├── dq_utility.cpython-310.pyc\\n│\\xa0\\xa0 └── the_pipeline.cpython-310.pyc\\n├── az_ca_pcoe_dq_rules_bioscript (4).csv\\n├── az_ca_pcoe_dq_rules_innomar (2).csv\\n├── brand_alignment_xref.csv\\n├── cities.csv\\n├── dq_utility.py\\n├── error_df.csv\\n└── temp_config (1).json\\n\\n1 directory, 14 files\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = subprocess.run(['tree', 'test_code/'], capture_output=True, text=True).stdout\n",
    "result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_code/to_test/\n",
      "├── TEST_DATA (1).csv\n",
      "├── __init__.py\n",
      "├── __pycache__\n",
      "│   ├── __init__.cpython-310.pyc\n",
      "│   ├── dq_check.cpython-310.pyc\n",
      "│   ├── dq_etl_glue.cpython-310.pyc\n",
      "│   ├── dq_utility.cpython-310.pyc\n",
      "│   └── the_pipeline.cpython-310.pyc\n",
      "├── az_ca_pcoe_dq_rules_bioscript (4).csv\n",
      "├── az_ca_pcoe_dq_rules_innomar (2).csv\n",
      "├── brand_alignment_xref.csv\n",
      "├── cities.csv\n",
      "├── dq_utility.py\n",
      "├── error_df.csv\n",
      "└── temp_config (1).json\n",
      "\n",
      "1 directory, 14 files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(subprocess.run(['tree', 'test_code/'], capture_output=True, text=True).stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('example.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Get the first and second level keys\n",
    "keys = set()\n",
    "for key1 in data:\n",
    "    keys.add(key1)\n",
    "    if isinstance(data[key1], dict):\n",
    "        for key2 in data[key1]:\n",
    "            keys.add(f\"{key1}.{key2}\")\n",
    "\n",
    "# Convert the set of keys to a string\n",
    "key_string = \"\\n\".join(keys)\n",
    "\n",
    "# Print the key string\n",
    "print(key_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------------------+---------+-------------+-------------+------------------+-----------------------------------------------------+----------+------------+---------------+--------------------+---------------------+-------------------------+-----------+-----------+\n",
      "|Patient Number|Current Patient Status|Current Patient Sub Status|Plan Type|Plan Priority|   Payer Name|Final Outcome Date|Days From First Call to Insurer To Final Outcome Date|Coverage %|Co Payment %|Plan Max Amount|Submitted To MD Date|Received From MD Date|Submitted To Insurer Date|Result Date|Expiry Date|\n",
      "+--------------+----------------------+--------------------------+---------+-------------+-------------+------------------+-----------------------------------------------------+----------+------------+---------------+--------------------+---------------------+-------------------------+-----------+-----------+\n",
      "|           001|                Active|              On Treatment|     Gold|         High|ABC Insurance|        2023-05-08|                                                   45|      80.0|        10.0|         5000.0|          2023-04-01|           2023-04-10|               2023-04-15| 2023-05-01| 2024-04-01|\n",
      "|           002|                Closed|                 Completed|   Silver|       Medium|XYZ Insurance|        2023-05-07|                                                   30|      70.0|        20.0|         4000.0|          2023-04-02|           2023-04-12|               2023-04-17| 2023-05-02| 2024-05-01|\n",
      "|           003|               Pending|                  Approval|   Bronze|          Low|PQR Insurance|        2023-05-09|                                                   60|      90.0|         5.0|         5500.0|          2023-04-03|           2023-04-13|               2023-04-18| 2023-05-03| 2024-03-01|\n",
      "+--------------+----------------------+--------------------------+---------+-------------+-------------+------------------+-----------------------------------------------------+----------+------------+---------------+--------------------+---------------------+-------------------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, round\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Patient Number\", StringType(), True),\n",
    "    StructField(\"Current Patient Status\", StringType(), True),\n",
    "    StructField(\"Current Patient Sub Status\", StringType(), True),\n",
    "    StructField(\"Plan Type\", StringType(), True),\n",
    "    StructField(\"Plan Priority\", StringType(), True),\n",
    "    StructField(\"Payer Name\", StringType(), True),\n",
    "    StructField(\"Final Outcome Date\", StringType(), True),\n",
    "    StructField(\"Days From First Call to Insurer To Final Outcome Date\", IntegerType(), True),\n",
    "    StructField(\"Coverage %\", DoubleType(), True),\n",
    "    StructField(\"Co Payment %\", DoubleType(), True),\n",
    "    StructField(\"Plan Max Amount\", DoubleType(), True),\n",
    "    StructField(\"Submitted To MD Date\", StringType(), True),\n",
    "    StructField(\"Received From MD Date\", StringType(), True),\n",
    "    StructField(\"Submitted To Insurer Date\", StringType(), True),\n",
    "    StructField(\"Result Date\", StringType(), True),\n",
    "    StructField(\"Expiry Date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create a PySpark DataFrame with fake data\n",
    "data = [\n",
    "    ('001', 'Active', 'On Treatment', 'Gold', 'High', 'ABC Insurance', '2023-05-08', 45, 80.0, 10.0, 5000.0, '2023-04-01', '2023-04-10', '2023-04-15', '2023-05-01', '2024-04-01'),\n",
    "    ('002', 'Closed', 'Completed', 'Silver', 'Medium', 'XYZ Insurance', '2023-05-07', 30, 70.0, 20.0, 4000.0, '2023-04-02', '2023-04-12', '2023-04-17', '2023-05-02', '2024-05-01'),\n",
    "    ('003', 'Pending', 'Approval', 'Bronze', 'Low', 'PQR Insurance', '2023-05-09', 60, 90.0, 5.0, 5500.0, '2023-04-03', '2023-04-13', '2023-04-18', '2023-05-03', '2024-03-01')\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Convert the date columns to DateType()\n",
    "df = df.withColumn(\"Final Outcome Date\", to_date(df[\"Final Outcome Date\"], 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"Submitted To MD Date\", to_date(df[\"Submitted To MD Date\"], 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"Received From MD Date\", to_date(df[\"Received From MD Date\"], 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"Submitted To Insurer Date\", to_date(df[\"Submitted To Insurer Date\"], 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"Result Date\", to_date(df[\"Result Date\"], 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"Expiry Date\", to_date(df[\"Expiry Date\"], 'yyyy-MM-dd'))\n",
    "# Show the data\n",
    "df.show()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1).write.parquet(\"test_data.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parDF1=spark.read.parquet(\"test_data.parquet/part-00000-f2355e81-923b-489e-bd7e-53fd89d044be-c000.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------------------+---------+-------------+-------------+------------------+-----------------------------------------------------+----------+------------+---------------+--------------------+---------------------+-------------------------+-----------+-----------+\n",
      "|Patient Number|Current Patient Status|Current Patient Sub Status|Plan Type|Plan Priority|   Payer Name|Final Outcome Date|Days From First Call to Insurer To Final Outcome Date|Coverage %|Co Payment %|Plan Max Amount|Submitted To MD Date|Received From MD Date|Submitted To Insurer Date|Result Date|Expiry Date|\n",
      "+--------------+----------------------+--------------------------+---------+-------------+-------------+------------------+-----------------------------------------------------+----------+------------+---------------+--------------------+---------------------+-------------------------+-----------+-----------+\n",
      "|           001|                Active|              On Treatment|     Gold|         High|ABC Insurance|        2023-05-08|                                                   45|      80.0|        10.0|         5000.0|          2023-04-01|           2023-04-10|               2023-04-15| 2023-05-01| 2024-04-01|\n",
      "|           002|                Closed|                 Completed|   Silver|       Medium|XYZ Insurance|        2023-05-07|                                                   30|      70.0|        20.0|         4000.0|          2023-04-02|           2023-04-12|               2023-04-17| 2023-05-02| 2024-05-01|\n",
      "|           003|               Pending|                  Approval|   Bronze|          Low|PQR Insurance|        2023-05-09|                                                   60|      90.0|         5.0|         5500.0|          2023-04-03|           2023-04-13|               2023-04-18| 2023-05-03| 2024-03-01|\n",
      "+--------------+----------------------+--------------------------+---------+-------------+-------------+------------------+-----------------------------------------------------+----------+------------+---------------+--------------------+---------------------+-------------------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathnesium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
