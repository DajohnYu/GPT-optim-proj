> [!IMPORTANT]
> This cookbook will not only help users of **ChatGPT for Business** (Enterprise & Team plan users) become experts in building GPTs, but share specific guides as examples to get you started on:
>
> **1. Expanding GPTs knowledge with internal knowledge bases or unstructured data (e.g. Confluence, Google Drive, Sharepoint, Slack)**
> 
> **2. Connecting with information in structured databases (e.g. Snowflake, Databricks, Salesforce, AWS)**
> 
> **3. Creating assistants who can execute tasks in 3rd party tools (e.g. Jira, Salesforce, NetSuite, Email)**

## Introduction to GPTs

ChatGPT is a great Swiss army knife, but when business users need to repetitiously complete the same tasks, supplement the model‚Äôs knowledge, or interact with external systems  - ChatGPT on its own can fall short. That is why OpenAI released ‚Äú[GPTs](https://openai.com/blog/introducing-gpts)‚Äù, allowing Plus and Enterprise users to create **tailored versions of ChatGPT** for personal or shared use (check out [Zapier](https://zapier.com/blog/gpt-assistant/) and [Canva‚Äôs](https://chat.openai.com/g/g-alKfVrz9K-canva) GPTs). 

Today ‚Äútailored‚Äù means:

- GPTs can have specific instructions (otherwise known as ‚Äúsystem messages‚Äù)
- GPTs can have extended knowledge (e.g. proprietary information or data the model was not previously trained on)
- GPTs can have additonal capabilities, to connect with specific OpenAI tools (e.g. browsing, image generation & Advanced Data analytics), plus 3rd party systems to execute functions, retrieve or send information

GPTs ultimately allow anyone to leverage the full power of GenAI without having to be a technical expert or know how to work with OpenAI‚Äôs API Platform 
>Note: if you are interested in leveraging similar capabilities with our API, please see the [Assistants API Overview](https://cookbook.openai.com/examples/assistants_api_overview_python).

## The anatomy of a GPT

The **core components** of a GPT available to builders are: 

(1) **model**, which you can think of as the ‚Äúgeneral intelligence‚Äù, this is defaulted to OpenAI‚Äôs GPT-4 Turbo and is not currently configurable

(2) **instructions**, which ensure that the model performs knows how to accomplish the task for which its built for each GPT - you can think of this being the ‚Äúsystem prompt‚Äù for your GPT.

(3) **supplementary knowledge**, which provides the model with specific information it did not have access to when trained.

> [!Note]
> How this works: files will be uploaded by users as ‚Äúknowledge‚Äù available for the GPT, this knowledge will stored as embeddings in a vector db hosted by OpenAI and accessed via a RAG method when the GPT recognizes it needs to access this knowledge. This knowledge is only available to that specific GPT for use, keeping your proprietary & sensitive information safe & sound.

(4) **capabilities**, which you can think of as ‚Äúsuper powers‚Äù you want given to the model - built & maintained exclusively by OpenAI; currently these are limited to: [web browsing](https://openai.com/blog/chatgpt-plugins#:~:text=study%20these%20impacts.-,Browsing,-Alpha), image generation with [DALL-E](https://openai.com/dall-e-3), plus ‚Äú[code interpreter](https://openai.com/blog/chatgpt-plugins#:~:text=of%20the%20ecosystem.-,Code%20interpreter,-Alpha)‚Äù - the ability to spin up a virtual machine to run python code generated by the model which enables mathematics, data analysis & visualization and more. 

>Note: these are different than the inherent capabilities that are always turned on, including: [vision](https://openai.com/research/gpt-4v-system-card), [robust file handling](https://help.openai.com/en/articles/8555545-file-uploads-with-gpts-and-advanced-data-analysis-in-chatgpt), [text to speech](https://openai.com/blog/chatgpt-can-now-see-hear-and-speak#:~:text=on%20all%20platforms.-,Speak%20with%20ChatGPT%20and%20have%20it%20talk%20back,-You%20can%20now)* and [voice](https://openai.com/blog/chatgpt-can-now-see-hear-and-speak#:~:text=voice%20and%20vision.-,Voice,-The%20new%20voice)* (* = only available on Mobile currently)

(5) **[actions](https://platform.openai.com/docs/actions)**, which allow the GPT to connect/integrate to 3rd party systems, so now the model can interact with the real world, on behalf of the user, leveraging their scopes and permissions (with the end user still in the loop, of course)

In addition to the core components, GPTs also allow the GPT builder administrative functions to manage the GPT, including: customization (name, description & avatar, conversation starters) and permissioning (share with: only me, only people in my organization, only people with the link, anyone with paid ChatGPT plan). We won‚Äôt go into these ones in detail - but you can checkout our [Help Center documentation](https://help.openai.com/en/articles/8554407-gpts-faq) for more information regarding the administrative & privacy side of GPTs.  

Once a builder has created their GPT, the end user will be able to spin up a conversation with the GPT in the exact same way they are used to interacting with ChatGPT today.

## Getting started building GPTs: Crawl, Walk, Run 

> [!Important]
>The following is intended to act as a notebook to demonstrate how to build GPTs that enhance value for business users of ChatGPT. Before getting started with these examples, we recommend checking out this guide to help you build and share a basic GPT - as we will skip over many of the basic concepts in our crawl, walk, run examples below:
>- Crawl: Build a GPT that leverages your internal knowledge in its responses (unstructed data)
>- Walk: Build a GPT that answers questions about your enterprise data (structured data)
>- Run: Build a GPT assistant who interacts with & executes tasks in 3rd party tools

### Crawl: Build a GPT that leverages your internal knowledge in its responses
Many times employees will prefer to ask questions of experts or submit tickets instead of performing searches in the various knowledge stores they have available to them (e.g. Google Docs, Sharepoint, Confluence, Slack, etc). Wonder why? Because experts are able to understand the context of a query and provide precise, specific & nuanced answers that often leverage a wide swath of knowledge. 

By connecting purpose-built GPTs to your various enterprise knowledge stores, you go beyond ‚ÄúEnterprise Search‚Äù to ‚ÄúEnterprise Expertise‚Äù that can generate a coherent and contextually relevant response. This not only significantly reduces the workload on staff who would normally be answering questions - allowing them to focus on more complex and unique issues that require human intervention, but provides immediate assistance to employees around the clock 24/7, which boosts productivity and employee satisfaction. 

> üí° Ideas for applicable GPT use cases you might create with this example: HR Generalist, IT Help Desk, Product Specialist, etc.

#### Example One: Connecting GPTs with Files (‚Äùstatic‚Äù knowledge) 
This example is great for you to replicate with any ‚Äústatic knowledge‚Äù you might have in your business and it helps provide a contrast of the advantages of getting precise, specific & nuanced answers vs performing a ‚ÄúCmd-F‚Äù on company policy documents or a Google Drive search. 

‚ÄúStatic knowledge‚Äù represents information that tends to change less frequently - such that exporting or downloading this information and uploading to your GPT on some cadence is a manageable process. This is especially great for institutional knowledge in HR, IT, or Product. 

We will start simple, creating an ‚ÄúHR Hero‚Äù that can provide employees with a pleasant HR Generalist available 24/7 to answers their questions around company policies and procedures, payroll, benefits, career development and more. 

1. We will start by locating and exporting or downloading the current versions of company policies, procedures, handbooks, manuals, and intranet. These can be in any file format (hint: even html exports of your intranet üòâ) when you export or download, but keep the number of exports to 20 files or less since that is the current system limitation.
> *[Hint: you can merge documents as long as you keep them to 512MB in size, you can even give ChatGPT documents and ask it to merge them for you]*

2. Now that you have these files its time to upload them to the knowledge section of your GPT. Easy, right? Behind the scenes we will convert these documents to embeddings, store them in a vector database that is hosted by OpenAI and will leverage RAG methods to effectively leverage this newly formed, underlying knowledge base. 
    
> üí° Fun Fact: When working with knowledge our model is post-trained to not only support RAG methods, but also knows when better performance would arise from ‚Äústuffing‚Äù the knowledge content into the context window for use. There will be cases where it will prefer this method. 
>
> A great example for when ‚Äústuffing‚Äù might be chosen by a model is when the request is related to content translation to another language. In this case the subtleties of language and cultural nuances are important, so having detailed context about these aspects will improve the accuracy and relevance of the response. Thus the model will choose stuffing vs. retrieval.

3. Time to build out some great instructions that will help the model use this information to generate a coherent and contextually relevant response rivaling that of a human. Here are some of our general best practices for crafting great instructional prompts when working with knowledge, including examples that we will include in our instructions for our ‚ÄúHR Hero‚Äù example.

| Best Practice | Example for ‚ÄúHR Hero‚Äù |
| --- | --- |
| Provide Context: Include relevant context in your instructions. This helps the model understand the exact nature when querying the knowledge, ensuring that the information retrieved is pertinent to your needs. | You are a member of our human resource team. Your job is to use only the knowledge you have available to answer employees questions in the most direct & pleasant manner. Display a tone of empathy and professionalism in all responses, recognizing the importance of HR inquiries in the employee experience. If you need more context from the employee to query the knowledge you have access to, you should ask them follow up questions when needed. If after you have queried the knowledge you find that you cannot generate an answer you should instruct the employee to ask their question in our slack #ask-hr channel.  |
| Be Specific and Detailed: Models leveraging embeddings to augment their knowledge perform best with specific, detailed queries. The more precise the query the model generates, the more accurate and relevant the information retrieved and generated will be. | Use the users input to generate the most specific and detailed query you can to search against the knowledge you have access to. For example, instead of 'leave policy', form the query 'What is the paternity leave policy for employees in the New York office?'. If the user‚Äôs question is unclear and you, ask follow-up questions to the user to build the best query before searching your knowledge to form the answer.  |
| Augment with Keywords: Include relevant keywords or phrases. This can help the retrieval component of the model to fetch the most relevant information from its database. | When processing the individuals question, you should augment the query you will build with specific terms that relate to what the employee asked. For example, if an employee asks about the company's new remote work policy, you should augment the query with keywords such as "remote work", "telecommuting guidelines", "work from home", and "employee remote policy". |
| Avoid Ambiguity: Ambiguous prompts can lead to vague or irrelevant responses. Be clear and unambiguous to guide the model towards the desired direction. | When forming your query avoid ambiguity, for example: rather than 'employee benefits', ask 'What are the health insurance options available for employees with families?‚Äô. You should then use your updated query to translate to the appropriate query language to use in your retrieval.  |
| Incorporate Multi-Step Reasoning: For complex queries, break down your request into multiple steps or questions. This can help the model process each part effectively and build a more comprehensive response. | If you are asked a complex question, break it down into multiple queries against the knowledge to form your answer. For example, first ask 'How is employee performance evaluated?', followed by 'How does this impact promotions?' use the knowledge returned from both to form the best response.  |
| Always cite source: Since the answer may come from multiple sources, the answer should always cite its sources. _Hint: you need to turn on Code Interpreter for this to work. Only use this when you are comfortable with your GPT end users being able to access the actual source documents you uploaded._   | Always provide a source citation when providing an answer using your knowledge.  |
| Bonus! Don‚Äôt use phrases in the negative in your instructions, ChatGPT doesn‚Äôt handle those well, instead handle them in the affirmative.  | For example, saying "Please include colorful illustrations in your response" is more straightforward for an AI to interpret and act on than saying "Don't leave out colorful illustrations in your response." The first instruction is direct and unambiguous, while the second one is a double negative that requires the AI to process the instruction and then invert it to understand the actual request. |

Here is what our finalized prompt looks like. You will enter yours into the ‚ÄúInstructions‚Äù text input inside the GPT‚Äôs ‚Äúconfigure‚Äù tab.

```markdown
You are a member of our human resource team and can use the knowledge uploaded to answer questions in a most direct & pleasant manner.

Use your GPT model to augment the query you will build with specific terms that relate to what the employee asked so you can find information in your knowledge to create an answer. 

Always provide a source citation when providing your answer using your knowledge. 

Display a tone of empathy and professionalism in all responses, recognizing the importance of HR inquiries in the employee experience. 

Use the users input to generate the most specific and detailed query you can to search against the knowledge you have access to. For example, instead of 'leave policy', form the query 'What is the paternity leave policy for employees in the New York office?'. If the user‚Äôs question is unclear and you, ask follow-up questions to the user to build the best query before searching your knowledge to form the answer. 

If you need more context from the employee to query the knowledge you have access to, you should ask them follow up questions when needed. If after you have queried the knowledge you find that you cannot generate an answer you should instruct the employee to ask their question in our slack #ask-hr channel. 

When forming your query avoid ambiguity, for example: rather than 'employee benefits', ask 'What are the health insurance options available for employees with families?‚Äô

If you are asked a complex question, break it down into multiple queries against the knowledge to form your answer. For example, first ask 'How is employee performance evaluated?', followed by 'How does this impact promotions?' use the knowledge returned from both to form the best response.
```

4.  Time to test our setup. We recommend creating a set of questions & expected answers - maybe even populated from your live Slack channels, emails, ticketing systems. These will be used to make sure we are getting the best results from our GPT. We will use this setup as an ‚Äúpseudo Eval framework‚Äù to help us understand what we might need to tweak in our instruction prompt. Here is one row as an example for ‚ÄúHR Hero‚Äù:

| Question | Human Answer | GPT Answer | Rating |
| --- | --- | --- | --- |
| When should I use sick leave vs. PTO? | You should use sick leave when you are out for longer than 5 days. This type of leave is specifically for medical or health-related reasons and can fall under the Family and Medical Leave Act (FMLA) and/or California Family Rights Act (CFRA), depending on the situation. We review each case of Medical/Sick leave and provide appropriate FMLA/CFRA documentation and guidance or may offer OpenAI-sponsored paid medical leave for a limited duration in many cases. The process involves emailing HR with a summary of the leave request, followed by a detailed discussion with HR to understand the leave's duration, rationale, etc., and then HR provides the necessary documentation and guidance. | Sick leave and Paid Time Off (PTO) are two different categories of time away from work, each with its own purpose and guidelines: <br/> Sick Leave: <br/> Purpose: Sick leave is specifically intended for health-related absences. This includes times when an employee is ill, needs to visit a doctor, is undergoing medical treatment, or needs to care for a sick family member. <br/> Duration: Depending on the company's policy, there might be a set number of sick days allowed per year. In some cases, sick leave can be extended for longer periods, especially if it falls under family and medical leave acts like FMLA or CFRA. <br/> Documentation: For longer sick leaves (more than 5 days), documentation or a doctor‚Äôs note might be required. The process might involve contacting HR for a review and further guidance. <br/> Compensation: Usually, sick leave is paid and does not affect the employee's vacation or personal time balances. <br/> Paid Time Off (PTO): <br/> Purpose: PTO is a more general category that covers various types of absences, including vacation, personal time, and sometimes even sick leave if the company has a combined PTO policy. It‚Äôs mainly used for planned time away from work, like vacations, personal days, or other non-work-related activities.<br/> Duration: PTO is typically accrued over time, with employees earning a certain number of PTO hours or days based on their tenure and company policy. <br/> Flexibility: PTO can typically be used at the employee‚Äôs discretion and scheduled in advance. <br/> Compensation: Like sick leave, PTO is also paid. <br/> In some companies, sick leave and PTO are distinct and separate, while in others, they may be combined into a single PTO policy where employees use their allotted time off for any purpose, including sickness. The specific policies regarding the accrual, use, and management of sick leave and PTO can vary widely depending on the company and the jurisdiction. | Low - Did not use our company documents  |

As you can see in our example, the answer was not satisfactory. We will edit our prompt to: ‚Äúyou ~~can~~ **`must only`** use the knowledge given to you to answer questions in the most direct & pleasant manner‚Ä¶‚Äù. 

This change gave us a satisfactory answer that performed ‚ÄúHigh‚Äù in our rating, you can see it below:
> The policy at OpenAI distinguishes Medical (Sick) Leave from general time off. Here are the key points:
>
> **Medical (Sick) Leave (Longer Than 5 Days)**:
> - This type of leave is specifically for medical or health-related reasons.
> - The leave can fall under the Family and Medical Leave Act (FMLA) and/or California Family Rights Act (CFRA), depending on the situation.
> - The HR department at OpenAI reviews each case of Medical/Sick leave and provides appropriate FMLA/CFRA documentation and guidance.
> - OpenAI may offer OpenAI-sponsored paid medical leave for a limited duration in many cases.
> - The process involves emailing HR with a summary of the leave request, followed by a detailed discussion with HR to understand the leave's duration, rationale, etc., and then HR provides the necessary documentation and guidance.

Continue to iterate until you feel that your instructions meet the expectations outlined in your pseudo eval framework.  

Let‚Äôs take a look under the hood at what it did to create this answer so you can understand what is happening:

- When the model receives a query or a question, it will use a retriever to run search(es) through the corpus of knowledge to find documents or passages that are most relevant to the query.
    - In our example it created 2 search queries: ‚ÄúSick Leave‚Äù and ‚ÄúPTO‚Äù
    - These queries returned 2 documents and then narrowed down to 3 passages of content relevant to that query - shown below.
```markdown
{
          "type": "file",
          "name": "Time Off Policy.pdf",
          "source": "my_files",
          "text": "REQUESTING MEDICAL (SICK) LEAVE (LONGER THAN 5 DAYS)\n\n\nDepending on the situation Medical / Sick leave can fall under the [FMLA and/or\nCFRA](https://www.edd.ca.gov/disability/faqs-fmla-cfra.htm#:~:text=The FMLA and the\nCFRATake medical leave for yourself.). HR will review your situation and if applicable will\nprovide FMLA/CFRA documentation in tandem with providing guidance on the Medical / Sick\nleave. Note that it is not mandatory to apply for FMLA or CFRA OpenAI may provide\nOpenAI-sponsored paid medical leave for a limited duration in many cases."
        }
{
          "type": "file",
          "name": "Time Off Policy.pdf",
          "source": "my_files",
          "text": "If OpenAI provides unlimited FTO why track FTO at all?\n\n\nHaving a central place where we can track and see which days employees will be out of the\noffice helps Exec Recruiting Ops HR managers etc. plan better for things like all-company\nmeetings events on-call calendars team roadmaps etc. It also helps HR and PeopleOps\nmeasure FTO utilization which is an important factor in the overall work-life balance of our\nworkforce. We‚Äôre also required to track time off by law!"
        }
{
          "type": "file",
          "name": "UK Benefits.pdf",
          "source": "my_files",
          "text": "What if I want to request time off that‚Äôs not FTO or sick time?\n\n\nIf you would like to request family or medical leave unpaid time off or other protected or\nunprotected leaves of absence you should not submit a FTO request pursuant to this policy.\nInstead please refer to the applicable policy in the OpenAI Employee Handbook and if you\nneed to request such time off please email hr@openai.com."
        }
```
Once the retriever has selected the most relevant pieces of content, the generator comes into play. The generator (GPT-4 Turbo in this case) is capable of generating human-like text. The generator takes both query and the retrieved content as input and generates a response. The response is formulated by considering both the input query and the information contained in the retrieved documents.

#### Example Two: Connecting GPTs with Confluence (‚Äúdynamic‚Äù knowledge)
You likely have a lot of dynamic knowledge that is ever changing & gets created and stored in purpose built products like: Confluence, Sharepoint, Google Drive, Slack, etc. This type of knowledge is highly valuable and likely something that you‚Äôd gain advantages of connecting to GenAI to leverage without having to build out RAG yourself. For this purpose, you can connect your GPT to your dynamic unstructured knowledge sources leveraging GPTs‚Äô ‚ÄúAction‚Äù function. 

Some important rules of thumb - you need to make sure that your knowledge source supports an API with advanced search capabilities. This typically includes the ability to use structured queries to search for the correct content to return plus the ability to return the content in the response. Furthermore, we may be unable to handle responses larger than 100,000 characters (approx. 200 single spaced pages) from these services, so you may need to implement middleware if the vendor itself doesn‚Äôt provide the capability to return relevant, smaller text chunks. *To note: there will be no error message indicating the 100,000 character cut off, it will just cut off and perform the retrieval, so you will need to execute some form of limiting on the query end to ensure there is not cut off.*  

If your source system does not have these capabilities (a relevant current example that does not support these would be Notion because it does not currently have and endpoint that supports queries of the content itself vs. just the title), you will need to implement the content search locally, which we will not cover in this cookbook but do plan to cover in its own cookbook in the future. ****Some examples of systems that have APIs with the advanced search capabilities needed are:

- **[Confluence‚Äôs Advanced searching using CQL](https://developer.atlassian.com/cloud/confluence/advanced-searching-using-cql/)**
- [**Google Drive API with Search**](https://developers.google.com/drive/api/guides/search-files)
- [**Sharepoint‚Äôs Search REST API**](https://learn.microsoft.com/en-us/sharepoint/dev/general-development/sharepoint-search-rest-api-overview)

For our example, we will create a ‚ÄúConfluence Savant‚Äù that connects our GPT to a Product Wiki to help employees get educated answers to their product questions. 

We will start by creating our Action, which will leverage an OpenAPI spec + authentication to access the relevant Confluence endpoints we need our GPT to be able to access to return relevant knowledge to answer questions. 

Let‚Äôs start with the spec and then finish with the auth.
1. **Creating the OpenAPI Spec**

The easiest way to create an OpenAPI spec would be to use our native capability to import the full schema from a URL. 

If we are less technically inclined, we might use ChatGPT to help us create the OpenAPI spec. For those of you who work regularly with integrations and are especially familiar with Swagger you likely can proceed to visiting the developer site, in our case: **[Confluence‚Äôs Advanced searching using CQL](https://developer.atlassian.com/cloud/confluence/advanced-searching-using-cql/)** and creating the OpenAPI spec to input yourself. 

For those less technically inclined, you can simply ask ChatGPT the following:
```markdown
Browse this page https://developer.atlassian.com/cloud/confluence/advanced-searching-using-cql/ and create an OpenAPI spec complete with a servers section and operation ids for searching for words inside of content in Confluence.
```
This should return a Spec that you can copy and paste into the Schema text box. Here is our example that ChatGPT provided, shown below (note: since no limits are used, this will limit the returned knowledge to the default of 25 articles)
```markdown
openapi: 3.0.0
info:
  title: Confluence Content Search API
  version: 1.0.0
  description: API for searching content in Confluence using CQL.

servers:
  - url: 'https://oaidemo.atlassian.net'
    description: Confluence Cloud Base URL
paths:
  /wiki/rest/api/content/search:
    get:
      operationId: searchConfluence
      summary: Search Content in Confluence
      description: >
        This endpoint allows you to search for content in Confluence using CQL.
        You can filter the content based on various fields using CQL syntax.
        Note that user-specific fields are not supported.
      parameters:
        - in: query
          name: cql
          schema:
            type: string
          description: The CQL query string to search for content, using fields like 'title', 'text', and operators like '~'.
          required: true
        - in: query
          required: true
          name: expand
          schema:
            type: string
            value: body.storage
      responses:
        '200':
          description: Successful response with the list of content that matches the CQL query.
          content:
            application/json:
              schema:
                type: object
                properties:
                  results:
                    type: array
                    items:
                      $ref: '#/components/schemas/Content'
                  start:
                    type: integer
                  limit:
                    type: integer
                  size:
                    type: integer
                  _links:
                    type: object
        '400':
          description: Bad request, e.g., invalid CQL syntax.
        '401':
          description: Unauthorized access.
        '403':
          description: Forbidden, user does not have permission to perform the operation.
        '500':
          description: Internal server error.

components:
  schemas:
    Content:
      type: object
      properties:
        id:
          type: string
        type:
          type: string
        status:
          type: string
        title:
          type: string
        body:
          type: object
          properties:
            storage:
              type: object
              properties:
                value:
                  type: string
                representation:
                  type: string
        _links:
          type: object
```
If you are struggling you can always also use [Swagger‚Äôs Editor](https://editor-next.swagger.io/) to create and test your spec or leverage postman to create your spec as follows:

- [ ] Get your request working in Postman - reference API docs, or inspect existing working API requests (perhaps in a websites inspect panel)
- [ ] [Add the request to a Collection](https://learning.postman.com/docs/collections/using-collections/)
- [ ] [Export the collection](https://learning.postman.com/docs/getting-started/importing-and-exporting/exporting-data/#export-collections)
- [ ]  Copy/paste the collection JSON into https://kevinswiber.github.io/postman2openapi/ to get a YAML-formatted OpenAPI spec
- [ ]  Copy/paste the spec into your custom action schema section

2. Setting up Authentication**

Once you have a working spec, you need to set up your authentication using the instructions given in the API documents. In our example, we want folks without Confluence licenses to be able to access the content, so we will use Basic Authentication instead of OAuth, by selecting API Key as our Authentication Type, entering in our key and then selecting ‚ÄúBasic‚Äù for Auth Type.
> [!Note]
> It is important to note, if you are choosing Basic Auth, you want to ensure that you are comfortable with anyone with access to the GPT, also having access to the source content you are connecting the GPT to. This likely means that you should not publish this GPT with the ‚Äúpublicly‚Äù or ‚Äúonly people with link‚Äù settings, but only chose to publish it within your Enterprise or Team workspace.

If you need to use OAuth 2.0, you can check out our Walk Guide examples, further along this cookbook, which show how to setup OAuth 2.0 with GPTs.

Now you need to give some basic instructions to the GPT so you can test the Action you just created. We are going to use the following, but you should refer to the [best practices for prompting instructions](https://www.notion.so/GPTs-for-Business-Cookbook-20cd502a4d604ab3b199d2f9121700a7?pvs=21) with unstructured knowledge that we used in our ‚ÄúHR Hero‚Äù previously to guide you.

```markdown
You are a "Confluence Savant", equipped with the ability to search our company's Product Wiki in Confluence to answer product-related questions. Your job is to provide accurate and detailed responses by retrieving information from the Product Wiki. Your responses should be clear, concise, and directly address the question asked. You have the capability to execute an action named "searchConfluence" that allows you to search for content within our Confluence Product Wiki using specific terms or phrases related to the user's question.

    - When you receive a query about product information, use the "searchConfluence" action to retrieve relevant content from the Product Wiki. Formulate your search query based on the user's question, using specific keywords or phrases to find the most pertinent information.
    - Once you receive the search results, review the content to ensure it matches the user's query. If necessary, refine your search query to retrieve more accurate results.
    - Provide a response that synthesizes the information from the Product Wiki, clearly answering the user's question. Your response should be easy to understand and directly related to the query.
    - If the query is complex or requires clarification, ask follow-up questions to the user to refine your understanding and improve the accuracy of your search.
    - If the information needed to answer the question is not available in the Product Wiki, inform the user and guide them to where they might find the answer, such as contacting a specific department or person in the company.

    Here is an example of how you might respond to a query:

    User: "What are the latest features of our XYZ product?"
    You: "The latest features of the XYZ product, as detailed in our Product Wiki, include [feature 1], [feature 2], and [feature 3]. These features were added in the recent update to enhance [specific functionalities]. For more detailed information, you can refer to the Product Wiki page [link to the specific Confluence page]."

Remember, your goal is to provide helpful, accurate, and relevant information to the user's query by effectively leveraging the Confluence Product Wiki.
```

3. Now that you have your instructions set, it's time to test the "Confluence Savant" GPT with real-world questions. Create a set of queries that employees might typically ask about your products and use these to evaluate the effectiveness of your GPT. Here's an example:

| Query | Expected Response | GPT Response | Rating |
| --- | --- | --- | --- |
| ‚ÄúI am getting a 401 error, how can I fix?‚Äù <br/> (Note: There are 3 articles in our data base around 401 errors, the response should include relevant information from multiple of the articles, just like a human would). | This error message indicates that the API key you are using in your request is not correct. This could happen for several reasons, such as: <br/> - You are using a typo or an extra space in your API key. <br/> - You are using an API key that belongs to a different organization. <br/> - You are using an API key that has been deleted or deactivated <br/> - Your API key might be cached. <br/> To resolve this error, please follow these steps: <br/> - Try clearing your browser's cache and cookies then try again.<br/> - Check that you are using the correct API key in your request header. Follow the instructions in our Authentication section to ensure your key is correctly formatted (i.e. 'Bearer <API_KEY>') <br/> - If you are unsure whether your API key is correct, you can generate a new one here. Make sure to replace your old API key in your codebase and follow our best practices. | A 401 error typically indicates an issue with invalid authentication. This error can occur due to various reasons, such as:<br/> Incorrect API Key: The API key used in your request might be incorrect. This can happen if there's a typo, an extra space in your API key, or if you're using an API key that belongs to a different organization, or one that has been deleted or deactivated. <br/> Cache Issues: Sometimes your browser might cache an old or invalid API key. <br/> To resolve a 401 error, you can try the following steps: <br/> Verify API Key: Make sure you are using the correct API key in your request header. The API key should be correctly formatted (i.e., 'Bearer <API_KEY>'). If you're unsure about your API key, consider generating a new one from here. Remember to replace your old API key in your codebase. <br/> Clear Cache: Try clearing your browser's cache and cookies, and then attempt your request again. <br/> Check for Typos: Ensure that there are no typographical errors in your API key. <br/> API Key Safety: Follow best practices for API key safety to avoid similar issues in the future. <br/> For more detailed information, you can refer to the Error Codes and 401 Errors documentation. (link) | High |

Continuously test and refine your instructions based on the results you get. This iterative process will help you fine-tune the GPT's ability to use dynamic knowledge from Confluence to provide accurate and helpful responses.

With these steps, you can create a powerful "Confluence Savant" that serves as an efficient and reliable resource for product-related queries within your organization. You can use this same logic to build out integrations with other unstructured knowledge sources that support advanced searching functionalities with their API. We plan to provide similar code snippets in the future to assist you with these.

### Walk: Build a GPT that answers questions about your enterprise data (structured data)
Building a GPT that can answer questions about your enterprise's structured data involves a few key steps and considerations. This capability is particularly beneficial for businesses that need to frequently ask questions about and interpret data from databases, CRM/ERP systems, financial records, etc. This section will act as a guide to help you build a GPT that can connect to your structured data set. 

Before we begin its important to clarify structured data as is highly organized and easily searchable, often stored in relational databases. Examples include SQL databases, CRM data, financial records, and sales data. Many structured databases are not accessible via APIs, which means you will need to leverage middleware to access the content - in this example we will not explore that option, but plan to expand this cookbook to include those examples in the near future. There are some leading structured databases that do (like Snowflake, Salesforce, SAP, etc.), and you can use the GPT's 'actions' capability to connect to these APIs. This allows the GPT to fetch data in real-time based on users prompts.

> [!Note]
> It is a best practice to work with endpoints for these databases that allow GPT‚Äôs intelligence to craft the appropriate Query Language to leverage to return the best information from the source system to answer the question the user asked. GPT-4 Turbo will be familiar with most of the standard query languages which popular structured data sources use such as SQL, Salesforce Object Query Language (SOQL), Snowflake SQL, SAQL. This means you don‚Äôt need to give your custom GPT instructions on how to format the query to be used to return information, it will automatically know, but you should tell it what Query Language to use.

Before you begin building you want to ensure Data Security and Privacy:

- **Access Control:** Make sure that only authorized personnel can query sensitive data. We strongly suggest you put OAuth into place, this will ensure users are forced to authenticate into the source data system.
    - Hint: you also may want to consider setting the **[Consequential flag](https://platform.openai.com/docs/actions/consequential-flag)**, to ensure all operations with the GPT "must always prompt the user for confirmation before running" and don't show an "always allow" button - both are new features of GPTs designed to give users more control).
- **Data Handling:** Clearly instruct the GPT on your data handling policies, helping to boost compliance with data privacy laws and internal regulations. This is not a perfect science, but many customers support adding instructions to the GPT to help cut down on things they might perceive as a risk surface depending on use case.

#### Example One: Answering Questions & Performing Analysis on CRM Data
Imagine a GPT named "DataWiz" designed to provide insights into sales performance. A user might ask, "What were our top-performing products last month?" DataWiz would interpret this query, access the sales database through an API or its internal knowledge base, run the appropriate data query(ies), and present a contextual response, such as a list of top products, total sales, and comparison with previous periods.

By following these steps, you can build a GPT that serves as a valuable tool for accessing and interpreting CRM data to enhance decision-making, and improve business efficiency.

We will start by connecting our GPT to Salesforce by setting up an ‚ÄúAction‚Äù. We will start with building the OpenAPI Spec and then configuring the Authentication.
1. Building the OpenAPI Spec to allow GPTs to Query Salesforce
There are several ways to generate an OpenAPI spec. In this example we will use one of my favorite hacks to generating quick specs, even for less technical folks - having ChatGPT help me create it! I will begin by chatting with normal ChatGPT to ask it about which APIs should I use to query SFDC and how to transform these into an OpenAPI spec. You can check out my conversation [here](https://chat.openai.com/share/e/40c0b4f8-6e35-4bff-9345-2814578bc4bd) to get some detailed inspiration on how you might do this. 

> *Hint: you will want to remove anything related to security in your paths or components objects. You don‚Äôt need those sections, as OpenAI‚Äôs authorization handles that, which you set up separately.* 

Our finalized spec looks like this:
```markdown
openapi: 3.0.0
info:
  title: Salesforce CRM Data Query API
  version: 1.0.0
  description: API for querying CRM data from Salesforce
servers:
  - url: 'https://**your-salesforce-instance**.salesforce.com'
    description: Salesforce API Endpoint

paths:
  /query:
    get:
      summary: Retrieve CRM data
      operationId: queryCrmData
      tags:
        - CRM
      parameters:
        - in: query
          name: soql
          schema:
            type: string
          required: true
          description: Salesforce Object Query Language (SOQL) query string
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                type: object
                properties:
                  totalSize:
                    type: integer
                    description: The total number of records that match the query
                  done:
                    type: boolean
                    description: Indicates if all records have been returned
                  nextRecordsUrl:
                    type: string
                    description: URL to the next batch of records (in case of pagination)
                    nullable: true
                  records:
                    type: array
                    description: Array of record objects returned by the query
                    items:
                      type: object
                      properties:
                        attributes:
                          type: object
                          description: Object containing the type and URL reference
                          properties:
                            type:
                              type: string
                            url:
                              type: string
                        # The following fields are examples, replace them with actual fields from your Salesforce object
                        Id:
                          type: string
                          description: Unique identifier for the record
                        Name:
                          type: string
                          description: Name of the record
                        CreatedDate:
                          type: string
                          format: date-time
                          description: Date when the record was created
                        # Add other fields as per your Salesforce object structure
                example:
                  totalSize: 1
                  done: true
                  records: 
                    - attributes: 
                        type: "Account"
                        url: "/services/data/vXX.X/sobjects/Account/001..."
                      Id: "001..."
                      Name: "Acme Corporation"
                      CreatedDate: "2023-01-01T12:00:00Z"
                  # Include an example response for better clarity
        '401':
          description: Unauthorized
        '400':
          description: Bad Request
        '500':
          description: Internal Server Error
security:
  - OAuth2:
      - read
      - write
```
2. **Setting up Salesforce OAuth 2.0 for GPT**
    
Now that we have our spec, we need to set up OAuth, since, in our case, we only want select folks to be able to access Sales data from our CRM. You can follow the steps below to do this:
    
- 2a . **Create a Connected App in Salesforce**
    
    First, you need to create a Connected App in your Salesforce org. This app acts as a bridge between your application and Salesforce's OAuth 2.0 service.
    
    - Log in to your Salesforce org.
    - Go to **Setup**.
    - In the **Quick Find** box, enter "App Manager".
    - Click on **App Manager**.
    - Click on **New Connected App**.
    - Enter the required details:
        - **Connected App Name**
        - **API Name**
        - **Contact Email**
    - In the **API (Enable OAuth Settings)** section:
        - Check **Enable OAuth Settings**.
        - Enter the **Callback URL**. This is the URL where Salesforce will redirect after authentication.
            - *Note: This should be the URL generate by your GPT, you can find it by navigating back to your GPT in ChatGPT and going opening the Authentication window in your GPT and selecting OAuth as your Authentication Type and clicking ‚ÄúSave‚Äù.*

    > <img width="300" alt="authenticationgpts" src="https://github.com/sabrina-de/OpenAI-ChatGPT-Enterprise/assets/152341307/8a3a216c-0784-44f7-ae48-c5105420de23">

    - *After you exit out of actions editor, you will be able to know see and copy the callback URL as shown in the images below.* ‚ö†Ô∏è *But in order to get the finalized callback url, you need to save your GPT and refresh the GPT creator page. Please note: if you delete your custom action and then create a new one, the URL changes.*

    > <img width="300" alt="redirectgpts" src="https://github.com/sabrina-de/OpenAI-ChatGPT-Enterprise/assets/152341307/82df48ac-3623-4f71-81eb-570dc4c20f9c">

     - Now back in SFDC, under **Selected OAuth Scopes**, add the scopes needed for your application (like "Full access (full)", "Access and manage your data (api)", ‚Äúrefresh_token‚Äù, etc.). You will also need to copy these into the Scope field in your GPT Authentication setup window.
     - Check the box for: **Enable Client Credentials Flow**
     - Click **Save**.

- 2b.  **Retrieve Client Credentials**
    
    After you create the Connected App, you'll receive a **Consumer Key** (Client ID) and **Consumer Secret**. You will need these for the authentication requests. You will copy and paste these into your GPT Authentication section that you set up in a previous step to get your callback URL.
    - **Implementing OAuth 2.0 in Your Application**
    
    There are various OAuth 2.0 flows, but the most common for server-to-server interactions is the Web Server Flow. Here's a simplified version:
    
    - Redirect users to Salesforce to log in *(note: this is your authentication URL to copy and paste into the Authentication window in your GPT)*
        - Generate a URL to Salesforce's authorization endpoint (**`https://login.salesforce.com/services/oauth2/authorize`** for production or **`https://test.salesforce.com/services/oauth2/authorize`** for sandbox) with your client ID and callback URL as query parameters.
          
    - Handle the callback:
        - After the user logs in, Salesforce redirects back to your callback URL with an authorization code.
        - Your server then exchanges this code for an access token by making a POST request to Salesforce's token request endpoint *(note: this is what you should copy into the Token URL in your GPT Authentication window and you should select Default as your* Token Exchange Method*)* (**`https://login.salesforce.com/services/oauth2/token`** for production or **`https://test.salesforce.com/services/oauth2/token`** for sandbox) with the authorization code, client ID, client secret, and redirect URI.

- 2c. **Testing and Debugging**
    
    - Use tools like Postman for testing OAuth flows and API requests, which gives you more visibility into the request than setting up in ChatGPT
    - Check Salesforce's documentation for any updates or changes in the API or authentication process.


3. **Creating Instructions for the GPT to Ask Questions Against Salesforce Data**

Time to build out some excellent instructions that will help the model use Salesforce data to generate coherent and contextually relevant responses. Here are some of our general best practices for crafting effective instructional prompts when working with structured data like Salesforce:
| Best Practice | Example for Salesforce Data Queries |
| --- | --- |
| Understand the Data Model: Ensure the model has a basic understanding of the Salesforce data structure. | You are programmed to query Salesforce data. Use your knowledge of Salesforce‚Äôs platform to act accordingly |
| Optimize Query Formulation: Instruct the model to translate user queries into specific Salesforce Object Query Language (SOQL) queries. | Form queries to send to your action using SOQL. For example, "SELECT TotalAmount FROM Opportunity WHERE Product_Name = 'X'." |
| Utilize Specific Keywords: Encourage using Salesforce-specific terminology in queries to enhance accuracy. | Include terms like 'Opportunity', 'Lead', 'Account', and 'Contact' in your queries to fetch precise data from Salesforce. |
| Break Down Complex Queries: Teach the model to deconstruct complicated requests into simpler sub-queries. | If a complex query is asked, break it down into steps and then perform. Example one: asked about customer satisfaction, first fetch sales data, followed by customer feedback ratings. Example two: for a complex query like forecasting sales trends, first retrieve historical sales data, then current market trends, and synthesize these for a comprehensive response. |
| Handle Errors and Alternatives: Guide the model on how to respond when data can't be accessed or is unavailable. | When you encounter errors suggest alternative methods or direct the user to human support if the required data cannot be retrieved. |
| Present Data Clearly: Instruct the model to present data in an easily understandable format. | Avoid technical jargon and present Salesforce data in a clear, concise manner suitable for the user‚Äôs expertise level. |
| Cite Sources: Ensure the model always cites the source of its information. | Always mention that the provided information is retrieved from Salesforce to maintain transparency, where possible cite the report or object used with a link to that data. |
| Leverage Retrieval for Context: Use the retrieval aspect to fetch relevant information from Salesforce as context for the response. | Instruct the model to retrieve specific data from Salesforce, like recent sales figures or customer interactions, to provide context for its generated responses. |

Based on these best practices, our instructional prompt for the GPT model ends up being:
```markdown
As a GPT model specialized in Salesforce data queries, your role is to respond to user requests with precise and relevant Salesforce data. 

Retrieve specific Salesforce data relevant to user queries, using precise and Salesforce-specific terminology. 

When receiving a query, convert it into a specific SOQL query using Salesforce-specific terminology. 

For complex requests, break down the query into simpler sub-queries. 

In situations where you cannot retrieve the required data, suggest alternative methods or guide the user to seek help from human support. 

Present data in a clear and understandable manner, and always cite Salesforce as the source of your information.This structured approach ensures that the GPT model effectively handles Salesforce data queries, maintains security and compliance, and continuously improves its responses based on user feedback.
```
Now that you have your instructions set, it's time to test & iterative the GPT using real-world questions. Create a set of questions that employees might typically ask about your sales and customer data and use these to evaluate the effectiveness of your GPT. Here's an example:
| Query | Expected Response | GPT Response | Rating |
| --- | --- | --- | --- |
| Of my accounts over 100K in spend are there any open support cases?  | 2 Open Under United Oil | Open Opportunities: <br/> ‚Ä¢ United Oil Installations: $270.000 <br/> ‚Ä¢ United Oil Refinery Generators: $270,000 <br/> ‚Ä¢ United Oil Plant Standby Generators: $675,000 <br/> ‚Ä¢ United Oil Office Portable Generators: $125,000 <br/> ‚Ä¢Open Cases: <br/> ‚Ä¢ Case Subject: "Design issue with mechanical rotor", Status: "New <br/> ‚Ä¢ Case Subject: "Seeking guidance on electrical wiring installation for GC5060*, Status: "New‚Äù | High |

Continuously test and refine your instructions based on the results you get. This iterative process will help you fine-tune the GPT's ability to use dynamic knowledge from Salesforce to provide accurate and helpful responses.

With these steps, you can connect your GPT to Salesforce CRM to create your "DataWiz" GPT that serves as an efficient and reliable resource for sales and customer based queries within your organization. You can use this same logic to build out integrations with other structured knowledge sources that support structured queries of the data with their API. We plan to provide similar code snippets in the future to assist you with these.

### **Run:** **Build a GPT assistant who interacts with & executes tasks in 3rd party tools**
    
Many times there are workflows that employees would benefit from having GenAI assist them with, but in order to do so, ChatGPT would need to be connected to 3rd party tools and given the ability to execute against these tools.  
    
By connecting purpose-built GPTs to your various Enterprise SaaS solutions, you can enable assistant like functionality that goes beyond the normal ‚Äúquestion and response‚Äù or chatbot paradigm currently associate with most GPT solutions. Building these sorts of GPTs not only significantly reduces the number of repetitive tasks staff must perform each day leading to enhanced productivity on what matters, but reduces errors and ensures things don‚Äôt fall through the cracks. 
    
> üí° Ideas for applicable GPT use cases you might create with this example: Task Manager or Customer Call Follow Up Assistant

#### Example One: Building a Task Assistant (connects to Asana + Slack)
In our example we are going to show how ChatGPT can act as an intern who attends meetings, recaps & assign action items via Asana & Slack & begin workings on tasks they are well suited to assist with. 

1. Building Actions with Multiple Endpoints

To do this we need to build actions with multiple endpoints. There are two ways to do this: first, directly in the GPT itself building multiple actions for each API (e.g. one for Slack and one for Asana in our case). Second option is to use a middle-wear provider. We are going to choose a middle-wear provider because it allows us to ‚Äúsee under the hood‚Äù in terms of the calls that are being created by the GPT and provides us additional clarity when it comes to debugging that ChatGPT‚Äôs GPT solution does not yet have. 

To begin, we will connect the Slack and Asana API endpoints to a Retool workflow, setting up the ‚Äústart trigger‚Äù for these endpoints as a Webhook, which we will utlimately connect to our GPT. We will review the Developer Docs for Slack and Asana to determine the best endpoints for our GPT. We know that we need to ‚Äúcreate tasks‚Äù in Asana so we will us the [`/tasks endpoint`](https://developers.asana.com/reference/createtask) and we need to ‚Äúpost messages‚Äù in Slack, so we will set up a [basic Slack app that can send messages](https://api.slack.com/messaging/sending) and can help us then leverage the `[chat.postMessage](https://api.slack.com/methods/chat.postMessage)` endpoint. 

Use the developer docs to setup the [RESTqueries](https://docs.retool.com/data-sources/quickstarts/api/rest) in Retool for each system. 

After setting up each RESTquery in Retool, we will use a ‚Äúbranch‚Äù from start trigger, as shown below that enables us to parse the response received by the webhook and send the relevant data to end endpoint.

> <img width="300" alt="retoolcookbook" src="https://github.com/sabrina-de/OpenAI-ChatGPT-Enterprise/assets/152341307/410f29ca-67c1-41a4-95c6-b0f7529b64ef">


2. Setting up your OpenAPI Spec to direct to multiple endpoints via a middlewear
After that we have our Retool workflow setup, we need to grab the url to for our webhook trigger and use this along with our knowledge of the format for both the Slack and Asana request bodies to build our OpenAPI spec to create our custom action. When we finish our spec will look like this:
```markdown
{
  "openapi": "3.0.0",
  "info": {
    "title": "Retool Workflow Start Trigger",
    "version": "1.0.0"
  },
  "servers": [
    {
      "url": "https://api.retool.com"
    }
  ],
  "paths": {
    "/v1/workflows/workflowID/startTrigger?workflowApiKey=key": {
      "post": {
        "operationId": "startWorkflow",
        "summary": "Start a Retool workflow trigger",
        "parameters": [],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "type": {
                    "type": "string"
                  },
                  "asana": {
                    "type": "object",
                    "properties": {
                      "data": {
                        "type": "object",
                        "properties": {
                          "workspace": {
                            "type": "string"
                          },
                          "name": {
                            "type": "string"
                          },
                          "asignee": {
                            "type": "string"
                          }
                        }
                      }
                    }
                  },
                  "slack": {
                    "type": "object",
                    "properties": {
                      "channel": {
                        "type": "string"
                      },
                      "text": {
                        "type": "string"
                      }
                    }
                  }
                },
                "required": [
                  "type"
                ]
              }
            }
          }
        }
      },
      "responses": {
        "200": {
          "description": "Trigger started successfully"
        }
      },
      "security": [
        {
          "apiKey": []
        }
      ]
    }
  }
}

No auth required
```
You might be surprised that is all you need to get going, but this is really the power of GPTs. You can think of it having a similar level of intelligence that we might on ‚Äúhow to form the response bodies‚Äù and what content to use when composing. So in our example, we just need to give it some light instructions on how we want it to be able to act + leverage these actions. 
> [!Note]
> We do recommend, when building more ‚Äúassistant‚Äù like functionality that you provide what good or success looks like in the instruction body.

3. Setting up instructions to work with multiple endpoints
As you read through the example below you should get a taste for how ‚Äúlight‚Äù on instructions you can be and still get a great result.
```markdown
Go through the meeting transcript to identify tasks. Identify which tasks you can start (by browsing the web, creating images, or writing and interpreting code) vs. which tasks the user must accomplish on their own. The user is named Sara.

Give a summary of tasks you've identified, as well as recommendations for who should complete the tasks (the user, or you).

Offer to create any tasks the user should complete in Asana. If the user agrees, use your custom action to create tasks in the asana workspace with the workspace id "1205857843515507". The assignee for all tasks should be "me". Share the resulting task_url you get in the response from the custom action with the user. Use the custom action to create the tasks one by one.

For tasks that you can help with, describe your plan for completing the task and then post that plan to the "marketing" slack channel. Use the custom action to share plans, one by one.

Success looks like this:

User: <provides meeting transcript>
You: <lists tasks that you can assist with, and tasks that the user should complete on their own>
You: <ask user if they would like you to create tasks in asana for the tasks they will need to complete on their own>
User: <responds in the affirmative>
You: <Use your custom action to create the first Asana task. When you get a response back from the custom action service, share the task_url with the user. Repeat these actions for each of the tasks>
You:<ask the user if that would like you to create a plan for each of the tasks you can assist with and share those plans in slack.>
You: <Create a plan. Use the custom action to share the plan in the "marketing" slack channel. Repeat this action for each of the tasks you can assist with.>
You: <ask the user if there's anything else you can do to help>
```
4. Testing
Now that we have set up our actions + instructions we are ready to test our system. In our example our assistant ‚Äúattends‚Äù meetings with us - so we are going to give it a meeting transcript to test with. You can use this transcript too, if you‚Äôd like to test.
```markdown
Agenda:

Review of last month's marketing campaigns
Discussion of upcoming holiday marketing strategy
Social media content calendar review
SEO strategy update
New graphic design requests
Budget review
Discussion:

Last Month's Campaigns

Increase in engagement on social media channels
PPC campaign on Google brought in 300 new leads
Holiday Marketing Strategy

Proposed having a special discount offer
Decision to collaborate with local businesses for cross-promotion
Social Media Calendar

Theme for the next month: "Winter Wonderland"
Sara to coordinate with Emma on creating visual content
SEO Strategy

Identified need to optimize website for mobile
Sara will research latest SEO trends and present findings next meeting
Graphic Design Requests

New banners needed for the holiday sale campaign
Sara to coordinate with Emma on this task
Budget Review

Currently under budget by 5%
Discussion about allocating extra budget towards influencer marketing

Action Items:
Sara to finalize the social media calendar by Nov 4, and coordinate with Emma on creating visual content.
Sara to conduct SEO trend research
Sara to design banners for holiday sale campaign
```
You likely will want to iterate on instructions based on the output of the model.
With these steps, you can create a powerful assistant GPT that can take action in the real world. You can use this same logic to build out integrations with other SaaS tools that have API endpoints. We plan to provide similar code snippets in the future to assist you with these.  
